'************************************************************************
'ALGLIB 3.9.0 (source code generated 2014-12-11)
'Copyright (c) Sergey Bochkanov (ALGLIB project).
'
'>>> SOURCE LICENSE >>>
'This program is free software; you can redistribute it and/or modify
'it under the terms of the GNU General Public License as published by
'the Free Software Foundation (www.fsf.org); either version 2 of the 
'License, or (at your option) any later version.
'
'This program is distributed in the hope that it will be useful,
'but WITHOUT ANY WARRANTY; without even the implied warranty of
'MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
'GNU General Public License for more details.
'
'A copy of the GNU General Public License is available at
'http://www.fsf.org/licensing/licenses
'>>> END OF LICENSE >>>
'************************************************************************

'#Pragma warning disable 162
'#Pragma warning disable 219

Public Partial Class alglib


	'************************************************************************
'    Optimal binary classification
'
'    Algorithms finds optimal (=with minimal cross-entropy) binary partition.
'    Internal subroutine.
'
'    INPUT PARAMETERS:
'        A       -   array[0..N-1], variable
'        C       -   array[0..N-1], class numbers (0 or 1).
'        N       -   array size
'
'    OUTPUT PARAMETERS:
'        Info    -   completetion code:
'                    * -3, all values of A[] are same (partition is impossible)
'                    * -2, one of C[] is incorrect (<0, >1)
'                    * -1, incorrect pararemets were passed (N<=0).
'                    *  1, OK
'        Threshold-  partiton boundary. Left part contains values which are
'                    strictly less than Threshold. Right part contains values
'                    which are greater than or equal to Threshold.
'        PAL, PBL-   probabilities P(0|v<Threshold) and P(1|v<Threshold)
'        PAR, PBR-   probabilities P(0|v>=Threshold) and P(1|v>=Threshold)
'        CVE     -   cross-validation estimate of cross-entropy
'
'      -- ALGLIB --
'         Copyright 22.05.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dsoptimalsplit2(a As Double(), c As Integer(), n As Integer, ByRef info As Integer, ByRef threshold As Double, ByRef pal As Double, _
		ByRef pbl As Double, ByRef par As Double, ByRef pbr As Double, ByRef cve As Double)
		info = 0
		threshold = 0
		pal = 0
		pbl = 0
		par = 0
		pbr = 0
		cve = 0
		bdss.dsoptimalsplit2(a, c, n, info, threshold, pal, _
			pbl, par, pbr, cve)
		Return
	End Sub

	'************************************************************************
'    Optimal partition, internal subroutine. Fast version.
'
'    Accepts:
'        A       array[0..N-1]       array of attributes     array[0..N-1]
'        C       array[0..N-1]       array of class labels
'        TiesBuf array[0..N]         temporaries (ties)
'        CntBuf  array[0..2*NC-1]    temporaries (counts)
'        Alpha                       centering factor (0<=alpha<=1, recommended value - 0.05)
'        BufR    array[0..N-1]       temporaries
'        BufI    array[0..N-1]       temporaries
'
'    Output:
'        Info    error code (">0"=OK, "<0"=bad)
'        RMS     training set RMS error
'        CVRMS   leave-one-out RMS error
'
'    Note:
'        content of all arrays is changed by subroutine;
'        it doesn't allocate temporaries.
'
'      -- ALGLIB --
'         Copyright 11.12.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dsoptimalsplit2fast(ByRef a As Double(), ByRef c As Integer(), ByRef tiesbuf As Integer(), ByRef cntbuf As Integer(), ByRef bufr As Double(), ByRef bufi As Integer(), _
		n As Integer, nc As Integer, alpha As Double, ByRef info As Integer, ByRef threshold As Double, ByRef rms As Double, _
		ByRef cvrms As Double)
		info = 0
		threshold = 0
		rms = 0
		cvrms = 0
		bdss.dsoptimalsplit2fast(a, c, tiesbuf, cntbuf, bufr, bufi, _
			n, nc, alpha, info, threshold, rms, _
			cvrms)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    This structure is a clusterization engine.
'
'    You should not try to access its fields directly.
'    Use ALGLIB functions in order to work with this object.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Class clusterizerstate
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New clustering.clusterizerstate()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New clusterizerstate(DirectCast(_innerobj.make_copy(), clustering.clusterizerstate))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As clustering.clusterizerstate
		Public ReadOnly Property innerobj() As clustering.clusterizerstate
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As clustering.clusterizerstate)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This structure  is used to store results of the agglomerative hierarchical
'    clustering (AHC).
'
'    Following information is returned:
'
'    * NPoints contains number of points in the original dataset
'
'    * Z contains information about merges performed  (see below).  Z  contains
'      indexes from the original (unsorted) dataset and it can be used when you
'      need to know what points were merged. However, it is not convenient when
'      you want to build a dendrograd (see below).
'
'    * if  you  want  to  build  dendrogram, you  can use Z, but it is not good
'      option, because Z contains  indexes from  unsorted  dataset.  Dendrogram
'      built from such dataset is likely to have intersections. So, you have to
'      reorder you points before building dendrogram.
'      Permutation which reorders point is returned in P. Another representation
'      of  merges,  which  is  more  convenient for dendorgram construction, is
'      returned in PM.
'
'    * more information on format of Z, P and PM can be found below and in the
'      examples from ALGLIB Reference Manual.
'
'    FORMAL DESCRIPTION OF FIELDS:
'        NPoints         number of points
'        Z               array[NPoints-1,2],  contains   indexes   of  clusters
'                        linked in pairs to  form  clustering  tree.  I-th  row
'                        corresponds to I-th merge:
'                        * Z[I,0] - index of the first cluster to merge
'                        * Z[I,1] - index of the second cluster to merge
'                        * Z[I,0]<Z[I,1]
'                        * clusters are  numbered  from 0 to 2*NPoints-2,  with
'                          indexes from 0 to NPoints-1 corresponding to  points
'                          of the original dataset, and indexes from NPoints to
'                          2*NPoints-2  correspond  to  clusters  generated  by
'                          subsequent  merges  (I-th  row  of Z creates cluster
'                          with index NPoints+I).
'
'                        IMPORTANT: indexes in Z[] are indexes in the ORIGINAL,
'                        unsorted dataset. In addition to  Z algorithm  outputs
'                        permutation which rearranges points in such  way  that
'                        subsequent merges are  performed  on  adjacent  points
'                        (such order is needed if you want to build dendrogram).
'                        However,  indexes  in  Z  are  related  to   original,
'                        unrearranged sequence of points.
'
'        P               array[NPoints], permutation which reorders points  for
'                        dendrogram  construction.  P[i] contains  index of the
'                        position  where  we  should  move  I-th  point  of the
'                        original dataset in order to apply merges PZ/PM.
'
'        PZ              same as Z, but for permutation of points given  by  P.
'                        The  only  thing  which  changed  are  indexes  of the
'                        original points; indexes of clusters remained same.
'
'        MergeDist       array[NPoints-1], contains distances between  clusters
'                        being merged (MergeDist[i] correspond to merge  stored
'                        in Z[i,...]).
'
'        PM              array[NPoints-1,6], another representation of  merges,
'                        which is suited for dendrogram construction. It  deals
'                        with rearranged points (permutation P is applied)  and
'                        represents merges in a form which different  from  one
'                        used by Z.
'                        For each I from 0 to NPoints-2, I-th row of PM represents
'                        merge performed on two clusters C0 and C1. Here:
'                        * C0 contains points with indexes PM[I,0]...PM[I,1]
'                        * C1 contains points with indexes PM[I,2]...PM[I,3]
'                        * indexes stored in PM are given for dataset sorted
'                          according to permutation P
'                        * PM[I,1]=PM[I,2]-1 (only adjacent clusters are merged)
'                        * PM[I,0]<=PM[I,1], PM[I,2]<=PM[I,3], i.e. both
'                          clusters contain at least one point
'                        * heights of "subdendrograms" corresponding  to  C0/C1
'                          are stored in PM[I,4]  and  PM[I,5].  Subdendrograms
'                          corresponding   to   single-point   clusters    have
'                          height=0. Dendrogram of the merge result has  height
'                          H=max(H0,H1)+1.
'
'    NOTE: there is one-to-one correspondence between merges described by Z and
'          PM. I-th row of Z describes same merge of clusters as I-th row of PM,
'          with "left" cluster from Z corresponding to the "left" one from PM.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Class ahcreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property npoints() As Integer
			Get
				Return _innerobj.npoints
			End Get
			Set
				_innerobj.npoints = value
			End Set
		End Property
		Public Property p() As Integer()
			Get
				Return _innerobj.p
			End Get
			Set
				_innerobj.p = value
			End Set
		End Property
		Public Property z() As Integer(,)
			Get
				Return _innerobj.z
			End Get
			Set
				_innerobj.z = value
			End Set
		End Property
		Public Property pz() As Integer(,)
			Get
				Return _innerobj.pz
			End Get
			Set
				_innerobj.pz = value
			End Set
		End Property
		Public Property pm() As Integer(,)
			Get
				Return _innerobj.pm
			End Get
			Set
				_innerobj.pm = value
			End Set
		End Property
		Public Property mergedist() As Double()
			Get
				Return _innerobj.mergedist
			End Get
			Set
				_innerobj.mergedist = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New clustering.ahcreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New ahcreport(DirectCast(_innerobj.make_copy(), clustering.ahcreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As clustering.ahcreport
		Public ReadOnly Property innerobj() As clustering.ahcreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As clustering.ahcreport)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This  structure   is  used  to  store  results of the k-means++ clustering
'    algorithm.
'
'    Following information is always returned:
'    * NPoints contains number of points in the original dataset
'    * TerminationType contains completion code, negative on failure, positive
'      on success
'    * K contains number of clusters
'
'    For positive TerminationType we return:
'    * NFeatures contains number of variables in the original dataset
'    * C, which contains centers found by algorithm
'    * CIdx, which maps points of the original dataset to clusters
'
'    FORMAL DESCRIPTION OF FIELDS:
'        NPoints         number of points, >=0
'        NFeatures       number of variables, >=1
'        TerminationType completion code:
'                        * -5 if  distance  type  is  anything  different  from
'                             Euclidean metric
'                        * -3 for degenerate dataset: a) less  than  K  distinct
'                             points, b) K=0 for non-empty dataset.
'                        * +1 for successful completion
'        K               number of clusters
'        C               array[K,NFeatures], rows of the array store centers
'        CIdx            array[NPoints], which contains cluster indexes
'
'      -- ALGLIB --
'         Copyright 27.11.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Class kmeansreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property npoints() As Integer
			Get
				Return _innerobj.npoints
			End Get
			Set
				_innerobj.npoints = value
			End Set
		End Property
		Public Property nfeatures() As Integer
			Get
				Return _innerobj.nfeatures
			End Get
			Set
				_innerobj.nfeatures = value
			End Set
		End Property
		Public Property terminationtype() As Integer
			Get
				Return _innerobj.terminationtype
			End Get
			Set
				_innerobj.terminationtype = value
			End Set
		End Property
		Public Property k() As Integer
			Get
				Return _innerobj.k
			End Get
			Set
				_innerobj.k = value
			End Set
		End Property
		Public Property c() As Double(,)
			Get
				Return _innerobj.c
			End Get
			Set
				_innerobj.c = value
			End Set
		End Property
		Public Property cidx() As Integer()
			Get
				Return _innerobj.cidx
			End Get
			Set
				_innerobj.cidx = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New clustering.kmeansreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New kmeansreport(DirectCast(_innerobj.make_copy(), clustering.kmeansreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As clustering.kmeansreport
		Public ReadOnly Property innerobj() As clustering.kmeansreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As clustering.kmeansreport)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'    This function initializes clusterizer object. Newly initialized object  is
'    empty, i.e. it does not contain dataset. You should use it as follows:
'    1. creation
'    2. dataset is added with ClusterizerSetPoints()
'    3. additional parameters are set
'    3. clusterization is performed with one of the clustering functions
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizercreate(ByRef s As clusterizerstate)
		s = New clusterizerstate()
		clustering.clusterizercreate(s.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function adds dataset to the clusterizer structure.
'
'    This function overrides all previous calls  of  ClusterizerSetPoints()  or
'    ClusterizerSetDistances().
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'        XY      -   array[NPoints,NFeatures], dataset
'        NPoints -   number of points, >=0
'        NFeatures-  number of features, >=1
'        DistType-   distance function:
'                    *  0    Chebyshev distance  (L-inf norm)
'                    *  1    city block distance (L1 norm)
'                    *  2    Euclidean distance  (L2 norm)
'                    * 10    Pearson correlation:
'                            dist(a,b) = 1-corr(a,b)
'                    * 11    Absolute Pearson correlation:
'                            dist(a,b) = 1-|corr(a,b)|
'                    * 12    Uncentered Pearson correlation (cosine of the angle):
'                            dist(a,b) = a'*b/(|a|*|b|)
'                    * 13    Absolute uncentered Pearson correlation
'                            dist(a,b) = |a'*b|/(|a|*|b|)
'                    * 20    Spearman rank correlation:
'                            dist(a,b) = 1-rankcorr(a,b)
'                    * 21    Absolute Spearman rank correlation
'                            dist(a,b) = 1-|rankcorr(a,b)|
'
'    NOTE 1: different distance functions have different performance penalty:
'            * Euclidean or Pearson correlation distances are the fastest ones
'            * Spearman correlation distance function is a bit slower
'            * city block and Chebyshev distances are order of magnitude slower
'
'            The reason behing difference in performance is that correlation-based
'            distance functions are computed using optimized linear algebra kernels,
'            while Chebyshev and city block distance functions are computed using
'            simple nested loops with two branches at each iteration.
'
'    NOTE 2: different clustering algorithms have different limitations:
'            * agglomerative hierarchical clustering algorithms may be used with
'              any kind of distance metric
'            * k-means++ clustering algorithm may be used only  with  Euclidean
'              distance function
'            Thus, list of specific clustering algorithms you may  use  depends
'            on distance function you specify when you set your dataset.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizersetpoints(s As clusterizerstate, xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer)

		clustering.clusterizersetpoints(s.innerobj, xy, npoints, nfeatures, disttype)
		Return
	End Sub
	Public Shared Sub clusterizersetpoints(s As clusterizerstate, xy As Double(,), disttype As Integer)
		Dim npoints As Integer
		Dim nfeatures As Integer


		npoints = ap.rows(xy)
		nfeatures = ap.cols(xy)
		clustering.clusterizersetpoints(s.innerobj, xy, npoints, nfeatures, disttype)

		Return
	End Sub

	'************************************************************************
'    This function adds dataset given by distance  matrix  to  the  clusterizer
'    structure. It is important that dataset is not  given  explicitly  -  only
'    distance matrix is given.
'
'    This function overrides all previous calls  of  ClusterizerSetPoints()  or
'    ClusterizerSetDistances().
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'        D       -   array[NPoints,NPoints], distance matrix given by its upper
'                    or lower triangle (main diagonal is  ignored  because  its
'                    entries are expected to be zero).
'        NPoints -   number of points
'        IsUpper -   whether upper or lower triangle of D is given.
'
'    NOTE 1: different clustering algorithms have different limitations:
'            * agglomerative hierarchical clustering algorithms may be used with
'              any kind of distance metric, including one  which  is  given  by
'              distance matrix
'            * k-means++ clustering algorithm may be used only  with  Euclidean
'              distance function and explicitly given points - it  can  not  be
'              used with dataset given by distance matrix
'            Thus, if you call this function, you will be unable to use k-means
'            clustering algorithm to process your problem.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizersetdistances(s As clusterizerstate, d As Double(,), npoints As Integer, isupper As Boolean)

		clustering.clusterizersetdistances(s.innerobj, d, npoints, isupper)
		Return
	End Sub
	Public Shared Sub clusterizersetdistances(s As clusterizerstate, d As Double(,), isupper As Boolean)
		Dim npoints As Integer
		If (ap.rows(d) <> ap.cols(d)) Then
			Throw New alglibexception("Error while calling 'clusterizersetdistances': looks like one of arguments has wrong size")
		End If

		npoints = ap.rows(d)
		clustering.clusterizersetdistances(s.innerobj, d, npoints, isupper)

		Return
	End Sub

	'************************************************************************
'    This function sets agglomerative hierarchical clustering algorithm
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'        Algo    -   algorithm type:
'                    * 0     complete linkage (default algorithm)
'                    * 1     single linkage
'                    * 2     unweighted average linkage
'                    * 3     weighted average linkage
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizersetahcalgo(s As clusterizerstate, algo As Integer)

		clustering.clusterizersetahcalgo(s.innerobj, algo)
		Return
	End Sub

	'************************************************************************
'    This  function  sets k-means++ properties : number of restarts and maximum
'    number of iterations per one run.
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'        Restarts-   restarts count, >=1.
'                    k-means++ algorithm performs several restarts and  chooses
'                    best set of centers (one with minimum squared distance).
'        MaxIts  -   maximum number of k-means iterations performed during  one
'                    run. >=0, zero value means that algorithm performs unlimited
'                    number of iterations.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizersetkmeanslimits(s As clusterizerstate, restarts As Integer, maxits As Integer)

		clustering.clusterizersetkmeanslimits(s.innerobj, restarts, maxits)
		Return
	End Sub

	'************************************************************************
'    This function performs agglomerative hierarchical clustering
'
'    COMMERCIAL EDITION OF ALGLIB:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function, which can be used from C++ and C#:
'      ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'      ! * multicore support
'      !
'      ! Agglomerative  hierarchical  clustering  algorithm  has  two   phases:
'      ! distance matrix calculation  and  clustering  itself. Only first phase
'      ! (distance matrix calculation) is accelerated by Intel MKL  and  multi-
'      ! threading. Thus, acceleration is significant only for  medium or high-
'      ! dimensional problems.
'      !
'      ! We recommend you to read 'Working with commercial version' section  of
'      ! ALGLIB Reference Manual in order to find out how to  use  performance-
'      ! related features provided by commercial edition of ALGLIB.
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'
'    OUTPUT PARAMETERS:
'        Rep     -   clustering results; see description of AHCReport
'                    structure for more information.
'
'    NOTE 1: hierarchical clustering algorithms require large amounts of memory.
'            In particular, this implementation needs  sizeof(double)*NPoints^2
'            bytes, which are used to store distance matrix. In  case  we  work
'            with user-supplied matrix, this amount is multiplied by 2 (we have
'            to store original matrix and to work with its copy).
'
'            For example, problem with 10000 points  would require 800M of RAM,
'            even when working in a 1-dimensional space.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizerrunahc(s As clusterizerstate, ByRef rep As ahcreport)
		rep = New ahcreport()
		clustering.clusterizerrunahc(s.innerobj, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_clusterizerrunahc(s As clusterizerstate, ByRef rep As ahcreport)
		rep = New ahcreport()
		clustering._pexec_clusterizerrunahc(s.innerobj, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function performs clustering by k-means++ algorithm.
'
'    You may change algorithm properties like number of restarts or  iterations
'    limit by calling ClusterizerSetKMeansLimits() functions.
'
'    INPUT PARAMETERS:
'        S       -   clusterizer state, initialized by ClusterizerCreate()
'        K       -   number of clusters, K>=0.
'                    K  can  be  zero only when algorithm is called  for  empty
'                    dataset,  in   this   case   completion  code  is  set  to
'                    success (+1).
'                    If  K=0  and  dataset  size  is  non-zero,  we   can   not
'                    meaningfully assign points to some center  (there  are  no
'                    centers because K=0) and  return  -3  as  completion  code
'                    (failure).
'
'    OUTPUT PARAMETERS:
'        Rep     -   clustering results; see description of KMeansReport
'                    structure for more information.
'
'    NOTE 1: k-means  clustering  can  be  performed  only  for  datasets  with
'            Euclidean  distance  function.  Algorithm  will  return   negative
'            completion code in Rep.TerminationType in case dataset  was  added
'            to clusterizer with DistType other than Euclidean (or dataset  was
'            specified by distance matrix instead of explicitly given points).
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizerrunkmeans(s As clusterizerstate, k As Integer, ByRef rep As kmeansreport)
		rep = New kmeansreport()
		clustering.clusterizerrunkmeans(s.innerobj, k, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function returns distance matrix for dataset
'
'    COMMERCIAL EDITION OF ALGLIB:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function, which can be used from C++ and C#:
'      ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'      ! * multicore support
'      !
'      ! Agglomerative  hierarchical  clustering  algorithm  has  two   phases:
'      ! distance matrix calculation  and  clustering  itself. Only first phase
'      ! (distance matrix calculation) is accelerated by Intel MKL  and  multi-
'      ! threading. Thus, acceleration is significant only for  medium or high-
'      ! dimensional problems.
'      !
'      ! We recommend you to read 'Working with commercial version' section  of
'      ! ALGLIB Reference Manual in order to find out how to  use  performance-
'      ! related features provided by commercial edition of ALGLIB.
'
'    INPUT PARAMETERS:
'        XY      -   array[NPoints,NFeatures], dataset
'        NPoints -   number of points, >=0
'        NFeatures-  number of features, >=1
'        DistType-   distance function:
'                    *  0    Chebyshev distance  (L-inf norm)
'                    *  1    city block distance (L1 norm)
'                    *  2    Euclidean distance  (L2 norm)
'                    * 10    Pearson correlation:
'                            dist(a,b) = 1-corr(a,b)
'                    * 11    Absolute Pearson correlation:
'                            dist(a,b) = 1-|corr(a,b)|
'                    * 12    Uncentered Pearson correlation (cosine of the angle):
'                            dist(a,b) = a'*b/(|a|*|b|)
'                    * 13    Absolute uncentered Pearson correlation
'                            dist(a,b) = |a'*b|/(|a|*|b|)
'                    * 20    Spearman rank correlation:
'                            dist(a,b) = 1-rankcorr(a,b)
'                    * 21    Absolute Spearman rank correlation
'                            dist(a,b) = 1-|rankcorr(a,b)|
'
'    OUTPUT PARAMETERS:
'        D       -   array[NPoints,NPoints], distance matrix
'                    (full matrix is returned, with lower and upper triangles)
'
'    NOTES: different distance functions have different performance penalty:
'           * Euclidean or Pearson correlation distances are the fastest ones
'           * Spearman correlation distance function is a bit slower
'           * city block and Chebyshev distances are order of magnitude slower
'
'           The reason behing difference in performance is that correlation-based
'           distance functions are computed using optimized linear algebra kernels,
'           while Chebyshev and city block distance functions are computed using
'           simple nested loops with two branches at each iteration.
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizergetdistances(xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer, ByRef d As Double(,))
		d = New Double(-1, -1) {}
		clustering.clusterizergetdistances(xy, npoints, nfeatures, disttype, d)
		Return
	End Sub


	Public Shared Sub smp_clusterizergetdistances(xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer, ByRef d As Double(,))
		d = New Double(-1, -1) {}
		clustering._pexec_clusterizergetdistances(xy, npoints, nfeatures, disttype, d)
		Return
	End Sub

	'************************************************************************
'    This function takes as input clusterization report Rep,  desired  clusters
'    count K, and builds top K clusters from hierarchical clusterization  tree.
'    It returns assignment of points to clusters (array of cluster indexes).
'
'    INPUT PARAMETERS:
'        Rep     -   report from ClusterizerRunAHC() performed on XY
'        K       -   desired number of clusters, 1<=K<=NPoints.
'                    K can be zero only when NPoints=0.
'
'    OUTPUT PARAMETERS:
'        CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                    0 to K-1) for I-th point of the dataset.
'        CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                    returned by this function to indexes used by  Rep.Z.  J-th
'                    cluster returned by this function corresponds to  CZ[J]-th
'                    cluster stored in Rep.Z/PZ/PM.
'                    It is guaranteed that CZ[I]<CZ[I+1].
'
'    NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'          Although  they  were  obtained  by  manipulation with top K nodes of
'          dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'          function does not return information about hierarchy.  Each  of  the
'          clusters stand on its own.
'
'    NOTE: Cluster indexes returned by this function  does  not  correspond  to
'          indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'          representation of the dataset (dendrogram), or you work with  "flat"
'          representation returned by this function.  Each  of  representations
'          has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'          while latter uses [0..K-1]), although  it  is  possible  to  perform
'          conversion from one system to another by means of CZ array, returned
'          by this function, which allows you to convert indexes stored in CIdx
'          to the numeration system used by Rep.Z.
'
'    NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'          it will perform many times faster than  for  K=100.  Its  worst-case
'          performance is O(N*K), although in average case  it  perform  better
'          (up to O(N*log(K))).
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizergetkclusters(rep As ahcreport, k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
		cidx = New Integer(-1) {}
		cz = New Integer(-1) {}
		clustering.clusterizergetkclusters(rep.innerobj, k, cidx, cz)
		Return
	End Sub

	'************************************************************************
'    This  function  accepts  AHC  report  Rep,  desired  minimum  intercluster
'    distance and returns top clusters from  hierarchical  clusterization  tree
'    which are separated by distance R or HIGHER.
'
'    It returns assignment of points to clusters (array of cluster indexes).
'
'    There is one more function with similar name - ClusterizerSeparatedByCorr,
'    which returns clusters with intercluster correlation equal to R  or  LOWER
'    (note: higher for distance, lower for correlation).
'
'    INPUT PARAMETERS:
'        Rep     -   report from ClusterizerRunAHC() performed on XY
'        R       -   desired minimum intercluster distance, R>=0
'
'    OUTPUT PARAMETERS:
'        K       -   number of clusters, 1<=K<=NPoints
'        CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                    0 to K-1) for I-th point of the dataset.
'        CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                    returned by this function to indexes used by  Rep.Z.  J-th
'                    cluster returned by this function corresponds to  CZ[J]-th
'                    cluster stored in Rep.Z/PZ/PM.
'                    It is guaranteed that CZ[I]<CZ[I+1].
'
'    NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'          Although  they  were  obtained  by  manipulation with top K nodes of
'          dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'          function does not return information about hierarchy.  Each  of  the
'          clusters stand on its own.
'
'    NOTE: Cluster indexes returned by this function  does  not  correspond  to
'          indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'          representation of the dataset (dendrogram), or you work with  "flat"
'          representation returned by this function.  Each  of  representations
'          has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'          while latter uses [0..K-1]), although  it  is  possible  to  perform
'          conversion from one system to another by means of CZ array, returned
'          by this function, which allows you to convert indexes stored in CIdx
'          to the numeration system used by Rep.Z.
'
'    NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'          it will perform many times faster than  for  K=100.  Its  worst-case
'          performance is O(N*K), although in average case  it  perform  better
'          (up to O(N*log(K))).
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizerseparatedbydist(rep As ahcreport, r As Double, ByRef k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
		k = 0
		cidx = New Integer(-1) {}
		cz = New Integer(-1) {}
		clustering.clusterizerseparatedbydist(rep.innerobj, r, k, cidx, cz)
		Return
	End Sub

	'************************************************************************
'    This  function  accepts  AHC  report  Rep,  desired  maximum  intercluster
'    correlation and returns top clusters from hierarchical clusterization tree
'    which are separated by correlation R or LOWER.
'
'    It returns assignment of points to clusters (array of cluster indexes).
'
'    There is one more function with similar name - ClusterizerSeparatedByDist,
'    which returns clusters with intercluster distance equal  to  R  or  HIGHER
'    (note: higher for distance, lower for correlation).
'
'    INPUT PARAMETERS:
'        Rep     -   report from ClusterizerRunAHC() performed on XY
'        R       -   desired maximum intercluster correlation, -1<=R<=+1
'
'    OUTPUT PARAMETERS:
'        K       -   number of clusters, 1<=K<=NPoints
'        CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                    0 to K-1) for I-th point of the dataset.
'        CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                    returned by this function to indexes used by  Rep.Z.  J-th
'                    cluster returned by this function corresponds to  CZ[J]-th
'                    cluster stored in Rep.Z/PZ/PM.
'                    It is guaranteed that CZ[I]<CZ[I+1].
'
'    NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'          Although  they  were  obtained  by  manipulation with top K nodes of
'          dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'          function does not return information about hierarchy.  Each  of  the
'          clusters stand on its own.
'
'    NOTE: Cluster indexes returned by this function  does  not  correspond  to
'          indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'          representation of the dataset (dendrogram), or you work with  "flat"
'          representation returned by this function.  Each  of  representations
'          has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'          while latter uses [0..K-1]), although  it  is  possible  to  perform
'          conversion from one system to another by means of CZ array, returned
'          by this function, which allows you to convert indexes stored in CIdx
'          to the numeration system used by Rep.Z.
'
'    NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'          it will perform many times faster than  for  K=100.  Its  worst-case
'          performance is O(N*K), although in average case  it  perform  better
'          (up to O(N*log(K))).
'
'      -- ALGLIB --
'         Copyright 10.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub clusterizerseparatedbycorr(rep As ahcreport, r As Double, ByRef k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
		k = 0
		cidx = New Integer(-1) {}
		cz = New Integer(-1) {}
		clustering.clusterizerseparatedbycorr(rep.innerobj, r, k, cidx, cz)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    k-means++ clusterization.
'    Backward compatibility function, we recommend to use CLUSTERING subpackage
'    as better replacement.
'
'      -- ALGLIB --
'         Copyright 21.03.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub kmeansgenerate(xy As Double(,), npoints As Integer, nvars As Integer, k As Integer, restarts As Integer, ByRef info As Integer, _
		ByRef c As Double(,), ByRef xyc As Integer())
		info = 0
		c = New Double(-1, -1) {}
		xyc = New Integer(-1) {}
		datacomp.kmeansgenerate(xy, npoints, nvars, k, restarts, info, _
			c, xyc)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'
'    ************************************************************************

	Public Class decisionforest
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New dforest.decisionforest()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New decisionforest(DirectCast(_innerobj.make_copy(), dforest.decisionforest))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As dforest.decisionforest
		Public ReadOnly Property innerobj() As dforest.decisionforest
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As dforest.decisionforest)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'
'    ************************************************************************

	Public Class dfreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property relclserror() As Double
			Get
				Return _innerobj.relclserror
			End Get
			Set
				_innerobj.relclserror = value
			End Set
		End Property
		Public Property avgce() As Double
			Get
				Return _innerobj.avgce
			End Get
			Set
				_innerobj.avgce = value
			End Set
		End Property
		Public Property rmserror() As Double
			Get
				Return _innerobj.rmserror
			End Get
			Set
				_innerobj.rmserror = value
			End Set
		End Property
		Public Property avgerror() As Double
			Get
				Return _innerobj.avgerror
			End Get
			Set
				_innerobj.avgerror = value
			End Set
		End Property
		Public Property avgrelerror() As Double
			Get
				Return _innerobj.avgrelerror
			End Get
			Set
				_innerobj.avgrelerror = value
			End Set
		End Property
		Public Property oobrelclserror() As Double
			Get
				Return _innerobj.oobrelclserror
			End Get
			Set
				_innerobj.oobrelclserror = value
			End Set
		End Property
		Public Property oobavgce() As Double
			Get
				Return _innerobj.oobavgce
			End Get
			Set
				_innerobj.oobavgce = value
			End Set
		End Property
		Public Property oobrmserror() As Double
			Get
				Return _innerobj.oobrmserror
			End Get
			Set
				_innerobj.oobrmserror = value
			End Set
		End Property
		Public Property oobavgerror() As Double
			Get
				Return _innerobj.oobavgerror
			End Get
			Set
				_innerobj.oobavgerror = value
			End Set
		End Property
		Public Property oobavgrelerror() As Double
			Get
				Return _innerobj.oobavgrelerror
			End Get
			Set
				_innerobj.oobavgrelerror = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New dforest.dfreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New dfreport(DirectCast(_innerobj.make_copy(), dforest.dfreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As dforest.dfreport
		Public ReadOnly Property innerobj() As dforest.dfreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As dforest.dfreport)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This function serializes data structure to string.
'
'    Important properties of s_out:
'    * it contains alphanumeric characters, dots, underscores, minus signs
'    * these symbols are grouped into words, which are separated by spaces
'      and Windows-style (CR+LF) newlines
'    * although  serializer  uses  spaces and CR+LF as separators, you can 
'      replace any separator character by arbitrary combination of spaces,
'      tabs, Windows or Unix newlines. It allows flexible reformatting  of
'      the  string  in  case you want to include it into text or XML file. 
'      But you should not insert separators into the middle of the "words"
'      nor you should change case of letters.
'    * s_out can be freely moved between 32-bit and 64-bit systems, little
'      and big endian machines, and so on. You can serialize structure  on
'      32-bit machine and unserialize it on 64-bit one (or vice versa), or
'      serialize  it  on  SPARC  and  unserialize  on  x86.  You  can also 
'      serialize  it  in  C# version of ALGLIB and unserialize in C++ one, 
'      and vice versa.
'    ************************************************************************

	Public Shared Sub dfserialize(obj As decisionforest, ByRef s_out As String)
		Dim s As New alglib.serializer()
		s.alloc_start()
		dforest.dfalloc(s, obj.innerobj)
		s.sstart_str()
		dforest.dfserialize(s, obj.innerobj)
		s.[stop]()
		s_out = s.get_string()
	End Sub


	'************************************************************************
'    This function unserializes data structure from string.
'    ************************************************************************

	Public Shared Sub dfunserialize(s_in As String, ByRef obj As decisionforest)
		Dim s As New alglib.serializer()
		obj = New decisionforest()
		s.ustart_str(s_in)
		dforest.dfunserialize(s, obj.innerobj)
		s.[stop]()
	End Sub

	'************************************************************************
'    This subroutine builds random decision forest.
'
'    INPUT PARAMETERS:
'        XY          -   training set
'        NPoints     -   training set size, NPoints>=1
'        NVars       -   number of independent variables, NVars>=1
'        NClasses    -   task type:
'                        * NClasses=1 - regression task with one
'                                       dependent variable
'                        * NClasses>1 - classification task with
'                                       NClasses classes.
'        NTrees      -   number of trees in a forest, NTrees>=1.
'                        recommended values: 50-100.
'        R           -   percent of a training set used to build
'                        individual trees. 0<R<=1.
'                        recommended values: 0.1 <= R <= 0.66.
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<1, NVars<1, NClasses<1, NTrees<1, R<=0
'                              or R>1).
'                        *  1, if task has been solved
'        DF          -   model built
'        Rep         -   training report, contains error on a training set
'                        and out-of-bag estimates of generalization error.
'
'      -- ALGLIB --
'         Copyright 19.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dfbuildrandomdecisionforest(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ntrees As Integer, r As Double, _
		ByRef info As Integer, ByRef df As decisionforest, ByRef rep As dfreport)
		info = 0
		df = New decisionforest()
		rep = New dfreport()
		dforest.dfbuildrandomdecisionforest(xy, npoints, nvars, nclasses, ntrees, r, _
			info, df.innerobj, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    This subroutine builds random decision forest.
'    This function gives ability to tune number of variables used when choosing
'    best split.
'
'    INPUT PARAMETERS:
'        XY          -   training set
'        NPoints     -   training set size, NPoints>=1
'        NVars       -   number of independent variables, NVars>=1
'        NClasses    -   task type:
'                        * NClasses=1 - regression task with one
'                                       dependent variable
'                        * NClasses>1 - classification task with
'                                       NClasses classes.
'        NTrees      -   number of trees in a forest, NTrees>=1.
'                        recommended values: 50-100.
'        NRndVars    -   number of variables used when choosing best split
'        R           -   percent of a training set used to build
'                        individual trees. 0<R<=1.
'                        recommended values: 0.1 <= R <= 0.66.
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<1, NVars<1, NClasses<1, NTrees<1, R<=0
'                              or R>1).
'                        *  1, if task has been solved
'        DF          -   model built
'        Rep         -   training report, contains error on a training set
'                        and out-of-bag estimates of generalization error.
'
'      -- ALGLIB --
'         Copyright 19.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dfbuildrandomdecisionforestx1(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ntrees As Integer, nrndvars As Integer, _
		r As Double, ByRef info As Integer, ByRef df As decisionforest, ByRef rep As dfreport)
		info = 0
		df = New decisionforest()
		rep = New dfreport()
		dforest.dfbuildrandomdecisionforestx1(xy, npoints, nvars, nclasses, ntrees, nrndvars, _
			r, info, df.innerobj, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Procesing
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        X       -   input vector,  array[0..NVars-1].
'
'    OUTPUT PARAMETERS:
'        Y       -   result. Regression estimate when solving regression  task,
'                    vector of posterior probabilities for classification task.
'
'    See also DFProcessI.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dfprocess(df As decisionforest, x As Double(), ByRef y As Double())

		dforest.dfprocess(df.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    'interactive' variant of DFProcess for languages like Python which support
'    constructs like "Y = DFProcessI(DF,X)" and interactive mode of interpreter
'
'    This function allocates new array on each call,  so  it  is  significantly
'    slower than its 'non-interactive' counterpart, but it is  more  convenient
'    when you call it from command line.
'
'      -- ALGLIB --
'         Copyright 28.02.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub dfprocessi(df As decisionforest, x As Double(), ByRef y As Double())
		y = New Double(-1) {}
		dforest.dfprocessi(df.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    Relative classification error on the test set
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        percent of incorrectly classified cases.
'        Zero if model solves regression task.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function dfrelclserror(df As decisionforest, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = dforest.dfrelclserror(df.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average cross-entropy (in bits per element) on the test set
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        CrossEntropy/(NPoints*LN(2)).
'        Zero if model solves regression task.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function dfavgce(df As decisionforest, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = dforest.dfavgce(df.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        root mean square error.
'        Its meaning for regression task is obvious. As for
'        classification task, RMS error means error when estimating posterior
'        probabilities.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function dfrmserror(df As decisionforest, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = dforest.dfrmserror(df.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average error on the test set
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        Its meaning for regression task is obvious. As for
'        classification task, it means average error when estimating posterior
'        probabilities.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function dfavgerror(df As decisionforest, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = dforest.dfavgerror(df.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average relative error on the test set
'
'    INPUT PARAMETERS:
'        DF      -   decision forest model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        Its meaning for regression task is obvious. As for
'        classification task, it means average relative error when estimating
'        posterior probability of belonging to the correct class.
'
'      -- ALGLIB --
'         Copyright 16.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function dfavgrelerror(df As decisionforest, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = dforest.dfavgrelerror(df.innerobj, xy, npoints)
		Return result
	End Function

End Class
Public Partial Class alglib


	'************************************************************************
'
'    ************************************************************************

	Public Class linearmodel
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New linreg.linearmodel()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New linearmodel(DirectCast(_innerobj.make_copy(), linreg.linearmodel))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As linreg.linearmodel
		Public ReadOnly Property innerobj() As linreg.linearmodel
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As linreg.linearmodel)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    LRReport structure contains additional information about linear model:
'    * C             -   covariation matrix,  array[0..NVars,0..NVars].
'                        C[i,j] = Cov(A[i],A[j])
'    * RMSError      -   root mean square error on a training set
'    * AvgError      -   average error on a training set
'    * AvgRelError   -   average relative error on a training set (excluding
'                        observations with zero function value).
'    * CVRMSError    -   leave-one-out cross-validation estimate of
'                        generalization error. Calculated using fast algorithm
'                        with O(NVars*NPoints) complexity.
'    * CVAvgError    -   cross-validation estimate of average error
'    * CVAvgRelError -   cross-validation estimate of average relative error
'
'    All other fields of the structure are intended for internal use and should
'    not be used outside ALGLIB.
'    ************************************************************************

	Public Class lrreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property c() As Double(,)
			Get
				Return _innerobj.c
			End Get
			Set
				_innerobj.c = value
			End Set
		End Property
		Public Property rmserror() As Double
			Get
				Return _innerobj.rmserror
			End Get
			Set
				_innerobj.rmserror = value
			End Set
		End Property
		Public Property avgerror() As Double
			Get
				Return _innerobj.avgerror
			End Get
			Set
				_innerobj.avgerror = value
			End Set
		End Property
		Public Property avgrelerror() As Double
			Get
				Return _innerobj.avgrelerror
			End Get
			Set
				_innerobj.avgrelerror = value
			End Set
		End Property
		Public Property cvrmserror() As Double
			Get
				Return _innerobj.cvrmserror
			End Get
			Set
				_innerobj.cvrmserror = value
			End Set
		End Property
		Public Property cvavgerror() As Double
			Get
				Return _innerobj.cvavgerror
			End Get
			Set
				_innerobj.cvavgerror = value
			End Set
		End Property
		Public Property cvavgrelerror() As Double
			Get
				Return _innerobj.cvavgrelerror
			End Get
			Set
				_innerobj.cvavgrelerror = value
			End Set
		End Property
		Public Property ncvdefects() As Integer
			Get
				Return _innerobj.ncvdefects
			End Get
			Set
				_innerobj.ncvdefects = value
			End Set
		End Property
		Public Property cvdefects() As Integer()
			Get
				Return _innerobj.cvdefects
			End Get
			Set
				_innerobj.cvdefects = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New linreg.lrreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New lrreport(DirectCast(_innerobj.make_copy(), linreg.lrreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As linreg.lrreport
		Public ReadOnly Property innerobj() As linreg.lrreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As linreg.lrreport)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'    Linear regression
'
'    Subroutine builds model:
'
'        Y = A(0)*X[0] + ... + A(N-1)*X[N-1] + A(N)
'
'    and model found in ALGLIB format, covariation matrix, training set  errors
'    (rms,  average,  average  relative)   and  leave-one-out  cross-validation
'    estimate of the generalization error. CV  estimate calculated  using  fast
'    algorithm with O(NPoints*NVars) complexity.
'
'    When  covariation  matrix  is  calculated  standard deviations of function
'    values are assumed to be equal to RMS error on the training set.
'
'    INPUT PARAMETERS:
'        XY          -   training set, array [0..NPoints-1,0..NVars]:
'                        * NVars columns - independent variables
'                        * last column - dependent variable
'        NPoints     -   training set size, NPoints>NVars+1
'        NVars       -   number of independent variables
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -255, in case of unknown internal error
'                        * -4, if internal SVD subroutine haven't converged
'                        * -1, if incorrect parameters was passed (NPoints<NVars+2, NVars<1).
'                        *  1, if subroutine successfully finished
'        LM          -   linear model in the ALGLIB format. Use subroutines of
'                        this unit to work with the model.
'        AR          -   additional results
'
'
'      -- ALGLIB --
'         Copyright 02.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrbuild(xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef lm As linearmodel, ByRef ar As lrreport)
		info = 0
		lm = New linearmodel()
		ar = New lrreport()
		linreg.lrbuild(xy, npoints, nvars, info, lm.innerobj, ar.innerobj)
		Return
	End Sub

	'************************************************************************
'    Linear regression
'
'    Variant of LRBuild which uses vector of standatd deviations (errors in
'    function values).
'
'    INPUT PARAMETERS:
'        XY          -   training set, array [0..NPoints-1,0..NVars]:
'                        * NVars columns - independent variables
'                        * last column - dependent variable
'        S           -   standard deviations (errors in function values)
'                        array[0..NPoints-1], S[i]>0.
'        NPoints     -   training set size, NPoints>NVars+1
'        NVars       -   number of independent variables
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -255, in case of unknown internal error
'                        * -4, if internal SVD subroutine haven't converged
'                        * -1, if incorrect parameters was passed (NPoints<NVars+2, NVars<1).
'                        * -2, if S[I]<=0
'                        *  1, if subroutine successfully finished
'        LM          -   linear model in the ALGLIB format. Use subroutines of
'                        this unit to work with the model.
'        AR          -   additional results
'
'
'      -- ALGLIB --
'         Copyright 02.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrbuilds(xy As Double(,), s As Double(), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef lm As linearmodel, _
		ByRef ar As lrreport)
		info = 0
		lm = New linearmodel()
		ar = New lrreport()
		linreg.lrbuilds(xy, s, npoints, nvars, info, lm.innerobj, _
			ar.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like LRBuildS, but builds model
'
'        Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
'
'    i.e. with zero constant term.
'
'      -- ALGLIB --
'         Copyright 30.10.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrbuildzs(xy As Double(,), s As Double(), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef lm As linearmodel, _
		ByRef ar As lrreport)
		info = 0
		lm = New linearmodel()
		ar = New lrreport()
		linreg.lrbuildzs(xy, s, npoints, nvars, info, lm.innerobj, _
			ar.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like LRBuild but builds model
'
'        Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
'
'    i.e. with zero constant term.
'
'      -- ALGLIB --
'         Copyright 30.10.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrbuildz(xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef lm As linearmodel, ByRef ar As lrreport)
		info = 0
		lm = New linearmodel()
		ar = New lrreport()
		linreg.lrbuildz(xy, npoints, nvars, info, lm.innerobj, ar.innerobj)
		Return
	End Sub

	'************************************************************************
'    Unpacks coefficients of linear model.
'
'    INPUT PARAMETERS:
'        LM          -   linear model in ALGLIB format
'
'    OUTPUT PARAMETERS:
'        V           -   coefficients, array[0..NVars]
'                        constant term (intercept) is stored in the V[NVars].
'        NVars       -   number of independent variables (one less than number
'                        of coefficients)
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrunpack(lm As linearmodel, ByRef v As Double(), ByRef nvars As Integer)
		v = New Double(-1) {}
		nvars = 0
		linreg.lrunpack(lm.innerobj, v, nvars)
		Return
	End Sub

	'************************************************************************
'    "Packs" coefficients and creates linear model in ALGLIB format (LRUnpack
'    reversed).
'
'    INPUT PARAMETERS:
'        V           -   coefficients, array[0..NVars]
'        NVars       -   number of independent variables
'
'    OUTPUT PAREMETERS:
'        LM          -   linear model.
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub lrpack(v As Double(), nvars As Integer, ByRef lm As linearmodel)
		lm = New linearmodel()
		linreg.lrpack(v, nvars, lm.innerobj)
		Return
	End Sub

	'************************************************************************
'    Procesing
'
'    INPUT PARAMETERS:
'        LM      -   linear model
'        X       -   input vector,  array[0..NVars-1].
'
'    Result:
'        value of linear model regression estimate
'
'      -- ALGLIB --
'         Copyright 03.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function lrprocess(lm As linearmodel, x As Double()) As Double

		Dim result As Double = linreg.lrprocess(lm.innerobj, x)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   linear model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        root mean square error.
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function lrrmserror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = linreg.lrrmserror(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   linear model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        average error.
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function lravgerror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = linreg.lravgerror(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   linear model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        average relative error.
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function lravgrelerror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = linreg.lravgrelerror(lm.innerobj, xy, npoints)
		Return result
	End Function

End Class
Public Partial Class alglib


	'************************************************************************
'    Filters: simple moving averages (unsymmetric).
'
'    This filter replaces array by results of SMA(K) filter. SMA(K) is defined
'    as filter which averages at most K previous points (previous - not points
'    AROUND central point) - or less, in case of the first K-1 points.
'
'    INPUT PARAMETERS:
'        X           -   array[N], array to process. It can be larger than N,
'                        in this case only first N points are processed.
'        N           -   points count, N>=0
'        K           -   K>=1 (K can be larger than N ,  such  cases  will  be
'                        correctly handled). Window width. K=1 corresponds  to
'                        identity transformation (nothing changes).
'
'    OUTPUT PARAMETERS:
'        X           -   array, whose first N elements were processed with SMA(K)
'
'    NOTE 1: this function uses efficient in-place  algorithm  which  does not
'            allocate temporary arrays.
'
'    NOTE 2: this algorithm makes only one pass through array and uses running
'            sum  to speed-up calculation of the averages. Additional measures
'            are taken to ensure that running sum on a long sequence  of  zero
'            elements will be correctly reset to zero even in the presence  of
'            round-off error.
'
'    NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
'            averages points after the current one. Only X[i], X[i-1], ... are
'            used when calculating new value of X[i]. We should also note that
'            this algorithm uses BOTH previous points and  current  one,  i.e.
'            new value of X[i] depends on BOTH previous point and X[i] itself.
'
'      -- ALGLIB --
'         Copyright 25.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub filtersma(ByRef x As Double(), n As Integer, k As Integer)

		filters.filtersma(x, n, k)
		Return
	End Sub
	Public Shared Sub filtersma(ByRef x As Double(), k As Integer)
		Dim n As Integer


		n = ap.len(x)
		filters.filtersma(x, n, k)

		Return
	End Sub

	'************************************************************************
'    Filters: exponential moving averages.
'
'    This filter replaces array by results of EMA(alpha) filter. EMA(alpha) is
'    defined as filter which replaces X[] by S[]:
'        S[0] = X[0]
'        S[t] = alpha*X[t] + (1-alpha)*S[t-1]
'
'    INPUT PARAMETERS:
'        X           -   array[N], array to process. It can be larger than N,
'                        in this case only first N points are processed.
'        N           -   points count, N>=0
'        alpha       -   0<alpha<=1, smoothing parameter.
'
'    OUTPUT PARAMETERS:
'        X           -   array, whose first N elements were processed
'                        with EMA(alpha)
'
'    NOTE 1: this function uses efficient in-place  algorithm  which  does not
'            allocate temporary arrays.
'
'    NOTE 2: this algorithm uses BOTH previous points and  current  one,  i.e.
'            new value of X[i] depends on BOTH previous point and X[i] itself.
'
'    NOTE 3: technical analytis users quite often work  with  EMA  coefficient
'            expressed in DAYS instead of fractions. If you want to  calculate
'            EMA(N), where N is a number of days, you can use alpha=2/(N+1).
'
'      -- ALGLIB --
'         Copyright 25.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub filterema(ByRef x As Double(), n As Integer, alpha As Double)

		filters.filterema(x, n, alpha)
		Return
	End Sub
	Public Shared Sub filterema(ByRef x As Double(), alpha As Double)
		Dim n As Integer


		n = ap.len(x)
		filters.filterema(x, n, alpha)

		Return
	End Sub

	'************************************************************************
'    Filters: linear regression moving averages.
'
'    This filter replaces array by results of LRMA(K) filter.
'
'    LRMA(K) is defined as filter which, for each data  point,  builds  linear
'    regression  model  using  K  prevous  points (point itself is included in
'    these K points) and calculates value of this linear model at the point in
'    question.
'
'    INPUT PARAMETERS:
'        X           -   array[N], array to process. It can be larger than N,
'                        in this case only first N points are processed.
'        N           -   points count, N>=0
'        K           -   K>=1 (K can be larger than N ,  such  cases  will  be
'                        correctly handled). Window width. K=1 corresponds  to
'                        identity transformation (nothing changes).
'
'    OUTPUT PARAMETERS:
'        X           -   array, whose first N elements were processed with SMA(K)
'
'    NOTE 1: this function uses efficient in-place  algorithm  which  does not
'            allocate temporary arrays.
'
'    NOTE 2: this algorithm makes only one pass through array and uses running
'            sum  to speed-up calculation of the averages. Additional measures
'            are taken to ensure that running sum on a long sequence  of  zero
'            elements will be correctly reset to zero even in the presence  of
'            round-off error.
'
'    NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
'            averages points after the current one. Only X[i], X[i-1], ... are
'            used when calculating new value of X[i]. We should also note that
'            this algorithm uses BOTH previous points and  current  one,  i.e.
'            new value of X[i] depends on BOTH previous point and X[i] itself.
'
'      -- ALGLIB --
'         Copyright 25.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub filterlrma(ByRef x As Double(), n As Integer, k As Integer)

		filters.filterlrma(x, n, k)
		Return
	End Sub
	Public Shared Sub filterlrma(ByRef x As Double(), k As Integer)
		Dim n As Integer


		n = ap.len(x)
		filters.filterlrma(x, n, k)

		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    Multiclass Fisher LDA
'
'    Subroutine finds coefficients of linear combination which optimally separates
'    training set on classes.
'
'    COMMERCIAL EDITION OF ALGLIB:
'
'      ! Commercial version of ALGLIB includes two important  improvements   of
'      ! this function, which can be used from C++ and C#:
'      ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'      ! * multithreading support
'      !
'      ! Intel MKL gives approximately constant  (with  respect  to  number  of
'      ! worker threads) acceleration factor which depends on CPU  being  used,
'      ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'      ! comparison. Best results are achieved  for  high-dimensional  problems
'      ! (NVars is at least 256).
'      !
'      ! Multithreading is used to  accelerate  initial  phase  of  LDA,  which
'      ! includes calculation of products of large matrices.  Again,  for  best
'      ! efficiency problem must be high-dimensional.
'      !
'      ! Generally, commercial ALGLIB is several times faster than  open-source
'      ! generic C edition, and many times faster than open-source C# edition.
'      !
'      ! We recommend you to read 'Working with commercial version' section  of
'      ! ALGLIB Reference Manual in order to find out how to  use  performance-
'      ! related features provided by commercial edition of ALGLIB.
'
'    INPUT PARAMETERS:
'        XY          -   training set, array[0..NPoints-1,0..NVars].
'                        First NVars columns store values of independent
'                        variables, next column stores number of class (from 0
'                        to NClasses-1) which dataset element belongs to. Fractional
'                        values are rounded to nearest integer.
'        NPoints     -   training set size, NPoints>=0
'        NVars       -   number of independent variables, NVars>=1
'        NClasses    -   number of classes, NClasses>=2
'
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -4, if internal EVD subroutine hasn't converged
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed (NPoints<0,
'                              NVars<1, NClasses<2)
'                        *  1, if task has been solved
'                        *  2, if there was a multicollinearity in training set,
'                              but task has been solved.
'        W           -   linear combination coefficients, array[0..NVars-1]
'
'      -- ALGLIB --
'         Copyright 31.05.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub fisherlda(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double())
		info = 0
		w = New Double(-1) {}
		lda.fisherlda(xy, npoints, nvars, nclasses, info, w)
		Return
	End Sub

	'************************************************************************
'    N-dimensional multiclass Fisher LDA
'
'    Subroutine finds coefficients of linear combinations which optimally separates
'    training set on classes. It returns N-dimensional basis whose vector are sorted
'    by quality of training set separation (in descending order).
'
'    COMMERCIAL EDITION OF ALGLIB:
'
'      ! Commercial version of ALGLIB includes two important  improvements   of
'      ! this function, which can be used from C++ and C#:
'      ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'      ! * multithreading support
'      !
'      ! Intel MKL gives approximately constant  (with  respect  to  number  of
'      ! worker threads) acceleration factor which depends on CPU  being  used,
'      ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'      ! comparison. Best results are achieved  for  high-dimensional  problems
'      ! (NVars is at least 256).
'      !
'      ! Multithreading is used to  accelerate  initial  phase  of  LDA,  which
'      ! includes calculation of products of large matrices.  Again,  for  best
'      ! efficiency problem must be high-dimensional.
'      !
'      ! Generally, commercial ALGLIB is several times faster than  open-source
'      ! generic C edition, and many times faster than open-source C# edition.
'      !
'      ! We recommend you to read 'Working with commercial version' section  of
'      ! ALGLIB Reference Manual in order to find out how to  use  performance-
'      ! related features provided by commercial edition of ALGLIB.
'
'    INPUT PARAMETERS:
'        XY          -   training set, array[0..NPoints-1,0..NVars].
'                        First NVars columns store values of independent
'                        variables, next column stores number of class (from 0
'                        to NClasses-1) which dataset element belongs to. Fractional
'                        values are rounded to nearest integer.
'        NPoints     -   training set size, NPoints>=0
'        NVars       -   number of independent variables, NVars>=1
'        NClasses    -   number of classes, NClasses>=2
'
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -4, if internal EVD subroutine hasn't converged
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed (NPoints<0,
'                              NVars<1, NClasses<2)
'                        *  1, if task has been solved
'                        *  2, if there was a multicollinearity in training set,
'                              but task has been solved.
'        W           -   basis, array[0..NVars-1,0..NVars-1]
'                        columns of matrix stores basis vectors, sorted by
'                        quality of training set separation (in descending order)
'
'      -- ALGLIB --
'         Copyright 31.05.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub fisherldan(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double(,))
		info = 0
		w = New Double(-1, -1) {}
		lda.fisherldan(xy, npoints, nvars, nclasses, info, w)
		Return
	End Sub


	Public Shared Sub smp_fisherldan(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double(,))
		info = 0
		w = New Double(-1, -1) {}
		lda._pexec_fisherldan(xy, npoints, nvars, nclasses, info, w)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    Model's errors:
'        * RelCLSError   -   fraction of misclassified cases.
'        * AvgCE         -   acerage cross-entropy
'        * RMSError      -   root-mean-square error
'        * AvgError      -   average error
'        * AvgRelError   -   average relative error
'
'    NOTE 1: RelCLSError/AvgCE are zero on regression problems.
'
'    NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
'            errors in prediction of posterior probabilities
'    ************************************************************************

	Public Class modelerrors
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property relclserror() As Double
			Get
				Return _innerobj.relclserror
			End Get
			Set
				_innerobj.relclserror = value
			End Set
		End Property
		Public Property avgce() As Double
			Get
				Return _innerobj.avgce
			End Get
			Set
				_innerobj.avgce = value
			End Set
		End Property
		Public Property rmserror() As Double
			Get
				Return _innerobj.rmserror
			End Get
			Set
				_innerobj.rmserror = value
			End Set
		End Property
		Public Property avgerror() As Double
			Get
				Return _innerobj.avgerror
			End Get
			Set
				_innerobj.avgerror = value
			End Set
		End Property
		Public Property avgrelerror() As Double
			Get
				Return _innerobj.avgrelerror
			End Get
			Set
				_innerobj.avgrelerror = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New mlpbase.modelerrors()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New modelerrors(DirectCast(_innerobj.make_copy(), mlpbase.modelerrors))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlpbase.modelerrors
		Public ReadOnly Property innerobj() As mlpbase.modelerrors
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlpbase.modelerrors)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'
'    ************************************************************************

	Public Class multilayerperceptron
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New mlpbase.multilayerperceptron()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New multilayerperceptron(DirectCast(_innerobj.make_copy(), mlpbase.multilayerperceptron))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlpbase.multilayerperceptron
		Public ReadOnly Property innerobj() As mlpbase.multilayerperceptron
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlpbase.multilayerperceptron)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This function serializes data structure to string.
'
'    Important properties of s_out:
'    * it contains alphanumeric characters, dots, underscores, minus signs
'    * these symbols are grouped into words, which are separated by spaces
'      and Windows-style (CR+LF) newlines
'    * although  serializer  uses  spaces and CR+LF as separators, you can 
'      replace any separator character by arbitrary combination of spaces,
'      tabs, Windows or Unix newlines. It allows flexible reformatting  of
'      the  string  in  case you want to include it into text or XML file. 
'      But you should not insert separators into the middle of the "words"
'      nor you should change case of letters.
'    * s_out can be freely moved between 32-bit and 64-bit systems, little
'      and big endian machines, and so on. You can serialize structure  on
'      32-bit machine and unserialize it on 64-bit one (or vice versa), or
'      serialize  it  on  SPARC  and  unserialize  on  x86.  You  can also 
'      serialize  it  in  C# version of ALGLIB and unserialize in C++ one, 
'      and vice versa.
'    ************************************************************************

	Public Shared Sub mlpserialize(obj As multilayerperceptron, ByRef s_out As String)
		Dim s As New alglib.serializer()
		s.alloc_start()
		mlpbase.mlpalloc(s, obj.innerobj)
		s.sstart_str()
		mlpbase.mlpserialize(s, obj.innerobj)
		s.[stop]()
		s_out = s.get_string()
	End Sub


	'************************************************************************
'    This function unserializes data structure from string.
'    ************************************************************************

	Public Shared Sub mlpunserialize(s_in As String, ByRef obj As multilayerperceptron)
		Dim s As New alglib.serializer()
		obj = New multilayerperceptron()
		s.ustart_str(s_in)
		mlpbase.mlpunserialize(s, obj.innerobj)
		s.[stop]()
	End Sub

	'************************************************************************
'    Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'    layers, with linear output layer. Network weights are  filled  with  small
'    random values.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreate0(nin As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreate0(nin, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same  as  MLPCreate0,  but  with  one  hidden  layer  (NHid  neurons) with
'    non-linear activation function. Output layer is linear.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreate1(nin As Integer, nhid As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreate1(nin, nhid, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreate0, but with two hidden layers (NHid1 and  NHid2  neurons)
'    with non-linear activation function. Output layer is linear.
'     $ALL
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreate2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreate2(nin, nhid1, nhid2, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'    layers with non-linear output layer. Network weights are filled with small
'    random values.
'
'    Activation function of the output layer takes values:
'
'        (B, +INF), if D>=0
'
'    or
'
'        (-INF, B), if D<0.
'
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreateb0(nin As Integer, nout As Integer, b As Double, d As Double, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreateb0(nin, nout, b, d, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateB0 but with non-linear hidden layer.
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreateb1(nin As Integer, nhid As Integer, nout As Integer, b As Double, d As Double, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreateb1(nin, nhid, nout, b, d, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateB0 but with two non-linear hidden layers.
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreateb2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, b As Double, d As Double, _
		ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreateb2(nin, nhid1, nhid2, nout, b, d, _
			network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'    layers with non-linear output layer. Network weights are filled with small
'    random values. Activation function of the output layer takes values [A,B].
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreater0(nin As Integer, nout As Integer, a As Double, b As Double, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreater0(nin, nout, a, b, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateR0, but with non-linear hidden layer.
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreater1(nin As Integer, nhid As Integer, nout As Integer, a As Double, b As Double, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreater1(nin, nhid, nout, a, b, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateR0, but with two non-linear hidden layers.
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreater2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, a As Double, b As Double, _
		ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreater2(nin, nhid1, nhid2, nout, a, b, _
			network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creates classifier network with NIn  inputs  and  NOut  possible  classes.
'    Network contains no hidden layers and linear output  layer  with  SOFTMAX-
'    normalization  (so  outputs  sums  up  to  1.0  and  converge to posterior
'    probabilities).
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreatec0(nin As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreatec0(nin, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateC0, but with one non-linear hidden layer.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreatec1(nin As Integer, nhid As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreatec1(nin, nhid, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Same as MLPCreateC0, but with two non-linear hidden layers.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreatec2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ByRef network As multilayerperceptron)
		network = New multilayerperceptron()
		mlpbase.mlpcreatec2(nin, nhid1, nhid2, nout, network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Copying of neural network
'
'    INPUT PARAMETERS:
'        Network1 -   original
'
'    OUTPUT PARAMETERS:
'        Network2 -   copy
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcopy(network1 As multilayerperceptron, ByRef network2 As multilayerperceptron)
		network2 = New multilayerperceptron()
		mlpbase.mlpcopy(network1.innerobj, network2.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function copies tunable  parameters (weights/means/sigmas)  from  one
'    network to another with same architecture. It  performs  some  rudimentary
'    checks that architectures are same, and throws exception if check fails.
'
'    It is intended for fast copying of states between two  network  which  are
'    known to have same geometry.
'
'    INPUT PARAMETERS:
'        Network1 -   source, must be correctly initialized
'        Network2 -   target, must have same architecture
'
'    OUTPUT PARAMETERS:
'        Network2 -   network state is copied from source to target
'
'      -- ALGLIB --
'         Copyright 20.06.2013 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcopytunableparameters(network1 As multilayerperceptron, network2 As multilayerperceptron)

		mlpbase.mlpcopytunableparameters(network1.innerobj, network2.innerobj)
		Return
	End Sub

	'************************************************************************
'    Randomization of neural network weights
'
'      -- ALGLIB --
'         Copyright 06.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlprandomize(network As multilayerperceptron)

		mlpbase.mlprandomize(network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Randomization of neural network weights and standartisator
'
'      -- ALGLIB --
'         Copyright 10.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlprandomizefull(network As multilayerperceptron)

		mlpbase.mlprandomizefull(network.innerobj)
		Return
	End Sub

	'************************************************************************
'    Internal subroutine.
'
'      -- ALGLIB --
'         Copyright 30.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpinitpreprocessor(network As multilayerperceptron, xy As Double(,), ssize As Integer)

		mlpbase.mlpinitpreprocessor(network.innerobj, xy, ssize)
		Return
	End Sub

	'************************************************************************
'    Returns information about initialized network: number of inputs, outputs,
'    weights.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpproperties(network As multilayerperceptron, ByRef nin As Integer, ByRef nout As Integer, ByRef wcount As Integer)
		nin = 0
		nout = 0
		wcount = 0
		mlpbase.mlpproperties(network.innerobj, nin, nout, wcount)
		Return
	End Sub

	'************************************************************************
'    Returns number of inputs.
'
'      -- ALGLIB --
'         Copyright 19.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetinputscount(network As multilayerperceptron) As Integer

		Dim result As Integer = mlpbase.mlpgetinputscount(network.innerobj)
		Return result
	End Function

	'************************************************************************
'    Returns number of outputs.
'
'      -- ALGLIB --
'         Copyright 19.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetoutputscount(network As multilayerperceptron) As Integer

		Dim result As Integer = mlpbase.mlpgetoutputscount(network.innerobj)
		Return result
	End Function

	'************************************************************************
'    Returns number of weights.
'
'      -- ALGLIB --
'         Copyright 19.10.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetweightscount(network As multilayerperceptron) As Integer

		Dim result As Integer = mlpbase.mlpgetweightscount(network.innerobj)
		Return result
	End Function

	'************************************************************************
'    Tells whether network is SOFTMAX-normalized (i.e. classifier) or not.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpissoftmax(network As multilayerperceptron) As Boolean

		Dim result As Boolean = mlpbase.mlpissoftmax(network.innerobj)
		Return result
	End Function

	'************************************************************************
'    This function returns total number of layers (including input, hidden and
'    output layers).
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetlayerscount(network As multilayerperceptron) As Integer

		Dim result As Integer = mlpbase.mlpgetlayerscount(network.innerobj)
		Return result
	End Function

	'************************************************************************
'    This function returns size of K-th layer.
'
'    K=0 corresponds to input layer, K=CNT-1 corresponds to output layer.
'
'    Size of the output layer is always equal to the number of outputs, although
'    when we have softmax-normalized network, last neuron doesn't have any
'    connections - it is just zero.
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetlayersize(network As multilayerperceptron, k As Integer) As Integer

		Dim result As Integer = mlpbase.mlpgetlayersize(network.innerobj, k)
		Return result
	End Function

	'************************************************************************
'    This function returns offset/scaling coefficients for I-th input of the
'    network.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        I           -   input index
'
'    OUTPUT PARAMETERS:
'        Mean        -   mean term
'        Sigma       -   sigma term, guaranteed to be nonzero.
'
'    I-th input is passed through linear transformation
'        IN[i] = (IN[i]-Mean)/Sigma
'    before feeding to the network
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgetinputscaling(network As multilayerperceptron, i As Integer, ByRef mean As Double, ByRef sigma As Double)
		mean = 0
		sigma = 0
		mlpbase.mlpgetinputscaling(network.innerobj, i, mean, sigma)
		Return
	End Sub

	'************************************************************************
'    This function returns offset/scaling coefficients for I-th output of the
'    network.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        I           -   input index
'
'    OUTPUT PARAMETERS:
'        Mean        -   mean term
'        Sigma       -   sigma term, guaranteed to be nonzero.
'
'    I-th output is passed through linear transformation
'        OUT[i] = OUT[i]*Sigma+Mean
'    before returning it to user. In case we have SOFTMAX-normalized network,
'    we return (Mean,Sigma)=(0.0,1.0).
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgetoutputscaling(network As multilayerperceptron, i As Integer, ByRef mean As Double, ByRef sigma As Double)
		mean = 0
		sigma = 0
		mlpbase.mlpgetoutputscaling(network.innerobj, i, mean, sigma)
		Return
	End Sub

	'************************************************************************
'    This function returns information about Ith neuron of Kth layer
'
'    INPUT PARAMETERS:
'        Network     -   network
'        K           -   layer index
'        I           -   neuron index (within layer)
'
'    OUTPUT PARAMETERS:
'        FKind       -   activation function type (used by MLPActivationFunction())
'                        this value is zero for input or linear neurons
'        Threshold   -   also called offset, bias
'                        zero for input neurons
'
'    NOTE: this function throws exception if layer or neuron with  given  index
'    do not exists.
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgetneuroninfo(network As multilayerperceptron, k As Integer, i As Integer, ByRef fkind As Integer, ByRef threshold As Double)
		fkind = 0
		threshold = 0
		mlpbase.mlpgetneuroninfo(network.innerobj, k, i, fkind, threshold)
		Return
	End Sub

	'************************************************************************
'    This function returns information about connection from I0-th neuron of
'    K0-th layer to I1-th neuron of K1-th layer.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        K0          -   layer index
'        I0          -   neuron index (within layer)
'        K1          -   layer index
'        I1          -   neuron index (within layer)
'
'    RESULT:
'        connection weight (zero for non-existent connections)
'
'    This function:
'    1. throws exception if layer or neuron with given index do not exists.
'    2. returns zero if neurons exist, but there is no connection between them
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpgetweight(network As multilayerperceptron, k0 As Integer, i0 As Integer, k1 As Integer, i1 As Integer) As Double

		Dim result As Double = mlpbase.mlpgetweight(network.innerobj, k0, i0, k1, i1)
		Return result
	End Function

	'************************************************************************
'    This function sets offset/scaling coefficients for I-th input of the
'    network.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        I           -   input index
'        Mean        -   mean term
'        Sigma       -   sigma term (if zero, will be replaced by 1.0)
'
'    NTE: I-th input is passed through linear transformation
'        IN[i] = (IN[i]-Mean)/Sigma
'    before feeding to the network. This function sets Mean and Sigma.
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetinputscaling(network As multilayerperceptron, i As Integer, mean As Double, sigma As Double)

		mlpbase.mlpsetinputscaling(network.innerobj, i, mean, sigma)
		Return
	End Sub

	'************************************************************************
'    This function sets offset/scaling coefficients for I-th output of the
'    network.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        I           -   input index
'        Mean        -   mean term
'        Sigma       -   sigma term (if zero, will be replaced by 1.0)
'
'    OUTPUT PARAMETERS:
'
'    NOTE: I-th output is passed through linear transformation
'        OUT[i] = OUT[i]*Sigma+Mean
'    before returning it to user. This function sets Sigma/Mean. In case we
'    have SOFTMAX-normalized network, you can not set (Sigma,Mean) to anything
'    other than(0.0,1.0) - this function will throw exception.
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetoutputscaling(network As multilayerperceptron, i As Integer, mean As Double, sigma As Double)

		mlpbase.mlpsetoutputscaling(network.innerobj, i, mean, sigma)
		Return
	End Sub

	'************************************************************************
'    This function modifies information about Ith neuron of Kth layer
'
'    INPUT PARAMETERS:
'        Network     -   network
'        K           -   layer index
'        I           -   neuron index (within layer)
'        FKind       -   activation function type (used by MLPActivationFunction())
'                        this value must be zero for input neurons
'                        (you can not set activation function for input neurons)
'        Threshold   -   also called offset, bias
'                        this value must be zero for input neurons
'                        (you can not set threshold for input neurons)
'
'    NOTES:
'    1. this function throws exception if layer or neuron with given index do
'       not exists.
'    2. this function also throws exception when you try to set non-linear
'       activation function for input neurons (any kind of network) or for output
'       neurons of classifier network.
'    3. this function throws exception when you try to set non-zero threshold for
'       input neurons (any kind of network).
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetneuroninfo(network As multilayerperceptron, k As Integer, i As Integer, fkind As Integer, threshold As Double)

		mlpbase.mlpsetneuroninfo(network.innerobj, k, i, fkind, threshold)
		Return
	End Sub

	'************************************************************************
'    This function modifies information about connection from I0-th neuron of
'    K0-th layer to I1-th neuron of K1-th layer.
'
'    INPUT PARAMETERS:
'        Network     -   network
'        K0          -   layer index
'        I0          -   neuron index (within layer)
'        K1          -   layer index
'        I1          -   neuron index (within layer)
'        W           -   connection weight (must be zero for non-existent
'                        connections)
'
'    This function:
'    1. throws exception if layer or neuron with given index do not exists.
'    2. throws exception if you try to set non-zero weight for non-existent
'       connection
'
'      -- ALGLIB --
'         Copyright 25.03.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetweight(network As multilayerperceptron, k0 As Integer, i0 As Integer, k1 As Integer, i1 As Integer, w As Double)

		mlpbase.mlpsetweight(network.innerobj, k0, i0, k1, i1, w)
		Return
	End Sub

	'************************************************************************
'    Neural network activation function
'
'    INPUT PARAMETERS:
'        NET         -   neuron input
'        K           -   function index (zero for linear function)
'
'    OUTPUT PARAMETERS:
'        F           -   function
'        DF          -   its derivative
'        D2F         -   its second derivative
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpactivationfunction(net As Double, k As Integer, ByRef f As Double, ByRef df As Double, ByRef d2f As Double)
		f = 0
		df = 0
		d2f = 0
		mlpbase.mlpactivationfunction(net, k, f, df, d2f)
		Return
	End Sub

	'************************************************************************
'    Procesing
'
'    INPUT PARAMETERS:
'        Network -   neural network
'        X       -   input vector,  array[0..NIn-1].
'
'    OUTPUT PARAMETERS:
'        Y       -   result. Regression estimate when solving regression  task,
'                    vector of posterior probabilities for classification task.
'
'    See also MLPProcessI
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpprocess(network As multilayerperceptron, x As Double(), ByRef y As Double())

		mlpbase.mlpprocess(network.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    'interactive'  variant  of  MLPProcess  for  languages  like  Python which
'    support constructs like "Y = MLPProcess(NN,X)" and interactive mode of the
'    interpreter
'
'    This function allocates new array on each call,  so  it  is  significantly
'    slower than its 'non-interactive' counterpart, but it is  more  convenient
'    when you call it from command line.
'
'      -- ALGLIB --
'         Copyright 21.09.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpprocessi(network As multilayerperceptron, x As Double(), ByRef y As Double())
		y = New Double(-1) {}
		mlpbase.mlpprocessi(network.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    Error of the neural network on dataset.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore systems.
'      ! Second improvement gives constant speedup (2-3x, depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'        sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlperror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlperror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlperror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Error of the neural network on dataset given by sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore systems.
'      ! Second improvement gives constant speedup (2-3x, depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        NPoints     -   points count, >=0
'
'    RESULT:
'        sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlperrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlperrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlperrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    Natural error function for neural network, internal subroutine.
'
'    NOTE: this function is single-threaded. Unlike other  error  function,  it
'    receives no speed-up from being executed in SMP mode.
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperrorn(network As multilayerperceptron, xy As Double(,), ssize As Integer) As Double

		Dim result As Double = mlpbase.mlperrorn(network.innerobj, xy, ssize)
		Return result
	End Function

	'************************************************************************
'    Classification error of the neural network on dataset.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'        classification error (number of misclassified cases)
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Integer

		Dim result As Integer = mlpbase.mlpclserror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Integer

		Dim result As Integer = mlpbase._pexec_mlpclserror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Relative classification error on the test set.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'    Percent   of incorrectly   classified  cases.  Works  both  for classifier
'    networks and general purpose networks used as classifiers.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 25.12.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlprelclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlprelclserror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlprelclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlprelclserror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Relative classification error on the test set given by sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. Sparse matrix must use CRS format
'                        for storage.
'        NPoints     -   points count, >=0.
'
'    RESULT:
'    Percent   of incorrectly   classified  cases.  Works  both  for classifier
'    networks and general purpose networks used as classifiers.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 09.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlprelclserrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlprelclserrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlprelclserrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlprelclserrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    Average cross-entropy  (in bits  per element) on the test set.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'    CrossEntropy/(NPoints*LN(2)).
'    Zero if network solves regression task.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 08.01.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgce(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgce(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgce(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgce(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average  cross-entropy  (in bits  per element)  on the  test set  given by
'    sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        NPoints     -   points count, >=0.
'
'    RESULT:
'    CrossEntropy/(NPoints*LN(2)).
'    Zero if network solves regression task.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 9.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgcesparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgcesparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgcesparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgcesparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set given.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'    Root mean  square error. Its meaning for regression task is obvious. As for
'    classification  task,  RMS  error  means  error  when estimating  posterior
'    probabilities.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlprmserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlprmserror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlprmserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlprmserror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set given by sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        NPoints     -   points count, >=0.
'
'    RESULT:
'    Root mean  square error. Its meaning for regression task is obvious. As for
'    classification  task,  RMS  error  means  error  when estimating  posterior
'    probabilities.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 09.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlprmserrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlprmserrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlprmserrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlprmserrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    Average absolute error on the test set.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'    Its meaning for regression task is obvious. As for classification task, it
'    means average error when estimating posterior probabilities.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 11.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgerror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgerror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average absolute error on the test set given by sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        NPoints     -   points count, >=0.
'
'    RESULT:
'    Its meaning for regression task is obvious. As for classification task, it
'    means average error when estimating posterior probabilities.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 09.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgerrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgerrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgerrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgerrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    Average relative error on the test set.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format;
'        NPoints     -   points count.
'
'    RESULT:
'    Its meaning for regression task is obvious. As for classification task, it
'    means  average  relative  error  when  estimating posterior probability of
'    belonging to the correct class.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 11.03.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgrelerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgrelerror(network.innerobj, xy, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgrelerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgrelerror(network.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average relative error on the test set given by sparse matrix.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network     -   neural network;
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        NPoints     -   points count, >=0.
'
'    RESULT:
'    Its meaning for regression task is obvious. As for classification task, it
'    means  average  relative  error  when  estimating posterior probability of
'    belonging to the correct class.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 09.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpavgrelerrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase.mlpavgrelerrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function


	Public Shared Function smp_mlpavgrelerrorsparse(network As multilayerperceptron, xy As sparsematrix, npoints As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlpavgrelerrorsparse(network.innerobj, xy.innerobj, npoints)
		Return result
	End Function

	'************************************************************************
'    Gradient calculation
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        X       -   input vector, length of array must be at least NIn
'        DesiredY-   desired outputs, length of array must be at least NOut
'        Grad    -   possibly preallocated array. If size of array is smaller
'                    than WCount, it will be reallocated. It is recommended to
'                    reuse previously allocated array to reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'        Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgrad(network As multilayerperceptron, x As Double(), desiredy As Double(), ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase.mlpgrad(network.innerobj, x, desiredy, e, grad)
		Return
	End Sub

	'************************************************************************
'    Gradient calculation (natural error function is used)
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        X       -   input vector, length of array must be at least NIn
'        DesiredY-   desired outputs, length of array must be at least NOut
'        Grad    -   possibly preallocated array. If size of array is smaller
'                    than WCount, it will be reallocated. It is recommended to
'                    reuse previously allocated array to reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, sum-of-squares for regression networks,
'                    cross-entropy for classification networks.
'        Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradn(network As multilayerperceptron, x As Double(), desiredy As Double(), ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase.mlpgradn(network.innerobj, x, desiredy, e, grad)
		Return
	End Sub

	'************************************************************************
'    Batch gradient calculation for a set of inputs/outputs
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset in dense format; one sample = one row:
'                    * first NIn columns contain inputs,
'                    * for regression problem, next NOut columns store
'                      desired outputs.
'                    * for classification problem, next column (just one!)
'                      stores class number.
'        SSize   -   number of elements in XY
'        Grad    -   possibly preallocated array. If size of array is smaller
'                    than WCount, it will be reallocated. It is recommended to
'                    reuse previously allocated array to reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'        Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase.mlpgradbatch(network.innerobj, xy, ssize, e, grad)
		Return
	End Sub


	Public Shared Sub smp_mlpgradbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase._pexec_mlpgradbatch(network.innerobj, xy, ssize, e, grad)
		Return
	End Sub

	'************************************************************************
'    Batch gradient calculation for a set  of inputs/outputs  given  by  sparse
'    matrices
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset in sparse format; one sample = one row:
'                    * MATRIX MUST BE STORED IN CRS FORMAT
'                    * first NIn columns contain inputs.
'                    * for regression problem, next NOut columns store
'                      desired outputs.
'                    * for classification problem, next column (just one!)
'                      stores class number.
'        SSize   -   number of elements in XY
'        Grad    -   possibly preallocated array. If size of array is smaller
'                    than WCount, it will be reallocated. It is recommended to
'                    reuse previously allocated array to reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'        Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'      -- ALGLIB --
'         Copyright 26.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradbatchsparse(network As multilayerperceptron, xy As sparsematrix, ssize As Integer, ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase.mlpgradbatchsparse(network.innerobj, xy.innerobj, ssize, e, grad)
		Return
	End Sub


	Public Shared Sub smp_mlpgradbatchsparse(network As multilayerperceptron, xy As sparsematrix, ssize As Integer, ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase._pexec_mlpgradbatchsparse(network.innerobj, xy.innerobj, ssize, e, grad)
		Return
	End Sub

	'************************************************************************
'    Batch gradient calculation for a subset of dataset
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset in dense format; one sample = one row:
'                    * first NIn columns contain inputs,
'                    * for regression problem, next NOut columns store
'                      desired outputs.
'                    * for classification problem, next column (just one!)
'                      stores class number.
'        SetSize -   real size of XY, SetSize>=0;
'        Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                    * Idx[I] stores row index in the original dataset which is
'                      given by XY. Gradient is calculated with respect to rows
'                      whose indexes are stored in Idx[].
'                    * Idx[]  must store correct indexes; this function  throws
'                      an  exception  in  case  incorrect index (less than 0 or
'                      larger than rows(XY)) is given
'                    * Idx[]  may  store  indexes  in  any  order and even with
'                      repetitions.
'        SubsetSize- number of elements in Idx[] array:
'                    * positive value means that subset given by Idx[] is processed
'                    * zero value results in zero gradient
'                    * negative value means that full dataset is processed
'        Grad      - possibly  preallocated array. If size of array is  smaller
'                    than WCount, it will be reallocated. It is  recommended to
'                    reuse  previously  allocated  array  to  reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E         - error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'        Grad      - gradient  of  E  with  respect   to  weights  of  network,
'                    array[WCount]
'
'      -- ALGLIB --
'         Copyright 26.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradbatchsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
		ByRef grad As Double())
		e = 0
		mlpbase.mlpgradbatchsubset(network.innerobj, xy, setsize, idx, subsetsize, e, _
			grad)
		Return
	End Sub


	Public Shared Sub smp_mlpgradbatchsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
		ByRef grad As Double())
		e = 0
		mlpbase._pexec_mlpgradbatchsubset(network.innerobj, xy, setsize, idx, subsetsize, e, _
			grad)
		Return
	End Sub

	'************************************************************************
'    Batch gradient calculation for a set of inputs/outputs  for  a  subset  of
'    dataset given by set of indexes.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset in sparse format; one sample = one row:
'                    * MATRIX MUST BE STORED IN CRS FORMAT
'                    * first NIn columns contain inputs,
'                    * for regression problem, next NOut columns store
'                      desired outputs.
'                    * for classification problem, next column (just one!)
'                      stores class number.
'        SetSize -   real size of XY, SetSize>=0;
'        Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                    * Idx[I] stores row index in the original dataset which is
'                      given by XY. Gradient is calculated with respect to rows
'                      whose indexes are stored in Idx[].
'                    * Idx[]  must store correct indexes; this function  throws
'                      an  exception  in  case  incorrect index (less than 0 or
'                      larger than rows(XY)) is given
'                    * Idx[]  may  store  indexes  in  any  order and even with
'                      repetitions.
'        SubsetSize- number of elements in Idx[] array:
'                    * positive value means that subset given by Idx[] is processed
'                    * zero value results in zero gradient
'                    * negative value means that full dataset is processed
'        Grad      - possibly  preallocated array. If size of array is  smaller
'                    than WCount, it will be reallocated. It is  recommended to
'                    reuse  previously  allocated  array  to  reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'        Grad    -   gradient  of  E  with  respect   to  weights  of  network,
'                    array[WCount]
'
'    NOTE: when  SubsetSize<0 is used full dataset by call MLPGradBatchSparse
'          function.
'
'      -- ALGLIB --
'         Copyright 26.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradbatchsparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
		ByRef grad As Double())
		e = 0
		mlpbase.mlpgradbatchsparsesubset(network.innerobj, xy.innerobj, setsize, idx, subsetsize, e, _
			grad)
		Return
	End Sub


	Public Shared Sub smp_mlpgradbatchsparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
		ByRef grad As Double())
		e = 0
		mlpbase._pexec_mlpgradbatchsparsesubset(network.innerobj, xy.innerobj, setsize, idx, subsetsize, e, _
			grad)
		Return
	End Sub

	'************************************************************************
'    Batch gradient calculation for a set of inputs/outputs
'    (natural error function is used)
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   set of inputs/outputs; one sample = one row;
'                    first NIn columns contain inputs,
'                    next NOut columns - desired outputs.
'        SSize   -   number of elements in XY
'        Grad    -   possibly preallocated array. If size of array is smaller
'                    than WCount, it will be reallocated. It is recommended to
'                    reuse previously allocated array to reduce allocation
'                    overhead.
'
'    OUTPUT PARAMETERS:
'        E       -   error function, sum-of-squares for regression networks,
'                    cross-entropy for classification networks.
'        Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'      -- ALGLIB --
'         Copyright 04.11.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpgradnbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
		e = 0
		mlpbase.mlpgradnbatch(network.innerobj, xy, ssize, e, grad)
		Return
	End Sub

	'************************************************************************
'    Batch Hessian calculation (natural error function) using R-algorithm.
'    Internal subroutine.
'
'      -- ALGLIB --
'         Copyright 26.01.2008 by Bochkanov Sergey.
'
'         Hessian calculation based on R-algorithm described in
'         "Fast Exact Multiplication by the Hessian",
'         B. A. Pearlmutter,
'         Neural Computation, 1994.
'    ************************************************************************

	Public Shared Sub mlphessiannbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double(), ByRef h As Double(,))
		e = 0
		mlpbase.mlphessiannbatch(network.innerobj, xy, ssize, e, grad, h)
		Return
	End Sub

	'************************************************************************
'    Batch Hessian calculation using R-algorithm.
'    Internal subroutine.
'
'      -- ALGLIB --
'         Copyright 26.01.2008 by Bochkanov Sergey.
'
'         Hessian calculation based on R-algorithm described in
'         "Fast Exact Multiplication by the Hessian",
'         B. A. Pearlmutter,
'         Neural Computation, 1994.
'    ************************************************************************

	Public Shared Sub mlphessianbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double(), ByRef h As Double(,))
		e = 0
		mlpbase.mlphessianbatch(network.innerobj, xy, ssize, e, grad, h)
		Return
	End Sub

	'************************************************************************
'    Calculation of all types of errors on subset of dataset.
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset; one sample = one row;
'                    first NIn columns contain inputs,
'                    next NOut columns - desired outputs.
'        SetSize -   real size of XY, SetSize>=0;
'        Subset  -   subset of SubsetSize elements, array[SubsetSize];
'        SubsetSize- number of elements in Subset[] array:
'                    * if SubsetSize>0, rows of XY with indices Subset[0]...
'                      ...Subset[SubsetSize-1] are processed
'                    * if SubsetSize=0, zeros are returned
'                    * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                      array is ignored in this case.
'
'    OUTPUT PARAMETERS:
'        Rep     -   it contains all type of errors.
'
'      -- ALGLIB --
'         Copyright 04.09.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpallerrorssubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer, ByRef rep As modelerrors)
		rep = New modelerrors()
		mlpbase.mlpallerrorssubset(network.innerobj, xy, setsize, subset, subsetsize, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_mlpallerrorssubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer, ByRef rep As modelerrors)
		rep = New modelerrors()
		mlpbase._pexec_mlpallerrorssubset(network.innerobj, xy, setsize, subset, subsetsize, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Calculation of all types of errors on subset of dataset.
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network -   network initialized with one of the network creation funcs
'        XY      -   original dataset given by sparse matrix;
'                    one sample = one row;
'                    first NIn columns contain inputs,
'                    next NOut columns - desired outputs.
'        SetSize -   real size of XY, SetSize>=0;
'        Subset  -   subset of SubsetSize elements, array[SubsetSize];
'        SubsetSize- number of elements in Subset[] array:
'                    * if SubsetSize>0, rows of XY with indices Subset[0]...
'                      ...Subset[SubsetSize-1] are processed
'                    * if SubsetSize=0, zeros are returned
'                    * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                      array is ignored in this case.
'
'    OUTPUT PARAMETERS:
'        Rep     -   it contains all type of errors.
'
'
'      -- ALGLIB --
'         Copyright 04.09.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpallerrorssparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer, ByRef rep As modelerrors)
		rep = New modelerrors()
		mlpbase.mlpallerrorssparsesubset(network.innerobj, xy.innerobj, setsize, subset, subsetsize, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_mlpallerrorssparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer, ByRef rep As modelerrors)
		rep = New modelerrors()
		mlpbase._pexec_mlpallerrorssparsesubset(network.innerobj, xy.innerobj, setsize, subset, subsetsize, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Error of the neural network on subset of dataset.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network   -     neural network;
'        XY        -     training  set,  see  below  for  information  on   the
'                        training set format;
'        SetSize   -     real size of XY, SetSize>=0;
'        Subset    -     subset of SubsetSize elements, array[SubsetSize];
'        SubsetSize-     number of elements in Subset[] array:
'                        * if SubsetSize>0, rows of XY with indices Subset[0]...
'                          ...Subset[SubsetSize-1] are processed
'                        * if SubsetSize=0, zeros are returned
'                        * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                          array is ignored in this case.
'
'    RESULT:
'        sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 04.09.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperrorsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer) As Double

		Dim result As Double = mlpbase.mlperrorsubset(network.innerobj, xy, setsize, subset, subsetsize)
		Return result
	End Function


	Public Shared Function smp_mlperrorsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlperrorsubset(network.innerobj, xy, setsize, subset, subsetsize)
		Return result
	End Function

	'************************************************************************
'    Error of the neural network on subset of sparse dataset.
'
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support
'      !
'      ! First improvement gives close-to-linear speedup on multicore  systems.
'      ! Second improvement gives constant speedup (2-3x depending on your CPU)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'
'    INPUT PARAMETERS:
'        Network   -     neural network;
'        XY        -     training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.  Sparse  matrix  must  use  CRS  format for
'                        storage.
'        SetSize   -     real size of XY, SetSize>=0;
'                        it is used when SubsetSize<0;
'        Subset    -     subset of SubsetSize elements, array[SubsetSize];
'        SubsetSize-     number of elements in Subset[] array:
'                        * if SubsetSize>0, rows of XY with indices Subset[0]...
'                          ...Subset[SubsetSize-1] are processed
'                        * if SubsetSize=0, zeros are returned
'                        * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                          array is ignored in this case.
'
'    RESULT:
'        sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    dataset format is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 04.09.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperrorsparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer) As Double

		Dim result As Double = mlpbase.mlperrorsparsesubset(network.innerobj, xy.innerobj, setsize, subset, subsetsize)
		Return result
	End Function


	Public Shared Function smp_mlperrorsparsesubset(network As multilayerperceptron, xy As sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer) As Double

		Dim result As Double = mlpbase._pexec_mlperrorsparsesubset(network.innerobj, xy.innerobj, setsize, subset, subsetsize)
		Return result
	End Function

End Class
Public Partial Class alglib


	'************************************************************************
'
'    ************************************************************************

	Public Class logitmodel
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New logit.logitmodel()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New logitmodel(DirectCast(_innerobj.make_copy(), logit.logitmodel))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As logit.logitmodel
		Public ReadOnly Property innerobj() As logit.logitmodel
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As logit.logitmodel)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    MNLReport structure contains information about training process:
'    * NGrad     -   number of gradient calculations
'    * NHess     -   number of Hessian calculations
'    ************************************************************************

	Public Class mnlreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property ngrad() As Integer
			Get
				Return _innerobj.ngrad
			End Get
			Set
				_innerobj.ngrad = value
			End Set
		End Property
		Public Property nhess() As Integer
			Get
				Return _innerobj.nhess
			End Get
			Set
				_innerobj.nhess = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New logit.mnlreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mnlreport(DirectCast(_innerobj.make_copy(), logit.mnlreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As logit.mnlreport
		Public ReadOnly Property innerobj() As logit.mnlreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As logit.mnlreport)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'    This subroutine trains logit model.
'
'    INPUT PARAMETERS:
'        XY          -   training set, array[0..NPoints-1,0..NVars]
'                        First NVars columns store values of independent
'                        variables, next column stores number of class (from 0
'                        to NClasses-1) which dataset element belongs to. Fractional
'                        values are rounded to nearest integer.
'        NPoints     -   training set size, NPoints>=1
'        NVars       -   number of independent variables, NVars>=1
'        NClasses    -   number of classes, NClasses>=2
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<NVars+2, NVars<1, NClasses<2).
'                        *  1, if task has been solved
'        LM          -   model built
'        Rep         -   training report
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mnltrainh(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef lm As logitmodel, _
		ByRef rep As mnlreport)
		info = 0
		lm = New logitmodel()
		rep = New mnlreport()
		logit.mnltrainh(xy, npoints, nvars, nclasses, info, lm.innerobj, _
			rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Procesing
'
'    INPUT PARAMETERS:
'        LM      -   logit model, passed by non-constant reference
'                    (some fields of structure are used as temporaries
'                    when calculating model output).
'        X       -   input vector,  array[0..NVars-1].
'        Y       -   (possibly) preallocated buffer; if size of Y is less than
'                    NClasses, it will be reallocated.If it is large enough, it
'                    is NOT reallocated, so we can save some time on reallocation.
'
'    OUTPUT PARAMETERS:
'        Y       -   result, array[0..NClasses-1]
'                    Vector of posterior probabilities for classification task.
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mnlprocess(lm As logitmodel, x As Double(), ByRef y As Double())

		logit.mnlprocess(lm.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    'interactive'  variant  of  MNLProcess  for  languages  like  Python which
'    support constructs like "Y = MNLProcess(LM,X)" and interactive mode of the
'    interpreter
'
'    This function allocates new array on each call,  so  it  is  significantly
'    slower than its 'non-interactive' counterpart, but it is  more  convenient
'    when you call it from command line.
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mnlprocessi(lm As logitmodel, x As Double(), ByRef y As Double())
		y = New Double(-1) {}
		logit.mnlprocessi(lm.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    Unpacks coefficients of logit model. Logit model have form:
'
'        P(class=i) = S(i) / (S(0) + S(1) + ... +S(M-1))
'              S(i) = Exp(A[i,0]*X[0] + ... + A[i,N-1]*X[N-1] + A[i,N]), when i<M-1
'            S(M-1) = 1
'
'    INPUT PARAMETERS:
'        LM          -   logit model in ALGLIB format
'
'    OUTPUT PARAMETERS:
'        V           -   coefficients, array[0..NClasses-2,0..NVars]
'        NVars       -   number of independent variables
'        NClasses    -   number of classes
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mnlunpack(lm As logitmodel, ByRef a As Double(,), ByRef nvars As Integer, ByRef nclasses As Integer)
		a = New Double(-1, -1) {}
		nvars = 0
		nclasses = 0
		logit.mnlunpack(lm.innerobj, a, nvars, nclasses)
		Return
	End Sub

	'************************************************************************
'    "Packs" coefficients and creates logit model in ALGLIB format (MNLUnpack
'    reversed).
'
'    INPUT PARAMETERS:
'        A           -   model (see MNLUnpack)
'        NVars       -   number of independent variables
'        NClasses    -   number of classes
'
'    OUTPUT PARAMETERS:
'        LM          -   logit model.
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mnlpack(a As Double(,), nvars As Integer, nclasses As Integer, ByRef lm As logitmodel)
		lm = New logitmodel()
		logit.mnlpack(a, nvars, nclasses, lm.innerobj)
		Return
	End Sub

	'************************************************************************
'    Average cross-entropy (in bits per element) on the test set
'
'    INPUT PARAMETERS:
'        LM      -   logit model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        CrossEntropy/(NPoints*ln(2)).
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlavgce(lm As logitmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = logit.mnlavgce(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Relative classification error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   logit model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        percent of incorrectly classified cases.
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlrelclserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = logit.mnlrelclserror(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   logit model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        root mean square error (error when estimating posterior probabilities).
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlrmserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = logit.mnlrmserror(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   logit model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        average error (error when estimating posterior probabilities).
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlavgerror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = logit.mnlavgerror(lm.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average relative error on the test set
'
'    INPUT PARAMETERS:
'        LM      -   logit model
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        average relative error (error when estimating posterior probabilities).
'
'      -- ALGLIB --
'         Copyright 30.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlavgrelerror(lm As logitmodel, xy As Double(,), ssize As Integer) As Double

		Dim result As Double = logit.mnlavgrelerror(lm.innerobj, xy, ssize)
		Return result
	End Function

	'************************************************************************
'    Classification error on test set = MNLRelClsError*NPoints
'
'      -- ALGLIB --
'         Copyright 10.09.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mnlclserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Integer

		Dim result As Integer = logit.mnlclserror(lm.innerobj, xy, npoints)
		Return result
	End Function

End Class
Public Partial Class alglib


	'************************************************************************
'    This structure is a MCPD (Markov Chains for Population Data) solver.
'
'    You should use ALGLIB functions in order to work with this object.
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Class mcpdstate
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New mcpd.mcpdstate()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mcpdstate(DirectCast(_innerobj.make_copy(), mcpd.mcpdstate))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mcpd.mcpdstate
		Public ReadOnly Property innerobj() As mcpd.mcpdstate
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mcpd.mcpdstate)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This structure is a MCPD training report:
'        InnerIterationsCount    -   number of inner iterations of the
'                                    underlying optimization algorithm
'        OuterIterationsCount    -   number of outer iterations of the
'                                    underlying optimization algorithm
'        NFEV                    -   number of merit function evaluations
'        TerminationType         -   termination type
'                                    (same as for MinBLEIC optimizer, positive
'                                    values denote success, negative ones -
'                                    failure)
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Class mcpdreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property inneriterationscount() As Integer
			Get
				Return _innerobj.inneriterationscount
			End Get
			Set
				_innerobj.inneriterationscount = value
			End Set
		End Property
		Public Property outeriterationscount() As Integer
			Get
				Return _innerobj.outeriterationscount
			End Get
			Set
				_innerobj.outeriterationscount = value
			End Set
		End Property
		Public Property nfev() As Integer
			Get
				Return _innerobj.nfev
			End Get
			Set
				_innerobj.nfev = value
			End Set
		End Property
		Public Property terminationtype() As Integer
			Get
				Return _innerobj.terminationtype
			End Get
			Set
				_innerobj.terminationtype = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New mcpd.mcpdreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mcpdreport(DirectCast(_innerobj.make_copy(), mcpd.mcpdreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mcpd.mcpdreport
		Public ReadOnly Property innerobj() As mcpd.mcpdreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mcpd.mcpdreport)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'    DESCRIPTION:
'
'    This function creates MCPD (Markov Chains for Population Data) solver.
'
'    This  solver  can  be  used  to find transition matrix P for N-dimensional
'    prediction  problem  where transition from X[i] to X[i+1] is  modelled  as
'        X[i+1] = P*X[i]
'    where X[i] and X[i+1] are N-dimensional population vectors (components  of
'    each X are non-negative), and P is a N*N transition matrix (elements of  P
'    are non-negative, each column sums to 1.0).
'
'    Such models arise when when:
'    * there is some population of individuals
'    * individuals can have different states
'    * individuals can transit from one state to another
'    * population size is constant, i.e. there is no new individuals and no one
'      leaves population
'    * you want to model transitions of individuals from one state into another
'
'    USAGE:
'
'    Here we give very brief outline of the MCPD. We strongly recommend you  to
'    read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
'    on data analysis which is available at http://www.alglib.net/dataanalysis/
'
'    1. User initializes algorithm state with MCPDCreate() call
'
'    2. User  adds  one  or  more  tracks -  sequences of states which describe
'       evolution of a system being modelled from different starting conditions
'
'    3. User may add optional boundary, equality  and/or  linear constraints on
'       the coefficients of P by calling one of the following functions:
'       * MCPDSetEC() to set equality constraints
'       * MCPDSetBC() to set bound constraints
'       * MCPDSetLC() to set linear constraints
'
'    4. Optionally,  user  may  set  custom  weights  for prediction errors (by
'       default, algorithm assigns non-equal, automatically chosen weights  for
'       errors in the prediction of different components of X). It can be  done
'       with a call of MCPDSetPredictionWeights() function.
'
'    5. User calls MCPDSolve() function which takes algorithm  state and
'       pointer (delegate, etc.) to callback function which calculates F/G.
'
'    6. User calls MCPDResults() to get solution
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>=1
'
'    OUTPUT PARAMETERS:
'        State   -   structure stores algorithm state
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdcreate(n As Integer, ByRef s As mcpdstate)
		s = New mcpdstate()
		mcpd.mcpdcreate(n, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    DESCRIPTION:
'
'    This function is a specialized version of MCPDCreate()  function,  and  we
'    recommend  you  to read comments for this function for general information
'    about MCPD solver.
'
'    This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'    for "Entry-state" model,  i.e. model  where transition from X[i] to X[i+1]
'    is modelled as
'        X[i+1] = P*X[i]
'    where
'        X[i] and X[i+1] are N-dimensional state vectors
'        P is a N*N transition matrix
'    and  one  selected component of X[] is called "entry" state and is treated
'    in a special way:
'        system state always transits from "entry" state to some another state
'        system state can not transit from any state into "entry" state
'    Such conditions basically mean that row of P which corresponds to  "entry"
'    state is zero.
'
'    Such models arise when:
'    * there is some population of individuals
'    * individuals can have different states
'    * individuals can transit from one state to another
'    * population size is NOT constant -  at every moment of time there is some
'      (unpredictable) amount of "new" individuals, which can transit into  one
'      of the states at the next turn, but still no one leaves population
'    * you want to model transitions of individuals from one state into another
'    * but you do NOT want to predict amount of "new"  individuals  because  it
'      does not depends on individuals already present (hence  system  can  not
'      transit INTO entry state - it can only transit FROM it).
'
'    This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'    http://www.alglib.net/dataanalysis/ for more data).
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>=2
'        EntryState- index of entry state, in 0..N-1
'
'    OUTPUT PARAMETERS:
'        State   -   structure stores algorithm state
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdcreateentry(n As Integer, entrystate As Integer, ByRef s As mcpdstate)
		s = New mcpdstate()
		mcpd.mcpdcreateentry(n, entrystate, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    DESCRIPTION:
'
'    This function is a specialized version of MCPDCreate()  function,  and  we
'    recommend  you  to read comments for this function for general information
'    about MCPD solver.
'
'    This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'    for "Exit-state" model,  i.e. model  where  transition from X[i] to X[i+1]
'    is modelled as
'        X[i+1] = P*X[i]
'    where
'        X[i] and X[i+1] are N-dimensional state vectors
'        P is a N*N transition matrix
'    and  one  selected component of X[] is called "exit"  state and is treated
'    in a special way:
'        system state can transit from any state into "exit" state
'        system state can not transit from "exit" state into any other state
'        transition operator discards "exit" state (makes it zero at each turn)
'    Such  conditions  basically  mean  that  column  of P which corresponds to
'    "exit" state is zero. Multiplication by such P may decrease sum of  vector
'    components.
'
'    Such models arise when:
'    * there is some population of individuals
'    * individuals can have different states
'    * individuals can transit from one state to another
'    * population size is NOT constant - individuals can move into "exit" state
'      and leave population at the next turn, but there are no new individuals
'    * amount of individuals which leave population can be predicted
'    * you want to model transitions of individuals from one state into another
'      (including transitions into the "exit" state)
'
'    This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'    http://www.alglib.net/dataanalysis/ for more data).
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>=2
'        ExitState-  index of exit state, in 0..N-1
'
'    OUTPUT PARAMETERS:
'        State   -   structure stores algorithm state
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdcreateexit(n As Integer, exitstate As Integer, ByRef s As mcpdstate)
		s = New mcpdstate()
		mcpd.mcpdcreateexit(n, exitstate, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    DESCRIPTION:
'
'    This function is a specialized version of MCPDCreate()  function,  and  we
'    recommend  you  to read comments for this function for general information
'    about MCPD solver.
'
'    This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'    for "Entry-Exit-states" model, i.e. model where  transition  from  X[i] to
'    X[i+1] is modelled as
'        X[i+1] = P*X[i]
'    where
'        X[i] and X[i+1] are N-dimensional state vectors
'        P is a N*N transition matrix
'    one selected component of X[] is called "entry" state and is treated in  a
'    special way:
'        system state always transits from "entry" state to some another state
'        system state can not transit from any state into "entry" state
'    and another one component of X[] is called "exit" state and is treated  in
'    a special way too:
'        system state can transit from any state into "exit" state
'        system state can not transit from "exit" state into any other state
'        transition operator discards "exit" state (makes it zero at each turn)
'    Such conditions basically mean that:
'        row of P which corresponds to "entry" state is zero
'        column of P which corresponds to "exit" state is zero
'    Multiplication by such P may decrease sum of vector components.
'
'    Such models arise when:
'    * there is some population of individuals
'    * individuals can have different states
'    * individuals can transit from one state to another
'    * population size is NOT constant
'    * at every moment of time there is some (unpredictable)  amount  of  "new"
'      individuals, which can transit into one of the states at the next turn
'    * some  individuals  can  move  (predictably)  into "exit" state and leave
'      population at the next turn
'    * you want to model transitions of individuals from one state into another,
'      including transitions from the "entry" state and into the "exit" state.
'    * but you do NOT want to predict amount of "new"  individuals  because  it
'      does not depends on individuals already present (hence  system  can  not
'      transit INTO entry state - it can only transit FROM it).
'
'    This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'    http://www.alglib.net/dataanalysis/ for more data).
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>=2
'        EntryState- index of entry state, in 0..N-1
'        ExitState-  index of exit state, in 0..N-1
'
'    OUTPUT PARAMETERS:
'        State   -   structure stores algorithm state
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdcreateentryexit(n As Integer, entrystate As Integer, exitstate As Integer, ByRef s As mcpdstate)
		s = New mcpdstate()
		mcpd.mcpdcreateentryexit(n, entrystate, exitstate, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    This  function  is  used to add a track - sequence of system states at the
'    different moments of its evolution.
'
'    You  may  add  one  or several tracks to the MCPD solver. In case you have
'    several tracks, they won't overwrite each other. For example,  if you pass
'    two tracks, A1-A2-A3 (system at t=A+1, t=A+2 and t=A+3) and B1-B2-B3, then
'    solver will try to model transitions from t=A+1 to t=A+2, t=A+2 to  t=A+3,
'    t=B+1 to t=B+2, t=B+2 to t=B+3. But it WONT mix these two tracks - i.e. it
'    wont try to model transition from t=A+3 to t=B+1.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        XY      -   track, array[K,N]:
'                    * I-th row is a state at t=I
'                    * elements of XY must be non-negative (exception will be
'                      thrown on negative elements)
'        K       -   number of points in a track
'                    * if given, only leading K rows of XY are used
'                    * if not given, automatically determined from size of XY
'
'    NOTES:
'
'    1. Track may contain either proportional or population data:
'       * with proportional data all rows of XY must sum to 1.0, i.e. we have
'         proportions instead of absolute population values
'       * with population data rows of XY contain population counts and generally
'         do not sum to 1.0 (although they still must be non-negative)
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdaddtrack(s As mcpdstate, xy As Double(,), k As Integer)

		mcpd.mcpdaddtrack(s.innerobj, xy, k)
		Return
	End Sub
	Public Shared Sub mcpdaddtrack(s As mcpdstate, xy As Double(,))
		Dim k As Integer


		k = ap.rows(xy)
		mcpd.mcpdaddtrack(s.innerobj, xy, k)

		Return
	End Sub

	'************************************************************************
'    This function is used to add equality constraints on the elements  of  the
'    transition matrix P.
'
'    MCPD solver has four types of constraints which can be placed on P:
'    * user-specified equality constraints (optional)
'    * user-specified bound constraints (optional)
'    * user-specified general linear constraints (optional)
'    * basic constraints (always present):
'      * non-negativity: P[i,j]>=0
'      * consistency: every column of P sums to 1.0
'
'    Final  constraints  which  are  passed  to  the  underlying  optimizer are
'    calculated  as  intersection  of all present constraints. For example, you
'    may specify boundary constraint on P[0,0] and equality one:
'        0.1<=P[0,0]<=0.9
'        P[0,0]=0.5
'    Such  combination  of  constraints  will  be  silently  reduced  to  their
'    intersection, which is P[0,0]=0.5.
'
'    This  function  can  be  used  to  place equality constraints on arbitrary
'    subset of elements of P. Set of constraints is specified by EC, which  may
'    contain either NAN's or finite numbers from [0,1]. NAN denotes absence  of
'    constraint, finite number denotes equality constraint on specific  element
'    of P.
'
'    You can also  use  MCPDAddEC()  function  which  allows  to  ADD  equality
'    constraint  for  one  element  of P without changing constraints for other
'    elements.
'
'    These functions (MCPDSetEC and MCPDAddEC) interact as follows:
'    * there is internal matrix of equality constraints which is stored in  the
'      MCPD solver
'    * MCPDSetEC() replaces this matrix by another one (SET)
'    * MCPDAddEC() modifies one element of this matrix and  leaves  other  ones
'      unchanged (ADD)
'    * thus  MCPDAddEC()  call  preserves  all  modifications  done by previous
'      calls,  while  MCPDSetEC()  completely discards all changes  done to the
'      equality constraints.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        EC      -   equality constraints, array[N,N]. Elements of  EC  can  be
'                    either NAN's or finite  numbers from  [0,1].  NAN  denotes
'                    absence  of  constraints,  while  finite  value    denotes
'                    equality constraint on the corresponding element of P.
'
'    NOTES:
'
'    1. infinite values of EC will lead to exception being thrown. Values  less
'    than 0.0 or greater than 1.0 will lead to error code being returned  after
'    call to MCPDSolve().
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsetec(s As mcpdstate, ec As Double(,))

		mcpd.mcpdsetec(s.innerobj, ec)
		Return
	End Sub

	'************************************************************************
'    This function is used to add equality constraints on the elements  of  the
'    transition matrix P.
'
'    MCPD solver has four types of constraints which can be placed on P:
'    * user-specified equality constraints (optional)
'    * user-specified bound constraints (optional)
'    * user-specified general linear constraints (optional)
'    * basic constraints (always present):
'      * non-negativity: P[i,j]>=0
'      * consistency: every column of P sums to 1.0
'
'    Final  constraints  which  are  passed  to  the  underlying  optimizer are
'    calculated  as  intersection  of all present constraints. For example, you
'    may specify boundary constraint on P[0,0] and equality one:
'        0.1<=P[0,0]<=0.9
'        P[0,0]=0.5
'    Such  combination  of  constraints  will  be  silently  reduced  to  their
'    intersection, which is P[0,0]=0.5.
'
'    This function can be used to ADD equality constraint for one element of  P
'    without changing constraints for other elements.
'
'    You  can  also  use  MCPDSetEC()  function  which  allows  you  to specify
'    arbitrary set of equality constraints in one call.
'
'    These functions (MCPDSetEC and MCPDAddEC) interact as follows:
'    * there is internal matrix of equality constraints which is stored in the
'      MCPD solver
'    * MCPDSetEC() replaces this matrix by another one (SET)
'    * MCPDAddEC() modifies one element of this matrix and leaves  other  ones
'      unchanged (ADD)
'    * thus  MCPDAddEC()  call  preserves  all  modifications done by previous
'      calls,  while  MCPDSetEC()  completely discards all changes done to the
'      equality constraints.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        I       -   row index of element being constrained
'        J       -   column index of element being constrained
'        C       -   value (constraint for P[I,J]).  Can  be  either  NAN  (no
'                    constraint) or finite value from [0,1].
'
'    NOTES:
'
'    1. infinite values of C  will lead to exception being thrown. Values  less
'    than 0.0 or greater than 1.0 will lead to error code being returned  after
'    call to MCPDSolve().
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdaddec(s As mcpdstate, i As Integer, j As Integer, c As Double)

		mcpd.mcpdaddec(s.innerobj, i, j, c)
		Return
	End Sub

	'************************************************************************
'    This function is used to add bound constraints  on  the  elements  of  the
'    transition matrix P.
'
'    MCPD solver has four types of constraints which can be placed on P:
'    * user-specified equality constraints (optional)
'    * user-specified bound constraints (optional)
'    * user-specified general linear constraints (optional)
'    * basic constraints (always present):
'      * non-negativity: P[i,j]>=0
'      * consistency: every column of P sums to 1.0
'
'    Final  constraints  which  are  passed  to  the  underlying  optimizer are
'    calculated  as  intersection  of all present constraints. For example, you
'    may specify boundary constraint on P[0,0] and equality one:
'        0.1<=P[0,0]<=0.9
'        P[0,0]=0.5
'    Such  combination  of  constraints  will  be  silently  reduced  to  their
'    intersection, which is P[0,0]=0.5.
'
'    This  function  can  be  used  to  place bound   constraints  on arbitrary
'    subset  of  elements  of  P.  Set of constraints is specified by BndL/BndU
'    matrices, which may contain arbitrary combination  of  finite  numbers  or
'    infinities (like -INF<x<=0.5 or 0.1<=x<+INF).
'
'    You can also use MCPDAddBC() function which allows to ADD bound constraint
'    for one element of P without changing constraints for other elements.
'
'    These functions (MCPDSetBC and MCPDAddBC) interact as follows:
'    * there is internal matrix of bound constraints which is stored in the
'      MCPD solver
'    * MCPDSetBC() replaces this matrix by another one (SET)
'    * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
'      unchanged (ADD)
'    * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
'      calls,  while  MCPDSetBC()  completely discards all changes  done to the
'      equality constraints.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        BndL    -   lower bounds constraints, array[N,N]. Elements of BndL can
'                    be finite numbers or -INF.
'        BndU    -   upper bounds constraints, array[N,N]. Elements of BndU can
'                    be finite numbers or +INF.
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsetbc(s As mcpdstate, bndl As Double(,), bndu As Double(,))

		mcpd.mcpdsetbc(s.innerobj, bndl, bndu)
		Return
	End Sub

	'************************************************************************
'    This function is used to add bound constraints  on  the  elements  of  the
'    transition matrix P.
'
'    MCPD solver has four types of constraints which can be placed on P:
'    * user-specified equality constraints (optional)
'    * user-specified bound constraints (optional)
'    * user-specified general linear constraints (optional)
'    * basic constraints (always present):
'      * non-negativity: P[i,j]>=0
'      * consistency: every column of P sums to 1.0
'
'    Final  constraints  which  are  passed  to  the  underlying  optimizer are
'    calculated  as  intersection  of all present constraints. For example, you
'    may specify boundary constraint on P[0,0] and equality one:
'        0.1<=P[0,0]<=0.9
'        P[0,0]=0.5
'    Such  combination  of  constraints  will  be  silently  reduced  to  their
'    intersection, which is P[0,0]=0.5.
'
'    This  function  can  be  used to ADD bound constraint for one element of P
'    without changing constraints for other elements.
'
'    You  can  also  use  MCPDSetBC()  function  which  allows to  place  bound
'    constraints  on arbitrary subset of elements of P.   Set of constraints is
'    specified  by  BndL/BndU matrices, which may contain arbitrary combination
'    of finite numbers or infinities (like -INF<x<=0.5 or 0.1<=x<+INF).
'
'    These functions (MCPDSetBC and MCPDAddBC) interact as follows:
'    * there is internal matrix of bound constraints which is stored in the
'      MCPD solver
'    * MCPDSetBC() replaces this matrix by another one (SET)
'    * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
'      unchanged (ADD)
'    * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
'      calls,  while  MCPDSetBC()  completely discards all changes  done to the
'      equality constraints.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        I       -   row index of element being constrained
'        J       -   column index of element being constrained
'        BndL    -   lower bound
'        BndU    -   upper bound
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdaddbc(s As mcpdstate, i As Integer, j As Integer, bndl As Double, bndu As Double)

		mcpd.mcpdaddbc(s.innerobj, i, j, bndl, bndu)
		Return
	End Sub

	'************************************************************************
'    This function is used to set linear equality/inequality constraints on the
'    elements of the transition matrix P.
'
'    This function can be used to set one or several general linear constraints
'    on the elements of P. Two types of constraints are supported:
'    * equality constraints
'    * inequality constraints (both less-or-equal and greater-or-equal)
'
'    Coefficients  of  constraints  are  specified  by  matrix  C (one  of  the
'    parameters).  One  row  of  C  corresponds  to  one  constraint.   Because
'    transition  matrix P has N*N elements,  we  need  N*N columns to store all
'    coefficients  (they  are  stored row by row), and one more column to store
'    right part - hence C has N*N+1 columns.  Constraint  kind is stored in the
'    CT array.
'
'    Thus, I-th linear constraint is
'        P[0,0]*C[I,0] + P[0,1]*C[I,1] + .. + P[0,N-1]*C[I,N-1] +
'            + P[1,0]*C[I,N] + P[1,1]*C[I,N+1] + ... +
'            + P[N-1,N-1]*C[I,N*N-1]  ?=?  C[I,N*N]
'    where ?=? can be either "=" (CT[i]=0), "<=" (CT[i]<0) or ">=" (CT[i]>0).
'
'    Your constraint may involve only some subset of P (less than N*N elements).
'    For example it can be something like
'        P[0,0] + P[0,1] = 0.5
'    In this case you still should pass matrix  with N*N+1 columns, but all its
'    elements (except for C[0,0], C[0,1] and C[0,N*N-1]) will be zero.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        C       -   array[K,N*N+1] - coefficients of constraints
'                    (see above for complete description)
'        CT      -   array[K] - constraint types
'                    (see above for complete description)
'        K       -   number of equality/inequality constraints, K>=0:
'                    * if given, only leading K elements of C/CT are used
'                    * if not given, automatically determined from sizes of C/CT
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsetlc(s As mcpdstate, c As Double(,), ct As Integer(), k As Integer)

		mcpd.mcpdsetlc(s.innerobj, c, ct, k)
		Return
	End Sub
	Public Shared Sub mcpdsetlc(s As mcpdstate, c As Double(,), ct As Integer())
		Dim k As Integer
		If (ap.rows(c) <> ap.len(ct)) Then
			Throw New alglibexception("Error while calling 'mcpdsetlc': looks like one of arguments has wrong size")
		End If

		k = ap.rows(c)
		mcpd.mcpdsetlc(s.innerobj, c, ct, k)

		Return
	End Sub

	'************************************************************************
'    This function allows to  tune  amount  of  Tikhonov  regularization  being
'    applied to your problem.
'
'    By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
'    small non-zero value,  P is transition matrix, prior_P is identity matrix,
'    ||X||^2 is a sum of squared elements of X.
'
'    This  function  allows  you to change coefficient r. You can  also  change
'    prior values with MCPDSetPrior() function.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        V       -   regularization  coefficient, finite non-negative value. It
'                    is  not  recommended  to specify zero value unless you are
'                    pretty sure that you want it.
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsettikhonovregularizer(s As mcpdstate, v As Double)

		mcpd.mcpdsettikhonovregularizer(s.innerobj, v)
		Return
	End Sub

	'************************************************************************
'    This  function  allows to set prior values used for regularization of your
'    problem.
'
'    By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
'    small non-zero value,  P is transition matrix, prior_P is identity matrix,
'    ||X||^2 is a sum of squared elements of X.
'
'    This  function  allows  you to change prior values prior_P. You  can  also
'    change r with MCPDSetTikhonovRegularizer() function.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        PP      -   array[N,N], matrix of prior values:
'                    1. elements must be real numbers from [0,1]
'                    2. columns must sum to 1.0.
'                    First property is checked (exception is thrown otherwise),
'                    while second one is not checked/enforced.
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsetprior(s As mcpdstate, pp As Double(,))

		mcpd.mcpdsetprior(s.innerobj, pp)
		Return
	End Sub

	'************************************************************************
'    This function is used to change prediction weights
'
'    MCPD solver scales prediction errors as follows
'        Error(P) = ||W*(y-P*x)||^2
'    where
'        x is a system state at time t
'        y is a system state at time t+1
'        P is a transition matrix
'        W is a diagonal scaling matrix
'
'    By default, weights are chosen in order  to  minimize  relative prediction
'    error instead of absolute one. For example, if one component of  state  is
'    about 0.5 in magnitude and another one is about 0.05, then algorithm  will
'    make corresponding weights equal to 2.0 and 20.0.
'
'    INPUT PARAMETERS:
'        S       -   solver
'        PW      -   array[N], weights:
'                    * must be non-negative values (exception will be thrown otherwise)
'                    * zero values will be replaced by automatically chosen values
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsetpredictionweights(s As mcpdstate, pw As Double())

		mcpd.mcpdsetpredictionweights(s.innerobj, pw)
		Return
	End Sub

	'************************************************************************
'    This function is used to start solution of the MCPD problem.
'
'    After return from this function, you can use MCPDResults() to get solution
'    and completion code.
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdsolve(s As mcpdstate)

		mcpd.mcpdsolve(s.innerobj)
		Return
	End Sub

	'************************************************************************
'    MCPD results
'
'    INPUT PARAMETERS:
'        State   -   algorithm state
'
'    OUTPUT PARAMETERS:
'        P       -   array[N,N], transition matrix
'        Rep     -   optimization report. You should check Rep.TerminationType
'                    in  order  to  distinguish  successful  termination  from
'                    unsuccessful one. Speaking short, positive values  denote
'                    success, negative ones are failures.
'                    More information about fields of this  structure  can  be
'                    found in the comments on MCPDReport datatype.
'
'
'      -- ALGLIB --
'         Copyright 23.05.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mcpdresults(s As mcpdstate, ByRef p As Double(,), ByRef rep As mcpdreport)
		p = New Double(-1, -1) {}
		rep = New mcpdreport()
		mcpd.mcpdresults(s.innerobj, p, rep.innerobj)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    Neural networks ensemble
'    ************************************************************************

	Public Class mlpensemble
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New mlpe.mlpensemble()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mlpensemble(DirectCast(_innerobj.make_copy(), mlpe.mlpensemble))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlpe.mlpensemble
		Public ReadOnly Property innerobj() As mlpe.mlpensemble
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlpe.mlpensemble)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This function serializes data structure to string.
'
'    Important properties of s_out:
'    * it contains alphanumeric characters, dots, underscores, minus signs
'    * these symbols are grouped into words, which are separated by spaces
'      and Windows-style (CR+LF) newlines
'    * although  serializer  uses  spaces and CR+LF as separators, you can 
'      replace any separator character by arbitrary combination of spaces,
'      tabs, Windows or Unix newlines. It allows flexible reformatting  of
'      the  string  in  case you want to include it into text or XML file. 
'      But you should not insert separators into the middle of the "words"
'      nor you should change case of letters.
'    * s_out can be freely moved between 32-bit and 64-bit systems, little
'      and big endian machines, and so on. You can serialize structure  on
'      32-bit machine and unserialize it on 64-bit one (or vice versa), or
'      serialize  it  on  SPARC  and  unserialize  on  x86.  You  can also 
'      serialize  it  in  C# version of ALGLIB and unserialize in C++ one, 
'      and vice versa.
'    ************************************************************************

	Public Shared Sub mlpeserialize(obj As mlpensemble, ByRef s_out As String)
		Dim s As New alglib.serializer()
		s.alloc_start()
		mlpe.mlpealloc(s, obj.innerobj)
		s.sstart_str()
		mlpe.mlpeserialize(s, obj.innerobj)
		s.[stop]()
		s_out = s.get_string()
	End Sub


	'************************************************************************
'    This function unserializes data structure from string.
'    ************************************************************************

	Public Shared Sub mlpeunserialize(s_in As String, ByRef obj As mlpensemble)
		Dim s As New alglib.serializer()
		obj = New mlpensemble()
		s.ustart_str(s_in)
		mlpe.mlpeunserialize(s, obj.innerobj)
		s.[stop]()
	End Sub

	'************************************************************************
'    Like MLPCreate0, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreate0(nin As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreate0(nin, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreate1, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreate1(nin As Integer, nhid As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreate1(nin, nhid, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreate2, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreate2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreate2(nin, nhid1, nhid2, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateB0, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreateb0(nin As Integer, nout As Integer, b As Double, d As Double, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreateb0(nin, nout, b, d, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateB1, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreateb1(nin As Integer, nhid As Integer, nout As Integer, b As Double, d As Double, ensemblesize As Integer, _
		ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreateb1(nin, nhid, nout, b, d, ensemblesize, _
			ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateB2, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreateb2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, b As Double, d As Double, _
		ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreateb2(nin, nhid1, nhid2, nout, b, d, _
			ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateR0, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreater0(nin As Integer, nout As Integer, a As Double, b As Double, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreater0(nin, nout, a, b, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateR1, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreater1(nin As Integer, nhid As Integer, nout As Integer, a As Double, b As Double, ensemblesize As Integer, _
		ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreater1(nin, nhid, nout, a, b, ensemblesize, _
			ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateR2, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreater2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, a As Double, b As Double, _
		ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreater2(nin, nhid1, nhid2, nout, a, b, _
			ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateC0, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreatec0(nin As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreatec0(nin, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateC1, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreatec1(nin As Integer, nhid As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreatec1(nin, nhid, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Like MLPCreateC2, but for ensembles.
'
'      -- ALGLIB --
'         Copyright 18.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreatec2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreatec2(nin, nhid1, nhid2, nout, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creates ensemble from network. Only network geometry is copied.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpecreatefromnetwork(network As multilayerperceptron, ensemblesize As Integer, ByRef ensemble As mlpensemble)
		ensemble = New mlpensemble()
		mlpe.mlpecreatefromnetwork(network.innerobj, ensemblesize, ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Randomization of MLP ensemble
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlperandomize(ensemble As mlpensemble)

		mlpe.mlperandomize(ensemble.innerobj)
		Return
	End Sub

	'************************************************************************
'    Return ensemble properties (number of inputs and outputs).
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpeproperties(ensemble As mlpensemble, ByRef nin As Integer, ByRef nout As Integer)
		nin = 0
		nout = 0
		mlpe.mlpeproperties(ensemble.innerobj, nin, nout)
		Return
	End Sub

	'************************************************************************
'    Return normalization type (whether ensemble is SOFTMAX-normalized or not).
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpeissoftmax(ensemble As mlpensemble) As Boolean

		Dim result As Boolean = mlpe.mlpeissoftmax(ensemble.innerobj)
		Return result
	End Function

	'************************************************************************
'    Procesing
'
'    INPUT PARAMETERS:
'        Ensemble-   neural networks ensemble
'        X       -   input vector,  array[0..NIn-1].
'        Y       -   (possibly) preallocated buffer; if size of Y is less than
'                    NOut, it will be reallocated. If it is large enough, it
'                    is NOT reallocated, so we can save some time on reallocation.
'
'
'    OUTPUT PARAMETERS:
'        Y       -   result. Regression estimate when solving regression  task,
'                    vector of posterior probabilities for classification task.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpeprocess(ensemble As mlpensemble, x As Double(), ByRef y As Double())

		mlpe.mlpeprocess(ensemble.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    'interactive'  variant  of  MLPEProcess  for  languages  like Python which
'    support constructs like "Y = MLPEProcess(LM,X)" and interactive mode of the
'    interpreter
'
'    This function allocates new array on each call,  so  it  is  significantly
'    slower than its 'non-interactive' counterpart, but it is  more  convenient
'    when you call it from command line.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpeprocessi(ensemble As mlpensemble, x As Double(), ByRef y As Double())
		y = New Double(-1) {}
		mlpe.mlpeprocessi(ensemble.innerobj, x, y)
		Return
	End Sub

	'************************************************************************
'    Relative classification error on the test set
'
'    INPUT PARAMETERS:
'        Ensemble-   ensemble
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        percent of incorrectly classified cases.
'        Works both for classifier betwork and for regression networks which
'    are used as classifiers.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlperelclserror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpe.mlperelclserror(ensemble.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average cross-entropy (in bits per element) on the test set
'
'    INPUT PARAMETERS:
'        Ensemble-   ensemble
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        CrossEntropy/(NPoints*LN(2)).
'        Zero if ensemble solves regression task.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpeavgce(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpe.mlpeavgce(ensemble.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    RMS error on the test set
'
'    INPUT PARAMETERS:
'        Ensemble-   ensemble
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        root mean square error.
'        Its meaning for regression task is obvious. As for classification task
'    RMS error means error when estimating posterior probabilities.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpermserror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpe.mlpermserror(ensemble.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average error on the test set
'
'    INPUT PARAMETERS:
'        Ensemble-   ensemble
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        Its meaning for regression task is obvious. As for classification task
'    it means average error when estimating posterior probabilities.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpeavgerror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpe.mlpeavgerror(ensemble.innerobj, xy, npoints)
		Return result
	End Function

	'************************************************************************
'    Average relative error on the test set
'
'    INPUT PARAMETERS:
'        Ensemble-   ensemble
'        XY      -   test set
'        NPoints -   test set size
'
'    RESULT:
'        Its meaning for regression task is obvious. As for classification task
'    it means average relative error when estimating posterior probabilities.
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpeavgrelerror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double

		Dim result As Double = mlpe.mlpeavgrelerror(ensemble.innerobj, xy, npoints)
		Return result
	End Function

End Class
Public Partial Class alglib


	'************************************************************************
'    Training report:
'        * RelCLSError   -   fraction of misclassified cases.
'        * AvgCE         -   acerage cross-entropy
'        * RMSError      -   root-mean-square error
'        * AvgError      -   average error
'        * AvgRelError   -   average relative error
'        * NGrad         -   number of gradient calculations
'        * NHess         -   number of Hessian calculations
'        * NCholesky     -   number of Cholesky decompositions
'
'    NOTE 1: RelCLSError/AvgCE are zero on regression problems.
'
'    NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
'            errors in prediction of posterior probabilities
'    ************************************************************************

	Public Class mlpreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property relclserror() As Double
			Get
				Return _innerobj.relclserror
			End Get
			Set
				_innerobj.relclserror = value
			End Set
		End Property
		Public Property avgce() As Double
			Get
				Return _innerobj.avgce
			End Get
			Set
				_innerobj.avgce = value
			End Set
		End Property
		Public Property rmserror() As Double
			Get
				Return _innerobj.rmserror
			End Get
			Set
				_innerobj.rmserror = value
			End Set
		End Property
		Public Property avgerror() As Double
			Get
				Return _innerobj.avgerror
			End Get
			Set
				_innerobj.avgerror = value
			End Set
		End Property
		Public Property avgrelerror() As Double
			Get
				Return _innerobj.avgrelerror
			End Get
			Set
				_innerobj.avgrelerror = value
			End Set
		End Property
		Public Property ngrad() As Integer
			Get
				Return _innerobj.ngrad
			End Get
			Set
				_innerobj.ngrad = value
			End Set
		End Property
		Public Property nhess() As Integer
			Get
				Return _innerobj.nhess
			End Get
			Set
				_innerobj.nhess = value
			End Set
		End Property
		Public Property ncholesky() As Integer
			Get
				Return _innerobj.ncholesky
			End Get
			Set
				_innerobj.ncholesky = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New mlptrain.mlpreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mlpreport(DirectCast(_innerobj.make_copy(), mlptrain.mlpreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlptrain.mlpreport
		Public ReadOnly Property innerobj() As mlptrain.mlpreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlptrain.mlpreport)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    Cross-validation estimates of generalization error
'    ************************************************************************

	Public Class mlpcvreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property relclserror() As Double
			Get
				Return _innerobj.relclserror
			End Get
			Set
				_innerobj.relclserror = value
			End Set
		End Property
		Public Property avgce() As Double
			Get
				Return _innerobj.avgce
			End Get
			Set
				_innerobj.avgce = value
			End Set
		End Property
		Public Property rmserror() As Double
			Get
				Return _innerobj.rmserror
			End Get
			Set
				_innerobj.rmserror = value
			End Set
		End Property
		Public Property avgerror() As Double
			Get
				Return _innerobj.avgerror
			End Get
			Set
				_innerobj.avgerror = value
			End Set
		End Property
		Public Property avgrelerror() As Double
			Get
				Return _innerobj.avgrelerror
			End Get
			Set
				_innerobj.avgrelerror = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New mlptrain.mlpcvreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mlpcvreport(DirectCast(_innerobj.make_copy(), mlptrain.mlpcvreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlptrain.mlpcvreport
		Public ReadOnly Property innerobj() As mlptrain.mlpcvreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlptrain.mlpcvreport)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    Trainer object for neural network.
'
'    You should not try to access fields of this object directly -  use  ALGLIB
'    functions to work with this object.
'    ************************************************************************

	Public Class mlptrainer
		Inherits alglibobject
		'
		' Public declarations
		'

		Public Sub New()
			_innerobj = New mlptrain.mlptrainer()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mlptrainer(DirectCast(_innerobj.make_copy(), mlptrain.mlptrainer))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mlptrain.mlptrainer
		Public ReadOnly Property innerobj() As mlptrain.mlptrainer
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mlptrain.mlptrainer)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'    Neural network training  using  modified  Levenberg-Marquardt  with  exact
'    Hessian calculation and regularization. Subroutine trains  neural  network
'    with restarts from random positions. Algorithm is well  suited  for  small
'    and medium scale problems (hundreds of weights).
'
'    INPUT PARAMETERS:
'        Network     -   neural network with initialized geometry
'        XY          -   training set
'        NPoints     -   training set size
'        Decay       -   weight decay constant, >=0.001
'                        Decay term 'Decay*||Weights||^2' is added to error
'                        function.
'                        If you don't know what Decay to choose, use 0.001.
'        Restarts    -   number of restarts from random position, >0.
'                        If you don't know what Restarts to choose, use 2.
'
'    OUTPUT PARAMETERS:
'        Network     -   trained neural network.
'        Info        -   return code:
'                        * -9, if internal matrix inverse subroutine failed
'                        * -2, if there is a point with class number
'                              outside of [0..NOut-1].
'                        * -1, if wrong parameters specified
'                              (NPoints<0, Restarts<1).
'                        *  2, if task has been solved.
'        Rep         -   training report
'
'      -- ALGLIB --
'         Copyright 10.03.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlptrainlm(network As multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
		ByRef rep As mlpreport)
		info = 0
		rep = New mlpreport()
		mlptrain.mlptrainlm(network.innerobj, xy, npoints, decay, restarts, info, _
			rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Neural  network  training  using  L-BFGS  algorithm  with  regularization.
'    Subroutine  trains  neural  network  with  restarts from random positions.
'    Algorithm  is  well  suited  for  problems  of  any dimensionality (memory
'    requirements and step complexity are linear by weights number).
'
'    INPUT PARAMETERS:
'        Network     -   neural network with initialized geometry
'        XY          -   training set
'        NPoints     -   training set size
'        Decay       -   weight decay constant, >=0.001
'                        Decay term 'Decay*||Weights||^2' is added to error
'                        function.
'                        If you don't know what Decay to choose, use 0.001.
'        Restarts    -   number of restarts from random position, >0.
'                        If you don't know what Restarts to choose, use 2.
'        WStep       -   stopping criterion. Algorithm stops if  step  size  is
'                        less than WStep. Recommended value - 0.01.  Zero  step
'                        size means stopping after MaxIts iterations.
'        MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
'                        iterations (NOT gradient  calculations).  Zero  MaxIts
'                        means stopping when step is sufficiently small.
'
'    OUTPUT PARAMETERS:
'        Network     -   trained neural network.
'        Info        -   return code:
'                        * -8, if both WStep=0 and MaxIts=0
'                        * -2, if there is a point with class number
'                              outside of [0..NOut-1].
'                        * -1, if wrong parameters specified
'                              (NPoints<0, Restarts<1).
'                        *  2, if task has been solved.
'        Rep         -   training report
'
'      -- ALGLIB --
'         Copyright 09.12.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlptrainlbfgs(network As multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
		maxits As Integer, ByRef info As Integer, ByRef rep As mlpreport)
		info = 0
		rep = New mlpreport()
		mlptrain.mlptrainlbfgs(network.innerobj, xy, npoints, decay, restarts, wstep, _
			maxits, info, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Neural network training using early stopping (base algorithm - L-BFGS with
'    regularization).
'
'    INPUT PARAMETERS:
'        Network     -   neural network with initialized geometry
'        TrnXY       -   training set
'        TrnSize     -   training set size, TrnSize>0
'        ValXY       -   validation set
'        ValSize     -   validation set size, ValSize>0
'        Decay       -   weight decay constant, >=0.001
'                        Decay term 'Decay*||Weights||^2' is added to error
'                        function.
'                        If you don't know what Decay to choose, use 0.001.
'        Restarts    -   number of restarts, either:
'                        * strictly positive number - algorithm make specified
'                          number of restarts from random position.
'                        * -1, in which case algorithm makes exactly one run
'                          from the initial state of the network (no randomization).
'                        If you don't know what Restarts to choose, choose one
'                        one the following:
'                        * -1 (deterministic start)
'                        * +1 (one random restart)
'                        * +5 (moderate amount of random restarts)
'
'    OUTPUT PARAMETERS:
'        Network     -   trained neural network.
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NOut-1].
'                        * -1, if wrong parameters specified
'                              (NPoints<0, Restarts<1, ...).
'                        *  2, task has been solved, stopping  criterion  met -
'                              sufficiently small step size.  Not expected  (we
'                              use  EARLY  stopping)  but  possible  and not an
'                              error.
'                        *  6, task has been solved, stopping  criterion  met -
'                              increasing of validation set error.
'        Rep         -   training report
'
'    NOTE:
'
'    Algorithm stops if validation set error increases for  a  long  enough  or
'    step size is small enought  (there  are  task  where  validation  set  may
'    decrease for eternity). In any case solution returned corresponds  to  the
'    minimum of validation set error.
'
'      -- ALGLIB --
'         Copyright 10.03.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlptraines(network As multilayerperceptron, trnxy As Double(,), trnsize As Integer, valxy As Double(,), valsize As Integer, decay As Double, _
		restarts As Integer, ByRef info As Integer, ByRef rep As mlpreport)
		info = 0
		rep = New mlpreport()
		mlptrain.mlptraines(network.innerobj, trnxy, trnsize, valxy, valsize, decay, _
			restarts, info, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Cross-validation estimate of generalization error.
'
'    Base algorithm - L-BFGS.
'
'    INPUT PARAMETERS:
'        Network     -   neural network with initialized geometry.   Network is
'                        not changed during cross-validation -  it is used only
'                        as a representative of its architecture.
'        XY          -   training set.
'        SSize       -   training set size
'        Decay       -   weight  decay, same as in MLPTrainLBFGS
'        Restarts    -   number of restarts, >0.
'                        restarts are counted for each partition separately, so
'                        total number of restarts will be Restarts*FoldsCount.
'        WStep       -   stopping criterion, same as in MLPTrainLBFGS
'        MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
'        FoldsCount  -   number of folds in k-fold cross-validation,
'                        2<=FoldsCount<=SSize.
'                        recommended value: 10.
'
'    OUTPUT PARAMETERS:
'        Info        -   return code, same as in MLPTrainLBFGS
'        Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
'        CVRep       -   generalization error estimates
'
'      -- ALGLIB --
'         Copyright 09.12.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpkfoldcvlbfgs(network As multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
		maxits As Integer, foldscount As Integer, ByRef info As Integer, ByRef rep As mlpreport, ByRef cvrep As mlpcvreport)
		info = 0
		rep = New mlpreport()
		cvrep = New mlpcvreport()
		mlptrain.mlpkfoldcvlbfgs(network.innerobj, xy, npoints, decay, restarts, wstep, _
			maxits, foldscount, info, rep.innerobj, cvrep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Cross-validation estimate of generalization error.
'
'    Base algorithm - Levenberg-Marquardt.
'
'    INPUT PARAMETERS:
'        Network     -   neural network with initialized geometry.   Network is
'                        not changed during cross-validation -  it is used only
'                        as a representative of its architecture.
'        XY          -   training set.
'        SSize       -   training set size
'        Decay       -   weight  decay, same as in MLPTrainLBFGS
'        Restarts    -   number of restarts, >0.
'                        restarts are counted for each partition separately, so
'                        total number of restarts will be Restarts*FoldsCount.
'        FoldsCount  -   number of folds in k-fold cross-validation,
'                        2<=FoldsCount<=SSize.
'                        recommended value: 10.
'
'    OUTPUT PARAMETERS:
'        Info        -   return code, same as in MLPTrainLBFGS
'        Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
'        CVRep       -   generalization error estimates
'
'      -- ALGLIB --
'         Copyright 09.12.2007 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpkfoldcvlm(network As multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, foldscount As Integer, _
		ByRef info As Integer, ByRef rep As mlpreport, ByRef cvrep As mlpcvreport)
		info = 0
		rep = New mlpreport()
		cvrep = New mlpcvreport()
		mlptrain.mlpkfoldcvlm(network.innerobj, xy, npoints, decay, restarts, foldscount, _
			info, rep.innerobj, cvrep.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function estimates generalization error using cross-validation on the
'    current dataset with current training settings.
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support (C++ computational core)
'      !
'      ! Second improvement gives constant  speedup (2-3X).  First  improvement
'      ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'      ! operations can be executed in parallel:
'      ! * FoldsCount cross-validation rounds (always)
'      ! * NRestarts training sessions performed within each of
'      !   cross-validation rounds (if NRestarts>1)
'      ! * gradient calculation over large dataset (if dataset is large enough)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        Network     -   neural network. It must have same number of inputs and
'                        output/classes as was specified during creation of the
'                        trainer object. Network is not changed  during  cross-
'                        validation and is not trained - it  is  used  only  as
'                        representative of its architecture. I.e., we  estimate
'                        generalization properties of  ARCHITECTURE,  not  some
'                        specific network.
'        NRestarts   -   number of restarts, >=0:
'                        * NRestarts>0  means  that  for  each cross-validation
'                          round   specified  number   of  random  restarts  is
'                          performed,  with  best  network  being  chosen after
'                          training.
'                        * NRestarts=0 is same as NRestarts=1
'        FoldsCount  -   number of folds in k-fold cross-validation:
'                        * 2<=FoldsCount<=size of dataset
'                        * recommended value: 10.
'                        * values larger than dataset size will be silently
'                          truncated down to dataset size
'
'    OUTPUT PARAMETERS:
'        Rep         -   structure which contains cross-validation estimates:
'                        * Rep.RelCLSError - fraction of misclassified cases.
'                        * Rep.AvgCE - acerage cross-entropy
'                        * Rep.RMSError - root-mean-square error
'                        * Rep.AvgError - average error
'                        * Rep.AvgRelError - average relative error
'
'    NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'          or subset with only one point  was  given,  zeros  are  returned  as
'          estimates.
'
'    NOTE: this method performs FoldsCount cross-validation  rounds,  each  one
'          with NRestarts random starts.  Thus,  FoldsCount*NRestarts  networks
'          are trained in total.
'
'    NOTE: Rep.RelCLSError/Rep.AvgCE are zero on regression problems.
'
'    NOTE: on classification problems Rep.RMSError/Rep.AvgError/Rep.AvgRelError
'          contain errors in prediction of posterior probabilities.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpkfoldcv(s As mlptrainer, network As multilayerperceptron, nrestarts As Integer, foldscount As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain.mlpkfoldcv(s.innerobj, network.innerobj, nrestarts, foldscount, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_mlpkfoldcv(s As mlptrainer, network As multilayerperceptron, nrestarts As Integer, foldscount As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain._pexec_mlpkfoldcv(s.innerobj, network.innerobj, nrestarts, foldscount, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creation of the network trainer object for regression networks
'
'    INPUT PARAMETERS:
'        NIn         -   number of inputs, NIn>=1
'        NOut        -   number of outputs, NOut>=1
'
'    OUTPUT PARAMETERS:
'        S           -   neural network trainer object.
'                        This structure can be used to train any regression
'                        network with NIn inputs and NOut outputs.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreatetrainer(nin As Integer, nout As Integer, ByRef s As mlptrainer)
		s = New mlptrainer()
		mlptrain.mlpcreatetrainer(nin, nout, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    Creation of the network trainer object for classification networks
'
'    INPUT PARAMETERS:
'        NIn         -   number of inputs, NIn>=1
'        NClasses    -   number of classes, NClasses>=2
'
'    OUTPUT PARAMETERS:
'        S           -   neural network trainer object.
'                        This structure can be used to train any classification
'                        network with NIn inputs and NOut outputs.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpcreatetrainercls(nin As Integer, nclasses As Integer, ByRef s As mlptrainer)
		s = New mlptrainer()
		mlptrain.mlpcreatetrainercls(nin, nclasses, s.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function sets "current dataset" of the trainer object to  one  passed
'    by user.
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed.
'        NPoints     -   points count, >=0.
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    datasetformat is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetdataset(s As mlptrainer, xy As Double(,), npoints As Integer)

		mlptrain.mlpsetdataset(s.innerobj, xy, npoints)
		Return
	End Sub

	'************************************************************************
'    This function sets "current dataset" of the trainer object to  one  passed
'    by user (sparse matrix is used to store dataset).
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        XY          -   training  set,  see  below  for  information  on   the
'                        training set format. This function checks  correctness
'                        of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                        correct) and throws exception when  incorrect  dataset
'                        is passed. Any  sparse  storage  format  can be  used:
'                        Hash-table, CRS...
'        NPoints     -   points count, >=0
'
'    DATASET FORMAT:
'
'    This  function  uses  two  different  dataset formats - one for regression
'    networks, another one for classification networks.
'
'    For regression networks with NIn inputs and NOut outputs following dataset
'    format is used:
'    * dataset is given by NPoints*(NIn+NOut) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, next NOut columns are outputs
'
'    For classification networks with NIn inputs and NClasses clases  following
'    datasetformat is used:
'    * dataset is given by NPoints*(NIn+1) matrix
'    * each row corresponds to one example
'    * first NIn columns are inputs, last column stores class number (from 0 to
'      NClasses-1).
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetsparsedataset(s As mlptrainer, xy As sparsematrix, npoints As Integer)

		mlptrain.mlpsetsparsedataset(s.innerobj, xy.innerobj, npoints)
		Return
	End Sub

	'************************************************************************
'    This function sets weight decay coefficient which is used for training.
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        Decay       -   weight  decay  coefficient,  >=0.  Weight  decay  term
'                        'Decay*||Weights||^2' is added to error  function.  If
'                        you don't know what Decay to choose, use 1.0E-3.
'                        Weight decay can be set to zero,  in this case network
'                        is trained without weight decay.
'
'    NOTE: by default network uses some small nonzero value for weight decay.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetdecay(s As mlptrainer, decay As Double)

		mlptrain.mlpsetdecay(s.innerobj, decay)
		Return
	End Sub

	'************************************************************************
'    This function sets stopping criteria for the optimizer.
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        WStep       -   stopping criterion. Algorithm stops if  step  size  is
'                        less than WStep. Recommended value - 0.01.  Zero  step
'                        size means stopping after MaxIts iterations.
'                        WStep>=0.
'        MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
'                        epochs (full passes over entire dataset).  Zero MaxIts
'                        means stopping when step is sufficiently small.
'                        MaxIts>=0.
'
'    NOTE: by default, WStep=0.005 and MaxIts=0 are used. These values are also
'          used when MLPSetCond() is called with WStep=0 and MaxIts=0.
'
'    NOTE: these stopping criteria are used for all kinds of neural training  -
'          from "conventional" networks to early stopping ensembles. When  used
'          for "conventional" networks, they are  used  as  the  only  stopping
'          criteria. When combined with early stopping, they used as ADDITIONAL
'          stopping criteria which can terminate early stopping algorithm.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetcond(s As mlptrainer, wstep As Double, maxits As Integer)

		mlptrain.mlpsetcond(s.innerobj, wstep, maxits)
		Return
	End Sub

	'************************************************************************
'    This function sets training algorithm: batch training using L-BFGS will be
'    used.
'
'    This algorithm:
'    * the most robust for small-scale problems, but may be too slow for  large
'      scale ones.
'    * perfoms full pass through the dataset before performing step
'    * uses conditions specified by MLPSetCond() for stopping
'    * is default one used by trainer object
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpsetalgobatch(s As mlptrainer)

		mlptrain.mlpsetalgobatch(s.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function trains neural network passed to this function, using current
'    dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
'    and current training settings. Training  from  NRestarts  random  starting
'    positions is performed, best network is chosen.
'
'    Training is performed using current training algorithm.
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support (C++ computational core)
'      !
'      ! Second improvement gives constant  speedup (2-3X).  First  improvement
'      ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'      ! operations can be executed in parallel:
'      ! * NRestarts training sessions performed within each of
'      !   cross-validation rounds (if NRestarts>1)
'      ! * gradient calculation over large dataset (if dataset is large enough)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        Network     -   neural network. It must have same number of inputs and
'                        output/classes as was specified during creation of the
'                        trainer object.
'        NRestarts   -   number of restarts, >=0:
'                        * NRestarts>0 means that specified  number  of  random
'                          restarts are performed, best network is chosen after
'                          training
'                        * NRestarts=0 means that current state of the  network
'                          is used for training.
'
'    OUTPUT PARAMETERS:
'        Network     -   trained network
'
'    NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'          network  is  filled  by zero  values.  Same  behavior  for functions
'          MLPStartTraining and MLPContinueTraining.
'
'    NOTE: this method uses sum-of-squares error function for training.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlptrainnetwork(s As mlptrainer, network As multilayerperceptron, nrestarts As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain.mlptrainnetwork(s.innerobj, network.innerobj, nrestarts, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_mlptrainnetwork(s As mlptrainer, network As multilayerperceptron, nrestarts As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain._pexec_mlptrainnetwork(s.innerobj, network.innerobj, nrestarts, rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
'               not recommend you to use it unless you are pretty sure that you
'               need ability to monitor training progress.
'
'    This function performs step-by-step training of the neural  network.  Here
'    "step-by-step" means that training  starts  with  MLPStartTraining() call,
'    and then user subsequently calls MLPContinueTraining() to perform one more
'    iteration of the training.
'
'    After call to this function trainer object remembers network and  is ready
'    to  train  it.  However,  no  training  is  performed  until first call to
'    MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
'    will advance training progress one iteration further.
'
'    EXAMPLE:
'        >
'        > ...initialize network and trainer object....
'        >
'        > MLPStartTraining(Trainer, Network, True)
'        > while MLPContinueTraining(Trainer, Network) do
'        >     ...visualize training progress...
'        >
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        Network     -   neural network. It must have same number of inputs and
'                        output/classes as was specified during creation of the
'                        trainer object.
'        RandomStart -   randomize network before training or not:
'                        * True  means  that  network  is  randomized  and  its
'                          initial state (one which was passed to  the  trainer
'                          object) is lost.
'                        * False  means  that  training  is  started  from  the
'                          current state of the network
'
'    OUTPUT PARAMETERS:
'        Network     -   neural network which is ready to training (weights are
'                        initialized, preprocessor is initialized using current
'                        training set)
'
'    NOTE: this method uses sum-of-squares error function for training.
'
'    NOTE: it is expected that trainer object settings are NOT  changed  during
'          step-by-step training, i.e. no  one  changes  stopping  criteria  or
'          training set during training. It is possible and there is no defense
'          against  such  actions,  but  algorithm  behavior  in  such cases is
'          undefined and can be unpredictable.
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpstarttraining(s As mlptrainer, network As multilayerperceptron, randomstart As Boolean)

		mlptrain.mlpstarttraining(s.innerobj, network.innerobj, randomstart)
		Return
	End Sub

	'************************************************************************
'    IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
'               not recommend you to use it unless you are pretty sure that you
'               need ability to monitor training progress.
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support (C++ computational core)
'      !
'      ! Second improvement gives constant  speedup (2-3X).  First  improvement
'      ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'      ! operations can be executed in parallel:
'      ! * gradient calculation over large dataset (if dataset is large enough)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'    This function performs step-by-step training of the neural  network.  Here
'    "step-by-step" means that training starts  with  MLPStartTraining()  call,
'    and then user subsequently calls MLPContinueTraining() to perform one more
'    iteration of the training.
'
'    This  function  performs  one  more  iteration of the training and returns
'    either True (training continues) or False (training stopped). In case True
'    was returned, Network weights are updated according to the  current  state
'    of the optimization progress. In case False was  returned,  no  additional
'    updates is performed (previous update of  the  network weights moved us to
'    the final point, and no additional updates is needed).
'
'    EXAMPLE:
'        >
'        > [initialize network and trainer object]
'        >
'        > MLPStartTraining(Trainer, Network, True)
'        > while MLPContinueTraining(Trainer, Network) do
'        >     [visualize training progress]
'        >
'
'    INPUT PARAMETERS:
'        S           -   trainer object
'        Network     -   neural  network  structure,  which  is  used to  store
'                        current state of the training process.
'
'    OUTPUT PARAMETERS:
'        Network     -   weights of the neural network  are  rewritten  by  the
'                        current approximation.
'
'    NOTE: this method uses sum-of-squares error function for training.
'
'    NOTE: it is expected that trainer object settings are NOT  changed  during
'          step-by-step training, i.e. no  one  changes  stopping  criteria  or
'          training set during training. It is possible and there is no defense
'          against  such  actions,  but  algorithm  behavior  in  such cases is
'          undefined and can be unpredictable.
'
'    NOTE: It  is  expected that Network is the same one which  was  passed  to
'          MLPStartTraining() function.  However,  THIS  function  checks  only
'          following:
'          * that number of network inputs is consistent with trainer object
'            settings
'          * that number of network outputs/classes is consistent with  trainer
'            object settings
'          * that number of network weights is the same as number of weights in
'            the network passed to MLPStartTraining() function
'          Exception is thrown when these conditions are violated.
'
'          It is also expected that you do not change state of the  network  on
'          your own - the only party who has right to change network during its
'          training is a trainer object. Any attempt to interfere with  trainer
'          may lead to unpredictable results.
'
'
'      -- ALGLIB --
'         Copyright 23.07.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Function mlpcontinuetraining(s As mlptrainer, network As multilayerperceptron) As Boolean

		Dim result As Boolean = mlptrain.mlpcontinuetraining(s.innerobj, network.innerobj)
		Return result
	End Function


	Public Shared Function smp_mlpcontinuetraining(s As mlptrainer, network As multilayerperceptron) As Boolean

		Dim result As Boolean = mlptrain._pexec_mlpcontinuetraining(s.innerobj, network.innerobj)
		Return result
	End Function

	'************************************************************************
'    Training neural networks ensemble using  bootstrap  aggregating (bagging).
'    Modified Levenberg-Marquardt algorithm is used as base training method.
'
'    INPUT PARAMETERS:
'        Ensemble    -   model with initialized geometry
'        XY          -   training set
'        NPoints     -   training set size
'        Decay       -   weight decay coefficient, >=0.001
'        Restarts    -   restarts, >0.
'
'    OUTPUT PARAMETERS:
'        Ensemble    -   trained model
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<0, Restarts<1).
'                        *  2, if task has been solved.
'        Rep         -   training report.
'        OOBErrors   -   out-of-bag generalization error estimate
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpebagginglm(ensemble As mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
		ByRef rep As mlpreport, ByRef ooberrors As mlpcvreport)
		info = 0
		rep = New mlpreport()
		ooberrors = New mlpcvreport()
		mlptrain.mlpebagginglm(ensemble.innerobj, xy, npoints, decay, restarts, info, _
			rep.innerobj, ooberrors.innerobj)
		Return
	End Sub

	'************************************************************************
'    Training neural networks ensemble using  bootstrap  aggregating (bagging).
'    L-BFGS algorithm is used as base training method.
'
'    INPUT PARAMETERS:
'        Ensemble    -   model with initialized geometry
'        XY          -   training set
'        NPoints     -   training set size
'        Decay       -   weight decay coefficient, >=0.001
'        Restarts    -   restarts, >0.
'        WStep       -   stopping criterion, same as in MLPTrainLBFGS
'        MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
'
'    OUTPUT PARAMETERS:
'        Ensemble    -   trained model
'        Info        -   return code:
'                        * -8, if both WStep=0 and MaxIts=0
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<0, Restarts<1).
'                        *  2, if task has been solved.
'        Rep         -   training report.
'        OOBErrors   -   out-of-bag generalization error estimate
'
'      -- ALGLIB --
'         Copyright 17.02.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpebagginglbfgs(ensemble As mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
		maxits As Integer, ByRef info As Integer, ByRef rep As mlpreport, ByRef ooberrors As mlpcvreport)
		info = 0
		rep = New mlpreport()
		ooberrors = New mlpcvreport()
		mlptrain.mlpebagginglbfgs(ensemble.innerobj, xy, npoints, decay, restarts, wstep, _
			maxits, info, rep.innerobj, ooberrors.innerobj)
		Return
	End Sub

	'************************************************************************
'    Training neural networks ensemble using early stopping.
'
'    INPUT PARAMETERS:
'        Ensemble    -   model with initialized geometry
'        XY          -   training set
'        NPoints     -   training set size
'        Decay       -   weight decay coefficient, >=0.001
'        Restarts    -   restarts, >0.
'
'    OUTPUT PARAMETERS:
'        Ensemble    -   trained model
'        Info        -   return code:
'                        * -2, if there is a point with class number
'                              outside of [0..NClasses-1].
'                        * -1, if incorrect parameters was passed
'                              (NPoints<0, Restarts<1).
'                        *  6, if task has been solved.
'        Rep         -   training report.
'        OOBErrors   -   out-of-bag generalization error estimate
'
'      -- ALGLIB --
'         Copyright 10.03.2009 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlpetraines(ensemble As mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
		ByRef rep As mlpreport)
		info = 0
		rep = New mlpreport()
		mlptrain.mlpetraines(ensemble.innerobj, xy, npoints, decay, restarts, info, _
			rep.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function trains neural network ensemble passed to this function using
'    current dataset and early stopping training algorithm. Each early stopping
'    round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
'    training rounds is performed in total).
'
'    FOR USERS OF COMMERCIAL EDITION:
'
'      ! Commercial version of ALGLIB includes two  important  improvements  of
'      ! this function:
'      ! * multicore support (C++ and C# computational cores)
'      ! * SSE support (C++ computational core)
'      !
'      ! Second improvement gives constant  speedup (2-3X).  First  improvement
'      ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'      ! operations can be executed in parallel:
'      ! * EnsembleSize  training  sessions  performed  for  each  of  ensemble
'      !   members (always parallelized)
'      ! * NRestarts  training  sessions  performed  within  each  of  training
'      !   sessions (if NRestarts>1)
'      ! * gradient calculation over large dataset (if dataset is large enough)
'      !
'      ! In order to use multicore features you have to:
'      ! * use commercial version of ALGLIB
'      ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'      !   multicore code will be used (for multicore support)
'      !
'      ! In order to use SSE features you have to:
'      ! * use commercial version of ALGLIB on Intel processors
'      ! * use C++ computational core
'      !
'      ! This note is given for users of commercial edition; if  you  use  GPL
'      ! edition, you still will be able to call smp-version of this function,
'      ! but all computations will be done serially.
'      !
'      ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'      ! called 'SMP support', before using parallel version of this function.
'
'    INPUT PARAMETERS:
'        S           -   trainer object;
'        Ensemble    -   neural network ensemble. It must have same  number  of
'                        inputs and outputs/classes  as  was  specified  during
'                        creation of the trainer object.
'        NRestarts   -   number of restarts, >=0:
'                        * NRestarts>0 means that specified  number  of  random
'                          restarts are performed during each ES round;
'                        * NRestarts=0 is silently replaced by 1.
'
'    OUTPUT PARAMETERS:
'        Ensemble    -   trained ensemble;
'        Rep         -   it contains all type of errors.
'
'    NOTE: this training method uses BOTH early stopping and weight decay!  So,
'          you should select weight decay before starting training just as  you
'          select it before training "conventional" networks.
'
'    NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'          or  single-point  dataset  was  passed,  ensemble  is filled by zero
'          values.
'
'    NOTE: this method uses sum-of-squares error function for training.
'
'      -- ALGLIB --
'         Copyright 22.08.2012 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mlptrainensemblees(s As mlptrainer, ensemble As mlpensemble, nrestarts As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain.mlptrainensemblees(s.innerobj, ensemble.innerobj, nrestarts, rep.innerobj)
		Return
	End Sub


	Public Shared Sub smp_mlptrainensemblees(s As mlptrainer, ensemble As mlpensemble, nrestarts As Integer, ByRef rep As mlpreport)
		rep = New mlpreport()
		mlptrain._pexec_mlptrainensemblees(s.innerobj, ensemble.innerobj, nrestarts, rep.innerobj)
		Return
	End Sub

End Class
Public Partial Class alglib


	'************************************************************************
'    Principal components analysis
'
'    Subroutine  builds  orthogonal  basis  where  first  axis  corresponds  to
'    direction with maximum variance, second axis maximizes variance in subspace
'    orthogonal to first axis and so on.
'
'    It should be noted that, unlike LDA, PCA does not use class labels.
'
'    COMMERCIAL EDITION OF ALGLIB:
'
'      ! Commercial version of ALGLIB includes one  important  improvement   of
'      ! this function, which can be used from C++ and C#:
'      ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'      !
'      ! Intel MKL gives approximately constant  (with  respect  to  number  of
'      ! worker threads) acceleration factor which depends on CPU  being  used,
'      ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'      ! comparison. Best results are achieved  for  high-dimensional  problems
'      ! (NVars is at least 256).
'      !
'      ! We recommend you to read 'Working with commercial version' section  of
'      ! ALGLIB Reference Manual in order to find out how to  use  performance-
'      ! related features provided by commercial edition of ALGLIB.
'
'    INPUT PARAMETERS:
'        X           -   dataset, array[0..NPoints-1,0..NVars-1].
'                        matrix contains ONLY INDEPENDENT VARIABLES.
'        NPoints     -   dataset size, NPoints>=0
'        NVars       -   number of independent variables, NVars>=1
'
'    OUTPUT PARAMETERS:
'        Info        -   return code:
'                        * -4, if SVD subroutine haven't converged
'                        * -1, if wrong parameters has been passed (NPoints<0,
'                              NVars<1)
'                        *  1, if task is solved
'        S2          -   array[0..NVars-1]. variance values corresponding
'                        to basis vectors.
'        V           -   array[0..NVars-1,0..NVars-1]
'                        matrix, whose columns store basis vectors.
'
'      -- ALGLIB --
'         Copyright 25.08.2008 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub pcabuildbasis(x As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef s2 As Double(), ByRef v As Double(,))
		info = 0
		s2 = New Double(-1) {}
		v = New Double(-1, -1) {}
		pca.pcabuildbasis(x, npoints, nvars, info, s2, v)
		Return
	End Sub

End Class
Public Partial Class alglib
	Public Class bdss
		Public Class cvreport
			Inherits apobject
			Public relclserror As Double
			Public avgce As Double
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New cvreport()
				_result.relclserror = relclserror
				_result.avgce = avgce
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				Return _result
			End Function
		End Class




		'************************************************************************
'        This set of routines (DSErrAllocate, DSErrAccumulate, DSErrFinish)
'        calculates different error functions (classification error, cross-entropy,
'        rms, avg, avg.rel errors).
'
'        1. DSErrAllocate prepares buffer.
'        2. DSErrAccumulate accumulates individual errors:
'            * Y contains predicted output (posterior probabilities for classification)
'            * DesiredY contains desired output (class number for classification)
'        3. DSErrFinish outputs results:
'           * Buf[0] contains relative classification error (zero for regression tasks)
'           * Buf[1] contains avg. cross-entropy (zero for regression tasks)
'           * Buf[2] contains rms error (regression, classification)
'           * Buf[3] contains average error (regression, classification)
'           * Buf[4] contains average relative error (regression, classification)
'           
'        NOTES(1):
'            "NClasses>0" means that we have classification task.
'            "NClasses<0" means regression task with -NClasses real outputs.
'
'        NOTES(2):
'            rms. avg, avg.rel errors for classification tasks are interpreted as
'            errors in posterior probabilities with respect to probabilities given
'            by training/test set.
'
'          -- ALGLIB --
'             Copyright 11.01.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dserrallocate(nclasses As Integer, ByRef buf As Double())
			buf = New Double(-1) {}

			buf = New Double(7) {}
			buf(0) = 0
			buf(1) = 0
			buf(2) = 0
			buf(3) = 0
			buf(4) = 0
			buf(5) = nclasses
			buf(6) = 0
			buf(7) = 0
		End Sub


		'************************************************************************
'        See DSErrAllocate for comments on this routine.
'
'          -- ALGLIB --
'             Copyright 11.01.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dserraccumulate(ByRef buf As Double(), y As Double(), desiredy As Double())
			Dim nclasses As Integer = 0
			Dim nout As Integer = 0
			Dim offs As Integer = 0
			Dim mmax As Integer = 0
			Dim rmax As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim ev As Double = 0

			offs = 5
			nclasses = CInt(System.Math.Truncate(System.Math.Round(buf(offs))))
			If nclasses > 0 Then

				'
				' Classification
				'
				rmax = CInt(System.Math.Truncate(System.Math.Round(desiredy(0))))
				mmax = 0
				For j = 1 To nclasses - 1
					If CDbl(y(j)) > CDbl(y(mmax)) Then
						mmax = j
					End If
				Next
				If mmax <> rmax Then
					buf(0) = buf(0) + 1
				End If
				If CDbl(y(rmax)) > CDbl(0) Then
					buf(1) = buf(1) - System.Math.Log(y(rmax))
				Else
					buf(1) = buf(1) + System.Math.Log(Math.maxrealnumber)
				End If
				For j = 0 To nclasses - 1
					v = y(j)
					If j = rmax Then
						ev = 1
					Else
						ev = 0
					End If
					buf(2) = buf(2) + Math.sqr(v - ev)
					buf(3) = buf(3) + System.Math.Abs(v - ev)
					If CDbl(ev) <> CDbl(0) Then
						buf(4) = buf(4) + System.Math.Abs((v - ev) / ev)
						buf(offs + 2) = buf(offs + 2) + 1
					End If
				Next
				buf(offs + 1) = buf(offs + 1) + 1
			Else

				'
				' Regression
				'
				nout = -nclasses
				rmax = 0
				For j = 1 To nout - 1
					If CDbl(desiredy(j)) > CDbl(desiredy(rmax)) Then
						rmax = j
					End If
				Next
				mmax = 0
				For j = 1 To nout - 1
					If CDbl(y(j)) > CDbl(y(mmax)) Then
						mmax = j
					End If
				Next
				If mmax <> rmax Then
					buf(0) = buf(0) + 1
				End If
				For j = 0 To nout - 1
					v = y(j)
					ev = desiredy(j)
					buf(2) = buf(2) + Math.sqr(v - ev)
					buf(3) = buf(3) + System.Math.Abs(v - ev)
					If CDbl(ev) <> CDbl(0) Then
						buf(4) = buf(4) + System.Math.Abs((v - ev) / ev)
						buf(offs + 2) = buf(offs + 2) + 1
					End If
				Next
				buf(offs + 1) = buf(offs + 1) + 1
			End If
		End Sub


		'************************************************************************
'        See DSErrAllocate for comments on this routine.
'
'          -- ALGLIB --
'             Copyright 11.01.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dserrfinish(ByRef buf As Double())
			Dim nout As Integer = 0
			Dim offs As Integer = 0

			offs = 5
			nout = System.Math.Abs(CInt(System.Math.Truncate(System.Math.Round(buf(offs)))))
			If CDbl(buf(offs + 1)) <> CDbl(0) Then
				buf(0) = buf(0) / buf(offs + 1)
				buf(1) = buf(1) / buf(offs + 1)
				buf(2) = System.Math.sqrt(buf(2) / (nout * buf(offs + 1)))
				buf(3) = buf(3) / (nout * buf(offs + 1))
			End If
			If CDbl(buf(offs + 2)) <> CDbl(0) Then
				buf(4) = buf(4) / buf(offs + 2)
			End If
		End Sub


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 19.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dsnormalize(ByRef xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef means As Double(), ByRef sigmas As Double())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim tmp As Double() = New Double(-1) {}
			Dim mean As Double = 0
			Dim variance As Double = 0
			Dim skewness As Double = 0
			Dim kurtosis As Double = 0
			Dim i_ As Integer = 0

			info = 0
			means = New Double(-1) {}
			sigmas = New Double(-1) {}


			'
			' Test parameters
			'
			If npoints <= 0 OrElse nvars < 1 Then
				info = -1
				Return
			End If
			info = 1

			'
			' Standartization
			'
			means = New Double(nvars - 1) {}
			sigmas = New Double(nvars - 1) {}
			tmp = New Double(npoints - 1) {}
			For j = 0 To nvars - 1
				For i_ = 0 To npoints - 1
					tmp(i_) = xy(i_, j)
				Next
				basestat.samplemoments(tmp, npoints, mean, variance, skewness, kurtosis)
				means(j) = mean
				sigmas(j) = System.Math.sqrt(variance)
				If CDbl(sigmas(j)) = CDbl(0) Then
					sigmas(j) = 1
				End If
				For i = 0 To npoints - 1
					xy(i, j) = (xy(i, j) - means(j)) / sigmas(j)
				Next
			Next
		End Sub


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 19.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dsnormalizec(xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef means As Double(), ByRef sigmas As Double())
			Dim j As Integer = 0
			Dim tmp As Double() = New Double(-1) {}
			Dim mean As Double = 0
			Dim variance As Double = 0
			Dim skewness As Double = 0
			Dim kurtosis As Double = 0
			Dim i_ As Integer = 0

			info = 0
			means = New Double(-1) {}
			sigmas = New Double(-1) {}


			'
			' Test parameters
			'
			If npoints <= 0 OrElse nvars < 1 Then
				info = -1
				Return
			End If
			info = 1

			'
			' Standartization
			'
			means = New Double(nvars - 1) {}
			sigmas = New Double(nvars - 1) {}
			tmp = New Double(npoints - 1) {}
			For j = 0 To nvars - 1
				For i_ = 0 To npoints - 1
					tmp(i_) = xy(i_, j)
				Next
				basestat.samplemoments(tmp, npoints, mean, variance, skewness, kurtosis)
				means(j) = mean
				sigmas(j) = System.Math.sqrt(variance)
				If CDbl(sigmas(j)) = CDbl(0) Then
					sigmas(j) = 1
				End If
			Next
		End Sub


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 19.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dsgetmeanmindistance(xy As Double(,), npoints As Integer, nvars As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim tmp As Double() = New Double(-1) {}
			Dim tmp2 As Double() = New Double(-1) {}
			Dim v As Double = 0
			Dim i_ As Integer = 0


			'
			' Test parameters
			'
			If npoints <= 0 OrElse nvars < 1 Then
				result = 0
				Return result
			End If

			'
			' Process
			'
			tmp = New Double(npoints - 1) {}
			For i = 0 To npoints - 1
				tmp(i) = Math.maxrealnumber
			Next
			tmp2 = New Double(nvars - 1) {}
			For i = 0 To npoints - 1
				For j = i + 1 To npoints - 1
					For i_ = 0 To nvars - 1
						tmp2(i_) = xy(i, i_)
					Next
					For i_ = 0 To nvars - 1
						tmp2(i_) = tmp2(i_) - xy(j, i_)
					Next
					v = 0.0
					For i_ = 0 To nvars - 1
						v += tmp2(i_) * tmp2(i_)
					Next
					v = System.Math.sqrt(v)
					tmp(i) = System.Math.Min(tmp(i), v)
					tmp(j) = System.Math.Min(tmp(j), v)
				Next
			Next
			result = 0
			For i = 0 To npoints - 1
				result = result + tmp(i) / npoints
			Next
			Return result
		End Function


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 19.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dstie(ByRef a As Double(), n As Integer, ByRef ties As Integer(), ByRef tiecount As Integer, ByRef p1 As Integer(), ByRef p2 As Integer())
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim tmp As Integer() = New Integer(-1) {}

			ties = New Integer(-1) {}
			tiecount = 0
			p1 = New Integer(-1) {}
			p2 = New Integer(-1) {}


			'
			' Special case
			'
			If n <= 0 Then
				tiecount = 0
				Return
			End If

			'
			' Sort A
			'
			tsort.tagsort(a, n, p1, p2)

			'
			' Process ties
			'
			tiecount = 1
			For i = 1 To n - 1
				If CDbl(a(i)) <> CDbl(a(i - 1)) Then
					tiecount = tiecount + 1
				End If
			Next
			ties = New Integer(tiecount) {}
			ties(0) = 0
			k = 1
			For i = 1 To n - 1
				If CDbl(a(i)) <> CDbl(a(i - 1)) Then
					ties(k) = i
					k = k + 1
				End If
			Next
			ties(tiecount) = n
		End Sub


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 11.12.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dstiefasti(ByRef a As Double(), ByRef b As Integer(), n As Integer, ByRef ties As Integer(), ByRef tiecount As Integer, ByRef bufr As Double(), _
			ByRef bufi As Integer())
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim tmp As Integer() = New Integer(-1) {}

			tiecount = 0


			'
			' Special case
			'
			If n <= 0 Then
				tiecount = 0
				Return
			End If

			'
			' Sort A
			'
			tsort.tagsortfasti(a, b, bufr, bufi, n)

			'
			' Process ties
			'
			ties(0) = 0
			k = 1
			For i = 1 To n - 1
				If CDbl(a(i)) <> CDbl(a(i - 1)) Then
					ties(k) = i
					k = k + 1
				End If
			Next
			ties(k) = n
			tiecount = k
		End Sub


		'************************************************************************
'        Optimal binary classification
'
'        Algorithms finds optimal (=with minimal cross-entropy) binary partition.
'        Internal subroutine.
'
'        INPUT PARAMETERS:
'            A       -   array[0..N-1], variable
'            C       -   array[0..N-1], class numbers (0 or 1).
'            N       -   array size
'
'        OUTPUT PARAMETERS:
'            Info    -   completetion code:
'                        * -3, all values of A[] are same (partition is impossible)
'                        * -2, one of C[] is incorrect (<0, >1)
'                        * -1, incorrect pararemets were passed (N<=0).
'                        *  1, OK
'            Threshold-  partiton boundary. Left part contains values which are
'                        strictly less than Threshold. Right part contains values
'                        which are greater than or equal to Threshold.
'            PAL, PBL-   probabilities P(0|v<Threshold) and P(1|v<Threshold)
'            PAR, PBR-   probabilities P(0|v>=Threshold) and P(1|v>=Threshold)
'            CVE     -   cross-validation estimate of cross-entropy
'
'          -- ALGLIB --
'             Copyright 22.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dsoptimalsplit2(a As Double(), c As Integer(), n As Integer, ByRef info As Integer, ByRef threshold As Double, ByRef pal As Double, _
			ByRef pbl As Double, ByRef par As Double, ByRef pbr As Double, ByRef cve As Double)
			Dim i As Integer = 0
			Dim t As Integer = 0
			Dim s As Double = 0
			Dim ties As Integer() = New Integer(-1) {}
			Dim tiecount As Integer = 0
			Dim p1 As Integer() = New Integer(-1) {}
			Dim p2 As Integer() = New Integer(-1) {}
			Dim k As Integer = 0
			Dim koptimal As Integer = 0
			Dim pak As Double = 0
			Dim pbk As Double = 0
			Dim cvoptimal As Double = 0
			Dim cv As Double = 0

			a = DirectCast(a.Clone(), Double())
			c = DirectCast(c.Clone(), Integer())
			info = 0
			threshold = 0
			pal = 0
			pbl = 0
			par = 0
			pbr = 0
			cve = 0


			'
			' Test for errors in inputs
			'
			If n <= 0 Then
				info = -1
				Return
			End If
			For i = 0 To n - 1
				If c(i) <> 0 AndAlso c(i) <> 1 Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Tie
			'
			dstie(a, n, ties, tiecount, p1, p2)
			For i = 0 To n - 1
				If p2(i) <> i Then
					t = c(i)
					c(i) = c(p2(i))
					c(p2(i)) = t
				End If
			Next

			'
			' Special case: number of ties is 1.
			'
			' NOTE: we assume that P[i,j] equals to 0 or 1,
			'       intermediate values are not allowed.
			'
			If tiecount = 1 Then
				info = -3
				Return
			End If

			'
			' General case, number of ties > 1
			'
			' NOTE: we assume that P[i,j] equals to 0 or 1,
			'       intermediate values are not allowed.
			'
			pal = 0
			pbl = 0
			par = 0
			pbr = 0
			For i = 0 To n - 1
				If c(i) = 0 Then
					par = par + 1
				End If
				If c(i) = 1 Then
					pbr = pbr + 1
				End If
			Next
			koptimal = -1
			cvoptimal = Math.maxrealnumber
			For k = 0 To tiecount - 2

				'
				' first, obtain information about K-th tie which is
				' moved from R-part to L-part
				'
				pak = 0
				pbk = 0
				For i = ties(k) To ties(k + 1) - 1
					If c(i) = 0 Then
						pak = pak + 1
					End If
					If c(i) = 1 Then
						pbk = pbk + 1
					End If
				Next

				'
				' Calculate cross-validation CE
				'
				cv = 0
				cv = cv - xlny(pal + pak, (pal + pak) / (pal + pak + pbl + pbk + 1))
				cv = cv - xlny(pbl + pbk, (pbl + pbk) / (pal + pak + 1 + pbl + pbk))
				cv = cv - xlny(par - pak, (par - pak) / (par - pak + pbr - pbk + 1))
				cv = cv - xlny(pbr - pbk, (pbr - pbk) / (par - pak + 1 + pbr - pbk))

				'
				' Compare with best
				'
				If CDbl(cv) < CDbl(cvoptimal) Then
					cvoptimal = cv
					koptimal = k
				End If

				'
				' update
				'
				pal = pal + pak
				pbl = pbl + pbk
				par = par - pak
				pbr = pbr - pbk
			Next
			cve = cvoptimal
			threshold = 0.5 * (a(ties(koptimal)) + a(ties(koptimal + 1)))
			pal = 0
			pbl = 0
			par = 0
			pbr = 0
			For i = 0 To n - 1
				If CDbl(a(i)) < CDbl(threshold) Then
					If c(i) = 0 Then
						pal = pal + 1
					Else
						pbl = pbl + 1
					End If
				Else
					If c(i) = 0 Then
						par = par + 1
					Else
						pbr = pbr + 1
					End If
				End If
			Next
			s = pal + pbl
			pal = pal / s
			pbl = pbl / s
			s = par + pbr
			par = par / s
			pbr = pbr / s
		End Sub


		'************************************************************************
'        Optimal partition, internal subroutine. Fast version.
'
'        Accepts:
'            A       array[0..N-1]       array of attributes     array[0..N-1]
'            C       array[0..N-1]       array of class labels
'            TiesBuf array[0..N]         temporaries (ties)
'            CntBuf  array[0..2*NC-1]    temporaries (counts)
'            Alpha                       centering factor (0<=alpha<=1, recommended value - 0.05)
'            BufR    array[0..N-1]       temporaries
'            BufI    array[0..N-1]       temporaries
'
'        Output:
'            Info    error code (">0"=OK, "<0"=bad)
'            RMS     training set RMS error
'            CVRMS   leave-one-out RMS error
'            
'        Note:
'            content of all arrays is changed by subroutine;
'            it doesn't allocate temporaries.
'
'          -- ALGLIB --
'             Copyright 11.12.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dsoptimalsplit2fast(ByRef a As Double(), ByRef c As Integer(), ByRef tiesbuf As Integer(), ByRef cntbuf As Integer(), ByRef bufr As Double(), ByRef bufi As Integer(), _
			n As Integer, nc As Integer, alpha As Double, ByRef info As Integer, ByRef threshold As Double, ByRef rms As Double, _
			ByRef cvrms As Double)
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim cl As Integer = 0
			Dim tiecount As Integer = 0
			Dim cbest As Double = 0
			Dim cc As Double = 0
			Dim koptimal As Integer = 0
			Dim sl As Integer = 0
			Dim sr As Integer = 0
			Dim v As Double = 0
			Dim w As Double = 0
			Dim x As Double = 0

			info = 0
			threshold = 0
			rms = 0
			cvrms = 0


			'
			' Test for errors in inputs
			'
			If n <= 0 OrElse nc < 2 Then
				info = -1
				Return
			End If
			For i = 0 To n - 1
				If c(i) < 0 OrElse c(i) >= nc Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Tie
			'
			dstiefasti(a, c, n, tiesbuf, tiecount, bufr, _
				bufi)

			'
			' Special case: number of ties is 1.
			'
			If tiecount = 1 Then
				info = -3
				Return
			End If

			'
			' General case, number of ties > 1
			'
			For i = 0 To 2 * nc - 1
				cntbuf(i) = 0
			Next
			For i = 0 To n - 1
				cntbuf(nc + c(i)) = cntbuf(nc + c(i)) + 1
			Next
			koptimal = -1
			threshold = a(n - 1)
			cbest = Math.maxrealnumber
			sl = 0
			sr = n
			For k = 0 To tiecount - 2

				'
				' first, move Kth tie from right to left
				'
				For i = tiesbuf(k) To tiesbuf(k + 1) - 1
					cl = c(i)
					cntbuf(cl) = cntbuf(cl) + 1
					cntbuf(nc + cl) = cntbuf(nc + cl) - 1
				Next
				sl = sl + (tiesbuf(k + 1) - tiesbuf(k))
				sr = sr - (tiesbuf(k + 1) - tiesbuf(k))

				'
				' Calculate RMS error
				'
				v = 0
				For i = 0 To nc - 1
					w = cntbuf(i)
					v = v + w * Math.sqr(w / sl - 1)
					v = v + (sl - w) * Math.sqr(w / sl)
					w = cntbuf(nc + i)
					v = v + w * Math.sqr(w / sr - 1)
					v = v + (sr - w) * Math.sqr(w / sr)
				Next
				v = System.Math.sqrt(v / (nc * n))

				'
				' Compare with best
				'
				x = CDbl(2 * sl) / CDbl(sl + sr) - 1
				cc = v * (1 - alpha + alpha * Math.sqr(x))
				If CDbl(cc) < CDbl(cbest) Then

					'
					' store split
					'
					rms = v
					koptimal = k
					cbest = cc

					'
					' calculate CVRMS error
					'
					cvrms = 0
					For i = 0 To nc - 1
						If sl > 1 Then
							w = cntbuf(i)
							cvrms = cvrms + w * Math.sqr((w - 1) / (sl - 1) - 1)
							cvrms = cvrms + (sl - w) * Math.sqr(w / (sl - 1))
						Else
							w = cntbuf(i)
							cvrms = cvrms + w * Math.sqr(CDbl(1) / CDbl(nc) - 1)
							cvrms = cvrms + (sl - w) * Math.sqr(CDbl(1) / CDbl(nc))
						End If
						If sr > 1 Then
							w = cntbuf(nc + i)
							cvrms = cvrms + w * Math.sqr((w - 1) / (sr - 1) - 1)
							cvrms = cvrms + (sr - w) * Math.sqr(w / (sr - 1))
						Else
							w = cntbuf(nc + i)
							cvrms = cvrms + w * Math.sqr(CDbl(1) / CDbl(nc) - 1)
							cvrms = cvrms + (sr - w) * Math.sqr(CDbl(1) / CDbl(nc))
						End If
					Next
					cvrms = System.Math.sqrt(cvrms / (nc * n))
				End If
			Next

			'
			' Calculate threshold.
			' Code is a bit complicated because there can be such
			' numbers that 0.5(A+B) equals to A or B (if A-B=epsilon)
			'
			threshold = 0.5 * (a(tiesbuf(koptimal)) + a(tiesbuf(koptimal + 1)))
			If CDbl(threshold) <= CDbl(a(tiesbuf(koptimal))) Then
				threshold = a(tiesbuf(koptimal + 1))
			End If
		End Sub


		'************************************************************************
'        Automatic non-optimal discretization, internal subroutine.
'
'          -- ALGLIB --
'             Copyright 22.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dssplitk(a As Double(), c As Integer(), n As Integer, nc As Integer, kmax As Integer, ByRef info As Integer, _
			ByRef thresholds As Double(), ByRef ni As Integer, ByRef cve As Double)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim j1 As Integer = 0
			Dim k As Integer = 0
			Dim ties As Integer() = New Integer(-1) {}
			Dim tiecount As Integer = 0
			Dim p1 As Integer() = New Integer(-1) {}
			Dim p2 As Integer() = New Integer(-1) {}
			Dim cnt As Integer() = New Integer(-1) {}
			Dim v2 As Double = 0
			Dim bestk As Integer = 0
			Dim bestcve As Double = 0
			Dim bestsizes As Integer() = New Integer(-1) {}
			Dim curcve As Double = 0
			Dim cursizes As Integer() = New Integer(-1) {}

			a = DirectCast(a.Clone(), Double())
			c = DirectCast(c.Clone(), Integer())
			info = 0
			thresholds = New Double(-1) {}
			ni = 0
			cve = 0


			'
			' Test for errors in inputs
			'
			If (n <= 0 OrElse nc < 2) OrElse kmax < 2 Then
				info = -1
				Return
			End If
			For i = 0 To n - 1
				If c(i) < 0 OrElse c(i) >= nc Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Tie
			'
			dstie(a, n, ties, tiecount, p1, p2)
			For i = 0 To n - 1
				If p2(i) <> i Then
					k = c(i)
					c(i) = c(p2(i))
					c(p2(i)) = k
				End If
			Next

			'
			' Special cases
			'
			If tiecount = 1 Then
				info = -3
				Return
			End If

			'
			' General case:
			' 0. allocate arrays
			'
			kmax = System.Math.Min(kmax, tiecount)
			bestsizes = New Integer(kmax - 1) {}
			cursizes = New Integer(kmax - 1) {}
			cnt = New Integer(nc - 1) {}

			'
			' General case:
			' 1. prepare "weak" solution (two subintervals, divided at median)
			'
			v2 = Math.maxrealnumber
			j = -1
			For i = 1 To tiecount - 1
				If CDbl(System.Math.Abs(ties(i) - 0.5 * (n - 1))) < CDbl(v2) Then
					v2 = System.Math.Abs(ties(i) - 0.5 * n)
					j = i
				End If
			Next
			alglib.ap.assert(j > 0, "DSSplitK: internal error #1!")
			bestk = 2
			bestsizes(0) = ties(j)
			bestsizes(1) = n - j
			bestcve = 0
			For i = 0 To nc - 1
				cnt(i) = 0
			Next
			For i = 0 To j - 1
				tieaddc(c, ties, i, nc, cnt)
			Next
			bestcve = bestcve + getcv(cnt, nc)
			For i = 0 To nc - 1
				cnt(i) = 0
			Next
			For i = j To tiecount - 1
				tieaddc(c, ties, i, nc, cnt)
			Next
			bestcve = bestcve + getcv(cnt, nc)

			'
			' General case:
			' 2. Use greedy algorithm to find sub-optimal split in O(KMax*N) time
			'
			For k = 2 To kmax

				'
				' Prepare greedy K-interval split
				'
				For i = 0 To k - 1
					cursizes(i) = 0
				Next
				i = 0
				j = 0
				While j <= tiecount - 1 AndAlso i <= k - 1

					'
					' Rule: I-th bin is empty, fill it
					'
					If cursizes(i) = 0 Then
						cursizes(i) = ties(j + 1) - ties(j)
						j = j + 1
						Continue While
					End If

					'
					' Rule: (K-1-I) bins left, (K-1-I) ties left (1 tie per bin); next bin
					'
					If tiecount - j = k - 1 - i Then
						i = i + 1
						Continue While
					End If

					'
					' Rule: last bin, always place in current
					'
					If i = k - 1 Then
						cursizes(i) = cursizes(i) + ties(j + 1) - ties(j)
						j = j + 1
						Continue While
					End If

					'
					' Place J-th tie in I-th bin, or leave for I+1-th bin.
					'
					If CDbl(System.Math.Abs(cursizes(i) + ties(j + 1) - ties(j) - CDbl(n) / CDbl(k))) < CDbl(System.Math.Abs(cursizes(i) - CDbl(n) / CDbl(k))) Then
						cursizes(i) = cursizes(i) + ties(j + 1) - ties(j)
						j = j + 1
					Else
						i = i + 1
					End If
				End While
				alglib.ap.assert(cursizes(k - 1) <> 0 AndAlso j = tiecount, "DSSplitK: internal error #1")

				'
				' Calculate CVE
				'
				curcve = 0
				j = 0
				For i = 0 To k - 1
					For j1 = 0 To nc - 1
						cnt(j1) = 0
					Next
					For j1 = j To j + cursizes(i) - 1
						cnt(c(j1)) = cnt(c(j1)) + 1
					Next
					curcve = curcve + getcv(cnt, nc)
					j = j + cursizes(i)
				Next

				'
				' Choose best variant
				'
				If CDbl(curcve) < CDbl(bestcve) Then
					For i = 0 To k - 1
						bestsizes(i) = cursizes(i)
					Next
					bestcve = curcve
					bestk = k
				End If
			Next

			'
			' Transform from sizes to thresholds
			'
			cve = bestcve
			ni = bestk
			thresholds = New Double(ni - 2) {}
			j = bestsizes(0)
			For i = 1 To bestk - 1
				thresholds(i - 1) = 0.5 * (a(j - 1) + a(j))
				j = j + bestsizes(i)
			Next
		End Sub


		'************************************************************************
'        Automatic optimal discretization, internal subroutine.
'
'          -- ALGLIB --
'             Copyright 22.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dsoptimalsplitk(a As Double(), c As Integer(), n As Integer, nc As Integer, kmax As Integer, ByRef info As Integer, _
			ByRef thresholds As Double(), ByRef ni As Integer, ByRef cve As Double)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim s As Integer = 0
			Dim jl As Integer = 0
			Dim jr As Integer = 0
			Dim v2 As Double = 0
			Dim ties As Integer() = New Integer(-1) {}
			Dim tiecount As Integer = 0
			Dim p1 As Integer() = New Integer(-1) {}
			Dim p2 As Integer() = New Integer(-1) {}
			Dim cvtemp As Double = 0
			Dim cnt As Integer() = New Integer(-1) {}
			Dim cnt2 As Integer() = New Integer(-1) {}
			Dim cv As Double(,) = New Double(-1, -1) {}
			Dim splits As Integer(,) = New Integer(-1, -1) {}
			Dim k As Integer = 0
			Dim koptimal As Integer = 0
			Dim cvoptimal As Double = 0

			a = DirectCast(a.Clone(), Double())
			c = DirectCast(c.Clone(), Integer())
			info = 0
			thresholds = New Double(-1) {}
			ni = 0
			cve = 0


			'
			' Test for errors in inputs
			'
			If (n <= 0 OrElse nc < 2) OrElse kmax < 2 Then
				info = -1
				Return
			End If
			For i = 0 To n - 1
				If c(i) < 0 OrElse c(i) >= nc Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Tie
			'
			dstie(a, n, ties, tiecount, p1, p2)
			For i = 0 To n - 1
				If p2(i) <> i Then
					k = c(i)
					c(i) = c(p2(i))
					c(p2(i)) = k
				End If
			Next

			'
			' Special cases
			'
			If tiecount = 1 Then
				info = -3
				Return
			End If

			'
			' General case
			' Use dynamic programming to find best split in O(KMax*NC*TieCount^2) time
			'
			kmax = System.Math.Min(kmax, tiecount)
			cv = New Double(kmax - 1, tiecount - 1) {}
			splits = New Integer(kmax - 1, tiecount - 1) {}
			cnt = New Integer(nc - 1) {}
			cnt2 = New Integer(nc - 1) {}
			For j = 0 To nc - 1
				cnt(j) = 0
			Next
			For j = 0 To tiecount - 1
				tieaddc(c, ties, j, nc, cnt)
				splits(0, j) = 0
				cv(0, j) = getcv(cnt, nc)
			Next
			For k = 1 To kmax - 1
				For j = 0 To nc - 1
					cnt(j) = 0
				Next

				'
				' Subtask size J in [K..TieCount-1]:
				' optimal K-splitting on ties from 0-th to J-th.
				'
				For j = k To tiecount - 1

					'
					' Update Cnt - let it contain classes of ties from K-th to J-th
					'
					tieaddc(c, ties, j, nc, cnt)

					'
					' Search for optimal split point S in [K..J]
					'
					For i = 0 To nc - 1
						cnt2(i) = cnt(i)
					Next
					cv(k, j) = cv(k - 1, j - 1) + getcv(cnt2, nc)
					splits(k, j) = j
					For s = k + 1 To j

						'
						' Update Cnt2 - let it contain classes of ties from S-th to J-th
						'
						tiesubc(c, ties, s - 1, nc, cnt2)

						'
						' Calculate CVE
						'
						cvtemp = cv(k - 1, s - 1) + getcv(cnt2, nc)
						If CDbl(cvtemp) < CDbl(cv(k, j)) Then
							cv(k, j) = cvtemp
							splits(k, j) = s
						End If
					Next
				Next
			Next

			'
			' Choose best partition, output result
			'
			koptimal = -1
			cvoptimal = Math.maxrealnumber
			For k = 0 To kmax - 1
				If CDbl(cv(k, tiecount - 1)) < CDbl(cvoptimal) Then
					cvoptimal = cv(k, tiecount - 1)
					koptimal = k
				End If
			Next
			alglib.ap.assert(koptimal >= 0, "DSOptimalSplitK: internal error #1!")
			If koptimal = 0 Then

				'
				' Special case: best partition is one big interval.
				' Even 2-partition is not better.
				' This is possible when dealing with "weak" predictor variables.
				'
				' Make binary split as close to the median as possible.
				'
				v2 = Math.maxrealnumber
				j = -1
				For i = 1 To tiecount - 1
					If CDbl(System.Math.Abs(ties(i) - 0.5 * (n - 1))) < CDbl(v2) Then
						v2 = System.Math.Abs(ties(i) - 0.5 * (n - 1))
						j = i
					End If
				Next
				alglib.ap.assert(j > 0, "DSOptimalSplitK: internal error #2!")
				thresholds = New Double(0) {}
				thresholds(0) = 0.5 * (a(ties(j - 1)) + a(ties(j)))
				ni = 2
				cve = 0
				For i = 0 To nc - 1
					cnt(i) = 0
				Next
				For i = 0 To j - 1
					tieaddc(c, ties, i, nc, cnt)
				Next
				cve = cve + getcv(cnt, nc)
				For i = 0 To nc - 1
					cnt(i) = 0
				Next
				For i = j To tiecount - 1
					tieaddc(c, ties, i, nc, cnt)
				Next
				cve = cve + getcv(cnt, nc)
			Else

				'
				' General case: 2 or more intervals
				'
				' NOTE: we initialize both JL and JR (left and right bounds),
				'       altough algorithm needs only JL.
				'
				thresholds = New Double(koptimal - 1) {}
				ni = koptimal + 1
				cve = cv(koptimal, tiecount - 1)
				jl = splits(koptimal, tiecount - 1)
				jr = tiecount - 1
				For k = koptimal To 1 Step -1
					thresholds(k - 1) = 0.5 * (a(ties(jl - 1)) + a(ties(jl)))
					jr = jl - 1
					jl = splits(k - 1, jl - 1)
				Next
				apserv.touchint(jr)
			End If
		End Sub


		'************************************************************************
'        Internal function
'        ************************************************************************

		Private Shared Function xlny(x As Double, y As Double) As Double
			Dim result As Double = 0

			If CDbl(x) = CDbl(0) Then
				result = 0
			Else
				result = x * System.Math.Log(y)
			End If
			Return result
		End Function


		'************************************************************************
'        Internal function,
'        returns number of samples of class I in Cnt[I]
'        ************************************************************************

		Private Shared Function getcv(cnt As Integer(), nc As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim s As Double = 0

			s = 0
			For i = 0 To nc - 1
				s = s + cnt(i)
			Next
			result = 0
			For i = 0 To nc - 1
				result = result - xlny(cnt(i), cnt(i) / (s + nc - 1))
			Next
			Return result
		End Function


		'************************************************************************
'        Internal function, adds number of samples of class I in tie NTie to Cnt[I]
'        ************************************************************************

		Private Shared Sub tieaddc(c As Integer(), ties As Integer(), ntie As Integer, nc As Integer, ByRef cnt As Integer())
			Dim i As Integer = 0

			For i = ties(ntie) To ties(ntie + 1) - 1
				cnt(c(i)) = cnt(c(i)) + 1
			Next
		End Sub


		'************************************************************************
'        Internal function, subtracts number of samples of class I in tie NTie to Cnt[I]
'        ************************************************************************

		Private Shared Sub tiesubc(c As Integer(), ties As Integer(), ntie As Integer, nc As Integer, ByRef cnt As Integer())
			Dim i As Integer = 0

			For i = ties(ntie) To ties(ntie + 1) - 1
				cnt(c(i)) = cnt(c(i)) - 1
			Next
		End Sub


	End Class
	Public Class clustering
		'************************************************************************
'        This structure is a clusterization engine.
'
'        You should not try to access its fields directly.
'        Use ALGLIB functions in order to work with this object.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Class clusterizerstate
			Inherits apobject
			Public npoints As Integer
			Public nfeatures As Integer
			Public disttype As Integer
			Public xy As Double(,)
			Public d As Double(,)
			Public ahcalgo As Integer
			Public kmeansrestarts As Integer
			Public kmeansmaxits As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				xy = New Double(-1, -1) {}
				d = New Double(-1, -1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New clusterizerstate()
				_result.npoints = npoints
				_result.nfeatures = nfeatures
				_result.disttype = disttype
				_result.xy = DirectCast(xy.Clone(), Double(,))
				_result.d = DirectCast(d.Clone(), Double(,))
				_result.ahcalgo = ahcalgo
				_result.kmeansrestarts = kmeansrestarts
				_result.kmeansmaxits = kmeansmaxits
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure  is used to store results of the agglomerative hierarchical
'        clustering (AHC).
'
'        Following information is returned:
'
'        * NPoints contains number of points in the original dataset
'
'        * Z contains information about merges performed  (see below).  Z  contains
'          indexes from the original (unsorted) dataset and it can be used when you
'          need to know what points were merged. However, it is not convenient when
'          you want to build a dendrograd (see below).
'          
'        * if  you  want  to  build  dendrogram, you  can use Z, but it is not good
'          option, because Z contains  indexes from  unsorted  dataset.  Dendrogram
'          built from such dataset is likely to have intersections. So, you have to
'          reorder you points before building dendrogram.
'          Permutation which reorders point is returned in P. Another representation
'          of  merges,  which  is  more  convenient for dendorgram construction, is
'          returned in PM.
'          
'        * more information on format of Z, P and PM can be found below and in the
'          examples from ALGLIB Reference Manual.
'
'        FORMAL DESCRIPTION OF FIELDS:
'            NPoints         number of points
'            Z               array[NPoints-1,2],  contains   indexes   of  clusters
'                            linked in pairs to  form  clustering  tree.  I-th  row
'                            corresponds to I-th merge:
'                            * Z[I,0] - index of the first cluster to merge
'                            * Z[I,1] - index of the second cluster to merge
'                            * Z[I,0]<Z[I,1]
'                            * clusters are  numbered  from 0 to 2*NPoints-2,  with
'                              indexes from 0 to NPoints-1 corresponding to  points
'                              of the original dataset, and indexes from NPoints to
'                              2*NPoints-2  correspond  to  clusters  generated  by
'                              subsequent  merges  (I-th  row  of Z creates cluster
'                              with index NPoints+I).
'                            
'                            IMPORTANT: indexes in Z[] are indexes in the ORIGINAL,
'                            unsorted dataset. In addition to  Z algorithm  outputs
'                            permutation which rearranges points in such  way  that
'                            subsequent merges are  performed  on  adjacent  points
'                            (such order is needed if you want to build dendrogram).
'                            However,  indexes  in  Z  are  related  to   original,
'                            unrearranged sequence of points.
'                            
'            P               array[NPoints], permutation which reorders points  for
'                            dendrogram  construction.  P[i] contains  index of the
'                            position  where  we  should  move  I-th  point  of the
'                            original dataset in order to apply merges PZ/PM.
'
'            PZ              same as Z, but for permutation of points given  by  P.
'                            The  only  thing  which  changed  are  indexes  of the
'                            original points; indexes of clusters remained same.
'                            
'            MergeDist       array[NPoints-1], contains distances between  clusters
'                            being merged (MergeDist[i] correspond to merge  stored
'                            in Z[i,...]).
'                            
'            PM              array[NPoints-1,6], another representation of  merges,
'                            which is suited for dendrogram construction. It  deals
'                            with rearranged points (permutation P is applied)  and
'                            represents merges in a form which different  from  one
'                            used by Z.
'                            For each I from 0 to NPoints-2, I-th row of PM represents
'                            merge performed on two clusters C0 and C1. Here:
'                            * C0 contains points with indexes PM[I,0]...PM[I,1]
'                            * C1 contains points with indexes PM[I,2]...PM[I,3]
'                            * indexes stored in PM are given for dataset sorted
'                              according to permutation P
'                            * PM[I,1]=PM[I,2]-1 (only adjacent clusters are merged)
'                            * PM[I,0]<=PM[I,1], PM[I,2]<=PM[I,3], i.e. both
'                              clusters contain at least one point
'                            * heights of "subdendrograms" corresponding  to  C0/C1
'                              are stored in PM[I,4]  and  PM[I,5].  Subdendrograms
'                              corresponding   to   single-point   clusters    have
'                              height=0. Dendrogram of the merge result has  height
'                              H=max(H0,H1)+1.
'
'        NOTE: there is one-to-one correspondence between merges described by Z and
'              PM. I-th row of Z describes same merge of clusters as I-th row of PM,
'              with "left" cluster from Z corresponding to the "left" one from PM.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Class ahcreport
			Inherits apobject
			Public npoints As Integer
			Public p As Integer()
			Public z As Integer(,)
			Public pz As Integer(,)
			Public pm As Integer(,)
			Public mergedist As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				p = New Integer(-1) {}
				z = New Integer(-1, -1) {}
				pz = New Integer(-1, -1) {}
				pm = New Integer(-1, -1) {}
				mergedist = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New ahcreport()
				_result.npoints = npoints
				_result.p = DirectCast(p.Clone(), Integer())
				_result.z = DirectCast(z.Clone(), Integer(,))
				_result.pz = DirectCast(pz.Clone(), Integer(,))
				_result.pm = DirectCast(pm.Clone(), Integer(,))
				_result.mergedist = DirectCast(mergedist.Clone(), Double())
				Return _result
			End Function
		End Class


		'************************************************************************
'        This  structure   is  used  to  store  results of the k-means++ clustering
'        algorithm.
'
'        Following information is always returned:
'        * NPoints contains number of points in the original dataset
'        * TerminationType contains completion code, negative on failure, positive
'          on success
'        * K contains number of clusters
'
'        For positive TerminationType we return:
'        * NFeatures contains number of variables in the original dataset
'        * C, which contains centers found by algorithm
'        * CIdx, which maps points of the original dataset to clusters
'
'        FORMAL DESCRIPTION OF FIELDS:
'            NPoints         number of points, >=0
'            NFeatures       number of variables, >=1
'            TerminationType completion code:
'                            * -5 if  distance  type  is  anything  different  from
'                                 Euclidean metric
'                            * -3 for degenerate dataset: a) less  than  K  distinct
'                                 points, b) K=0 for non-empty dataset.
'                            * +1 for successful completion
'            K               number of clusters
'            C               array[K,NFeatures], rows of the array store centers
'            CIdx            array[NPoints], which contains cluster indexes
'            
'          -- ALGLIB --
'             Copyright 27.11.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Class kmeansreport
			Inherits apobject
			Public npoints As Integer
			Public nfeatures As Integer
			Public terminationtype As Integer
			Public k As Integer
			Public c As Double(,)
			Public cidx As Integer()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				c = New Double(-1, -1) {}
				cidx = New Integer(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New kmeansreport()
				_result.npoints = npoints
				_result.nfeatures = nfeatures
				_result.terminationtype = terminationtype
				_result.k = k
				_result.c = DirectCast(c.Clone(), Double(,))
				_result.cidx = DirectCast(cidx.Clone(), Integer())
				Return _result
			End Function
		End Class




		Public Const parallelcomplexity As Integer = 200000


		'************************************************************************
'        This function initializes clusterizer object. Newly initialized object  is
'        empty, i.e. it does not contain dataset. You should use it as follows:
'        1. creation
'        2. dataset is added with ClusterizerSetPoints()
'        3. additional parameters are set
'        3. clusterization is performed with one of the clustering functions
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizercreate(s As clusterizerstate)
			s.npoints = 0
			s.nfeatures = 0
			s.disttype = 2
			s.ahcalgo = 0
			s.kmeansrestarts = 1
			s.kmeansmaxits = 0
		End Sub


		'************************************************************************
'        This function adds dataset to the clusterizer structure.
'
'        This function overrides all previous calls  of  ClusterizerSetPoints()  or
'        ClusterizerSetDistances().
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            XY      -   array[NPoints,NFeatures], dataset
'            NPoints -   number of points, >=0
'            NFeatures-  number of features, >=1
'            DistType-   distance function:
'                        *  0    Chebyshev distance  (L-inf norm)
'                        *  1    city block distance (L1 norm)
'                        *  2    Euclidean distance  (L2 norm)
'                        * 10    Pearson correlation:
'                                dist(a,b) = 1-corr(a,b)
'                        * 11    Absolute Pearson correlation:
'                                dist(a,b) = 1-|corr(a,b)|
'                        * 12    Uncentered Pearson correlation (cosine of the angle):
'                                dist(a,b) = a'*b/(|a|*|b|)
'                        * 13    Absolute uncentered Pearson correlation
'                                dist(a,b) = |a'*b|/(|a|*|b|)
'                        * 20    Spearman rank correlation:
'                                dist(a,b) = 1-rankcorr(a,b)
'                        * 21    Absolute Spearman rank correlation
'                                dist(a,b) = 1-|rankcorr(a,b)|
'
'        NOTE 1: different distance functions have different performance penalty:
'                * Euclidean or Pearson correlation distances are the fastest ones
'                * Spearman correlation distance function is a bit slower
'                * city block and Chebyshev distances are order of magnitude slower
'               
'                The reason behing difference in performance is that correlation-based
'                distance functions are computed using optimized linear algebra kernels,
'                while Chebyshev and city block distance functions are computed using
'                simple nested loops with two branches at each iteration.
'                
'        NOTE 2: different clustering algorithms have different limitations:
'                * agglomerative hierarchical clustering algorithms may be used with
'                  any kind of distance metric
'                * k-means++ clustering algorithm may be used only  with  Euclidean
'                  distance function
'                Thus, list of specific clustering algorithms you may  use  depends
'                on distance function you specify when you set your dataset.
'               
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizersetpoints(s As clusterizerstate, xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer)
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert((((((((disttype = 0 OrElse disttype = 1) OrElse disttype = 2) OrElse disttype = 10) OrElse disttype = 11) OrElse disttype = 12) OrElse disttype = 13) OrElse disttype = 20) OrElse disttype = 21, "ClusterizerSetPoints: incorrect DistType")
			alglib.ap.assert(npoints >= 0, "ClusterizerSetPoints: NPoints<0")
			alglib.ap.assert(nfeatures >= 1, "ClusterizerSetPoints: NFeatures<1")
			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "ClusterizerSetPoints: Rows(XY)<NPoints")
			alglib.ap.assert(alglib.ap.cols(xy) >= nfeatures, "ClusterizerSetPoints: Cols(XY)<NFeatures")
			alglib.ap.assert(apserv.apservisfinitematrix(xy, npoints, nfeatures), "ClusterizerSetPoints: XY contains NAN/INF")
			s.npoints = npoints
			s.nfeatures = nfeatures
			s.disttype = disttype
			apserv.rmatrixsetlengthatleast(s.xy, npoints, nfeatures)
			For i = 0 To npoints - 1
				For i_ = 0 To nfeatures - 1
					s.xy(i, i_) = xy(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        This function adds dataset given by distance  matrix  to  the  clusterizer
'        structure. It is important that dataset is not  given  explicitly  -  only
'        distance matrix is given.
'
'        This function overrides all previous calls  of  ClusterizerSetPoints()  or
'        ClusterizerSetDistances().
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            D       -   array[NPoints,NPoints], distance matrix given by its upper
'                        or lower triangle (main diagonal is  ignored  because  its
'                        entries are expected to be zero).
'            NPoints -   number of points
'            IsUpper -   whether upper or lower triangle of D is given.
'                
'        NOTE 1: different clustering algorithms have different limitations:
'                * agglomerative hierarchical clustering algorithms may be used with
'                  any kind of distance metric, including one  which  is  given  by
'                  distance matrix
'                * k-means++ clustering algorithm may be used only  with  Euclidean
'                  distance function and explicitly given points - it  can  not  be
'                  used with dataset given by distance matrix
'                Thus, if you call this function, you will be unable to use k-means
'                clustering algorithm to process your problem.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizersetdistances(s As clusterizerstate, d As Double(,), npoints As Integer, isupper As Boolean)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim j0 As Integer = 0
			Dim j1 As Integer = 0

			alglib.ap.assert(npoints >= 0, "ClusterizerSetDistances: NPoints<0")
			alglib.ap.assert(alglib.ap.rows(d) >= npoints, "ClusterizerSetDistances: Rows(D)<NPoints")
			alglib.ap.assert(alglib.ap.cols(d) >= npoints, "ClusterizerSetDistances: Cols(D)<NPoints")
			s.npoints = npoints
			s.nfeatures = 0
			s.disttype = -1
			apserv.rmatrixsetlengthatleast(s.d, npoints, npoints)
			For i = 0 To npoints - 1
				If isupper Then
					j0 = i + 1
					j1 = npoints - 1
				Else
					j0 = 0
					j1 = i - 1
				End If
				For j = j0 To j1
					alglib.ap.assert(Math.isfinite(d(i, j)) AndAlso CDbl(d(i, j)) >= CDbl(0), "ClusterizerSetDistances: D contains infinite, NAN or negative elements")
					s.d(i, j) = d(i, j)
					s.d(j, i) = d(i, j)
				Next
				s.d(i, i) = 0
			Next
		End Sub


		'************************************************************************
'        This function sets agglomerative hierarchical clustering algorithm
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            Algo    -   algorithm type:
'                        * 0     complete linkage (default algorithm)
'                        * 1     single linkage
'                        * 2     unweighted average linkage
'                        * 3     weighted average linkage
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizersetahcalgo(s As clusterizerstate, algo As Integer)
			alglib.ap.assert(((algo = 0 OrElse algo = 1) OrElse algo = 2) OrElse algo = 3, "ClusterizerSetHCAlgo: incorrect algorithm type")
			s.ahcalgo = algo
		End Sub


		'************************************************************************
'        This  function  sets k-means++ properties : number of restarts and maximum
'        number of iterations per one run.
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            Restarts-   restarts count, >=1.
'                        k-means++ algorithm performs several restarts and  chooses
'                        best set of centers (one with minimum squared distance).
'            MaxIts  -   maximum number of k-means iterations performed during  one
'                        run. >=0, zero value means that algorithm performs unlimited
'                        number of iterations.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizersetkmeanslimits(s As clusterizerstate, restarts As Integer, maxits As Integer)
			alglib.ap.assert(restarts >= 1, "ClusterizerSetKMeansLimits: Restarts<=0")
			alglib.ap.assert(maxits >= 0, "ClusterizerSetKMeansLimits: MaxIts<0")
			s.kmeansrestarts = restarts
			s.kmeansmaxits = maxits
		End Sub


		'************************************************************************
'        This function performs agglomerative hierarchical clustering
'
'        COMMERCIAL EDITION OF ALGLIB:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function, which can be used from C++ and C#:
'          ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'          ! * multicore support
'          !
'          ! Agglomerative  hierarchical  clustering  algorithm  has  two   phases:
'          ! distance matrix calculation  and  clustering  itself. Only first phase
'          ! (distance matrix calculation) is accelerated by Intel MKL  and  multi-
'          ! threading. Thus, acceleration is significant only for  medium or high-
'          ! dimensional problems.
'          !
'          ! We recommend you to read 'Working with commercial version' section  of
'          ! ALGLIB Reference Manual in order to find out how to  use  performance-
'          ! related features provided by commercial edition of ALGLIB.
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'
'        OUTPUT PARAMETERS:
'            Rep     -   clustering results; see description of AHCReport
'                        structure for more information.
'
'        NOTE 1: hierarchical clustering algorithms require large amounts of memory.
'                In particular, this implementation needs  sizeof(double)*NPoints^2
'                bytes, which are used to store distance matrix. In  case  we  work
'                with user-supplied matrix, this amount is multiplied by 2 (we have
'                to store original matrix and to work with its copy).
'                
'                For example, problem with 10000 points  would require 800M of RAM,
'                even when working in a 1-dimensional space.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizerrunahc(s As clusterizerstate, rep As ahcreport)
			Dim npoints As Integer = 0
			Dim nfeatures As Integer = 0
			Dim d As Double(,) = New Double(-1, -1) {}

			npoints = s.npoints
			nfeatures = s.nfeatures

			'
			' Fill Rep.NPoints, quick exit when NPoints<=1
			'
			rep.npoints = npoints
			If npoints = 0 Then
				rep.p = New Integer(-1) {}
				rep.z = New Integer(-1, -1) {}
				rep.pz = New Integer(-1, -1) {}
				rep.pm = New Integer(-1, -1) {}
				rep.mergedist = New Double(-1) {}
				Return
			End If
			If npoints = 1 Then
				rep.p = New Integer(0) {}
				rep.z = New Integer(-1, -1) {}
				rep.pz = New Integer(-1, -1) {}
				rep.pm = New Integer(-1, -1) {}
				rep.mergedist = New Double(-1) {}
				rep.p(0) = 0
				Return
			End If

			'
			' More than one point
			'
			If s.disttype = -1 Then

				'
				' Run clusterizer with user-supplied distance matrix
				'
				clusterizerrunahcinternal(s, s.d, rep)
				Return
			Else

				'
				' Build distance matrix D.
				'
				clusterizergetdistances(s.xy, npoints, nfeatures, s.disttype, d)

				'
				' Run clusterizer
				'
				clusterizerrunahcinternal(s, d, rep)
				Return
			End If
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_clusterizerrunahc(s As clusterizerstate, rep As ahcreport)
			clusterizerrunahc(s, rep)
		End Sub


		'************************************************************************
'        This function performs clustering by k-means++ algorithm.
'
'        You may change algorithm properties like number of restarts or  iterations
'        limit by calling ClusterizerSetKMeansLimits() functions.
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            K       -   number of clusters, K>=0.
'                        K  can  be  zero only when algorithm is called  for  empty
'                        dataset,  in   this   case   completion  code  is  set  to
'                        success (+1).
'                        If  K=0  and  dataset  size  is  non-zero,  we   can   not
'                        meaningfully assign points to some center  (there  are  no
'                        centers because K=0) and  return  -3  as  completion  code
'                        (failure).
'
'        OUTPUT PARAMETERS:
'            Rep     -   clustering results; see description of KMeansReport
'                        structure for more information.
'
'        NOTE 1: k-means  clustering  can  be  performed  only  for  datasets  with
'                Euclidean  distance  function.  Algorithm  will  return   negative
'                completion code in Rep.TerminationType in case dataset  was  added
'                to clusterizer with DistType other than Euclidean (or dataset  was
'                specified by distance matrix instead of explicitly given points).
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizerrunkmeans(s As clusterizerstate, k As Integer, rep As kmeansreport)
			Dim dummy As Double(,) = New Double(-1, -1) {}

			alglib.ap.assert(k >= 0, "ClusterizerRunKMeans: K<0")

			'
			' Incorrect distance type
			'
			If s.disttype <> 2 Then
				rep.npoints = s.npoints
				rep.terminationtype = -5
				rep.k = k
				Return
			End If

			'
			' K>NPoints or (K=0 and NPoints>0)
			'
			If k > s.npoints OrElse (k = 0 AndAlso s.npoints > 0) Then
				rep.npoints = s.npoints
				rep.terminationtype = -3
				rep.k = k
				Return
			End If

			'
			' No points
			'
			If s.npoints = 0 Then
				rep.npoints = 0
				rep.terminationtype = 1
				rep.k = k
				Return
			End If

			'
			' Normal case:
			' 1<=K<=NPoints, Euclidean distance 
			'
			rep.npoints = s.npoints
			rep.nfeatures = s.nfeatures
			rep.k = k
			rep.npoints = s.npoints
			rep.nfeatures = s.nfeatures
			kmeansgenerateinternal(s.xy, s.npoints, s.nfeatures, k, s.kmeansmaxits, s.kmeansrestarts, _
				rep.terminationtype, dummy, False, rep.c, True, rep.cidx)
		End Sub


		'************************************************************************
'        This function returns distance matrix for dataset
'
'        COMMERCIAL EDITION OF ALGLIB:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function, which can be used from C++ and C#:
'          ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'          ! * multicore support
'          !
'          ! Agglomerative  hierarchical  clustering  algorithm  has  two   phases:
'          ! distance matrix calculation  and  clustering  itself. Only first phase
'          ! (distance matrix calculation) is accelerated by Intel MKL  and  multi-
'          ! threading. Thus, acceleration is significant only for  medium or high-
'          ! dimensional problems.
'          !
'          ! We recommend you to read 'Working with commercial version' section  of
'          ! ALGLIB Reference Manual in order to find out how to  use  performance-
'          ! related features provided by commercial edition of ALGLIB.
'
'        INPUT PARAMETERS:
'            XY      -   array[NPoints,NFeatures], dataset
'            NPoints -   number of points, >=0
'            NFeatures-  number of features, >=1
'            DistType-   distance function:
'                        *  0    Chebyshev distance  (L-inf norm)
'                        *  1    city block distance (L1 norm)
'                        *  2    Euclidean distance  (L2 norm)
'                        * 10    Pearson correlation:
'                                dist(a,b) = 1-corr(a,b)
'                        * 11    Absolute Pearson correlation:
'                                dist(a,b) = 1-|corr(a,b)|
'                        * 12    Uncentered Pearson correlation (cosine of the angle):
'                                dist(a,b) = a'*b/(|a|*|b|)
'                        * 13    Absolute uncentered Pearson correlation
'                                dist(a,b) = |a'*b|/(|a|*|b|)
'                        * 20    Spearman rank correlation:
'                                dist(a,b) = 1-rankcorr(a,b)
'                        * 21    Absolute Spearman rank correlation
'                                dist(a,b) = 1-|rankcorr(a,b)|
'
'        OUTPUT PARAMETERS:
'            D       -   array[NPoints,NPoints], distance matrix
'                        (full matrix is returned, with lower and upper triangles)
'
'        NOTES: different distance functions have different performance penalty:
'               * Euclidean or Pearson correlation distances are the fastest ones
'               * Spearman correlation distance function is a bit slower
'               * city block and Chebyshev distances are order of magnitude slower
'               
'               The reason behing difference in performance is that correlation-based
'               distance functions are computed using optimized linear algebra kernels,
'               while Chebyshev and city block distance functions are computed using
'               simple nested loops with two branches at each iteration.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizergetdistances(xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer, ByRef d As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim vr As Double = 0
			Dim tmpxy As Double(,) = New Double(-1, -1) {}
			Dim tmpx As Double() = New Double(-1) {}
			Dim tmpy As Double() = New Double(-1) {}
			Dim diagbuf As Double() = New Double(-1) {}
			Dim buf As New apserv.apbuffers()
			Dim i_ As Integer = 0

			d = New Double(-1, -1) {}

			alglib.ap.assert(nfeatures >= 1, "ClusterizerGetDistances: NFeatures<1")
			alglib.ap.assert(npoints >= 0, "ClusterizerGetDistances: NPoints<1")
			alglib.ap.assert((((((((disttype = 0 OrElse disttype = 1) OrElse disttype = 2) OrElse disttype = 10) OrElse disttype = 11) OrElse disttype = 12) OrElse disttype = 13) OrElse disttype = 20) OrElse disttype = 21, "ClusterizerGetDistances: incorrect DistType")
			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "ClusterizerGetDistances: Rows(XY)<NPoints")
			alglib.ap.assert(alglib.ap.cols(xy) >= nfeatures, "ClusterizerGetDistances: Cols(XY)<NFeatures")
			alglib.ap.assert(apserv.apservisfinitematrix(xy, npoints, nfeatures), "ClusterizerGetDistances: XY contains NAN/INF")

			'
			' Quick exit
			'
			If npoints = 0 Then
				Return
			End If
			If npoints = 1 Then
				d = New Double(0, 0) {}
				d(0, 0) = 0
				Return
			End If

			'
			' Build distance matrix D.
			'
			If disttype = 0 OrElse disttype = 1 Then

				'
				' Chebyshev or city-block distances:
				' * recursively calculate upper triangle (with main diagonal)
				' * copy it to the bottom part of the matrix
				'
				d = New Double(npoints - 1, npoints - 1) {}
				evaluatedistancematrixrec(xy, nfeatures, disttype, d, 0, npoints, _
					0, npoints)
				ablas.rmatrixenforcesymmetricity(d, npoints, True)
				Return
			End If
			If disttype = 2 Then

				'
				' Euclidean distance
				'
				' NOTE: parallelization is done within RMatrixSYRK
				'
				d = New Double(npoints - 1, npoints - 1) {}
				tmpxy = New Double(npoints - 1, nfeatures - 1) {}
				tmpx = New Double(nfeatures - 1) {}
				diagbuf = New Double(npoints - 1) {}
				For j = 0 To nfeatures - 1
					tmpx(j) = 0.0
				Next
				v = CDbl(1) / CDbl(npoints)
				For i = 0 To npoints - 1
					For i_ = 0 To nfeatures - 1
						tmpx(i_) = tmpx(i_) + v * xy(i, i_)
					Next
				Next
				For i = 0 To npoints - 1
					For i_ = 0 To nfeatures - 1
						tmpxy(i, i_) = xy(i, i_)
					Next
					For i_ = 0 To nfeatures - 1
						tmpxy(i, i_) = tmpxy(i, i_) - tmpx(i_)
					Next
				Next
				ablas.rmatrixsyrk(npoints, nfeatures, 1.0, tmpxy, 0, 0, _
					0, 0.0, d, 0, 0, True)
				For i = 0 To npoints - 1
					diagbuf(i) = d(i, i)
				Next
				For i = 0 To npoints - 1
					d(i, i) = 0.0
					For j = i + 1 To npoints - 1
						v = System.Math.sqrt(System.Math.Max(diagbuf(i) + diagbuf(j) - 2 * d(i, j), 0.0))
						d(i, j) = v
					Next
				Next
				ablas.rmatrixenforcesymmetricity(d, npoints, True)
				Return
			End If
			If disttype = 10 OrElse disttype = 11 Then

				'
				' Absolute/nonabsolute Pearson correlation distance
				'
				' NOTE: parallelization is done within PearsonCorrM, which calls RMatrixSYRK internally
				'
				d = New Double(npoints - 1, npoints - 1) {}
				diagbuf = New Double(npoints - 1) {}
				tmpxy = New Double(npoints - 1, nfeatures - 1) {}
				For i = 0 To npoints - 1
					v = 0.0
					For j = 0 To nfeatures - 1
						v = v + xy(i, j)
					Next
					v = v / nfeatures
					For j = 0 To nfeatures - 1
						tmpxy(i, j) = xy(i, j) - v
					Next
				Next
				ablas.rmatrixsyrk(npoints, nfeatures, 1.0, tmpxy, 0, 0, _
					0, 0.0, d, 0, 0, True)
				For i = 0 To npoints - 1
					diagbuf(i) = d(i, i)
				Next
				For i = 0 To npoints - 1
					d(i, i) = 0.0
					For j = i + 1 To npoints - 1
						v = d(i, j) / System.Math.sqrt(diagbuf(i) * diagbuf(j))
						If disttype = 10 Then
							v = 1 - v
						Else
							v = 1 - System.Math.Abs(v)
						End If
						v = System.Math.Max(v, 0.0)
						d(i, j) = v
					Next
				Next
				ablas.rmatrixenforcesymmetricity(d, npoints, True)
				Return
			End If
			If disttype = 12 OrElse disttype = 13 Then

				'
				' Absolute/nonabsolute uncentered Pearson correlation distance
				'
				' NOTE: parallelization is done within RMatrixSYRK
				'
				d = New Double(npoints - 1, npoints - 1) {}
				diagbuf = New Double(npoints - 1) {}
				ablas.rmatrixsyrk(npoints, nfeatures, 1.0, xy, 0, 0, _
					0, 0.0, d, 0, 0, True)
				For i = 0 To npoints - 1
					diagbuf(i) = d(i, i)
				Next
				For i = 0 To npoints - 1
					d(i, i) = 0.0
					For j = i + 1 To npoints - 1
						v = d(i, j) / System.Math.sqrt(diagbuf(i) * diagbuf(j))
						If disttype = 13 Then
							v = System.Math.Abs(v)
						End If
						v = System.Math.Min(v, 1.0)
						d(i, j) = 1 - v
					Next
				Next
				ablas.rmatrixenforcesymmetricity(d, npoints, True)
				Return
			End If
			If disttype = 20 OrElse disttype = 21 Then

				'
				' Spearman rank correlation
				'
				' NOTE: parallelization of correlation matrix is done within
				'       PearsonCorrM, which calls RMatrixSYRK internally
				'
				d = New Double(npoints - 1, npoints - 1) {}
				diagbuf = New Double(npoints - 1) {}
				tmpxy = New Double(npoints - 1, nfeatures - 1) {}
				ablas.rmatrixcopy(npoints, nfeatures, xy, 0, 0, tmpxy, _
					0, 0)
				basestat.rankdatacentered(tmpxy, npoints, nfeatures)
				ablas.rmatrixsyrk(npoints, nfeatures, 1.0, tmpxy, 0, 0, _
					0, 0.0, d, 0, 0, True)
				For i = 0 To npoints - 1
					If CDbl(d(i, i)) > CDbl(0) Then
						diagbuf(i) = 1 / System.Math.sqrt(d(i, i))
					Else
						diagbuf(i) = 0.0
					End If
				Next
				For i = 0 To npoints - 1
					v = diagbuf(i)
					d(i, i) = 0.0
					For j = i + 1 To npoints - 1
						vv = d(i, j) * v * diagbuf(j)
						If disttype = 20 Then
							vr = 1 - vv
						Else
							vr = 1 - System.Math.Abs(vv)
						End If
						If CDbl(vr) < CDbl(0) Then
							vr = 0.0
						End If
						d(i, j) = vr
					Next
				Next
				ablas.rmatrixenforcesymmetricity(d, npoints, True)
				Return
			End If
			alglib.ap.assert(False)
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_clusterizergetdistances(xy As Double(,), npoints As Integer, nfeatures As Integer, disttype As Integer, ByRef d As Double(,))
			clusterizergetdistances(xy, npoints, nfeatures, disttype, d)
		End Sub


		'************************************************************************
'        This function takes as input clusterization report Rep,  desired  clusters
'        count K, and builds top K clusters from hierarchical clusterization  tree.
'        It returns assignment of points to clusters (array of cluster indexes).
'
'        INPUT PARAMETERS:
'            Rep     -   report from ClusterizerRunAHC() performed on XY
'            K       -   desired number of clusters, 1<=K<=NPoints.
'                        K can be zero only when NPoints=0.
'
'        OUTPUT PARAMETERS:
'            CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                        0 to K-1) for I-th point of the dataset.
'            CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                        returned by this function to indexes used by  Rep.Z.  J-th
'                        cluster returned by this function corresponds to  CZ[J]-th
'                        cluster stored in Rep.Z/PZ/PM.
'                        It is guaranteed that CZ[I]<CZ[I+1].
'
'        NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'              Although  they  were  obtained  by  manipulation with top K nodes of
'              dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'              function does not return information about hierarchy.  Each  of  the
'              clusters stand on its own.
'              
'        NOTE: Cluster indexes returned by this function  does  not  correspond  to
'              indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'              representation of the dataset (dendrogram), or you work with  "flat"
'              representation returned by this function.  Each  of  representations
'              has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'              while latter uses [0..K-1]), although  it  is  possible  to  perform
'              conversion from one system to another by means of CZ array, returned
'              by this function, which allows you to convert indexes stored in CIdx
'              to the numeration system used by Rep.Z.
'              
'        NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'              it will perform many times faster than  for  K=100.  Its  worst-case
'              performance is O(N*K), although in average case  it  perform  better
'              (up to O(N*log(K))).
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizergetkclusters(rep As ahcreport, k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
			Dim i As Integer = 0
			Dim mergeidx As Integer = 0
			Dim i0 As Integer = 0
			Dim i1 As Integer = 0
			Dim t As Integer = 0
			Dim presentclusters As Boolean() = New Boolean(-1) {}
			Dim clusterindexes As Integer() = New Integer(-1) {}
			Dim clustersizes As Integer() = New Integer(-1) {}
			Dim tmpidx As Integer() = New Integer(-1) {}
			Dim npoints As Integer = 0

			cidx = New Integer(-1) {}
			cz = New Integer(-1) {}

			npoints = rep.npoints
			alglib.ap.assert(npoints >= 0, "ClusterizerGetKClusters: internal error in Rep integrity")
			alglib.ap.assert(k >= 0, "ClusterizerGetKClusters: K<=0")
			alglib.ap.assert(k <= npoints, "ClusterizerGetKClusters: K>NPoints")
			alglib.ap.assert(k > 0 OrElse npoints = 0, "ClusterizerGetKClusters: K<=0")
			alglib.ap.assert(npoints = rep.npoints, "ClusterizerGetKClusters: NPoints<>Rep.NPoints")

			'
			' Quick exit
			'
			If npoints = 0 Then
				Return
			End If
			If npoints = 1 Then
				cz = New Integer(0) {}
				cidx = New Integer(0) {}
				cz(0) = 0
				cidx(0) = 0
				Return
			End If

			'
			' Replay merges, from top to bottom,
			' keep track of clusters being present at the moment
			'
			presentclusters = New Boolean(2 * npoints - 2) {}
			tmpidx = New Integer(npoints - 1) {}
			For i = 0 To 2 * npoints - 3
				presentclusters(i) = False
			Next
			presentclusters(2 * npoints - 2) = True
			For i = 0 To npoints - 1
				tmpidx(i) = 2 * npoints - 2
			Next
			For mergeidx = npoints - 2 To npoints - k Step -1

				'
				' Update information about clusters being present at the moment
				'
				presentclusters(npoints + mergeidx) = False
				presentclusters(rep.z(mergeidx, 0)) = True
				presentclusters(rep.z(mergeidx, 1)) = True

				'
				' Update TmpIdx according to the current state of the dataset
				'
				' NOTE: TmpIdx contains cluster indexes from [0..2*NPoints-2];
				'       we will convert them to [0..K-1] later.
				'
				i0 = rep.pm(mergeidx, 0)
				i1 = rep.pm(mergeidx, 1)
				t = rep.z(mergeidx, 0)
				For i = i0 To i1
					tmpidx(i) = t
				Next
				i0 = rep.pm(mergeidx, 2)
				i1 = rep.pm(mergeidx, 3)
				t = rep.z(mergeidx, 1)
				For i = i0 To i1
					tmpidx(i) = t
				Next
			Next

			'
			' Fill CZ - array which allows us to convert cluster indexes
			' from one system to another.
			'
			cz = New Integer(k - 1) {}
			clusterindexes = New Integer(2 * npoints - 2) {}
			t = 0
			For i = 0 To 2 * npoints - 2
				If presentclusters(i) Then
					cz(t) = i
					clusterindexes(i) = t
					t = t + 1
				End If
			Next
			alglib.ap.assert(t = k, "ClusterizerGetKClusters: internal error")

			'
			' Convert indexes stored in CIdx
			'
			cidx = New Integer(npoints - 1) {}
			For i = 0 To npoints - 1
				cidx(i) = clusterindexes(tmpidx(rep.p(i)))
			Next
		End Sub


		'************************************************************************
'        This  function  accepts  AHC  report  Rep,  desired  minimum  intercluster
'        distance and returns top clusters from  hierarchical  clusterization  tree
'        which are separated by distance R or HIGHER.
'
'        It returns assignment of points to clusters (array of cluster indexes).
'
'        There is one more function with similar name - ClusterizerSeparatedByCorr,
'        which returns clusters with intercluster correlation equal to R  or  LOWER
'        (note: higher for distance, lower for correlation).
'
'        INPUT PARAMETERS:
'            Rep     -   report from ClusterizerRunAHC() performed on XY
'            R       -   desired minimum intercluster distance, R>=0
'
'        OUTPUT PARAMETERS:
'            K       -   number of clusters, 1<=K<=NPoints
'            CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                        0 to K-1) for I-th point of the dataset.
'            CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                        returned by this function to indexes used by  Rep.Z.  J-th
'                        cluster returned by this function corresponds to  CZ[J]-th
'                        cluster stored in Rep.Z/PZ/PM.
'                        It is guaranteed that CZ[I]<CZ[I+1].
'
'        NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'              Although  they  were  obtained  by  manipulation with top K nodes of
'              dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'              function does not return information about hierarchy.  Each  of  the
'              clusters stand on its own.
'              
'        NOTE: Cluster indexes returned by this function  does  not  correspond  to
'              indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'              representation of the dataset (dendrogram), or you work with  "flat"
'              representation returned by this function.  Each  of  representations
'              has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'              while latter uses [0..K-1]), although  it  is  possible  to  perform
'              conversion from one system to another by means of CZ array, returned
'              by this function, which allows you to convert indexes stored in CIdx
'              to the numeration system used by Rep.Z.
'              
'        NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'              it will perform many times faster than  for  K=100.  Its  worst-case
'              performance is O(N*K), although in average case  it  perform  better
'              (up to O(N*log(K))).
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizerseparatedbydist(rep As ahcreport, r As Double, ByRef k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
			k = 0
			cidx = New Integer(-1) {}
			cz = New Integer(-1) {}

			alglib.ap.assert(Math.isfinite(r) AndAlso CDbl(r) >= CDbl(0), "ClusterizerSeparatedByDist: R is infinite or less than 0")
			k = 1
			While k < rep.npoints AndAlso CDbl(rep.mergedist(rep.npoints - 1 - k)) >= CDbl(r)
				k = k + 1
			End While
			clusterizergetkclusters(rep, k, cidx, cz)
		End Sub


		'************************************************************************
'        This  function  accepts  AHC  report  Rep,  desired  maximum  intercluster
'        correlation and returns top clusters from hierarchical clusterization tree
'        which are separated by correlation R or LOWER.
'
'        It returns assignment of points to clusters (array of cluster indexes).
'
'        There is one more function with similar name - ClusterizerSeparatedByDist,
'        which returns clusters with intercluster distance equal  to  R  or  HIGHER
'        (note: higher for distance, lower for correlation).
'
'        INPUT PARAMETERS:
'            Rep     -   report from ClusterizerRunAHC() performed on XY
'            R       -   desired maximum intercluster correlation, -1<=R<=+1
'
'        OUTPUT PARAMETERS:
'            K       -   number of clusters, 1<=K<=NPoints
'            CIdx    -   array[NPoints], I-th element contains cluster index  (from
'                        0 to K-1) for I-th point of the dataset.
'            CZ      -   array[K]. This array allows  to  convert  cluster  indexes
'                        returned by this function to indexes used by  Rep.Z.  J-th
'                        cluster returned by this function corresponds to  CZ[J]-th
'                        cluster stored in Rep.Z/PZ/PM.
'                        It is guaranteed that CZ[I]<CZ[I+1].
'
'        NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
'              Although  they  were  obtained  by  manipulation with top K nodes of
'              dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
'              function does not return information about hierarchy.  Each  of  the
'              clusters stand on its own.
'              
'        NOTE: Cluster indexes returned by this function  does  not  correspond  to
'              indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
'              representation of the dataset (dendrogram), or you work with  "flat"
'              representation returned by this function.  Each  of  representations
'              has its own clusters indexing system (former uses [0, 2*NPoints-2]),
'              while latter uses [0..K-1]), although  it  is  possible  to  perform
'              conversion from one system to another by means of CZ array, returned
'              by this function, which allows you to convert indexes stored in CIdx
'              to the numeration system used by Rep.Z.
'              
'        NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
'              it will perform many times faster than  for  K=100.  Its  worst-case
'              performance is O(N*K), although in average case  it  perform  better
'              (up to O(N*log(K))).
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub clusterizerseparatedbycorr(rep As ahcreport, r As Double, ByRef k As Integer, ByRef cidx As Integer(), ByRef cz As Integer())
			k = 0
			cidx = New Integer(-1) {}
			cz = New Integer(-1) {}

			alglib.ap.assert((Math.isfinite(r) AndAlso CDbl(r) >= CDbl(-1)) AndAlso CDbl(r) <= CDbl(1), "ClusterizerSeparatedByCorr: R is infinite or less than 0")
			k = 1
			While k < rep.npoints AndAlso CDbl(rep.mergedist(rep.npoints - 1 - k)) >= CDbl(1 - r)
				k = k + 1
			End While
			clusterizergetkclusters(rep, k, cidx, cz)
		End Sub


		'************************************************************************
'        K-means++ clusterization
'
'        INPUT PARAMETERS:
'            XY          -   dataset, array [0..NPoints-1,0..NVars-1].
'            NPoints     -   dataset size, NPoints>=K
'            NVars       -   number of variables, NVars>=1
'            K           -   desired number of clusters, K>=1
'            Restarts    -   number of restarts, Restarts>=1
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -3, if task is degenerate (number of distinct points is
'                                  less than K)
'                            * -1, if incorrect NPoints/NFeatures/K/Restarts was passed
'                            *  1, if subroutine finished successfully
'            CCol        -   array[0..NVars-1,0..K-1].matrix whose columns store
'                            cluster's centers
'            NeedCCol    -   True in case caller requires to store result in CCol
'            CRow        -   array[0..K-1,0..NVars-1], same as CCol, but centers are
'                            stored in rows
'            NeedCRow    -   True in case caller requires to store result in CCol
'            XYC         -   array[NPoints], which contains cluster indexes
'
'          -- ALGLIB --
'             Copyright 21.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub kmeansgenerateinternal(xy As Double(,), npoints As Integer, nvars As Integer, k As Integer, maxits As Integer, restarts As Integer, _
			ByRef info As Integer, ByRef ccol As Double(,), needccol As Boolean, ByRef crow As Double(,), needcrow As Boolean, ByRef xyc As Integer())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ct As Double(,) = New Double(-1, -1) {}
			Dim ctbest As Double(,) = New Double(-1, -1) {}
			Dim xycbest As Integer() = New Integer(-1) {}
			Dim e As Double = 0
			Dim eprev As Double = 0
			Dim ebest As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim tmp As Double() = New Double(-1) {}
			Dim d2 As Double() = New Double(-1) {}
			Dim p As Double() = New Double(-1) {}
			Dim csizes As Integer() = New Integer(-1) {}
			Dim cbusy As Boolean() = New Boolean(-1) {}
			Dim v As Double = 0
			Dim cclosest As Integer = 0
			Dim dclosest As Double = 0
			Dim work As Double() = New Double(-1) {}
			Dim waschanges As New Boolean()
			Dim zerosizeclusters As New Boolean()
			Dim pass As Integer = 0
			Dim itcnt As Integer = 0
			Dim rs As New hqrnd.hqrndstate()
			Dim i_ As Integer = 0

			info = 0
			ccol = New Double(-1, -1) {}
			crow = New Double(-1, -1) {}
			xyc = New Integer(-1) {}


			'
			' Test parameters
			'
			If ((npoints < k OrElse nvars < 1) OrElse k < 1) OrElse restarts < 1 Then
				info = -1
				Return
			End If

			'
			' TODO: special case K=1
			' TODO: special case K=NPoints
			'
			info = 1

			'
			' Multiple passes of k-means++ algorithm
			'
			ct = New Double(k - 1, nvars - 1) {}
			ctbest = New Double(k - 1, nvars - 1) {}
			xyc = New Integer(npoints - 1) {}
			xycbest = New Integer(npoints - 1) {}
			d2 = New Double(npoints - 1) {}
			p = New Double(npoints - 1) {}
			tmp = New Double(nvars - 1) {}
			csizes = New Integer(k - 1) {}
			cbusy = New Boolean(k - 1) {}
			ebest = Math.maxrealnumber
			hqrnd.hqrndrandomize(rs)
			For pass = 1 To restarts

				'
				' Select initial centers  using k-means++ algorithm
				' 1. Choose first center at random
				' 2. Choose next centers using their distance from centers already chosen
				'
				' Note that for performance reasons centers are stored in ROWS of CT, not
				' in columns. We'll transpose CT in the end and store it in the C.
				'
				i = hqrnd.hqrnduniformi(rs, npoints)
				For i_ = 0 To nvars - 1
					ct(0, i_) = xy(i, i_)
				Next
				cbusy(0) = True
				For i = 1 To k - 1
					cbusy(i) = False
				Next
				If Not selectcenterpp(xy, npoints, nvars, ct, cbusy, k, _
					d2, p, tmp) Then
					info = -3
					Return
				End If

				'
				' Update centers:
				' 2. update center positions
				'
				For i = 0 To npoints - 1
					xyc(i) = -1
				Next
				eprev = Math.maxrealnumber
				itcnt = 0
				e = 0
				While maxits = 0 OrElse itcnt < maxits

					'
					' Update iteration counter
					'
					itcnt = itcnt + 1

					'
					' fill XYC with center numbers
					'
					waschanges = False
					For i = 0 To npoints - 1
						cclosest = -1
						dclosest = Math.maxrealnumber
						For j = 0 To k - 1
							For i_ = 0 To nvars - 1
								tmp(i_) = xy(i, i_)
							Next
							For i_ = 0 To nvars - 1
								tmp(i_) = tmp(i_) - ct(j, i_)
							Next
							v = 0.0
							For i_ = 0 To nvars - 1
								v += tmp(i_) * tmp(i_)
							Next
							If CDbl(v) < CDbl(dclosest) Then
								cclosest = j
								dclosest = v
							End If
						Next
						If xyc(i) <> cclosest Then
							waschanges = True
						End If
						xyc(i) = cclosest
					Next

					'
					' Update centers
					'
					For j = 0 To k - 1
						csizes(j) = 0
					Next
					For i = 0 To k - 1
						For j = 0 To nvars - 1
							ct(i, j) = 0
						Next
					Next
					For i = 0 To npoints - 1
						csizes(xyc(i)) = csizes(xyc(i)) + 1
						For i_ = 0 To nvars - 1
							ct(xyc(i), i_) = ct(xyc(i), i_) + xy(i, i_)
						Next
					Next
					zerosizeclusters = False
					For j = 0 To k - 1
						If csizes(j) <> 0 Then
							v = CDbl(1) / CDbl(csizes(j))
							For i_ = 0 To nvars - 1
								ct(j, i_) = v * ct(j, i_)
							Next
						End If
						cbusy(j) = csizes(j) <> 0
						zerosizeclusters = zerosizeclusters OrElse csizes(j) = 0
					Next
					If zerosizeclusters Then

						'
						' Some clusters have zero size - rare, but possible.
						' We'll choose new centers for such clusters using k-means++ rule
						' and restart algorithm
						'
						If Not selectcenterpp(xy, npoints, nvars, ct, cbusy, k, _
							d2, p, tmp) Then
							info = -3
							Return
						End If
						Continue While
					End If

					'
					' Stop if one of two conditions is met:
					' 1. nothing has changed during iteration
					' 2. energy function increased 
					'
					e = 0
					For i = 0 To npoints - 1
						For i_ = 0 To nvars - 1
							tmp(i_) = xy(i, i_)
						Next
						For i_ = 0 To nvars - 1
							tmp(i_) = tmp(i_) - ct(xyc(i), i_)
						Next
						v = 0.0
						For i_ = 0 To nvars - 1
							v += tmp(i_) * tmp(i_)
						Next
						e = e + v
					Next
					If Not waschanges OrElse CDbl(e) >= CDbl(eprev) Then
						Exit While
					End If

					'
					' Update EPrev
					'
					eprev = e
				End While

				'
				' 3. Calculate E, compare with best centers found so far
				'
				If CDbl(e) < CDbl(ebest) Then

					'
					' store partition.
					'
					ebest = e
					blas.copymatrix(ct, 0, k - 1, 0, nvars - 1, ctbest, _
						0, k - 1, 0, nvars - 1)
					For i = 0 To npoints - 1
						xycbest(i) = xyc(i)
					Next
				End If
			Next

			'
			' Copy and transpose
			'
			If needccol Then
				ccol = New Double(nvars - 1, k - 1) {}
				blas.copyandtranspose(ctbest, 0, k - 1, 0, nvars - 1, ccol, _
					0, nvars - 1, 0, k - 1)
			End If
			If needcrow Then
				crow = New Double(k - 1, nvars - 1) {}
				ablas.rmatrixcopy(k, nvars, ctbest, 0, 0, crow, _
					0, 0)
			End If
			For i = 0 To npoints - 1
				xyc(i) = xycbest(i)
			Next
		End Sub


		'************************************************************************
'        Select center for a new cluster using k-means++ rule
'        ************************************************************************

		Private Shared Function selectcenterpp(xy As Double(,), npoints As Integer, nvars As Integer, ByRef centers As Double(,), ByRef busycenters As Boolean(), ccnt As Integer, _
			ByRef d2 As Double(), ByRef p As Double(), ByRef tmp As Double()) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim cc As Integer = 0
			Dim v As Double = 0
			Dim s As Double = 0
			Dim i_ As Integer = 0

			result = True
			For cc = 0 To ccnt - 1
				If Not busycenters(cc) Then

					'
					' fill D2
					'
					For i = 0 To npoints - 1
						d2(i) = Math.maxrealnumber
						For j = 0 To ccnt - 1
							If busycenters(j) Then
								For i_ = 0 To nvars - 1
									tmp(i_) = xy(i, i_)
								Next
								For i_ = 0 To nvars - 1
									tmp(i_) = tmp(i_) - centers(j, i_)
								Next
								v = 0.0
								For i_ = 0 To nvars - 1
									v += tmp(i_) * tmp(i_)
								Next
								If CDbl(v) < CDbl(d2(i)) Then
									d2(i) = v
								End If
							End If
						Next
					Next

					'
					' calculate P (non-cumulative)
					'
					s = 0
					For i = 0 To npoints - 1
						s = s + d2(i)
					Next
					If CDbl(s) = CDbl(0) Then
						result = False
						Return result
					End If
					s = 1 / s
					For i_ = 0 To npoints - 1
						p(i_) = s * d2(i_)
					Next

					'
					' choose one of points with probability P
					' random number within (0,1) is generated and
					' inverse empirical CDF is used to randomly choose a point.
					'
					s = 0
					v = Math.randomreal()
					For i = 0 To npoints - 1
						s = s + p(i)
						If CDbl(v) <= CDbl(s) OrElse i = npoints - 1 Then
							For i_ = 0 To nvars - 1
								centers(cc, i_) = xy(i, i_)
							Next
							busycenters(cc) = True
							Exit For
						End If
					Next
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        This  function  performs  agglomerative  hierarchical  clustering    using
'        precomputed  distance  matrix.  Internal  function,  should  not be called
'        directly.
'
'        INPUT PARAMETERS:
'            S       -   clusterizer state, initialized by ClusterizerCreate()
'            D       -   distance matrix, array[S.NFeatures,S.NFeatures]
'                        Contents of the matrix is destroyed during
'                        algorithm operation.
'
'        OUTPUT PARAMETERS:
'            Rep     -   clustering results; see description of AHCReport
'                        structure for more information.
'
'          -- ALGLIB --
'             Copyright 10.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub clusterizerrunahcinternal(s As clusterizerstate, ByRef d As Double(,), rep As ahcreport)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim mergeidx As Integer = 0
			Dim c0 As Integer = 0
			Dim c1 As Integer = 0
			Dim s0 As Integer = 0
			Dim s1 As Integer = 0
			Dim ar As Integer = 0
			Dim br As Integer = 0
			Dim npoints As Integer = 0
			Dim cidx As Integer() = New Integer(-1) {}
			Dim csizes As Integer() = New Integer(-1) {}
			Dim nnidx As Integer() = New Integer(-1) {}
			Dim cinfo As Integer(,) = New Integer(-1, -1) {}

			npoints = s.npoints

			'
			' Fill Rep.NPoints, quick exit when NPoints<=1
			'
			rep.npoints = npoints
			If npoints = 0 Then
				rep.p = New Integer(-1) {}
				rep.z = New Integer(-1, -1) {}
				rep.pz = New Integer(-1, -1) {}
				rep.pm = New Integer(-1, -1) {}
				rep.mergedist = New Double(-1) {}
				Return
			End If
			If npoints = 1 Then
				rep.p = New Integer(0) {}
				rep.z = New Integer(-1, -1) {}
				rep.pz = New Integer(-1, -1) {}
				rep.pm = New Integer(-1, -1) {}
				rep.mergedist = New Double(-1) {}
				rep.p(0) = 0
				Return
			End If
			rep.z = New Integer(npoints - 2, 1) {}
			rep.mergedist = New Double(npoints - 2) {}

			'
			' Build list of nearest neighbors
			'
			nnidx = New Integer(npoints - 1) {}
			For i = 0 To npoints - 1

				'
				' Calculate index of the nearest neighbor
				'
				k = -1
				v = Math.maxrealnumber
				For j = 0 To npoints - 1
					If j <> i AndAlso CDbl(d(i, j)) < CDbl(v) Then
						k = j
						v = d(i, j)
					End If
				Next
				alglib.ap.assert(CDbl(v) < CDbl(Math.maxrealnumber), "ClusterizerRunAHC: internal error")
				nnidx(i) = k
			Next

			'
			' Distance matrix is built, perform merges.
			'
			' NOTE 1: CIdx is array[NPoints] which maps rows/columns of the
			'         distance matrix D to indexes of clusters. Values of CIdx
			'         from [0,NPoints) denote single-point clusters, and values
			'         from [NPoints,2*NPoints-1) denote ones obtained by merging
			'         smaller clusters. Negative calues correspond to absent clusters.
			'
			'         Initially it contains [0...NPoints-1], after each merge
			'         one element of CIdx (one with index C0) is replaced by
			'         NPoints+MergeIdx, and another one with index C1 is
			'         rewritten by -1.
			' 
			' NOTE 2: CSizes is array[NPoints] which stores sizes of clusters.
			'         
			'
			cidx = New Integer(npoints - 1) {}
			csizes = New Integer(npoints - 1) {}
			For i = 0 To npoints - 1
				cidx(i) = i
				csizes(i) = 1
			Next
			For mergeidx = 0 To npoints - 2

				'
				' Select pair of clusters (C0,C1) with CIdx[C0]<CIdx[C1] to merge.
				'
				c0 = -1
				c1 = -1
				v = Math.maxrealnumber
				For i = 0 To npoints - 1
					If cidx(i) >= 0 Then
						If CDbl(d(i, nnidx(i))) < CDbl(v) Then
							c0 = i
							c1 = nnidx(i)
							v = d(i, nnidx(i))
						End If
					End If
				Next
				alglib.ap.assert(CDbl(v) < CDbl(Math.maxrealnumber), "ClusterizerRunAHC: internal error")
				If cidx(c0) > cidx(c1) Then
					i = c1
					c1 = c0
					c0 = i
				End If

				'
				' Fill one row of Rep.Z and one element of Rep.MergeDist
				'
				rep.z(mergeidx, 0) = cidx(c0)
				rep.z(mergeidx, 1) = cidx(c1)
				rep.mergedist(mergeidx) = v

				'
				' Update distance matrix:
				' * row/column C0 are updated by distances to the new cluster
				' * row/column C1 are considered empty (we can fill them by zeros,
				'   but do not want to spend time - we just ignore them)
				'
				' NOTE: it is important to update distance matrix BEFORE CIdx/CSizes
				'       are updated.
				'
				alglib.ap.assert(((s.ahcalgo = 0 OrElse s.ahcalgo = 1) OrElse s.ahcalgo = 2) OrElse s.ahcalgo = 3, "ClusterizerRunAHC: internal error")
				For i = 0 To npoints - 1
					If i <> c0 AndAlso i <> c1 Then
						If s.ahcalgo = 0 Then
							d(i, c0) = System.Math.Max(d(i, c0), d(i, c1))
						End If
						If s.ahcalgo = 1 Then
							d(i, c0) = System.Math.Min(d(i, c0), d(i, c1))
						End If
						If s.ahcalgo = 2 Then
							d(i, c0) = (csizes(c0) * d(i, c0) + csizes(c1) * d(i, c1)) / (csizes(c0) + csizes(c1))
						End If
						If s.ahcalgo = 3 Then
							d(i, c0) = (d(i, c0) + d(i, c1)) / 2
						End If
						d(c0, i) = d(i, c0)
					End If
				Next

				'
				' Update CIdx and CSizes
				'
				cidx(c0) = npoints + mergeidx
				cidx(c1) = -1
				csizes(c0) = csizes(c0) + csizes(c1)
				csizes(c1) = 0

				'
				' Update nearest neighbors array:
				' * update nearest neighbors of everything except for C0/C1
				' * update neighbors of C0/C1
				'
				For i = 0 To npoints - 1
					If (cidx(i) >= 0 AndAlso i <> c0) AndAlso (nnidx(i) = c0 OrElse nnidx(i) = c1) Then

						'
						' I-th cluster which is distinct from C0/C1 has former C0/C1 cluster as its nearest
						' neighbor. We handle this issue depending on specific AHC algorithm being used.
						'
						If s.ahcalgo = 1 Then

							'
							' Single linkage. Merging of two clusters together
							' does NOT change distances between new cluster and
							' other clusters.
							'
							' The only thing we have to do is to update nearest neighbor index
							'
							nnidx(i) = c0
						Else

							'
							' Something other than single linkage. We have to re-examine
							' all the row to find nearest neighbor.
							'
							k = -1
							v = Math.maxrealnumber
							For j = 0 To npoints - 1
								If (cidx(j) >= 0 AndAlso j <> i) AndAlso CDbl(d(i, j)) < CDbl(v) Then
									k = j
									v = d(i, j)
								End If
							Next
							alglib.ap.assert(CDbl(v) < CDbl(Math.maxrealnumber) OrElse mergeidx = npoints - 2, "ClusterizerRunAHC: internal error")
							nnidx(i) = k
						End If
					End If
				Next
				k = -1
				v = Math.maxrealnumber
				For j = 0 To npoints - 1
					If (cidx(j) >= 0 AndAlso j <> c0) AndAlso CDbl(d(c0, j)) < CDbl(v) Then
						k = j
						v = d(c0, j)
					End If
				Next
				alglib.ap.assert(CDbl(v) < CDbl(Math.maxrealnumber) OrElse mergeidx = npoints - 2, "ClusterizerRunAHC: internal error")
				nnidx(c0) = k
			Next

			'
			' Calculate Rep.P and Rep.PM.
			'
			' In order to do that, we fill CInfo matrix - (2*NPoints-1)*3 matrix,
			' with I-th row containing:
			' * CInfo[I,0]     -   size of I-th cluster
			' * CInfo[I,1]     -   beginning of I-th cluster
			' * CInfo[I,2]     -   end of I-th cluster
			' * CInfo[I,3]     -   height of I-th cluster
			'
			' We perform it as follows:
			' * first NPoints clusters have unit size (CInfo[I,0]=1) and zero
			'   height (CInfo[I,3]=0)
			' * we replay NPoints-1 merges from first to last and fill sizes of
			'   corresponding clusters (new size is a sum of sizes of clusters
			'   being merged) and height (new height is max(heights)+1).
			' * now we ready to determine locations of clusters. Last cluster
			'   spans entire dataset, we know it. We replay merges from last to
			'   first, during each merge we already know location of the merge
			'   result, and we can position first cluster to the left part of
			'   the result, and second cluster to the right part.
			'
			rep.p = New Integer(npoints - 1) {}
			rep.pm = New Integer(npoints - 2, 5) {}
			cinfo = New Integer(2 * npoints - 2, 3) {}
			For i = 0 To npoints - 1
				cinfo(i, 0) = 1
				cinfo(i, 3) = 0
			Next
			For i = 0 To npoints - 2
				cinfo(npoints + i, 0) = cinfo(rep.z(i, 0), 0) + cinfo(rep.z(i, 1), 0)
				cinfo(npoints + i, 3) = System.Math.Max(cinfo(rep.z(i, 0), 3), cinfo(rep.z(i, 1), 3)) + 1
			Next
			cinfo(2 * npoints - 2, 1) = 0
			cinfo(2 * npoints - 2, 2) = npoints - 1
			For i = npoints - 2 To 0 Step -1

				'
				' We merge C0 which spans [A0,B0] and C1 (spans [A1,B1]),
				' with unknown A0, B0, A1, B1. However, we know that result
				' is CR, which spans [AR,BR] with known AR/BR, and we know
				' sizes of C0, C1, CR (denotes as S0, S1, SR).
				'
				c0 = rep.z(i, 0)
				c1 = rep.z(i, 1)
				s0 = cinfo(c0, 0)
				s1 = cinfo(c1, 0)
				ar = cinfo(npoints + i, 1)
				br = cinfo(npoints + i, 2)
				cinfo(c0, 1) = ar
				cinfo(c0, 2) = ar + s0 - 1
				cinfo(c1, 1) = br - (s1 - 1)
				cinfo(c1, 2) = br
				rep.pm(i, 0) = cinfo(c0, 1)
				rep.pm(i, 1) = cinfo(c0, 2)
				rep.pm(i, 2) = cinfo(c1, 1)
				rep.pm(i, 3) = cinfo(c1, 2)
				rep.pm(i, 4) = cinfo(c0, 3)
				rep.pm(i, 5) = cinfo(c1, 3)
			Next
			For i = 0 To npoints - 1
				alglib.ap.assert(cinfo(i, 1) = cinfo(i, 2))
				rep.p(i) = cinfo(i, 1)
			Next

			'
			' Calculate Rep.PZ
			'
			rep.pz = New Integer(npoints - 2, 1) {}
			For i = 0 To npoints - 2
				rep.pz(i, 0) = rep.z(i, 0)
				rep.pz(i, 1) = rep.z(i, 1)
				If rep.pz(i, 0) < npoints Then
					rep.pz(i, 0) = rep.p(rep.pz(i, 0))
				End If
				If rep.pz(i, 1) < npoints Then
					rep.pz(i, 1) = rep.p(rep.pz(i, 1))
				End If
			Next
		End Sub


		'************************************************************************
'        This function recursively evaluates distance matrix  for  SOME  (not all!)
'        distance types.
'
'        INPUT PARAMETERS:
'            XY      -   array[?,NFeatures], dataset
'            NFeatures-  number of features, >=1
'            DistType-   distance function:
'                        *  0    Chebyshev distance  (L-inf norm)
'                        *  1    city block distance (L1 norm)
'            D       -   preallocated output matrix
'            I0,I1   -   half interval of rows to calculate: [I0,I1) is processed
'            J0,J1   -   half interval of cols to calculate: [J0,J1) is processed
'
'        OUTPUT PARAMETERS:
'            D       -   array[NPoints,NPoints], distance matrix
'                        upper triangle and main diagonal are initialized with
'                        data.
'
'        NOTE: intersection of [I0,I1) and [J0,J1)  may  completely  lie  in  upper
'              triangle, only partially intersect with it, or have zero intersection.
'              In any case, only intersection of submatrix given by [I0,I1)*[J0,J1)
'              with upper triangle of the matrix is evaluated.
'              
'              Say, for 4x4 distance matrix A:
'              * [0,2)*[0,2) will result in evaluation of A00, A01, A11
'              * [2,4)*[2,4) will result in evaluation of A22, A23, A32, A33
'              * [2,4)*[0,2) will result in evaluation of empty set of elements
'              
'
'          -- ALGLIB --
'             Copyright 07.04.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub evaluatedistancematrixrec(xy As Double(,), nfeatures As Integer, disttype As Integer, d As Double(,), i0 As Integer, i1 As Integer, _
			j0 As Integer, j1 As Integer)
			Dim rcomplexity As Double = 0
			Dim len0 As Integer = 0
			Dim len1 As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0

			alglib.ap.assert(disttype = 0 OrElse disttype = 1, "EvaluateDistanceMatrixRec: incorrect DistType")

			'
			' Normalize J0/J1:
			' * J0:=max(J0,I0) - we ignore lower triangle
			' * J1:=max(J1,J0) - normalize J1
			'
			j0 = System.Math.Max(j0, i0)
			j1 = System.Math.Max(j1, j0)
			If j1 <= j0 OrElse i1 <= i0 Then
				Return
			End If

			'
			' Try to process in parallel. Two condtions must hold in order to
			' activate parallel processing:
			' 1. I1-I0>2 or J1-J0>2
			' 2. (I1-I0)*(J1-J0)*NFeatures>=ParallelComplexity
			'
			' NOTE: all quantities are converted to reals in order to avoid
			'       integer overflow during multiplication
			'
			' NOTE: strict inequality in (1) is necessary to reduce task to 2x2
			'       basecases. In future versions we will be able to handle such
			'       basecases more efficiently than 1x1 cases.
			'
			rcomplexity = i1 - i0
			rcomplexity = rcomplexity * (j1 - j0)
			rcomplexity = rcomplexity * nfeatures
			If CDbl(rcomplexity) >= CDbl(parallelcomplexity) AndAlso (i1 - i0 > 2 OrElse j1 - j0 > 2) Then

				'
				' Recursive division along largest of dimensions
				'
				If i1 - i0 > j1 - j0 Then
					apserv.splitlengtheven(i1 - i0, len0, len1)
					evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i0 + len0, _
						j0, j1)
					evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0 + len0, i1, _
						j0, j1)
				Else
					apserv.splitlengtheven(j1 - j0, len0, len1)
					evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i1, _
						j0, j0 + len0)
					evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i1, _
						j0 + len0, j1)
				End If
				Return
			End If

			'
			' Sequential processing
			'
			For i = i0 To i1 - 1
				For j = j0 To j1 - 1
					If j >= i Then
						v = 0.0
						If disttype = 0 Then
							For k = 0 To nfeatures - 1
								vv = xy(i, k) - xy(j, k)
								If CDbl(vv) < CDbl(0) Then
									vv = -vv
								End If
								If CDbl(vv) > CDbl(v) Then
									v = vv
								End If
							Next
						End If
						If disttype = 1 Then
							For k = 0 To nfeatures - 1
								vv = xy(i, k) - xy(j, k)
								If CDbl(vv) < CDbl(0) Then
									vv = -vv
								End If
								v = v + vv
							Next
						End If
						d(i, j) = v
					End If
				Next
			Next
		End Sub


	End Class
	Public Class datacomp
		'************************************************************************
'        k-means++ clusterization.
'        Backward compatibility function, we recommend to use CLUSTERING subpackage
'        as better replacement.
'
'          -- ALGLIB --
'             Copyright 21.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub kmeansgenerate(xy As Double(,), npoints As Integer, nvars As Integer, k As Integer, restarts As Integer, ByRef info As Integer, _
			ByRef c As Double(,), ByRef xyc As Integer())
			Dim dummy As Double(,) = New Double(-1, -1) {}

			info = 0
			c = New Double(-1, -1) {}
			xyc = New Integer(-1) {}

			clustering.kmeansgenerateinternal(xy, npoints, nvars, k, 0, restarts, _
				info, c, True, dummy, False, xyc)
		End Sub


	End Class
	Public Class dforest
		Public Class decisionforest
			Inherits apobject
			Public nvars As Integer
			Public nclasses As Integer
			Public ntrees As Integer
			Public bufsize As Integer
			Public trees As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				trees = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New decisionforest()
				_result.nvars = nvars
				_result.nclasses = nclasses
				_result.ntrees = ntrees
				_result.bufsize = bufsize
				_result.trees = DirectCast(trees.Clone(), Double())
				Return _result
			End Function
		End Class


		Public Class dfreport
			Inherits apobject
			Public relclserror As Double
			Public avgce As Double
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public oobrelclserror As Double
			Public oobavgce As Double
			Public oobrmserror As Double
			Public oobavgerror As Double
			Public oobavgrelerror As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New dfreport()
				_result.relclserror = relclserror
				_result.avgce = avgce
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				_result.oobrelclserror = oobrelclserror
				_result.oobavgce = oobavgce
				_result.oobrmserror = oobrmserror
				_result.oobavgerror = oobavgerror
				_result.oobavgrelerror = oobavgrelerror
				Return _result
			End Function
		End Class


		Public Class dfinternalbuffers
			Inherits apobject
			Public treebuf As Double()
			Public idxbuf As Integer()
			Public tmpbufr As Double()
			Public tmpbufr2 As Double()
			Public tmpbufi As Integer()
			Public classibuf As Integer()
			Public sortrbuf As Double()
			Public sortrbuf2 As Double()
			Public sortibuf As Integer()
			Public varpool As Integer()
			Public evsbin As Boolean()
			Public evssplits As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				treebuf = New Double(-1) {}
				idxbuf = New Integer(-1) {}
				tmpbufr = New Double(-1) {}
				tmpbufr2 = New Double(-1) {}
				tmpbufi = New Integer(-1) {}
				classibuf = New Integer(-1) {}
				sortrbuf = New Double(-1) {}
				sortrbuf2 = New Double(-1) {}
				sortibuf = New Integer(-1) {}
				varpool = New Integer(-1) {}
				evsbin = New Boolean(-1) {}
				evssplits = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New dfinternalbuffers()
				_result.treebuf = DirectCast(treebuf.Clone(), Double())
				_result.idxbuf = DirectCast(idxbuf.Clone(), Integer())
				_result.tmpbufr = DirectCast(tmpbufr.Clone(), Double())
				_result.tmpbufr2 = DirectCast(tmpbufr2.Clone(), Double())
				_result.tmpbufi = DirectCast(tmpbufi.Clone(), Integer())
				_result.classibuf = DirectCast(classibuf.Clone(), Integer())
				_result.sortrbuf = DirectCast(sortrbuf.Clone(), Double())
				_result.sortrbuf2 = DirectCast(sortrbuf2.Clone(), Double())
				_result.sortibuf = DirectCast(sortibuf.Clone(), Integer())
				_result.varpool = DirectCast(varpool.Clone(), Integer())
				_result.evsbin = DirectCast(evsbin.Clone(), Boolean())
				_result.evssplits = DirectCast(evssplits.Clone(), Double())
				Return _result
			End Function
		End Class




		Public Const innernodewidth As Integer = 3
		Public Const leafnodewidth As Integer = 2
		Public Const dfusestrongsplits As Integer = 1
		Public Const dfuseevs As Integer = 2
		Public Const dffirstversion As Integer = 0


		'************************************************************************
'        This subroutine builds random decision forest.
'
'        INPUT PARAMETERS:
'            XY          -   training set
'            NPoints     -   training set size, NPoints>=1
'            NVars       -   number of independent variables, NVars>=1
'            NClasses    -   task type:
'                            * NClasses=1 - regression task with one
'                                           dependent variable
'                            * NClasses>1 - classification task with
'                                           NClasses classes.
'            NTrees      -   number of trees in a forest, NTrees>=1.
'                            recommended values: 50-100.
'            R           -   percent of a training set used to build
'                            individual trees. 0<R<=1.
'                            recommended values: 0.1 <= R <= 0.66.
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<1, NVars<1, NClasses<1, NTrees<1, R<=0
'                                  or R>1).
'                            *  1, if task has been solved
'            DF          -   model built
'            Rep         -   training report, contains error on a training set
'                            and out-of-bag estimates of generalization error.
'
'          -- ALGLIB --
'             Copyright 19.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfbuildrandomdecisionforest(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ntrees As Integer, r As Double, _
			ByRef info As Integer, df As decisionforest, rep As dfreport)
			Dim samplesize As Integer = 0

			info = 0

			If CDbl(r) <= CDbl(0) OrElse CDbl(r) > CDbl(1) Then
				info = -1
				Return
			End If
			samplesize = System.Math.Max(CInt(System.Math.Truncate(System.Math.Round(r * npoints))), 1)
			dfbuildinternal(xy, npoints, nvars, nclasses, ntrees, samplesize, _
				System.Math.Max(nvars \ 2, 1), dfusestrongsplits + dfuseevs, info, df, rep)
		End Sub


		'************************************************************************
'        This subroutine builds random decision forest.
'        This function gives ability to tune number of variables used when choosing
'        best split.
'
'        INPUT PARAMETERS:
'            XY          -   training set
'            NPoints     -   training set size, NPoints>=1
'            NVars       -   number of independent variables, NVars>=1
'            NClasses    -   task type:
'                            * NClasses=1 - regression task with one
'                                           dependent variable
'                            * NClasses>1 - classification task with
'                                           NClasses classes.
'            NTrees      -   number of trees in a forest, NTrees>=1.
'                            recommended values: 50-100.
'            NRndVars    -   number of variables used when choosing best split
'            R           -   percent of a training set used to build
'                            individual trees. 0<R<=1.
'                            recommended values: 0.1 <= R <= 0.66.
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<1, NVars<1, NClasses<1, NTrees<1, R<=0
'                                  or R>1).
'                            *  1, if task has been solved
'            DF          -   model built
'            Rep         -   training report, contains error on a training set
'                            and out-of-bag estimates of generalization error.
'
'          -- ALGLIB --
'             Copyright 19.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfbuildrandomdecisionforestx1(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ntrees As Integer, nrndvars As Integer, _
			r As Double, ByRef info As Integer, df As decisionforest, rep As dfreport)
			Dim samplesize As Integer = 0

			info = 0

			If CDbl(r) <= CDbl(0) OrElse CDbl(r) > CDbl(1) Then
				info = -1
				Return
			End If
			If nrndvars <= 0 OrElse nrndvars > nvars Then
				info = -1
				Return
			End If
			samplesize = System.Math.Max(CInt(System.Math.Truncate(System.Math.Round(r * npoints))), 1)
			dfbuildinternal(xy, npoints, nvars, nclasses, ntrees, samplesize, _
				nrndvars, dfusestrongsplits + dfuseevs, info, df, rep)
		End Sub


		Public Shared Sub dfbuildinternal(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ntrees As Integer, samplesize As Integer, _
			nfeatures As Integer, flags As Integer, ByRef info As Integer, df As decisionforest, rep As dfreport)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim tmpi As Integer = 0
			Dim lasttreeoffs As Integer = 0
			Dim offs As Integer = 0
			Dim ooboffs As Integer = 0
			Dim treesize As Integer = 0
			Dim nvarsinpool As Integer = 0
			Dim useevs As New Boolean()
			Dim bufs As New dfinternalbuffers()
			Dim permbuf As Integer() = New Integer(-1) {}
			Dim oobbuf As Double() = New Double(-1) {}
			Dim oobcntbuf As Integer() = New Integer(-1) {}
			Dim xys As Double(,) = New Double(-1, -1) {}
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim oobcnt As Integer = 0
			Dim oobrelcnt As Integer = 0
			Dim v As Double = 0
			Dim vmin As Double = 0
			Dim vmax As Double = 0
			Dim bflag As New Boolean()
			Dim rs As New hqrnd.hqrndstate()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			info = 0


			'
			' Test for inputs
			'
			If (((((npoints < 1 OrElse samplesize < 1) OrElse samplesize > npoints) OrElse nvars < 1) OrElse nclasses < 1) OrElse ntrees < 1) OrElse nfeatures < 1 Then
				info = -1
				Return
			End If
			If nclasses > 1 Then
				For i = 0 To npoints - 1
					If CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) >= nclasses Then
						info = -2
						Return
					End If
				Next
			End If
			info = 1

			'
			' Flags
			'
			useevs = flags \ dfuseevs Mod 2 <> 0

			'
			' Allocate data, prepare header
			'
			treesize = 1 + innernodewidth * (samplesize - 1) + leafnodewidth * samplesize
			permbuf = New Integer(npoints - 1) {}
			bufs.treebuf = New Double(treesize - 1) {}
			bufs.idxbuf = New Integer(npoints - 1) {}
			bufs.tmpbufr = New Double(npoints - 1) {}
			bufs.tmpbufr2 = New Double(npoints - 1) {}
			bufs.tmpbufi = New Integer(npoints - 1) {}
			bufs.sortrbuf = New Double(npoints - 1) {}
			bufs.sortrbuf2 = New Double(npoints - 1) {}
			bufs.sortibuf = New Integer(npoints - 1) {}
			bufs.varpool = New Integer(nvars - 1) {}
			bufs.evsbin = New Boolean(nvars - 1) {}
			bufs.evssplits = New Double(nvars - 1) {}
			bufs.classibuf = New Integer(2 * nclasses - 1) {}
			oobbuf = New Double(nclasses * npoints - 1) {}
			oobcntbuf = New Integer(npoints - 1) {}
			df.trees = New Double(ntrees * treesize - 1) {}
			xys = New Double(samplesize - 1, nvars) {}
			x = New Double(nvars - 1) {}
			y = New Double(nclasses - 1) {}
			For i = 0 To npoints - 1
				permbuf(i) = i
			Next
			For i = 0 To npoints * nclasses - 1
				oobbuf(i) = 0
			Next
			For i = 0 To npoints - 1
				oobcntbuf(i) = 0
			Next

			'
			' Prepare variable pool and EVS (extended variable selection/splitting) buffers
			' (whether EVS is turned on or not):
			' 1. detect binary variables and pre-calculate splits for them
			' 2. detect variables with non-distinct values and exclude them from pool
			'
			For i = 0 To nvars - 1
				bufs.varpool(i) = i
			Next
			nvarsinpool = nvars
			If useevs Then
				For j = 0 To nvars - 1
					vmin = xy(0, j)
					vmax = vmin
					For i = 0 To npoints - 1
						v = xy(i, j)
						vmin = System.Math.Min(vmin, v)
						vmax = System.Math.Max(vmax, v)
					Next
					If CDbl(vmin) = CDbl(vmax) Then

						'
						' exclude variable from pool
						'
						bufs.varpool(j) = bufs.varpool(nvarsinpool - 1)
						bufs.varpool(nvarsinpool - 1) = -1
						nvarsinpool = nvarsinpool - 1
						Continue For
					End If
					bflag = False
					For i = 0 To npoints - 1
						v = xy(i, j)
						If CDbl(v) <> CDbl(vmin) AndAlso CDbl(v) <> CDbl(vmax) Then
							bflag = True
							Exit For
						End If
					Next
					If bflag Then

						'
						' non-binary variable
						'
						bufs.evsbin(j) = False
					Else

						'
						' Prepare
						'
						bufs.evsbin(j) = True
						bufs.evssplits(j) = 0.5 * (vmin + vmax)
						If CDbl(bufs.evssplits(j)) <= CDbl(vmin) Then
							bufs.evssplits(j) = vmax
						End If
					End If
				Next
			End If

			'
			' RANDOM FOREST FORMAT
			' W[0]         -   size of array
			' W[1]         -   version number
			' W[2]         -   NVars
			' W[3]         -   NClasses (1 for regression)
			' W[4]         -   NTrees
			' W[5]         -   trees offset
			'
			'
			' TREE FORMAT
			' W[Offs]      -   size of sub-array
			'     node info:
			' W[K+0]       -   variable number        (-1 for leaf mode)
			' W[K+1]       -   threshold              (class/value for leaf node)
			' W[K+2]       -   ">=" branch index      (absent for leaf node)
			'
			'
			df.nvars = nvars
			df.nclasses = nclasses
			df.ntrees = ntrees

			'
			' Build forest
			'
			hqrnd.hqrndrandomize(rs)
			offs = 0
			For i = 0 To ntrees - 1

				'
				' Prepare sample
				'
				For k = 0 To samplesize - 1
					j = k + hqrnd.hqrnduniformi(rs, npoints - k)
					tmpi = permbuf(k)
					permbuf(k) = permbuf(j)
					permbuf(j) = tmpi
					j = permbuf(k)
					For i_ = 0 To nvars
						xys(k, i_) = xy(j, i_)
					Next
				Next

				'
				' build tree, copy
				'
				dfbuildtree(xys, samplesize, nvars, nclasses, nfeatures, nvarsinpool, _
					flags, bufs, rs)
				j = CInt(System.Math.Truncate(System.Math.Round(bufs.treebuf(0))))
				i1_ = (0) - (offs)
				For i_ = offs To offs + j - 1
					df.trees(i_) = bufs.treebuf(i_ + i1_)
				Next
				lasttreeoffs = offs
				offs = offs + j

				'
				' OOB estimates
				'
				For k = samplesize To npoints - 1
					For j = 0 To nclasses - 1
						y(j) = 0
					Next
					j = permbuf(k)
					For i_ = 0 To nvars - 1
						x(i_) = xy(j, i_)
					Next
					dfprocessinternal(df, lasttreeoffs, x, y)
					i1_ = (0) - (j * nclasses)
					For i_ = j * nclasses To (j + 1) * nclasses - 1
						oobbuf(i_) = oobbuf(i_) + y(i_ + i1_)
					Next
					oobcntbuf(j) = oobcntbuf(j) + 1
				Next
			Next
			df.bufsize = offs

			'
			' Normalize OOB results
			'
			For i = 0 To npoints - 1
				If oobcntbuf(i) <> 0 Then
					v = CDbl(1) / CDbl(oobcntbuf(i))
					For i_ = i * nclasses To i * nclasses + nclasses - 1
						oobbuf(i_) = v * oobbuf(i_)
					Next
				End If
			Next

			'
			' Calculate training set estimates
			'
			rep.relclserror = dfrelclserror(df, xy, npoints)
			rep.avgce = dfavgce(df, xy, npoints)
			rep.rmserror = dfrmserror(df, xy, npoints)
			rep.avgerror = dfavgerror(df, xy, npoints)
			rep.avgrelerror = dfavgrelerror(df, xy, npoints)

			'
			' Calculate OOB estimates.
			'
			rep.oobrelclserror = 0
			rep.oobavgce = 0
			rep.oobrmserror = 0
			rep.oobavgerror = 0
			rep.oobavgrelerror = 0
			oobcnt = 0
			oobrelcnt = 0
			For i = 0 To npoints - 1
				If oobcntbuf(i) <> 0 Then
					ooboffs = i * nclasses
					If nclasses > 1 Then

						'
						' classification-specific code
						'
						k = CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars))))
						tmpi = 0
						For j = 1 To nclasses - 1
							If CDbl(oobbuf(ooboffs + j)) > CDbl(oobbuf(ooboffs + tmpi)) Then
								tmpi = j
							End If
						Next
						If tmpi <> k Then
							rep.oobrelclserror = rep.oobrelclserror + 1
						End If
						If CDbl(oobbuf(ooboffs + k)) <> CDbl(0) Then
							rep.oobavgce = rep.oobavgce - System.Math.Log(oobbuf(ooboffs + k))
						Else
							rep.oobavgce = rep.oobavgce - System.Math.Log(Math.minrealnumber)
						End If
						For j = 0 To nclasses - 1
							If j = k Then
								rep.oobrmserror = rep.oobrmserror + Math.sqr(oobbuf(ooboffs + j) - 1)
								rep.oobavgerror = rep.oobavgerror + System.Math.Abs(oobbuf(ooboffs + j) - 1)
								rep.oobavgrelerror = rep.oobavgrelerror + System.Math.Abs(oobbuf(ooboffs + j) - 1)
								oobrelcnt = oobrelcnt + 1
							Else
								rep.oobrmserror = rep.oobrmserror + Math.sqr(oobbuf(ooboffs + j))
								rep.oobavgerror = rep.oobavgerror + System.Math.Abs(oobbuf(ooboffs + j))
							End If
						Next
					Else

						'
						' regression-specific code
						'
						rep.oobrmserror = rep.oobrmserror + Math.sqr(oobbuf(ooboffs) - xy(i, nvars))
						rep.oobavgerror = rep.oobavgerror + System.Math.Abs(oobbuf(ooboffs) - xy(i, nvars))
						If CDbl(xy(i, nvars)) <> CDbl(0) Then
							rep.oobavgrelerror = rep.oobavgrelerror + System.Math.Abs((oobbuf(ooboffs) - xy(i, nvars)) / xy(i, nvars))
							oobrelcnt = oobrelcnt + 1
						End If
					End If

					'
					' update OOB estimates count.
					'
					oobcnt = oobcnt + 1
				End If
			Next
			If oobcnt > 0 Then
				rep.oobrelclserror = rep.oobrelclserror / oobcnt
				rep.oobavgce = rep.oobavgce / oobcnt
				rep.oobrmserror = System.Math.sqrt(rep.oobrmserror / (oobcnt * nclasses))
				rep.oobavgerror = rep.oobavgerror / (oobcnt * nclasses)
				If oobrelcnt > 0 Then
					rep.oobavgrelerror = rep.oobavgrelerror / oobrelcnt
				End If
			End If
		End Sub


		'************************************************************************
'        Procesing
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            X       -   input vector,  array[0..NVars-1].
'
'        OUTPUT PARAMETERS:
'            Y       -   result. Regression estimate when solving regression  task,
'                        vector of posterior probabilities for classification task.
'
'        See also DFProcessI.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfprocess(df As decisionforest, x As Double(), ByRef y As Double())
			Dim offs As Integer = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0


			'
			' Proceed
			'
			If alglib.ap.len(y) < df.nclasses Then
				y = New Double(df.nclasses - 1) {}
			End If
			offs = 0
			For i = 0 To df.nclasses - 1
				y(i) = 0
			Next
			For i = 0 To df.ntrees - 1

				'
				' Process basic tree
				'
				dfprocessinternal(df, offs, x, y)

				'
				' Next tree
				'
				offs = offs + CInt(System.Math.Truncate(System.Math.Round(df.trees(offs))))
			Next
			v = CDbl(1) / CDbl(df.ntrees)
			For i_ = 0 To df.nclasses - 1
				y(i_) = v * y(i_)
			Next
		End Sub


		'************************************************************************
'        'interactive' variant of DFProcess for languages like Python which support
'        constructs like "Y = DFProcessI(DF,X)" and interactive mode of interpreter
'
'        This function allocates new array on each call,  so  it  is  significantly
'        slower than its 'non-interactive' counterpart, but it is  more  convenient
'        when you call it from command line.
'
'          -- ALGLIB --
'             Copyright 28.02.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfprocessi(df As decisionforest, x As Double(), ByRef y As Double())
			y = New Double(-1) {}

			dfprocess(df, x, y)
		End Sub


		'************************************************************************
'        Relative classification error on the test set
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            percent of incorrectly classified cases.
'            Zero if model solves regression task.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dfrelclserror(df As decisionforest, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			result = CDbl(dfclserror(df, xy, npoints)) / CDbl(npoints)
			Return result
		End Function


		'************************************************************************
'        Average cross-entropy (in bits per element) on the test set
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            CrossEntropy/(NPoints*LN(2)).
'            Zero if model solves regression task.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dfavgce(df As decisionforest, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim tmpi As Integer = 0
			Dim i_ As Integer = 0

			x = New Double(df.nvars - 1) {}
			y = New Double(df.nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1
				For i_ = 0 To df.nvars - 1
					x(i_) = xy(i, i_)
				Next
				dfprocess(df, x, y)
				If df.nclasses > 1 Then

					'
					' classification-specific code
					'
					k = CInt(System.Math.Truncate(System.Math.Round(xy(i, df.nvars))))
					tmpi = 0
					For j = 1 To df.nclasses - 1
						If CDbl(y(j)) > CDbl(y(tmpi)) Then
							tmpi = j
						End If
					Next
					If CDbl(y(k)) <> CDbl(0) Then
						result = result - System.Math.Log(y(k))
					Else
						result = result - System.Math.Log(Math.minrealnumber)
					End If
				End If
			Next
			result = result / npoints
			Return result
		End Function


		'************************************************************************
'        RMS error on the test set
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            root mean square error.
'            Its meaning for regression task is obvious. As for
'            classification task, RMS error means error when estimating posterior
'            probabilities.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dfrmserror(df As decisionforest, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim tmpi As Integer = 0
			Dim i_ As Integer = 0

			x = New Double(df.nvars - 1) {}
			y = New Double(df.nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1
				For i_ = 0 To df.nvars - 1
					x(i_) = xy(i, i_)
				Next
				dfprocess(df, x, y)
				If df.nclasses > 1 Then

					'
					' classification-specific code
					'
					k = CInt(System.Math.Truncate(System.Math.Round(xy(i, df.nvars))))
					tmpi = 0
					For j = 1 To df.nclasses - 1
						If CDbl(y(j)) > CDbl(y(tmpi)) Then
							tmpi = j
						End If
					Next
					For j = 0 To df.nclasses - 1
						If j = k Then
							result = result + Math.sqr(y(j) - 1)
						Else
							result = result + Math.sqr(y(j))
						End If
					Next
				Else

					'
					' regression-specific code
					'
					result = result + Math.sqr(y(0) - xy(i, df.nvars))
				End If
			Next
			result = System.Math.sqrt(result / (npoints * df.nclasses))
			Return result
		End Function


		'************************************************************************
'        Average error on the test set
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            Its meaning for regression task is obvious. As for
'            classification task, it means average error when estimating posterior
'            probabilities.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dfavgerror(df As decisionforest, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim i_ As Integer = 0

			x = New Double(df.nvars - 1) {}
			y = New Double(df.nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1
				For i_ = 0 To df.nvars - 1
					x(i_) = xy(i, i_)
				Next
				dfprocess(df, x, y)
				If df.nclasses > 1 Then

					'
					' classification-specific code
					'
					k = CInt(System.Math.Truncate(System.Math.Round(xy(i, df.nvars))))
					For j = 0 To df.nclasses - 1
						If j = k Then
							result = result + System.Math.Abs(y(j) - 1)
						Else
							result = result + System.Math.Abs(y(j))
						End If
					Next
				Else

					'
					' regression-specific code
					'
					result = result + System.Math.Abs(y(0) - xy(i, df.nvars))
				End If
			Next
			result = result / (npoints * df.nclasses)
			Return result
		End Function


		'************************************************************************
'        Average relative error on the test set
'
'        INPUT PARAMETERS:
'            DF      -   decision forest model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            Its meaning for regression task is obvious. As for
'            classification task, it means average relative error when estimating
'            posterior probability of belonging to the correct class.
'
'          -- ALGLIB --
'             Copyright 16.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function dfavgrelerror(df As decisionforest, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim relcnt As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim i_ As Integer = 0

			x = New Double(df.nvars - 1) {}
			y = New Double(df.nclasses - 1) {}
			result = 0
			relcnt = 0
			For i = 0 To npoints - 1
				For i_ = 0 To df.nvars - 1
					x(i_) = xy(i, i_)
				Next
				dfprocess(df, x, y)
				If df.nclasses > 1 Then

					'
					' classification-specific code
					'
					k = CInt(System.Math.Truncate(System.Math.Round(xy(i, df.nvars))))
					For j = 0 To df.nclasses - 1
						If j = k Then
							result = result + System.Math.Abs(y(j) - 1)
							relcnt = relcnt + 1
						End If
					Next
				Else

					'
					' regression-specific code
					'
					If CDbl(xy(i, df.nvars)) <> CDbl(0) Then
						result = result + System.Math.Abs((y(0) - xy(i, df.nvars)) / xy(i, df.nvars))
						relcnt = relcnt + 1
					End If
				End If
			Next
			If relcnt > 0 Then
				result = result / relcnt
			End If
			Return result
		End Function


		'************************************************************************
'        Copying of DecisionForest strucure
'
'        INPUT PARAMETERS:
'            DF1 -   original
'
'        OUTPUT PARAMETERS:
'            DF2 -   copy
'
'          -- ALGLIB --
'             Copyright 13.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfcopy(df1 As decisionforest, df2 As decisionforest)
			Dim i_ As Integer = 0

			df2.nvars = df1.nvars
			df2.nclasses = df1.nclasses
			df2.ntrees = df1.ntrees
			df2.bufsize = df1.bufsize
			df2.trees = New Double(df1.bufsize - 1) {}
			For i_ = 0 To df1.bufsize - 1
				df2.trees(i_) = df1.trees(i_)
			Next
		End Sub


		'************************************************************************
'        Serializer: allocation
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfalloc(s As alglib.serializer, forest As decisionforest)
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			apserv.allocrealarray(s, forest.trees, forest.bufsize)
		End Sub


		'************************************************************************
'        Serializer: serialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfserialize(s As alglib.serializer, forest As decisionforest)
			s.serialize_int(scodes.getrdfserializationcode())
			s.serialize_int(dffirstversion)
			s.serialize_int(forest.nvars)
			s.serialize_int(forest.nclasses)
			s.serialize_int(forest.ntrees)
			s.serialize_int(forest.bufsize)
			apserv.serializerealarray(s, forest.trees, forest.bufsize)
		End Sub


		'************************************************************************
'        Serializer: unserialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub dfunserialize(s As alglib.serializer, forest As decisionforest)
			Dim i0 As Integer = 0
			Dim i1 As Integer = 0


			'
			' check correctness of header
			'
			i0 = s.unserialize_int()
			alglib.ap.assert(i0 = scodes.getrdfserializationcode(), "DFUnserialize: stream header corrupted")
			i1 = s.unserialize_int()
			alglib.ap.assert(i1 = dffirstversion, "DFUnserialize: stream header corrupted")

			'
			' Unserialize data
			'
			forest.nvars = s.unserialize_int()
			forest.nclasses = s.unserialize_int()
			forest.ntrees = s.unserialize_int()
			forest.bufsize = s.unserialize_int()
			apserv.unserializerealarray(s, forest.trees)
		End Sub


		'************************************************************************
'        Classification error
'        ************************************************************************

		Private Shared Function dfclserror(df As decisionforest, xy As Double(,), npoints As Integer) As Integer
			Dim result As Integer = 0
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim tmpi As Integer = 0
			Dim i_ As Integer = 0

			If df.nclasses <= 1 Then
				result = 0
				Return result
			End If
			x = New Double(df.nvars - 1) {}
			y = New Double(df.nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1
				For i_ = 0 To df.nvars - 1
					x(i_) = xy(i, i_)
				Next
				dfprocess(df, x, y)
				k = CInt(System.Math.Truncate(System.Math.Round(xy(i, df.nvars))))
				tmpi = 0
				For j = 1 To df.nclasses - 1
					If CDbl(y(j)) > CDbl(y(tmpi)) Then
						tmpi = j
					End If
				Next
				If tmpi <> k Then
					result = result + 1
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        Internal subroutine for processing one decision tree starting at Offs
'        ************************************************************************

		Private Shared Sub dfprocessinternal(df As decisionforest, offs As Integer, x As Double(), ByRef y As Double())
			Dim k As Integer = 0
			Dim idx As Integer = 0


			'
			' Set pointer to the root
			'
			k = offs + 1

			'
			' Navigate through the tree
			'
			While True
				If CDbl(df.trees(k)) = CDbl(-1) Then
					If df.nclasses = 1 Then
						y(0) = y(0) + df.trees(k + 1)
					Else
						idx = CInt(System.Math.Truncate(System.Math.Round(df.trees(k + 1))))
						y(idx) = y(idx) + 1
					End If
					Exit While
				End If
				If CDbl(x(CInt(System.Math.Truncate(System.Math.Round(df.trees(k)))))) < CDbl(df.trees(k + 1)) Then
					k = k + innernodewidth
				Else
					k = offs + CInt(System.Math.Truncate(System.Math.Round(df.trees(k + 2))))
				End If
			End While
		End Sub


		'************************************************************************
'        Builds one decision tree. Just a wrapper for the DFBuildTreeRec.
'        ************************************************************************

		Private Shared Sub dfbuildtree(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, nfeatures As Integer, nvarsinpool As Integer, _
			flags As Integer, bufs As dfinternalbuffers, rs As hqrnd.hqrndstate)
			Dim numprocessed As Integer = 0
			Dim i As Integer = 0

			alglib.ap.assert(npoints > 0)

			'
			' Prepare IdxBuf. It stores indices of the training set elements.
			' When training set is being split, contents of IdxBuf is
			' correspondingly reordered so we can know which elements belong
			' to which branch of decision tree.
			'
			For i = 0 To npoints - 1
				bufs.idxbuf(i) = i
			Next

			'
			' Recursive procedure
			'
			numprocessed = 1
			dfbuildtreerec(xy, npoints, nvars, nclasses, nfeatures, nvarsinpool, _
				flags, numprocessed, 0, npoints - 1, bufs, rs)
			bufs.treebuf(0) = numprocessed
		End Sub


		'************************************************************************
'        Builds one decision tree (internal recursive subroutine)
'
'        Parameters:
'            TreeBuf     -   large enough array, at least TreeSize
'            IdxBuf      -   at least NPoints elements
'            TmpBufR     -   at least NPoints
'            TmpBufR2    -   at least NPoints
'            TmpBufI     -   at least NPoints
'            TmpBufI2    -   at least NPoints+1
'        ************************************************************************

		Private Shared Sub dfbuildtreerec(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, nfeatures As Integer, nvarsinpool As Integer, _
			flags As Integer, ByRef numprocessed As Integer, idx1 As Integer, idx2 As Integer, bufs As dfinternalbuffers, rs As hqrnd.hqrndstate)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim bflag As New Boolean()
			Dim i1 As Integer = 0
			Dim i2 As Integer = 0
			Dim info As Integer = 0
			Dim sl As Double = 0
			Dim sr As Double = 0
			Dim w As Double = 0
			Dim idxbest As Integer = 0
			Dim ebest As Double = 0
			Dim tbest As Double = 0
			Dim varcur As Integer = 0
			Dim s As Double = 0
			Dim v As Double = 0
			Dim v1 As Double = 0
			Dim v2 As Double = 0
			Dim threshold As Double = 0
			Dim oldnp As Integer = 0
			Dim currms As Double = 0
			Dim useevs As New Boolean()


			'
			' these initializers are not really necessary,
			' but without them compiler complains about uninitialized locals
			'
			tbest = 0

			'
			' Prepare
			'
			alglib.ap.assert(npoints > 0)
			alglib.ap.assert(idx2 >= idx1)
			useevs = flags \ dfuseevs Mod 2 <> 0

			'
			' Leaf node
			'
			If idx2 = idx1 Then
				bufs.treebuf(numprocessed) = -1
				bufs.treebuf(numprocessed + 1) = xy(bufs.idxbuf(idx1), nvars)
				numprocessed = numprocessed + leafnodewidth
				Return
			End If

			'
			' Non-leaf node.
			' Select random variable, prepare split:
			' 1. prepare default solution - no splitting, class at random
			' 2. investigate possible splits, compare with default/best
			'
			idxbest = -1
			If nclasses > 1 Then

				'
				' default solution for classification
				'
				For i = 0 To nclasses - 1
					bufs.classibuf(i) = 0
				Next
				s = idx2 - idx1 + 1
				For i = idx1 To idx2
					j = CInt(System.Math.Truncate(System.Math.Round(xy(bufs.idxbuf(i), nvars))))
					bufs.classibuf(j) = bufs.classibuf(j) + 1
				Next
				ebest = 0
				For i = 0 To nclasses - 1
					ebest = ebest + bufs.classibuf(i) * Math.sqr(1 - bufs.classibuf(i) / s) + (s - bufs.classibuf(i)) * Math.sqr(bufs.classibuf(i) / s)
				Next
				ebest = System.Math.sqrt(ebest / (nclasses * (idx2 - idx1 + 1)))
			Else

				'
				' default solution for regression
				'
				v = 0
				For i = idx1 To idx2
					v = v + xy(bufs.idxbuf(i), nvars)
				Next
				v = v / (idx2 - idx1 + 1)
				ebest = 0
				For i = idx1 To idx2
					ebest = ebest + Math.sqr(xy(bufs.idxbuf(i), nvars) - v)
				Next
				ebest = System.Math.sqrt(ebest / (idx2 - idx1 + 1))
			End If
			i = 0
			While i <= System.Math.Min(nfeatures, nvarsinpool) - 1

				'
				' select variables from pool
				'
				j = i + hqrnd.hqrnduniformi(rs, nvarsinpool - i)
				k = bufs.varpool(i)
				bufs.varpool(i) = bufs.varpool(j)
				bufs.varpool(j) = k
				varcur = bufs.varpool(i)

				'
				' load variable values to working array
				'
				' apply EVS preprocessing: if all variable values are same,
				' variable is excluded from pool.
				'
				' This is necessary for binary pre-splits (see later) to work.
				'
				For j = idx1 To idx2
					bufs.tmpbufr(j - idx1) = xy(bufs.idxbuf(j), varcur)
				Next
				If useevs Then
					bflag = False
					v = bufs.tmpbufr(0)
					For j = 0 To idx2 - idx1
						If CDbl(bufs.tmpbufr(j)) <> CDbl(v) Then
							bflag = True
							Exit For
						End If
					Next
					If Not bflag Then

						'
						' exclude variable from pool,
						' go to the next iteration.
						' I is not increased.
						'
						k = bufs.varpool(i)
						bufs.varpool(i) = bufs.varpool(nvarsinpool - 1)
						bufs.varpool(nvarsinpool - 1) = k
						nvarsinpool = nvarsinpool - 1
						Continue While
					End If
				End If

				'
				' load labels to working array
				'
				If nclasses > 1 Then
					For j = idx1 To idx2
						bufs.tmpbufi(j - idx1) = CInt(System.Math.Truncate(System.Math.Round(xy(bufs.idxbuf(j), nvars))))
					Next
				Else
					For j = idx1 To idx2
						bufs.tmpbufr2(j - idx1) = xy(bufs.idxbuf(j), nvars)
					Next
				End If

				'
				' calculate split
				'
				If useevs AndAlso bufs.evsbin(varcur) Then

					'
					' Pre-calculated splits for binary variables.
					' Threshold is already known, just calculate RMS error
					'
					threshold = bufs.evssplits(varcur)
					If nclasses > 1 Then

						'
						' classification-specific code
						'
						For j = 0 To 2 * nclasses - 1
							bufs.classibuf(j) = 0
						Next
						sl = 0
						sr = 0
						For j = 0 To idx2 - idx1
							k = bufs.tmpbufi(j)
							If CDbl(bufs.tmpbufr(j)) < CDbl(threshold) Then
								bufs.classibuf(k) = bufs.classibuf(k) + 1
								sl = sl + 1
							Else
								bufs.classibuf(k + nclasses) = bufs.classibuf(k + nclasses) + 1
								sr = sr + 1
							End If
						Next
						alglib.ap.assert(CDbl(sl) <> CDbl(0) AndAlso CDbl(sr) <> CDbl(0), "DFBuildTreeRec: something strange!")
						currms = 0
						For j = 0 To nclasses - 1
							w = bufs.classibuf(j)
							currms = currms + w * Math.sqr(w / sl - 1)
							currms = currms + (sl - w) * Math.sqr(w / sl)
							w = bufs.classibuf(nclasses + j)
							currms = currms + w * Math.sqr(w / sr - 1)
							currms = currms + (sr - w) * Math.sqr(w / sr)
						Next
						currms = System.Math.sqrt(currms / (nclasses * (idx2 - idx1 + 1)))
					Else

						'
						' regression-specific code
						'
						sl = 0
						sr = 0
						v1 = 0
						v2 = 0
						For j = 0 To idx2 - idx1
							If CDbl(bufs.tmpbufr(j)) < CDbl(threshold) Then
								v1 = v1 + bufs.tmpbufr2(j)
								sl = sl + 1
							Else
								v2 = v2 + bufs.tmpbufr2(j)
								sr = sr + 1
							End If
						Next
						alglib.ap.assert(CDbl(sl) <> CDbl(0) AndAlso CDbl(sr) <> CDbl(0), "DFBuildTreeRec: something strange!")
						v1 = v1 / sl
						v2 = v2 / sr
						currms = 0
						For j = 0 To idx2 - idx1
							If CDbl(bufs.tmpbufr(j)) < CDbl(threshold) Then
								currms = currms + Math.sqr(v1 - bufs.tmpbufr2(j))
							Else
								currms = currms + Math.sqr(v2 - bufs.tmpbufr2(j))
							End If
						Next
						currms = System.Math.sqrt(currms / (idx2 - idx1 + 1))
					End If
					info = 1
				Else

					'
					' Generic splits
					'
					If nclasses > 1 Then
						dfsplitc(bufs.tmpbufr, bufs.tmpbufi, bufs.classibuf, idx2 - idx1 + 1, nclasses, dfusestrongsplits, _
							info, threshold, currms, bufs.sortrbuf, bufs.sortibuf)
					Else
						dfsplitr(bufs.tmpbufr, bufs.tmpbufr2, idx2 - idx1 + 1, dfusestrongsplits, info, threshold, _
							currms, bufs.sortrbuf, bufs.sortrbuf2)
					End If
				End If
				If info > 0 Then
					If CDbl(currms) <= CDbl(ebest) Then
						ebest = currms
						idxbest = varcur
						tbest = threshold
					End If
				End If

				'
				' Next iteration
				'
				i = i + 1
			End While

			'
			' to split or not to split
			'
			If idxbest < 0 Then

				'
				' All values are same, cannot split.
				'
				bufs.treebuf(numprocessed) = -1
				If nclasses > 1 Then

					'
					' Select random class label (randomness allows us to
					' approximate distribution of the classes)
					'
					bufs.treebuf(numprocessed + 1) = CInt(System.Math.Truncate(System.Math.Round(xy(bufs.idxbuf(idx1 + hqrnd.hqrnduniformi(rs, idx2 - idx1 + 1)), nvars))))
				Else

					'
					' Select average (for regression task).
					'
					v = 0
					For i = idx1 To idx2
						v = v + xy(bufs.idxbuf(i), nvars) / (idx2 - idx1 + 1)
					Next
					bufs.treebuf(numprocessed + 1) = v
				End If
				numprocessed = numprocessed + leafnodewidth
			Else

				'
				' we can split
				'
				bufs.treebuf(numprocessed) = idxbest
				bufs.treebuf(numprocessed + 1) = tbest
				i1 = idx1
				i2 = idx2
				While i1 <= i2

					'
					' Reorder indices so that left partition is in [Idx1..I1-1],
					' and right partition is in [I2+1..Idx2]
					'
					If CDbl(xy(bufs.idxbuf(i1), idxbest)) < CDbl(tbest) Then
						i1 = i1 + 1
						Continue While
					End If
					If CDbl(xy(bufs.idxbuf(i2), idxbest)) >= CDbl(tbest) Then
						i2 = i2 - 1
						Continue While
					End If
					j = bufs.idxbuf(i1)
					bufs.idxbuf(i1) = bufs.idxbuf(i2)
					bufs.idxbuf(i2) = j
					i1 = i1 + 1
					i2 = i2 - 1
				End While
				oldnp = numprocessed
				numprocessed = numprocessed + innernodewidth
				dfbuildtreerec(xy, npoints, nvars, nclasses, nfeatures, nvarsinpool, _
					flags, numprocessed, idx1, i1 - 1, bufs, rs)
				bufs.treebuf(oldnp + 2) = numprocessed
				dfbuildtreerec(xy, npoints, nvars, nclasses, nfeatures, nvarsinpool, _
					flags, numprocessed, i2 + 1, idx2, bufs, rs)
			End If
		End Sub


		'************************************************************************
'        Makes split on attribute
'        ************************************************************************

		Private Shared Sub dfsplitc(ByRef x As Double(), ByRef c As Integer(), ByRef cntbuf As Integer(), n As Integer, nc As Integer, flags As Integer, _
			ByRef info As Integer, ByRef threshold As Double, ByRef e As Double, ByRef sortrbuf As Double(), ByRef sortibuf As Integer())
			Dim i As Integer = 0
			Dim neq As Integer = 0
			Dim nless As Integer = 0
			Dim ngreater As Integer = 0
			Dim q As Integer = 0
			Dim qmin As Integer = 0
			Dim qmax As Integer = 0
			Dim qcnt As Integer = 0
			Dim cursplit As Double = 0
			Dim nleft As Integer = 0
			Dim v As Double = 0
			Dim cure As Double = 0
			Dim w As Double = 0
			Dim sl As Double = 0
			Dim sr As Double = 0

			info = 0
			threshold = 0
			e = 0

			tsort.tagsortfasti(x, c, sortrbuf, sortibuf, n)
			e = Math.maxrealnumber
			threshold = 0.5 * (x(0) + x(n - 1))
			info = -3
			If flags \ dfusestrongsplits Mod 2 = 0 Then

				'
				' weak splits, split at half
				'
				qcnt = 2
				qmin = 1
				qmax = 1
			Else

				'
				' strong splits: choose best quartile
				'
				qcnt = 4
				qmin = 1
				qmax = 3
			End If
			For q = qmin To qmax
				cursplit = x(n * q \ qcnt)
				neq = 0
				nless = 0
				ngreater = 0
				For i = 0 To n - 1
					If CDbl(x(i)) < CDbl(cursplit) Then
						nless = nless + 1
					End If
					If CDbl(x(i)) = CDbl(cursplit) Then
						neq = neq + 1
					End If
					If CDbl(x(i)) > CDbl(cursplit) Then
						ngreater = ngreater + 1
					End If
				Next
				alglib.ap.assert(neq <> 0, "DFSplitR: NEq=0, something strange!!!")
				If nless <> 0 OrElse ngreater <> 0 Then

					'
					' set threshold between two partitions, with
					' some tweaking to avoid problems with floating point
					' arithmetics.
					'
					' The problem is that when you calculates C = 0.5*(A+B) there
					' can be no C which lies strictly between A and B (for example,
					' there is no floating point number which is
					' greater than 1 and less than 1+eps). In such situations
					' we choose right side as theshold (remember that
					' points which lie on threshold falls to the right side).
					'
					If nless < ngreater Then
						cursplit = 0.5 * (x(nless + neq - 1) + x(nless + neq))
						nleft = nless + neq
						If CDbl(cursplit) <= CDbl(x(nless + neq - 1)) Then
							cursplit = x(nless + neq)
						End If
					Else
						cursplit = 0.5 * (x(nless - 1) + x(nless))
						nleft = nless
						If CDbl(cursplit) <= CDbl(x(nless - 1)) Then
							cursplit = x(nless)
						End If
					End If
					info = 1
					cure = 0
					For i = 0 To 2 * nc - 1
						cntbuf(i) = 0
					Next
					For i = 0 To nleft - 1
						cntbuf(c(i)) = cntbuf(c(i)) + 1
					Next
					For i = nleft To n - 1
						cntbuf(nc + c(i)) = cntbuf(nc + c(i)) + 1
					Next
					sl = nleft
					sr = n - nleft
					v = 0
					For i = 0 To nc - 1
						w = cntbuf(i)
						v = v + w * Math.sqr(w / sl - 1)
						v = v + (sl - w) * Math.sqr(w / sl)
						w = cntbuf(nc + i)
						v = v + w * Math.sqr(w / sr - 1)
						v = v + (sr - w) * Math.sqr(w / sr)
					Next
					cure = System.Math.sqrt(v / (nc * n))
					If CDbl(cure) < CDbl(e) Then
						threshold = cursplit
						e = cure
					End If
				End If
			Next
		End Sub


		'************************************************************************
'        Makes split on attribute
'        ************************************************************************

		Private Shared Sub dfsplitr(ByRef x As Double(), ByRef y As Double(), n As Integer, flags As Integer, ByRef info As Integer, ByRef threshold As Double, _
			ByRef e As Double, ByRef sortrbuf As Double(), ByRef sortrbuf2 As Double())
			Dim i As Integer = 0
			Dim neq As Integer = 0
			Dim nless As Integer = 0
			Dim ngreater As Integer = 0
			Dim q As Integer = 0
			Dim qmin As Integer = 0
			Dim qmax As Integer = 0
			Dim qcnt As Integer = 0
			Dim cursplit As Double = 0
			Dim nleft As Integer = 0
			Dim v As Double = 0
			Dim cure As Double = 0

			info = 0
			threshold = 0
			e = 0

			tsort.tagsortfastr(x, y, sortrbuf, sortrbuf2, n)
			e = Math.maxrealnumber
			threshold = 0.5 * (x(0) + x(n - 1))
			info = -3
			If flags \ dfusestrongsplits Mod 2 = 0 Then

				'
				' weak splits, split at half
				'
				qcnt = 2
				qmin = 1
				qmax = 1
			Else

				'
				' strong splits: choose best quartile
				'
				qcnt = 4
				qmin = 1
				qmax = 3
			End If
			For q = qmin To qmax
				cursplit = x(n * q \ qcnt)
				neq = 0
				nless = 0
				ngreater = 0
				For i = 0 To n - 1
					If CDbl(x(i)) < CDbl(cursplit) Then
						nless = nless + 1
					End If
					If CDbl(x(i)) = CDbl(cursplit) Then
						neq = neq + 1
					End If
					If CDbl(x(i)) > CDbl(cursplit) Then
						ngreater = ngreater + 1
					End If
				Next
				alglib.ap.assert(neq <> 0, "DFSplitR: NEq=0, something strange!!!")
				If nless <> 0 OrElse ngreater <> 0 Then

					'
					' set threshold between two partitions, with
					' some tweaking to avoid problems with floating point
					' arithmetics.
					'
					' The problem is that when you calculates C = 0.5*(A+B) there
					' can be no C which lies strictly between A and B (for example,
					' there is no floating point number which is
					' greater than 1 and less than 1+eps). In such situations
					' we choose right side as theshold (remember that
					' points which lie on threshold falls to the right side).
					'
					If nless < ngreater Then
						cursplit = 0.5 * (x(nless + neq - 1) + x(nless + neq))
						nleft = nless + neq
						If CDbl(cursplit) <= CDbl(x(nless + neq - 1)) Then
							cursplit = x(nless + neq)
						End If
					Else
						cursplit = 0.5 * (x(nless - 1) + x(nless))
						nleft = nless
						If CDbl(cursplit) <= CDbl(x(nless - 1)) Then
							cursplit = x(nless)
						End If
					End If
					info = 1
					cure = 0
					v = 0
					For i = 0 To nleft - 1
						v = v + y(i)
					Next
					v = v / nleft
					For i = 0 To nleft - 1
						cure = cure + Math.sqr(y(i) - v)
					Next
					v = 0
					For i = nleft To n - 1
						v = v + y(i)
					Next
					v = v / (n - nleft)
					For i = nleft To n - 1
						cure = cure + Math.sqr(y(i) - v)
					Next
					cure = System.Math.sqrt(cure / n)
					If CDbl(cure) < CDbl(e) Then
						threshold = cursplit
						e = cure
					End If
				End If
			Next
		End Sub


	End Class
	Public Class linreg
		Public Class linearmodel
			Inherits apobject
			Public w As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				w = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New linearmodel()
				_result.w = DirectCast(w.Clone(), Double())
				Return _result
			End Function
		End Class


		'************************************************************************
'        LRReport structure contains additional information about linear model:
'        * C             -   covariation matrix,  array[0..NVars,0..NVars].
'                            C[i,j] = Cov(A[i],A[j])
'        * RMSError      -   root mean square error on a training set
'        * AvgError      -   average error on a training set
'        * AvgRelError   -   average relative error on a training set (excluding
'                            observations with zero function value).
'        * CVRMSError    -   leave-one-out cross-validation estimate of
'                            generalization error. Calculated using fast algorithm
'                            with O(NVars*NPoints) complexity.
'        * CVAvgError    -   cross-validation estimate of average error
'        * CVAvgRelError -   cross-validation estimate of average relative error
'
'        All other fields of the structure are intended for internal use and should
'        not be used outside ALGLIB.
'        ************************************************************************

		Public Class lrreport
			Inherits apobject
			Public c As Double(,)
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public cvrmserror As Double
			Public cvavgerror As Double
			Public cvavgrelerror As Double
			Public ncvdefects As Integer
			Public cvdefects As Integer()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				c = New Double(-1, -1) {}
				cvdefects = New Integer(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New lrreport()
				_result.c = DirectCast(c.Clone(), Double(,))
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				_result.cvrmserror = cvrmserror
				_result.cvavgerror = cvavgerror
				_result.cvavgrelerror = cvavgrelerror
				_result.ncvdefects = ncvdefects
				_result.cvdefects = DirectCast(cvdefects.Clone(), Integer())
				Return _result
			End Function
		End Class




		Public Const lrvnum As Integer = 5


		'************************************************************************
'        Linear regression
'
'        Subroutine builds model:
'
'            Y = A(0)*X[0] + ... + A(N-1)*X[N-1] + A(N)
'
'        and model found in ALGLIB format, covariation matrix, training set  errors
'        (rms,  average,  average  relative)   and  leave-one-out  cross-validation
'        estimate of the generalization error. CV  estimate calculated  using  fast
'        algorithm with O(NPoints*NVars) complexity.
'
'        When  covariation  matrix  is  calculated  standard deviations of function
'        values are assumed to be equal to RMS error on the training set.
'
'        INPUT PARAMETERS:
'            XY          -   training set, array [0..NPoints-1,0..NVars]:
'                            * NVars columns - independent variables
'                            * last column - dependent variable
'            NPoints     -   training set size, NPoints>NVars+1
'            NVars       -   number of independent variables
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -255, in case of unknown internal error
'                            * -4, if internal SVD subroutine haven't converged
'                            * -1, if incorrect parameters was passed (NPoints<NVars+2, NVars<1).
'                            *  1, if subroutine successfully finished
'            LM          -   linear model in the ALGLIB format. Use subroutines of
'                            this unit to work with the model.
'            AR          -   additional results
'
'
'          -- ALGLIB --
'             Copyright 02.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrbuild(xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, lm As linearmodel, ar As lrreport)
			Dim s As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim sigma2 As Double = 0
			Dim i_ As Integer = 0

			info = 0

			If npoints <= nvars + 1 OrElse nvars < 1 Then
				info = -1
				Return
			End If
			s = New Double(npoints - 1) {}
			For i = 0 To npoints - 1
				s(i) = 1
			Next
			lrbuilds(xy, s, npoints, nvars, info, lm, _
				ar)
			If info < 0 Then
				Return
			End If
			sigma2 = Math.sqr(ar.rmserror) * npoints / (npoints - nvars - 1)
			For i = 0 To nvars
				For i_ = 0 To nvars
					ar.c(i, i_) = sigma2 * ar.c(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        Linear regression
'
'        Variant of LRBuild which uses vector of standatd deviations (errors in
'        function values).
'
'        INPUT PARAMETERS:
'            XY          -   training set, array [0..NPoints-1,0..NVars]:
'                            * NVars columns - independent variables
'                            * last column - dependent variable
'            S           -   standard deviations (errors in function values)
'                            array[0..NPoints-1], S[i]>0.
'            NPoints     -   training set size, NPoints>NVars+1
'            NVars       -   number of independent variables
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -255, in case of unknown internal error
'                            * -4, if internal SVD subroutine haven't converged
'                            * -1, if incorrect parameters was passed (NPoints<NVars+2, NVars<1).
'                            * -2, if S[I]<=0
'                            *  1, if subroutine successfully finished
'            LM          -   linear model in the ALGLIB format. Use subroutines of
'                            this unit to work with the model.
'            AR          -   additional results
'
'
'          -- ALGLIB --
'             Copyright 02.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrbuilds(xy As Double(,), s As Double(), npoints As Integer, nvars As Integer, ByRef info As Integer, lm As linearmodel, _
			ar As lrreport)
			Dim xyi As Double(,) = New Double(-1, -1) {}
			Dim x As Double() = New Double(-1) {}
			Dim means As Double() = New Double(-1) {}
			Dim sigmas As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim mean As Double = 0
			Dim variance As Double = 0
			Dim skewness As Double = 0
			Dim kurtosis As Double = 0
			Dim i_ As Integer = 0

			info = 0


			'
			' Test parameters
			'
			If npoints <= nvars + 1 OrElse nvars < 1 Then
				info = -1
				Return
			End If

			'
			' Copy data, add one more column (constant term)
			'
			xyi = New Double(npoints - 1, nvars + 1) {}
			For i = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					xyi(i, i_) = xy(i, i_)
				Next
				xyi(i, nvars) = 1
				xyi(i, nvars + 1) = xy(i, nvars)
			Next

			'
			' Standartization
			'
			x = New Double(npoints - 1) {}
			means = New Double(nvars - 1) {}
			sigmas = New Double(nvars - 1) {}
			For j = 0 To nvars - 1
				For i_ = 0 To npoints - 1
					x(i_) = xy(i_, j)
				Next
				basestat.samplemoments(x, npoints, mean, variance, skewness, kurtosis)
				means(j) = mean
				sigmas(j) = System.Math.sqrt(variance)
				If CDbl(sigmas(j)) = CDbl(0) Then
					sigmas(j) = 1
				End If
				For i = 0 To npoints - 1
					xyi(i, j) = (xyi(i, j) - means(j)) / sigmas(j)
				Next
			Next

			'
			' Internal processing
			'
			lrinternal(xyi, s, npoints, nvars + 1, info, lm, _
				ar)
			If info < 0 Then
				Return
			End If

			'
			' Un-standartization
			'
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			For j = 0 To nvars - 1

				'
				' Constant term is updated (and its covariance too,
				' since it gets some variance from J-th component)
				'
				lm.w(offs + nvars) = lm.w(offs + nvars) - lm.w(offs + j) * means(j) / sigmas(j)
				v = means(j) / sigmas(j)
				For i_ = 0 To nvars
					ar.c(nvars, i_) = ar.c(nvars, i_) - v * ar.c(j, i_)
				Next
				For i_ = 0 To nvars
					ar.c(i_, nvars) = ar.c(i_, nvars) - v * ar.c(i_, j)
				Next

				'
				' J-th term is updated
				'
				lm.w(offs + j) = lm.w(offs + j) / sigmas(j)
				v = 1 / sigmas(j)
				For i_ = 0 To nvars
					ar.c(j, i_) = v * ar.c(j, i_)
				Next
				For i_ = 0 To nvars
					ar.c(i_, j) = v * ar.c(i_, j)
				Next
			Next
		End Sub


		'************************************************************************
'        Like LRBuildS, but builds model
'
'            Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
'
'        i.e. with zero constant term.
'
'          -- ALGLIB --
'             Copyright 30.10.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrbuildzs(xy As Double(,), s As Double(), npoints As Integer, nvars As Integer, ByRef info As Integer, lm As linearmodel, _
			ar As lrreport)
			Dim xyi As Double(,) = New Double(-1, -1) {}
			Dim x As Double() = New Double(-1) {}
			Dim c As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim mean As Double = 0
			Dim variance As Double = 0
			Dim skewness As Double = 0
			Dim kurtosis As Double = 0
			Dim i_ As Integer = 0

			info = 0


			'
			' Test parameters
			'
			If npoints <= nvars + 1 OrElse nvars < 1 Then
				info = -1
				Return
			End If

			'
			' Copy data, add one more column (constant term)
			'
			xyi = New Double(npoints - 1, nvars + 1) {}
			For i = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					xyi(i, i_) = xy(i, i_)
				Next
				xyi(i, nvars) = 0
				xyi(i, nvars + 1) = xy(i, nvars)
			Next

			'
			' Standartization: unusual scaling
			'
			x = New Double(npoints - 1) {}
			c = New Double(nvars - 1) {}
			For j = 0 To nvars - 1
				For i_ = 0 To npoints - 1
					x(i_) = xy(i_, j)
				Next
				basestat.samplemoments(x, npoints, mean, variance, skewness, kurtosis)
				If CDbl(System.Math.Abs(mean)) > CDbl(System.Math.sqrt(variance)) Then

					'
					' variation is relatively small, it is better to
					' bring mean value to 1
					'
					c(j) = mean
				Else

					'
					' variation is large, it is better to bring variance to 1
					'
					If CDbl(variance) = CDbl(0) Then
						variance = 1
					End If
					c(j) = System.Math.sqrt(variance)
				End If
				For i = 0 To npoints - 1
					xyi(i, j) = xyi(i, j) / c(j)
				Next
			Next

			'
			' Internal processing
			'
			lrinternal(xyi, s, npoints, nvars + 1, info, lm, _
				ar)
			If info < 0 Then
				Return
			End If

			'
			' Un-standartization
			'
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			For j = 0 To nvars - 1

				'
				' J-th term is updated
				'
				lm.w(offs + j) = lm.w(offs + j) / c(j)
				v = 1 / c(j)
				For i_ = 0 To nvars
					ar.c(j, i_) = v * ar.c(j, i_)
				Next
				For i_ = 0 To nvars
					ar.c(i_, j) = v * ar.c(i_, j)
				Next
			Next
		End Sub


		'************************************************************************
'        Like LRBuild but builds model
'
'            Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
'
'        i.e. with zero constant term.
'
'          -- ALGLIB --
'             Copyright 30.10.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrbuildz(xy As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, lm As linearmodel, ar As lrreport)
			Dim s As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim sigma2 As Double = 0
			Dim i_ As Integer = 0

			info = 0

			If npoints <= nvars + 1 OrElse nvars < 1 Then
				info = -1
				Return
			End If
			s = New Double(npoints - 1) {}
			For i = 0 To npoints - 1
				s(i) = 1
			Next
			lrbuildzs(xy, s, npoints, nvars, info, lm, _
				ar)
			If info < 0 Then
				Return
			End If
			sigma2 = Math.sqr(ar.rmserror) * npoints / (npoints - nvars - 1)
			For i = 0 To nvars
				For i_ = 0 To nvars
					ar.c(i, i_) = sigma2 * ar.c(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        Unpacks coefficients of linear model.
'
'        INPUT PARAMETERS:
'            LM          -   linear model in ALGLIB format
'
'        OUTPUT PARAMETERS:
'            V           -   coefficients, array[0..NVars]
'                            constant term (intercept) is stored in the V[NVars].
'            NVars       -   number of independent variables (one less than number
'                            of coefficients)
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrunpack(lm As linearmodel, ByRef v As Double(), ByRef nvars As Integer)
			Dim offs As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			v = New Double(-1) {}
			nvars = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = lrvnum, "LINREG: Incorrect LINREG version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			v = New Double(nvars) {}
			i1_ = (offs) - (0)
			For i_ = 0 To nvars
				v(i_) = lm.w(i_ + i1_)
			Next
		End Sub


		'************************************************************************
'        "Packs" coefficients and creates linear model in ALGLIB format (LRUnpack
'        reversed).
'
'        INPUT PARAMETERS:
'            V           -   coefficients, array[0..NVars]
'            NVars       -   number of independent variables
'
'        OUTPUT PAREMETERS:
'            LM          -   linear model.
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrpack(v As Double(), nvars As Integer, lm As linearmodel)
			Dim offs As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			lm.w = New Double(4 + nvars) {}
			offs = 4
			lm.w(0) = 4 + nvars + 1
			lm.w(1) = lrvnum
			lm.w(2) = nvars
			lm.w(3) = offs
			i1_ = (0) - (offs)
			For i_ = offs To offs + nvars
				lm.w(i_) = v(i_ + i1_)
			Next
		End Sub


		'************************************************************************
'        Procesing
'
'        INPUT PARAMETERS:
'            LM      -   linear model
'            X       -   input vector,  array[0..NVars-1].
'
'        Result:
'            value of linear model regression estimate
'
'          -- ALGLIB --
'             Copyright 03.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function lrprocess(lm As linearmodel, x As Double()) As Double
			Dim result As Double = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim nvars As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = lrvnum, "LINREG: Incorrect LINREG version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			i1_ = (offs) - (0)
			v = 0.0
			For i_ = 0 To nvars - 1
				v += x(i_) * lm.w(i_ + i1_)
			Next
			result = v + lm.w(offs + nvars)
			Return result
		End Function


		'************************************************************************
'        RMS error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   linear model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            root mean square error.
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function lrrmserror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim nvars As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = lrvnum, "LINREG: Incorrect LINREG version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			result = 0
			For i = 0 To npoints - 1
				i1_ = (offs) - (0)
				v = 0.0
				For i_ = 0 To nvars - 1
					v += xy(i, i_) * lm.w(i_ + i1_)
				Next
				v = v + lm.w(offs + nvars)
				result = result + Math.sqr(v - xy(i, nvars))
			Next
			result = System.Math.sqrt(result / npoints)
			Return result
		End Function


		'************************************************************************
'        Average error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   linear model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            average error.
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function lravgerror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim nvars As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = lrvnum, "LINREG: Incorrect LINREG version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			result = 0
			For i = 0 To npoints - 1
				i1_ = (offs) - (0)
				v = 0.0
				For i_ = 0 To nvars - 1
					v += xy(i, i_) * lm.w(i_ + i1_)
				Next
				v = v + lm.w(offs + nvars)
				result = result + System.Math.Abs(v - xy(i, nvars))
			Next
			result = result / npoints
			Return result
		End Function


		'************************************************************************
'        RMS error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   linear model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            average relative error.
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function lravgrelerror(lm As linearmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim offs As Integer = 0
			Dim nvars As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = lrvnum, "LINREG: Incorrect LINREG version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			result = 0
			k = 0
			For i = 0 To npoints - 1
				If CDbl(xy(i, nvars)) <> CDbl(0) Then
					i1_ = (offs) - (0)
					v = 0.0
					For i_ = 0 To nvars - 1
						v += xy(i, i_) * lm.w(i_ + i1_)
					Next
					v = v + lm.w(offs + nvars)
					result = result + System.Math.Abs((v - xy(i, nvars)) / xy(i, nvars))
					k = k + 1
				End If
			Next
			If k <> 0 Then
				result = result / k
			End If
			Return result
		End Function


		'************************************************************************
'        Copying of LinearModel strucure
'
'        INPUT PARAMETERS:
'            LM1 -   original
'
'        OUTPUT PARAMETERS:
'            LM2 -   copy
'
'          -- ALGLIB --
'             Copyright 15.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub lrcopy(lm1 As linearmodel, lm2 As linearmodel)
			Dim k As Integer = 0
			Dim i_ As Integer = 0

			k = CInt(System.Math.Truncate(System.Math.Round(lm1.w(0))))
			lm2.w = New Double(k - 1) {}
			For i_ = 0 To k - 1
				lm2.w(i_) = lm1.w(i_)
			Next
		End Sub


		Public Shared Sub lrlines(xy As Double(,), s As Double(), n As Integer, ByRef info As Integer, ByRef a As Double, ByRef b As Double, _
			ByRef vara As Double, ByRef varb As Double, ByRef covab As Double, ByRef corrab As Double, ByRef p As Double)
			Dim i As Integer = 0
			Dim ss As Double = 0
			Dim sx As Double = 0
			Dim sxx As Double = 0
			Dim sy As Double = 0
			Dim stt As Double = 0
			Dim e1 As Double = 0
			Dim e2 As Double = 0
			Dim t As Double = 0
			Dim chi2 As Double = 0

			info = 0
			a = 0
			b = 0
			vara = 0
			varb = 0
			covab = 0
			corrab = 0
			p = 0

			If n < 2 Then
				info = -1
				Return
			End If
			For i = 0 To n - 1
				If CDbl(s(i)) <= CDbl(0) Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Calculate S, SX, SY, SXX
			'
			ss = 0
			sx = 0
			sy = 0
			sxx = 0
			For i = 0 To n - 1
				t = Math.sqr(s(i))
				ss = ss + 1 / t
				sx = sx + xy(i, 0) / t
				sy = sy + xy(i, 1) / t
				sxx = sxx + Math.sqr(xy(i, 0)) / t
			Next

			'
			' Test for condition number
			'
			t = System.Math.sqrt(4 * Math.sqr(sx) + Math.sqr(ss - sxx))
			e1 = 0.5 * (ss + sxx + t)
			e2 = 0.5 * (ss + sxx - t)
			If CDbl(System.Math.Min(e1, e2)) <= CDbl(1000 * Math.machineepsilon * System.Math.Max(e1, e2)) Then
				info = -3
				Return
			End If

			'
			' Calculate A, B
			'
			a = 0
			b = 0
			stt = 0
			For i = 0 To n - 1
				t = (xy(i, 0) - sx / ss) / s(i)
				b = b + t * xy(i, 1) / s(i)
				stt = stt + Math.sqr(t)
			Next
			b = b / stt
			a = (sy - sx * b) / ss

			'
			' Calculate goodness-of-fit
			'
			If n > 2 Then
				chi2 = 0
				For i = 0 To n - 1
					chi2 = chi2 + Math.sqr((xy(i, 1) - a - b * xy(i, 0)) / s(i))
				Next
				p = igammaf.incompletegammac(CDbl(n - 2) / CDbl(2), chi2 / 2)
			Else
				p = 1
			End If

			'
			' Calculate other parameters
			'
			vara = (1 + Math.sqr(sx) / (ss * stt)) / ss
			varb = 1 / stt
			covab = -(sx / (ss * stt))
			corrab = covab / System.Math.sqrt(vara * varb)
		End Sub


		Public Shared Sub lrline(xy As Double(,), n As Integer, ByRef info As Integer, ByRef a As Double, ByRef b As Double)
			Dim s As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim vara As Double = 0
			Dim varb As Double = 0
			Dim covab As Double = 0
			Dim corrab As Double = 0
			Dim p As Double = 0

			info = 0
			a = 0
			b = 0

			If n < 2 Then
				info = -1
				Return
			End If
			s = New Double(n - 1) {}
			For i = 0 To n - 1
				s(i) = 1
			Next
			lrlines(xy, s, n, info, a, b, _
				vara, varb, covab, corrab, p)
		End Sub


		'************************************************************************
'        Internal linear regression subroutine
'        ************************************************************************

		Private Shared Sub lrinternal(xy As Double(,), s As Double(), npoints As Integer, nvars As Integer, ByRef info As Integer, lm As linearmodel, _
			ar As lrreport)
			Dim a As Double(,) = New Double(-1, -1) {}
			Dim u As Double(,) = New Double(-1, -1) {}
			Dim vt As Double(,) = New Double(-1, -1) {}
			Dim vm As Double(,) = New Double(-1, -1) {}
			Dim xym As Double(,) = New Double(-1, -1) {}
			Dim b As Double() = New Double(-1) {}
			Dim sv As Double() = New Double(-1) {}
			Dim t As Double() = New Double(-1) {}
			Dim svi As Double() = New Double(-1) {}
			Dim work As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim ncv As Integer = 0
			Dim na As Integer = 0
			Dim nacv As Integer = 0
			Dim r As Double = 0
			Dim p As Double = 0
			Dim epstol As Double = 0
			Dim ar2 As New lrreport()
			Dim offs As Integer = 0
			Dim tlm As New linearmodel()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			info = 0

			epstol = 1000

			'
			' Check for errors in data
			'
			If npoints < nvars OrElse nvars < 1 Then
				info = -1
				Return
			End If
			For i = 0 To npoints - 1
				If CDbl(s(i)) <= CDbl(0) Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Create design matrix
			'
			a = New Double(npoints - 1, nvars - 1) {}
			b = New Double(npoints - 1) {}
			For i = 0 To npoints - 1
				r = 1 / s(i)
				For i_ = 0 To nvars - 1
					a(i, i_) = r * xy(i, i_)
				Next
				b(i) = xy(i, nvars) / s(i)
			Next

			'
			' Allocate W:
			' W[0]     array size
			' W[1]     version number, 0
			' W[2]     NVars (minus 1, to be compatible with external representation)
			' W[3]     coefficients offset
			'
			lm.w = New Double(4 + nvars - 1) {}
			offs = 4
			lm.w(0) = 4 + nvars
			lm.w(1) = lrvnum
			lm.w(2) = nvars - 1
			lm.w(3) = offs

			'
			' Solve problem using SVD:
			'
			' 0. check for degeneracy (different types)
			' 1. A = U*diag(sv)*V'
			' 2. T = b'*U
			' 3. w = SUM((T[i]/sv[i])*V[..,i])
			' 4. cov(wi,wj) = SUM(Vji*Vjk/sv[i]^2,K=1..M)
			'
			' see $15.4 of "Numerical Recipes in C" for more information
			'
			t = New Double(nvars - 1) {}
			svi = New Double(nvars - 1) {}
			ar.c = New Double(nvars - 1, nvars - 1) {}
			vm = New Double(nvars - 1, nvars - 1) {}
			If Not svd.rmatrixsvd(a, npoints, nvars, 1, 1, 2, _
				sv, u, vt) Then
				info = -4
				Return
			End If
			If CDbl(sv(0)) <= CDbl(0) Then

				'
				' Degenerate case: zero design matrix.
				'
				For i = offs To offs + nvars - 1
					lm.w(i) = 0
				Next
				ar.rmserror = lrrmserror(lm, xy, npoints)
				ar.avgerror = lravgerror(lm, xy, npoints)
				ar.avgrelerror = lravgrelerror(lm, xy, npoints)
				ar.cvrmserror = ar.rmserror
				ar.cvavgerror = ar.avgerror
				ar.cvavgrelerror = ar.avgrelerror
				ar.ncvdefects = 0
				ar.cvdefects = New Integer(nvars - 1) {}
				ar.c = New Double(nvars - 1, nvars - 1) {}
				For i = 0 To nvars - 1
					For j = 0 To nvars - 1
						ar.c(i, j) = 0
					Next
				Next
				Return
			End If
			If CDbl(sv(nvars - 1)) <= CDbl(epstol * Math.machineepsilon * sv(0)) Then

				'
				' Degenerate case, non-zero design matrix.
				'
				' We can leave it and solve task in SVD least squares fashion.
				' Solution and covariance matrix will be obtained correctly,
				' but CV error estimates - will not. It is better to reduce
				' it to non-degenerate task and to obtain correct CV estimates.
				'
				For k = nvars To 1 Step -1
					If CDbl(sv(k - 1)) > CDbl(epstol * Math.machineepsilon * sv(0)) Then

						'
						' Reduce
						'
						xym = New Double(npoints - 1, k) {}
						For i = 0 To npoints - 1
							For j = 0 To k - 1
								r = 0.0
								For i_ = 0 To nvars - 1
									r += xy(i, i_) * vt(j, i_)
								Next
								xym(i, j) = r
							Next
							xym(i, k) = xy(i, nvars)
						Next

						'
						' Solve
						'
						lrinternal(xym, s, npoints, k, info, tlm, _
							ar2)
						If info <> 1 Then
							Return
						End If

						'
						' Convert back to un-reduced format
						'
						For j = 0 To nvars - 1
							lm.w(offs + j) = 0
						Next
						For j = 0 To k - 1
							r = tlm.w(offs + j)
							i1_ = (0) - (offs)
							For i_ = offs To offs + nvars - 1
								lm.w(i_) = lm.w(i_) + r * vt(j, i_ + i1_)
							Next
						Next
						ar.rmserror = ar2.rmserror
						ar.avgerror = ar2.avgerror
						ar.avgrelerror = ar2.avgrelerror
						ar.cvrmserror = ar2.cvrmserror
						ar.cvavgerror = ar2.cvavgerror
						ar.cvavgrelerror = ar2.cvavgrelerror
						ar.ncvdefects = ar2.ncvdefects
						ar.cvdefects = New Integer(nvars - 1) {}
						For j = 0 To ar.ncvdefects - 1
							ar.cvdefects(j) = ar2.cvdefects(j)
						Next
						ar.c = New Double(nvars - 1, nvars - 1) {}
						work = New Double(nvars) {}
						blas.matrixmatrixmultiply(ar2.c, 0, k - 1, 0, k - 1, False, _
							vt, 0, k - 1, 0, nvars - 1, False, _
							1.0, vm, 0, k - 1, 0, nvars - 1, _
							0.0, work)
						blas.matrixmatrixmultiply(vt, 0, k - 1, 0, nvars - 1, True, _
							vm, 0, k - 1, 0, nvars - 1, False, _
							1.0, ar.c, 0, nvars - 1, 0, nvars - 1, _
							0.0, work)
						Return
					End If
				Next
				info = -255
				Return
			End If
			For i = 0 To nvars - 1
				If CDbl(sv(i)) > CDbl(epstol * Math.machineepsilon * sv(0)) Then
					svi(i) = 1 / sv(i)
				Else
					svi(i) = 0
				End If
			Next
			For i = 0 To nvars - 1
				t(i) = 0
			Next
			For i = 0 To npoints - 1
				r = b(i)
				For i_ = 0 To nvars - 1
					t(i_) = t(i_) + r * u(i, i_)
				Next
			Next
			For i = 0 To nvars - 1
				lm.w(offs + i) = 0
			Next
			For i = 0 To nvars - 1
				r = t(i) * svi(i)
				i1_ = (0) - (offs)
				For i_ = offs To offs + nvars - 1
					lm.w(i_) = lm.w(i_) + r * vt(i, i_ + i1_)
				Next
			Next
			For j = 0 To nvars - 1
				r = svi(j)
				For i_ = 0 To nvars - 1
					vm(i_, j) = r * vt(j, i_)
				Next
			Next
			For i = 0 To nvars - 1
				For j = i To nvars - 1
					r = 0.0
					For i_ = 0 To nvars - 1
						r += vm(i, i_) * vm(j, i_)
					Next
					ar.c(i, j) = r
					ar.c(j, i) = r
				Next
			Next

			'
			' Leave-1-out cross-validation error.
			'
			' NOTATIONS:
			' A            design matrix
			' A*x = b      original linear least squares task
			' U*S*V'       SVD of A
			' ai           i-th row of the A
			' bi           i-th element of the b
			' xf           solution of the original LLS task
			'
			' Cross-validation error of i-th element from a sample is
			' calculated using following formula:
			'
			'     ERRi = ai*xf - (ai*xf-bi*(ui*ui'))/(1-ui*ui')     (1)
			'
			' This formula can be derived from normal equations of the
			' original task
			'
			'     (A'*A)x = A'*b                                    (2)
			'
			' by applying modification (zeroing out i-th row of A) to (2):
			'
			'     (A-ai)'*(A-ai) = (A-ai)'*b
			'
			' and using Sherman-Morrison formula for updating matrix inverse
			'
			' NOTE 1: b is not zeroed out since it is much simpler and
			' does not influence final result.
			'
			' NOTE 2: some design matrices A have such ui that 1-ui*ui'=0.
			' Formula (1) can't be applied for such cases and they are skipped
			' from CV calculation (which distorts resulting CV estimate).
			' But from the properties of U we can conclude that there can
			' be no more than NVars such vectors. Usually
			' NVars << NPoints, so in a normal case it only slightly
			' influences result.
			'
			ncv = 0
			na = 0
			nacv = 0
			ar.rmserror = 0
			ar.avgerror = 0
			ar.avgrelerror = 0
			ar.cvrmserror = 0
			ar.cvavgerror = 0
			ar.cvavgrelerror = 0
			ar.ncvdefects = 0
			ar.cvdefects = New Integer(nvars - 1) {}
			For i = 0 To npoints - 1

				'
				' Error on a training set
				'
				i1_ = (offs) - (0)
				r = 0.0
				For i_ = 0 To nvars - 1
					r += xy(i, i_) * lm.w(i_ + i1_)
				Next
				ar.rmserror = ar.rmserror + Math.sqr(r - xy(i, nvars))
				ar.avgerror = ar.avgerror + System.Math.Abs(r - xy(i, nvars))
				If CDbl(xy(i, nvars)) <> CDbl(0) Then
					ar.avgrelerror = ar.avgrelerror + System.Math.Abs((r - xy(i, nvars)) / xy(i, nvars))
					na = na + 1
				End If

				'
				' Error using fast leave-one-out cross-validation
				'
				p = 0.0
				For i_ = 0 To nvars - 1
					p += u(i, i_) * u(i, i_)
				Next
				If CDbl(p) > CDbl(1 - epstol * Math.machineepsilon) Then
					ar.cvdefects(ar.ncvdefects) = i
					ar.ncvdefects = ar.ncvdefects + 1
					Continue For
				End If
				r = s(i) * (r / s(i) - b(i) * p) / (1 - p)
				ar.cvrmserror = ar.cvrmserror + Math.sqr(r - xy(i, nvars))
				ar.cvavgerror = ar.cvavgerror + System.Math.Abs(r - xy(i, nvars))
				If CDbl(xy(i, nvars)) <> CDbl(0) Then
					ar.cvavgrelerror = ar.cvavgrelerror + System.Math.Abs((r - xy(i, nvars)) / xy(i, nvars))
					nacv = nacv + 1
				End If
				ncv = ncv + 1
			Next
			If ncv = 0 Then

				'
				' Something strange: ALL ui are degenerate.
				' Unexpected...
				'
				info = -255
				Return
			End If
			ar.rmserror = System.Math.sqrt(ar.rmserror / npoints)
			ar.avgerror = ar.avgerror / npoints
			If na <> 0 Then
				ar.avgrelerror = ar.avgrelerror / na
			End If
			ar.cvrmserror = System.Math.sqrt(ar.cvrmserror / ncv)
			ar.cvavgerror = ar.cvavgerror / ncv
			If nacv <> 0 Then
				ar.cvavgrelerror = ar.cvavgrelerror / nacv
			End If
		End Sub


	End Class
	Public Class filters
		'************************************************************************
'        Filters: simple moving averages (unsymmetric).
'
'        This filter replaces array by results of SMA(K) filter. SMA(K) is defined
'        as filter which averages at most K previous points (previous - not points
'        AROUND central point) - or less, in case of the first K-1 points.
'
'        INPUT PARAMETERS:
'            X           -   array[N], array to process. It can be larger than N,
'                            in this case only first N points are processed.
'            N           -   points count, N>=0
'            K           -   K>=1 (K can be larger than N ,  such  cases  will  be
'                            correctly handled). Window width. K=1 corresponds  to
'                            identity transformation (nothing changes).
'
'        OUTPUT PARAMETERS:
'            X           -   array, whose first N elements were processed with SMA(K)
'
'        NOTE 1: this function uses efficient in-place  algorithm  which  does not
'                allocate temporary arrays.
'
'        NOTE 2: this algorithm makes only one pass through array and uses running
'                sum  to speed-up calculation of the averages. Additional measures
'                are taken to ensure that running sum on a long sequence  of  zero
'                elements will be correctly reset to zero even in the presence  of
'                round-off error.
'
'        NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
'                averages points after the current one. Only X[i], X[i-1], ... are
'                used when calculating new value of X[i]. We should also note that
'                this algorithm uses BOTH previous points and  current  one,  i.e.
'                new value of X[i] depends on BOTH previous point and X[i] itself.
'
'          -- ALGLIB --
'             Copyright 25.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub filtersma(ByRef x As Double(), n As Integer, k As Integer)
			Dim i As Integer = 0
			Dim runningsum As Double = 0
			Dim termsinsum As Double = 0
			Dim zeroprefix As Integer = 0
			Dim v As Double = 0

			alglib.ap.assert(n >= 0, "FilterSMA: N<0")
			alglib.ap.assert(alglib.ap.len(x) >= n, "FilterSMA: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "FilterSMA: X contains INF or NAN")
			alglib.ap.assert(k >= 1, "FilterSMA: K<1")

			'
			' Quick exit, if necessary
			'
			If n <= 1 OrElse k = 1 Then
				Return
			End If

			'
			' Prepare variables (see below for explanation)
			'
			runningsum = 0.0
			termsinsum = 0
			For i = System.Math.Max(n - k, 0) To n - 1
				runningsum = runningsum + x(i)
				termsinsum = termsinsum + 1
			Next
			i = System.Math.Max(n - k, 0)
			zeroprefix = 0
			While i <= n - 1 AndAlso CDbl(x(i)) = CDbl(0)
				zeroprefix = zeroprefix + 1
				i = i + 1
			End While

			'
			' General case: we assume that N>1 and K>1
			'
			' Make one pass through all elements. At the beginning of
			' the iteration we have:
			' * I              element being processed
			' * RunningSum     current value of the running sum
			'                  (including I-th element)
			' * TermsInSum     number of terms in sum, 0<=TermsInSum<=K
			' * ZeroPrefix     length of the sequence of zero elements
			'                  which starts at X[I-K+1] and continues towards X[I].
			'                  Equal to zero in case X[I-K+1] is non-zero.
			'                  This value is used to make RunningSum exactly zero
			'                  when it follows from the problem properties.
			'
			For i = n - 1 To 0 Step -1

				'
				' Store new value of X[i], save old value in V
				'
				v = x(i)
				x(i) = runningsum / termsinsum

				'
				' Update RunningSum and TermsInSum
				'
				If i - k >= 0 Then
					runningsum = runningsum - v + x(i - k)
				Else
					runningsum = runningsum - v
					termsinsum = termsinsum - 1
				End If

				'
				' Update ZeroPrefix.
				' In case we have ZeroPrefix=TermsInSum,
				' RunningSum is reset to zero.
				'
				If i - k >= 0 Then
					If CDbl(x(i - k)) <> CDbl(0) Then
						zeroprefix = 0
					Else
						zeroprefix = System.Math.Min(zeroprefix + 1, k)
					End If
				Else
					zeroprefix = System.Math.Min(zeroprefix, i + 1)
				End If
				If CDbl(zeroprefix) = CDbl(termsinsum) Then
					runningsum = 0
				End If
			Next
		End Sub


		'************************************************************************
'        Filters: exponential moving averages.
'
'        This filter replaces array by results of EMA(alpha) filter. EMA(alpha) is
'        defined as filter which replaces X[] by S[]:
'            S[0] = X[0]
'            S[t] = alpha*X[t] + (1-alpha)*S[t-1]
'
'        INPUT PARAMETERS:
'            X           -   array[N], array to process. It can be larger than N,
'                            in this case only first N points are processed.
'            N           -   points count, N>=0
'            alpha       -   0<alpha<=1, smoothing parameter.
'
'        OUTPUT PARAMETERS:
'            X           -   array, whose first N elements were processed
'                            with EMA(alpha)
'
'        NOTE 1: this function uses efficient in-place  algorithm  which  does not
'                allocate temporary arrays.
'
'        NOTE 2: this algorithm uses BOTH previous points and  current  one,  i.e.
'                new value of X[i] depends on BOTH previous point and X[i] itself.
'
'        NOTE 3: technical analytis users quite often work  with  EMA  coefficient
'                expressed in DAYS instead of fractions. If you want to  calculate
'                EMA(N), where N is a number of days, you can use alpha=2/(N+1).
'
'          -- ALGLIB --
'             Copyright 25.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub filterema(ByRef x As Double(), n As Integer, alpha As Double)
			Dim i As Integer = 0

			alglib.ap.assert(n >= 0, "FilterEMA: N<0")
			alglib.ap.assert(alglib.ap.len(x) >= n, "FilterEMA: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "FilterEMA: X contains INF or NAN")
			alglib.ap.assert(CDbl(alpha) > CDbl(0), "FilterEMA: Alpha<=0")
			alglib.ap.assert(CDbl(alpha) <= CDbl(1), "FilterEMA: Alpha>1")

			'
			' Quick exit, if necessary
			'
			If n <= 1 OrElse CDbl(alpha) = CDbl(1) Then
				Return
			End If

			'
			' Process
			'
			For i = 1 To n - 1
				x(i) = alpha * x(i) + (1 - alpha) * x(i - 1)
			Next
		End Sub


		'************************************************************************
'        Filters: linear regression moving averages.
'
'        This filter replaces array by results of LRMA(K) filter.
'
'        LRMA(K) is defined as filter which, for each data  point,  builds  linear
'        regression  model  using  K  prevous  points (point itself is included in
'        these K points) and calculates value of this linear model at the point in
'        question.
'
'        INPUT PARAMETERS:
'            X           -   array[N], array to process. It can be larger than N,
'                            in this case only first N points are processed.
'            N           -   points count, N>=0
'            K           -   K>=1 (K can be larger than N ,  such  cases  will  be
'                            correctly handled). Window width. K=1 corresponds  to
'                            identity transformation (nothing changes).
'
'        OUTPUT PARAMETERS:
'            X           -   array, whose first N elements were processed with SMA(K)
'
'        NOTE 1: this function uses efficient in-place  algorithm  which  does not
'                allocate temporary arrays.
'
'        NOTE 2: this algorithm makes only one pass through array and uses running
'                sum  to speed-up calculation of the averages. Additional measures
'                are taken to ensure that running sum on a long sequence  of  zero
'                elements will be correctly reset to zero even in the presence  of
'                round-off error.
'
'        NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
'                averages points after the current one. Only X[i], X[i-1], ... are
'                used when calculating new value of X[i]. We should also note that
'                this algorithm uses BOTH previous points and  current  one,  i.e.
'                new value of X[i] depends on BOTH previous point and X[i] itself.
'
'          -- ALGLIB --
'             Copyright 25.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub filterlrma(ByRef x As Double(), n As Integer, k As Integer)
			Dim i As Integer = 0
			Dim m As Integer = 0
			Dim xy As Double(,) = New Double(-1, -1) {}
			Dim s As Double() = New Double(-1) {}
			Dim info As Integer = 0
			Dim a As Double = 0
			Dim b As Double = 0
			Dim vara As Double = 0
			Dim varb As Double = 0
			Dim covab As Double = 0
			Dim corrab As Double = 0
			Dim p As Double = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(n >= 0, "FilterLRMA: N<0")
			alglib.ap.assert(alglib.ap.len(x) >= n, "FilterLRMA: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "FilterLRMA: X contains INF or NAN")
			alglib.ap.assert(k >= 1, "FilterLRMA: K<1")

			'
			' Quick exit, if necessary:
			' * either N is equal to 1 (nothing to average)
			' * or K is 1 (only point itself is used) or 2 (model is too simple,
			'   we will always get identity transformation)
			'
			If n <= 1 OrElse k <= 2 Then
				Return
			End If

			'
			' General case: K>2, N>1.
			' We do not process points with I<2 because first two points (I=0 and I=1) will be
			' left unmodified by LRMA filter in any case.
			'
			xy = New Double(k - 1, 1) {}
			s = New Double(k - 1) {}
			For i = 0 To k - 1
				xy(i, 0) = i
				s(i) = 1.0
			Next
			For i = n - 1 To 2 Step -1
				m = System.Math.Min(i + 1, k)
				i1_ = (i - m + 1) - (0)
				For i_ = 0 To m - 1
					xy(i_, 1) = x(i_ + i1_)
				Next
				linreg.lrlines(xy, s, m, info, a, b, _
					vara, varb, covab, corrab, p)
				alglib.ap.assert(info = 1, "FilterLRMA: internal error")
				x(i) = a + b * (m - 1)
			Next
		End Sub


	End Class
	Public Class lda
		'************************************************************************
'        Multiclass Fisher LDA
'
'        Subroutine finds coefficients of linear combination which optimally separates
'        training set on classes.
'
'        COMMERCIAL EDITION OF ALGLIB:
'
'          ! Commercial version of ALGLIB includes two important  improvements   of
'          ! this function, which can be used from C++ and C#:
'          ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'          ! * multithreading support
'          !
'          ! Intel MKL gives approximately constant  (with  respect  to  number  of
'          ! worker threads) acceleration factor which depends on CPU  being  used,
'          ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'          ! comparison. Best results are achieved  for  high-dimensional  problems
'          ! (NVars is at least 256).
'          !
'          ! Multithreading is used to  accelerate  initial  phase  of  LDA,  which
'          ! includes calculation of products of large matrices.  Again,  for  best
'          ! efficiency problem must be high-dimensional.
'          !
'          ! Generally, commercial ALGLIB is several times faster than  open-source
'          ! generic C edition, and many times faster than open-source C# edition.
'          !
'          ! We recommend you to read 'Working with commercial version' section  of
'          ! ALGLIB Reference Manual in order to find out how to  use  performance-
'          ! related features provided by commercial edition of ALGLIB.
'
'        INPUT PARAMETERS:
'            XY          -   training set, array[0..NPoints-1,0..NVars].
'                            First NVars columns store values of independent
'                            variables, next column stores number of class (from 0
'                            to NClasses-1) which dataset element belongs to. Fractional
'                            values are rounded to nearest integer.
'            NPoints     -   training set size, NPoints>=0
'            NVars       -   number of independent variables, NVars>=1
'            NClasses    -   number of classes, NClasses>=2
'
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -4, if internal EVD subroutine hasn't converged
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed (NPoints<0,
'                                  NVars<1, NClasses<2)
'                            *  1, if task has been solved
'                            *  2, if there was a multicollinearity in training set,
'                                  but task has been solved.
'            W           -   linear combination coefficients, array[0..NVars-1]
'
'          -- ALGLIB --
'             Copyright 31.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub fisherlda(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double())
			Dim w2 As Double(,) = New Double(-1, -1) {}
			Dim i_ As Integer = 0

			info = 0
			w = New Double(-1) {}

			fisherldan(xy, npoints, nvars, nclasses, info, w2)
			If info > 0 Then
				w = New Double(nvars - 1) {}
				For i_ = 0 To nvars - 1
					w(i_) = w2(i_, 0)
				Next
			End If
		End Sub


		'************************************************************************
'        N-dimensional multiclass Fisher LDA
'
'        Subroutine finds coefficients of linear combinations which optimally separates
'        training set on classes. It returns N-dimensional basis whose vector are sorted
'        by quality of training set separation (in descending order).
'
'        COMMERCIAL EDITION OF ALGLIB:
'
'          ! Commercial version of ALGLIB includes two important  improvements   of
'          ! this function, which can be used from C++ and C#:
'          ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'          ! * multithreading support
'          !
'          ! Intel MKL gives approximately constant  (with  respect  to  number  of
'          ! worker threads) acceleration factor which depends on CPU  being  used,
'          ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'          ! comparison. Best results are achieved  for  high-dimensional  problems
'          ! (NVars is at least 256).
'          !
'          ! Multithreading is used to  accelerate  initial  phase  of  LDA,  which
'          ! includes calculation of products of large matrices.  Again,  for  best
'          ! efficiency problem must be high-dimensional.
'          !
'          ! Generally, commercial ALGLIB is several times faster than  open-source
'          ! generic C edition, and many times faster than open-source C# edition.
'          !
'          ! We recommend you to read 'Working with commercial version' section  of
'          ! ALGLIB Reference Manual in order to find out how to  use  performance-
'          ! related features provided by commercial edition of ALGLIB.
'
'        INPUT PARAMETERS:
'            XY          -   training set, array[0..NPoints-1,0..NVars].
'                            First NVars columns store values of independent
'                            variables, next column stores number of class (from 0
'                            to NClasses-1) which dataset element belongs to. Fractional
'                            values are rounded to nearest integer.
'            NPoints     -   training set size, NPoints>=0
'            NVars       -   number of independent variables, NVars>=1
'            NClasses    -   number of classes, NClasses>=2
'
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -4, if internal EVD subroutine hasn't converged
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed (NPoints<0,
'                                  NVars<1, NClasses<2)
'                            *  1, if task has been solved
'                            *  2, if there was a multicollinearity in training set,
'                                  but task has been solved.
'            W           -   basis, array[0..NVars-1,0..NVars-1]
'                            columns of matrix stores basis vectors, sorted by
'                            quality of training set separation (in descending order)
'
'          -- ALGLIB --
'             Copyright 31.05.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub fisherldan(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim m As Integer = 0
			Dim v As Double = 0
			Dim c As Integer() = New Integer(-1) {}
			Dim mu As Double() = New Double(-1) {}
			Dim muc As Double(,) = New Double(-1, -1) {}
			Dim nc As Integer() = New Integer(-1) {}
			Dim sw As Double(,) = New Double(-1, -1) {}
			Dim st As Double(,) = New Double(-1, -1) {}
			Dim z As Double(,) = New Double(-1, -1) {}
			Dim z2 As Double(,) = New Double(-1, -1) {}
			Dim tm As Double(,) = New Double(-1, -1) {}
			Dim sbroot As Double(,) = New Double(-1, -1) {}
			Dim a As Double(,) = New Double(-1, -1) {}
			Dim xyc As Double(,) = New Double(-1, -1) {}
			Dim xyproj As Double(,) = New Double(-1, -1) {}
			Dim wproj As Double(,) = New Double(-1, -1) {}
			Dim tf As Double() = New Double(-1) {}
			Dim d As Double() = New Double(-1) {}
			Dim d2 As Double() = New Double(-1) {}
			Dim work As Double() = New Double(-1) {}
			Dim i_ As Integer = 0

			info = 0
			w = New Double(-1, -1) {}


			'
			' Test data
			'
			If (npoints < 0 OrElse nvars < 1) OrElse nclasses < 2 Then
				info = -1
				Return
			End If
			For i = 0 To npoints - 1
				If CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) >= nclasses Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Special case: NPoints<=1
			' Degenerate task.
			'
			If npoints <= 1 Then
				info = 2
				w = New Double(nvars - 1, nvars - 1) {}
				For i = 0 To nvars - 1
					For j = 0 To nvars - 1
						If i = j Then
							w(i, j) = 1
						Else
							w(i, j) = 0
						End If
					Next
				Next
				Return
			End If

			'
			' Prepare temporaries
			'
			tf = New Double(nvars - 1) {}
			work = New Double(System.Math.Max(nvars, npoints)) {}
			xyc = New Double(npoints - 1, nvars - 1) {}

			'
			' Convert class labels from reals to integers (just for convenience)
			'
			c = New Integer(npoints - 1) {}
			For i = 0 To npoints - 1
				c(i) = CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars))))
			Next

			'
			' Calculate class sizes, class means
			'
			mu = New Double(nvars - 1) {}
			muc = New Double(nclasses - 1, nvars - 1) {}
			nc = New Integer(nclasses - 1) {}
			For j = 0 To nvars - 1
				mu(j) = 0
			Next
			For i = 0 To nclasses - 1
				nc(i) = 0
				For j = 0 To nvars - 1
					muc(i, j) = 0
				Next
			Next
			For i = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					mu(i_) = mu(i_) + xy(i, i_)
				Next
				For i_ = 0 To nvars - 1
					muc(c(i), i_) = muc(c(i), i_) + xy(i, i_)
				Next
				nc(c(i)) = nc(c(i)) + 1
			Next
			For i = 0 To nclasses - 1
				v = CDbl(1) / CDbl(nc(i))
				For i_ = 0 To nvars - 1
					muc(i, i_) = v * muc(i, i_)
				Next
			Next
			v = CDbl(1) / CDbl(npoints)
			For i_ = 0 To nvars - 1
				mu(i_) = v * mu(i_)
			Next

			'
			' Create ST matrix
			'
			st = New Double(nvars - 1, nvars - 1) {}
			For i = 0 To nvars - 1
				For j = 0 To nvars - 1
					st(i, j) = 0
				Next
			Next
			For k = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					xyc(k, i_) = xy(k, i_)
				Next
				For i_ = 0 To nvars - 1
					xyc(k, i_) = xyc(k, i_) - mu(i_)
				Next
			Next
			ablas.rmatrixgemm(nvars, nvars, npoints, 1.0, xyc, 0, _
				0, 1, xyc, 0, 0, 0, _
				0.0, st, 0, 0)

			'
			' Create SW matrix
			'
			sw = New Double(nvars - 1, nvars - 1) {}
			For i = 0 To nvars - 1
				For j = 0 To nvars - 1
					sw(i, j) = 0
				Next
			Next
			For k = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					xyc(k, i_) = xy(k, i_)
				Next
				For i_ = 0 To nvars - 1
					xyc(k, i_) = xyc(k, i_) - muc(c(k), i_)
				Next
			Next
			ablas.rmatrixgemm(nvars, nvars, npoints, 1.0, xyc, 0, _
				0, 1, xyc, 0, 0, 0, _
				0.0, sw, 0, 0)

			'
			' Maximize ratio J=(w'*ST*w)/(w'*SW*w).
			'
			' First, make transition from w to v such that w'*ST*w becomes v'*v:
			'    v  = root(ST)*w = R*w
			'    R  = root(D)*Z'
			'    w  = (root(ST)^-1)*v = RI*v
			'    RI = Z*inv(root(D))
			'    J  = (v'*v)/(v'*(RI'*SW*RI)*v)
			'    ST = Z*D*Z'
			'
			'    so we have
			'
			'    J = (v'*v) / (v'*(inv(root(D))*Z'*SW*Z*inv(root(D)))*v)  =
			'      = (v'*v) / (v'*A*v)
			'
			If Not evd.smatrixevd(st, nvars, 1, True, d, z) Then
				info = -4
				Return
			End If
			w = New Double(nvars - 1, nvars - 1) {}
			If CDbl(d(nvars - 1)) <= CDbl(0) OrElse CDbl(d(0)) <= CDbl(1000 * Math.machineepsilon * d(nvars - 1)) Then

				'
				' Special case: D[NVars-1]<=0
				' Degenerate task (all variables takes the same value).
				'
				If CDbl(d(nvars - 1)) <= CDbl(0) Then
					info = 2
					For i = 0 To nvars - 1
						For j = 0 To nvars - 1
							If i = j Then
								w(i, j) = 1
							Else
								w(i, j) = 0
							End If
						Next
					Next
					Return
				End If

				'
				' Special case: degenerate ST matrix, multicollinearity found.
				' Since we know ST eigenvalues/vectors we can translate task to
				' non-degenerate form.
				'
				' Let WG is orthogonal basis of the non zero variance subspace
				' of the ST and let WZ is orthogonal basis of the zero variance
				' subspace.
				'
				' Projection on WG allows us to use LDA on reduced M-dimensional
				' subspace, N-M vectors of WZ allows us to update reduced LDA
				' factors to full N-dimensional subspace.
				'
				m = 0
				For k = 0 To nvars - 1
					If CDbl(d(k)) <= CDbl(1000 * Math.machineepsilon * d(nvars - 1)) Then
						m = k + 1
					End If
				Next
				alglib.ap.assert(m <> 0, "FisherLDAN: internal error #1")
				xyproj = New Double(npoints - 1, nvars - m) {}
				ablas.rmatrixgemm(npoints, nvars - m, nvars, 1.0, xy, 0, _
					0, 0, z, 0, m, 0, _
					0.0, xyproj, 0, 0)
				For i = 0 To npoints - 1
					xyproj(i, nvars - m) = xy(i, nvars)
				Next
				fisherldan(xyproj, npoints, nvars - m, nclasses, info, wproj)
				If info < 0 Then
					Return
				End If
				ablas.rmatrixgemm(nvars, nvars - m, nvars - m, 1.0, z, 0, _
					m, 0, wproj, 0, 0, 0, _
					0.0, w, 0, 0)
				For k = nvars - m To nvars - 1
					For i_ = 0 To nvars - 1
						w(i_, k) = z(i_, k - (nvars - m))
					Next
				Next
				info = 2
			Else

				'
				' General case: no multicollinearity
				'
				tm = New Double(nvars - 1, nvars - 1) {}
				a = New Double(nvars - 1, nvars - 1) {}
				ablas.rmatrixgemm(nvars, nvars, nvars, 1.0, sw, 0, _
					0, 0, z, 0, 0, 0, _
					0.0, tm, 0, 0)
				ablas.rmatrixgemm(nvars, nvars, nvars, 1.0, z, 0, _
					0, 1, tm, 0, 0, 0, _
					0.0, a, 0, 0)
				For i = 0 To nvars - 1
					For j = 0 To nvars - 1
						a(i, j) = a(i, j) / System.Math.sqrt(d(i) * d(j))
					Next
				Next
				If Not evd.smatrixevd(a, nvars, 1, True, d2, z2) Then
					info = -4
					Return
				End If
				For i = 0 To nvars - 1
					For k = 0 To nvars - 1
						z2(i, k) = z2(i, k) / System.Math.sqrt(d(i))
					Next
				Next
				ablas.rmatrixgemm(nvars, nvars, nvars, 1.0, z, 0, _
					0, 0, z2, 0, 0, 0, _
					0.0, w, 0, 0)
			End If

			'
			' Post-processing:
			' * normalization
			' * converting to non-negative form, if possible
			'
			For k = 0 To nvars - 1
				v = 0.0
				For i_ = 0 To nvars - 1
					v += w(i_, k) * w(i_, k)
				Next
				v = 1 / System.Math.sqrt(v)
				For i_ = 0 To nvars - 1
					w(i_, k) = v * w(i_, k)
				Next
				v = 0
				For i = 0 To nvars - 1
					v = v + w(i, k)
				Next
				If CDbl(v) < CDbl(0) Then
					For i_ = 0 To nvars - 1
						w(i_, k) = -1 * w(i_, k)
					Next
				End If
			Next
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_fisherldan(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, ByRef w As Double(,))
			fisherldan(xy, npoints, nvars, nclasses, info, w)
		End Sub


	End Class
	Public Class mlpbase
		'************************************************************************
'        Model's errors:
'            * RelCLSError   -   fraction of misclassified cases.
'            * AvgCE         -   acerage cross-entropy
'            * RMSError      -   root-mean-square error
'            * AvgError      -   average error
'            * AvgRelError   -   average relative error
'            
'        NOTE 1: RelCLSError/AvgCE are zero on regression problems.
'
'        NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
'                errors in prediction of posterior probabilities
'        ************************************************************************

		Public Class modelerrors
			Inherits apobject
			Public relclserror As Double
			Public avgce As Double
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New modelerrors()
				_result.relclserror = relclserror
				_result.avgce = avgce
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure is used to store MLP error and gradient.
'        ************************************************************************

		Public Class smlpgrad
			Inherits apobject
			Public f As Double
			Public g As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				g = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New smlpgrad()
				_result.f = f
				_result.g = DirectCast(g.Clone(), Double())
				Return _result
			End Function
		End Class


		Public Class multilayerperceptron
			Inherits apobject
			Public hlnetworktype As Integer
			Public hlnormtype As Integer
			Public hllayersizes As Integer()
			Public hlconnections As Integer()
			Public hlneurons As Integer()
			Public structinfo As Integer()
			Public weights As Double()
			Public columnmeans As Double()
			Public columnsigmas As Double()
			Public neurons As Double()
			Public dfdnet As Double()
			Public derror As Double()
			Public x As Double()
			Public y As Double()
			Public xy As Double(,)
			Public xyrow As Double()
			Public nwbuf As Double()
			Public integerbuf As Integer()
			Public err As modelerrors
			Public rndbuf As Double()
			Public buf As alglib.smp.shared_pool
			Public gradbuf As alglib.smp.shared_pool
			Public dummydxy As Double(,)
			Public dummysxy As sparse.sparsematrix
			Public dummyidx As Integer()
			Public dummypool As alglib.smp.shared_pool
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				hllayersizes = New Integer(-1) {}
				hlconnections = New Integer(-1) {}
				hlneurons = New Integer(-1) {}
				structinfo = New Integer(-1) {}
				weights = New Double(-1) {}
				columnmeans = New Double(-1) {}
				columnsigmas = New Double(-1) {}
				neurons = New Double(-1) {}
				dfdnet = New Double(-1) {}
				derror = New Double(-1) {}
				x = New Double(-1) {}
				y = New Double(-1) {}
				xy = New Double(-1, -1) {}
				xyrow = New Double(-1) {}
				nwbuf = New Double(-1) {}
				integerbuf = New Integer(-1) {}
				err = New modelerrors()
				rndbuf = New Double(-1) {}
				buf = New alglib.smp.shared_pool()
				gradbuf = New alglib.smp.shared_pool()
				dummydxy = New Double(-1, -1) {}
				dummysxy = New sparse.sparsematrix()
				dummyidx = New Integer(-1) {}
				dummypool = New alglib.smp.shared_pool()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New multilayerperceptron()
				_result.hlnetworktype = hlnetworktype
				_result.hlnormtype = hlnormtype
				_result.hllayersizes = DirectCast(hllayersizes.Clone(), Integer())
				_result.hlconnections = DirectCast(hlconnections.Clone(), Integer())
				_result.hlneurons = DirectCast(hlneurons.Clone(), Integer())
				_result.structinfo = DirectCast(structinfo.Clone(), Integer())
				_result.weights = DirectCast(weights.Clone(), Double())
				_result.columnmeans = DirectCast(columnmeans.Clone(), Double())
				_result.columnsigmas = DirectCast(columnsigmas.Clone(), Double())
				_result.neurons = DirectCast(neurons.Clone(), Double())
				_result.dfdnet = DirectCast(dfdnet.Clone(), Double())
				_result.derror = DirectCast(derror.Clone(), Double())
				_result.x = DirectCast(x.Clone(), Double())
				_result.y = DirectCast(y.Clone(), Double())
				_result.xy = DirectCast(xy.Clone(), Double(,))
				_result.xyrow = DirectCast(xyrow.Clone(), Double())
				_result.nwbuf = DirectCast(nwbuf.Clone(), Double())
				_result.integerbuf = DirectCast(integerbuf.Clone(), Integer())
				_result.err = DirectCast(err.make_copy(), modelerrors)
				_result.rndbuf = DirectCast(rndbuf.Clone(), Double())
				_result.buf = DirectCast(buf.make_copy(), alglib.smp.shared_pool)
				_result.gradbuf = DirectCast(gradbuf.make_copy(), alglib.smp.shared_pool)
				_result.dummydxy = DirectCast(dummydxy.Clone(), Double(,))
				_result.dummysxy = DirectCast(dummysxy.make_copy(), sparse.sparsematrix)
				_result.dummyidx = DirectCast(dummyidx.Clone(), Integer())
				_result.dummypool = DirectCast(dummypool.make_copy(), alglib.smp.shared_pool)
				Return _result
			End Function
		End Class




		Public Const mlpvnum As Integer = 7
		Public Const mlpfirstversion As Integer = 0
		Public Const nfieldwidth As Integer = 4
		Public Const hlconnfieldwidth As Integer = 5
		Public Const hlnfieldwidth As Integer = 4
		Public Const gradbasecasecost As Integer = 50000
		Public Const microbatchsize As Integer = 64


		'************************************************************************
'        This function returns number of weights  updates  which  is  required  for
'        gradient calculation problem to be splitted.
'        ************************************************************************

		Public Shared Function mlpgradsplitcost() As Integer
			Dim result As Integer = 0

			result = gradbasecasecost
			Return result
		End Function


		'************************************************************************
'        This  function  returns  number  of elements in subset of dataset which is
'        required for gradient calculation problem to be splitted.
'        ************************************************************************

		Public Shared Function mlpgradsplitsize() As Integer
			Dim result As Integer = 0

			result = microbatchsize
			Return result
		End Function


		'************************************************************************
'        Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'        layers, with linear output layer. Network weights are  filled  with  small
'        random values.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreate0(nin As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			layerscount = 1 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(-5, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, 0, 0, nout, False, _
				True)
		End Sub


		'************************************************************************
'        Same  as  MLPCreate0,  but  with  one  hidden  layer  (NHid  neurons) with
'        non-linear activation function. Output layer is linear.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreate1(nin As Integer, nhid As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			layerscount = 1 + 3 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(-5, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid, 0, nout, False, _
				True)
		End Sub


		'************************************************************************
'        Same as MLPCreate0, but with two hidden layers (NHid1 and  NHid2  neurons)
'        with non-linear activation function. Output layer is linear.
'         $ALL
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreate2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			layerscount = 1 + 3 + 3 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid2, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(-5, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid1, nhid2, nout, False, _
				True)
		End Sub


		'************************************************************************
'        Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'        layers with non-linear output layer. Network weights are filled with small
'        random values.
'
'        Activation function of the output layer takes values:
'
'            (B, +INF), if D>=0
'
'        or
'
'            (-INF, B), if D<0.
'
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreateb0(nin As Integer, nout As Integer, b As Double, d As Double, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3
			If CDbl(d) >= CDbl(0) Then
				d = 1
			Else
				d = -1
			End If

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(3, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, 0, 0, nout, False, _
				False)

			'
			' Turn on ouputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = b
				network.columnsigmas(i) = d
			Next
		End Sub


		'************************************************************************
'        Same as MLPCreateB0 but with non-linear hidden layer.
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreateb1(nin As Integer, nhid As Integer, nout As Integer, b As Double, d As Double, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3 + 3
			If CDbl(d) >= CDbl(0) Then
				d = 1
			Else
				d = -1
			End If

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(3, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid, 0, nout, False, _
				False)

			'
			' Turn on ouputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = b
				network.columnsigmas(i) = d
			Next
		End Sub


		'************************************************************************
'        Same as MLPCreateB0 but with two non-linear hidden layers.
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreateb2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, b As Double, d As Double, _
			network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3 + 3 + 3
			If CDbl(d) >= CDbl(0) Then
				d = 1
			Else
				d = -1
			End If

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid2, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(3, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid1, nhid2, nout, False, _
				False)

			'
			' Turn on ouputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = b
				network.columnsigmas(i) = d
			Next
		End Sub


		'************************************************************************
'        Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
'        layers with non-linear output layer. Network weights are filled with small
'        random values. Activation function of the output layer takes values [A,B].
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreater0(nin As Integer, nout As Integer, a As Double, b As Double, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, 0, 0, nout, False, _
				False)

			'
			' Turn on outputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = 0.5 * (a + b)
				network.columnsigmas(i) = 0.5 * (a - b)
			Next
		End Sub


		'************************************************************************
'        Same as MLPCreateR0, but with non-linear hidden layer.
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreater1(nin As Integer, nhid As Integer, nout As Integer, a As Double, b As Double, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid, 0, nout, False, _
				False)

			'
			' Turn on outputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = 0.5 * (a + b)
				network.columnsigmas(i) = 0.5 * (a - b)
			Next
		End Sub


		'************************************************************************
'        Same as MLPCreateR0, but with two non-linear hidden layers.
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreater2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, a As Double, b As Double, _
			network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0
			Dim i As Integer = 0

			layerscount = 1 + 3 + 3 + 3

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid2, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, False, network)
			fillhighlevelinformation(network, nin, nhid1, nhid2, nout, False, _
				False)

			'
			' Turn on outputs shift/scaling.
			'
			For i = nin To nin + nout - 1
				network.columnmeans(i) = 0.5 * (a + b)
				network.columnsigmas(i) = 0.5 * (a - b)
			Next
		End Sub


		'************************************************************************
'        Creates classifier network with NIn  inputs  and  NOut  possible  classes.
'        Network contains no hidden layers and linear output  layer  with  SOFTMAX-
'        normalization  (so  outputs  sums  up  to  1.0  and  converge to posterior
'        probabilities).
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreatec0(nin As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			alglib.ap.assert(nout >= 2, "MLPCreateC0: NOut<2!")
			layerscount = 1 + 2 + 1

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout - 1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addzerolayer(lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, True, network)
			fillhighlevelinformation(network, nin, 0, 0, nout, True, _
				True)
		End Sub


		'************************************************************************
'        Same as MLPCreateC0, but with one non-linear hidden layer.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreatec1(nin As Integer, nhid As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			alglib.ap.assert(nout >= 2, "MLPCreateC1: NOut<2!")
			layerscount = 1 + 3 + 2 + 1

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout - 1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addzerolayer(lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, True, network)
			fillhighlevelinformation(network, nin, nhid, 0, nout, True, _
				True)
		End Sub


		'************************************************************************
'        Same as MLPCreateC0, but with two non-linear hidden layers.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreatec2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, network As multilayerperceptron)
			Dim lsizes As Integer() = New Integer(-1) {}
			Dim ltypes As Integer() = New Integer(-1) {}
			Dim lconnfirst As Integer() = New Integer(-1) {}
			Dim lconnlast As Integer() = New Integer(-1) {}
			Dim layerscount As Integer = 0
			Dim lastproc As Integer = 0

			alglib.ap.assert(nout >= 2, "MLPCreateC2: NOut<2!")
			layerscount = 1 + 3 + 3 + 2 + 1

			'
			' Allocate arrays
			'
			lsizes = New Integer(layerscount - 1) {}
			ltypes = New Integer(layerscount - 1) {}
			lconnfirst = New Integer(layerscount - 1) {}
			lconnlast = New Integer(layerscount - 1) {}

			'
			' Layers
			'
			addinputlayer(nin, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nhid2, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addactivationlayer(1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addbiasedsummatorlayer(nout - 1, lsizes, ltypes, lconnfirst, lconnlast, lastproc)
			addzerolayer(lsizes, ltypes, lconnfirst, lconnlast, lastproc)

			'
			' Create
			'
			mlpcreate(nin, nout, lsizes, ltypes, lconnfirst, lconnlast, _
				layerscount, True, network)
			fillhighlevelinformation(network, nin, nhid1, nhid2, nout, True, _
				True)
		End Sub


		'************************************************************************
'        Copying of neural network
'
'        INPUT PARAMETERS:
'            Network1 -   original
'
'        OUTPUT PARAMETERS:
'            Network2 -   copy
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcopy(network1 As multilayerperceptron, network2 As multilayerperceptron)
			mlpcopyshared(network1, network2)
		End Sub


		'************************************************************************
'        Copying of neural network (second parameter is passed as shared object).
'
'        INPUT PARAMETERS:
'            Network1 -   original
'
'        OUTPUT PARAMETERS:
'            Network2 -   copy
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcopyshared(network1 As multilayerperceptron, network2 As multilayerperceptron)
			Dim wcount As Integer = 0
			Dim i As Integer = 0
			Dim buf As New hpccores.mlpbuffers()
			Dim sgrad As New smlpgrad()


			'
			' Copy scalar and array fields
			'
			network2.hlnetworktype = network1.hlnetworktype
			network2.hlnormtype = network1.hlnormtype
			apserv.copyintegerarray(network1.hllayersizes, network2.hllayersizes)
			apserv.copyintegerarray(network1.hlconnections, network2.hlconnections)
			apserv.copyintegerarray(network1.hlneurons, network2.hlneurons)
			apserv.copyintegerarray(network1.structinfo, network2.structinfo)
			apserv.copyrealarray(network1.weights, network2.weights)
			apserv.copyrealarray(network1.columnmeans, network2.columnmeans)
			apserv.copyrealarray(network1.columnsigmas, network2.columnsigmas)
			apserv.copyrealarray(network1.neurons, network2.neurons)
			apserv.copyrealarray(network1.dfdnet, network2.dfdnet)
			apserv.copyrealarray(network1.derror, network2.derror)
			apserv.copyrealarray(network1.x, network2.x)
			apserv.copyrealarray(network1.y, network2.y)
			apserv.copyrealarray(network1.nwbuf, network2.nwbuf)
			apserv.copyintegerarray(network1.integerbuf, network2.integerbuf)

			'
			' copy buffers
			'
			wcount = mlpgetweightscount(network1)
			alglib.smp.ae_shared_pool_set_seed(network2.buf, buf)
			sgrad.g = New Double(wcount - 1) {}
			sgrad.f = 0.0
			For i = 0 To wcount - 1
				sgrad.g(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_set_seed(network2.gradbuf, sgrad)
		End Sub


		'************************************************************************
'        This function compares architectures of neural networks.  Only  geometries
'        are compared, weights and other parameters are not tested.
'
'          -- ALGLIB --
'             Copyright 20.06.2013 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpsamearchitecture(network1 As multilayerperceptron, network2 As multilayerperceptron) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim ninfo As Integer = 0

			alglib.ap.assert(alglib.ap.len(network1.structinfo) > 0 AndAlso alglib.ap.len(network1.structinfo) >= network1.structinfo(0), "MLPSameArchitecture: Network1 is uninitialized")
			alglib.ap.assert(alglib.ap.len(network2.structinfo) > 0 AndAlso alglib.ap.len(network2.structinfo) >= network2.structinfo(0), "MLPSameArchitecture: Network2 is uninitialized")
			result = False
			If network1.structinfo(0) <> network2.structinfo(0) Then
				Return result
			End If
			ninfo = network1.structinfo(0)
			For i = 0 To ninfo - 1
				If network1.structinfo(i) <> network2.structinfo(i) Then
					Return result
				End If
			Next
			result = True
			Return result
		End Function


		'************************************************************************
'        This function copies tunable  parameters (weights/means/sigmas)  from  one
'        network to another with same architecture. It  performs  some  rudimentary
'        checks that architectures are same, and throws exception if check fails.
'
'        It is intended for fast copying of states between two  network  which  are
'        known to have same geometry.
'
'        INPUT PARAMETERS:
'            Network1 -   source, must be correctly initialized
'            Network2 -   target, must have same architecture
'
'        OUTPUT PARAMETERS:
'            Network2 -   network state is copied from source to target
'
'          -- ALGLIB --
'             Copyright 20.06.2013 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcopytunableparameters(network1 As multilayerperceptron, network2 As multilayerperceptron)
			Dim i As Integer = 0
			Dim ninfo As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0

			alglib.ap.assert(alglib.ap.len(network1.structinfo) > 0 AndAlso alglib.ap.len(network1.structinfo) >= network1.structinfo(0), "MLPCopyTunableParameters: Network1 is uninitialized")
			alglib.ap.assert(alglib.ap.len(network2.structinfo) > 0 AndAlso alglib.ap.len(network2.structinfo) >= network2.structinfo(0), "MLPCopyTunableParameters: Network2 is uninitialized")
			alglib.ap.assert(network1.structinfo(0) = network2.structinfo(0), "MLPCopyTunableParameters: Network1 geometry differs from that of Network2")
			ninfo = network1.structinfo(0)
			For i = 0 To ninfo - 1
				alglib.ap.assert(network1.structinfo(i) = network2.structinfo(i), "MLPCopyTunableParameters: Network1 geometry differs from that of Network2")
			Next
			mlpproperties(network1, nin, nout, wcount)
			For i = 0 To wcount - 1
				network2.weights(i) = network1.weights(i)
			Next
			If mlpissoftmax(network1) Then
				For i = 0 To nin - 1
					network2.columnmeans(i) = network1.columnmeans(i)
					network2.columnsigmas(i) = network1.columnsigmas(i)
				Next
			Else
				For i = 0 To nin + nout - 1
					network2.columnmeans(i) = network1.columnmeans(i)
					network2.columnsigmas(i) = network1.columnsigmas(i)
				Next
			End If
		End Sub


		'************************************************************************
'        This  function  exports  tunable   parameters  (weights/means/sigmas) from
'        network to contiguous array. Nothing is guaranteed about array format, the
'        only thing you can count for is that MLPImportTunableParameters() will  be
'        able to parse it.
'
'        It is intended for fast copying of states between network and backup array
'
'        INPUT PARAMETERS:
'            Network     -   source, must be correctly initialized
'            P           -   array to use. If its size is enough to store data,  it
'                            is reused.
'
'        OUTPUT PARAMETERS:
'            P           -   array which stores network parameters, resized if needed
'            PCount      -   number of parameters stored in array.
'
'          -- ALGLIB --
'             Copyright 20.06.2013 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpexporttunableparameters(network As multilayerperceptron, ByRef p As Double(), ByRef pcount As Integer)
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0

			pcount = 0

			alglib.ap.assert(alglib.ap.len(network.structinfo) > 0 AndAlso alglib.ap.len(network.structinfo) >= network.structinfo(0), "MLPExportTunableParameters: Network is uninitialized")
			mlpproperties(network, nin, nout, wcount)
			If mlpissoftmax(network) Then
				pcount = wcount + 2 * nin
				apserv.rvectorsetlengthatleast(p, pcount)
				k = 0
				For i = 0 To wcount - 1
					p(k) = network.weights(i)
					k = k + 1
				Next
				For i = 0 To nin - 1
					p(k) = network.columnmeans(i)
					k = k + 1
					p(k) = network.columnsigmas(i)
					k = k + 1
				Next
			Else
				pcount = wcount + 2 * (nin + nout)
				apserv.rvectorsetlengthatleast(p, pcount)
				k = 0
				For i = 0 To wcount - 1
					p(k) = network.weights(i)
					k = k + 1
				Next
				For i = 0 To nin + nout - 1
					p(k) = network.columnmeans(i)
					k = k + 1
					p(k) = network.columnsigmas(i)
					k = k + 1
				Next
			End If
		End Sub


		'************************************************************************
'        This  function imports  tunable   parameters  (weights/means/sigmas) which
'        were exported by MLPExportTunableParameters().
'
'        It is intended for fast copying of states between network and backup array
'
'        INPUT PARAMETERS:
'            Network     -   target:
'                            * must be correctly initialized
'                            * must have same geometry as network used to export params
'            P           -   array with parameters
'
'          -- ALGLIB --
'             Copyright 20.06.2013 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpimporttunableparameters(network As multilayerperceptron, p As Double())
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0

			alglib.ap.assert(alglib.ap.len(network.structinfo) > 0 AndAlso alglib.ap.len(network.structinfo) >= network.structinfo(0), "MLPImportTunableParameters: Network is uninitialized")
			mlpproperties(network, nin, nout, wcount)
			If mlpissoftmax(network) Then
				k = 0
				For i = 0 To wcount - 1
					network.weights(i) = p(k)
					k = k + 1
				Next
				For i = 0 To nin - 1
					network.columnmeans(i) = p(k)
					k = k + 1
					network.columnsigmas(i) = p(k)
					k = k + 1
				Next
			Else
				k = 0
				For i = 0 To wcount - 1
					network.weights(i) = p(k)
					k = k + 1
				Next
				For i = 0 To nin + nout - 1
					network.columnmeans(i) = p(k)
					k = k + 1
					network.columnsigmas(i) = p(k)
					k = k + 1
				Next
			End If
		End Sub


		'************************************************************************
'        Serialization of MultiLayerPerceptron strucure
'
'        INPUT PARAMETERS:
'            Network -   original
'
'        OUTPUT PARAMETERS:
'            RA      -   array of real numbers which stores network,
'                        array[0..RLen-1]
'            RLen    -   RA lenght
'
'          -- ALGLIB --
'             Copyright 29.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpserializeold(network As multilayerperceptron, ByRef ra As Double(), ByRef rlen As Integer)
			Dim i As Integer = 0
			Dim ssize As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim sigmalen As Integer = 0
			Dim offs As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			ra = New Double(-1) {}
			rlen = 0


			'
			' Unload info
			'
			ssize = network.structinfo(0)
			nin = network.structinfo(1)
			nout = network.structinfo(2)
			wcount = network.structinfo(4)
			If mlpissoftmax(network) Then
				sigmalen = nin
			Else
				sigmalen = nin + nout
			End If

			'
			'  RA format:
			'      LEN         DESRC.
			'      1           RLen
			'      1           version (MLPVNum)
			'      1           StructInfo size
			'      SSize       StructInfo
			'      WCount      Weights
			'      SigmaLen    ColumnMeans
			'      SigmaLen    ColumnSigmas
			'
			rlen = 3 + ssize + wcount + 2 * sigmalen
			ra = New Double(rlen - 1) {}
			ra(0) = rlen
			ra(1) = mlpvnum
			ra(2) = ssize
			offs = 3
			For i = 0 To ssize - 1
				ra(offs + i) = network.structinfo(i)
			Next
			offs = offs + ssize
			i1_ = (0) - (offs)
			For i_ = offs To offs + wcount - 1
				ra(i_) = network.weights(i_ + i1_)
			Next
			offs = offs + wcount
			i1_ = (0) - (offs)
			For i_ = offs To offs + sigmalen - 1
				ra(i_) = network.columnmeans(i_ + i1_)
			Next
			offs = offs + sigmalen
			i1_ = (0) - (offs)
			For i_ = offs To offs + sigmalen - 1
				ra(i_) = network.columnsigmas(i_ + i1_)
			Next
			offs = offs + sigmalen
		End Sub


		'************************************************************************
'        Unserialization of MultiLayerPerceptron strucure
'
'        INPUT PARAMETERS:
'            RA      -   real array which stores network
'
'        OUTPUT PARAMETERS:
'            Network -   restored network
'
'          -- ALGLIB --
'             Copyright 29.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpunserializeold(ra As Double(), network As multilayerperceptron)
			Dim i As Integer = 0
			Dim ssize As Integer = 0
			Dim ntotal As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim sigmalen As Integer = 0
			Dim offs As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(ra(1)))) = mlpvnum, "MLPUnserialize: incorrect array!")

			'
			' Unload StructInfo from IA
			'
			offs = 3
			ssize = CInt(System.Math.Truncate(System.Math.Round(ra(2))))
			network.structinfo = New Integer(ssize - 1) {}
			For i = 0 To ssize - 1
				network.structinfo(i) = CInt(System.Math.Truncate(System.Math.Round(ra(offs + i))))
			Next
			offs = offs + ssize

			'
			' Unload info from StructInfo
			'
			ssize = network.structinfo(0)
			nin = network.structinfo(1)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			wcount = network.structinfo(4)
			If network.structinfo(6) = 0 Then
				sigmalen = nin + nout
			Else
				sigmalen = nin
			End If

			'
			' Allocate space for other fields
			'
			network.weights = New Double(wcount - 1) {}
			network.columnmeans = New Double(sigmalen - 1) {}
			network.columnsigmas = New Double(sigmalen - 1) {}
			network.neurons = New Double(ntotal - 1) {}
			network.nwbuf = New Double(System.Math.Max(wcount, 2 * nout) - 1) {}
			network.dfdnet = New Double(ntotal - 1) {}
			network.x = New Double(nin - 1) {}
			network.y = New Double(nout - 1) {}
			network.derror = New Double(ntotal - 1) {}

			'
			' Copy parameters from RA
			'
			i1_ = (offs) - (0)
			For i_ = 0 To wcount - 1
				network.weights(i_) = ra(i_ + i1_)
			Next
			offs = offs + wcount
			i1_ = (offs) - (0)
			For i_ = 0 To sigmalen - 1
				network.columnmeans(i_) = ra(i_ + i1_)
			Next
			offs = offs + sigmalen
			i1_ = (offs) - (0)
			For i_ = 0 To sigmalen - 1
				network.columnsigmas(i_) = ra(i_ + i1_)
			Next
			offs = offs + sigmalen
		End Sub


		'************************************************************************
'        Randomization of neural network weights
'
'          -- ALGLIB --
'             Copyright 06.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlprandomize(network As multilayerperceptron)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim r As New hqrnd.hqrndstate()
			Dim entrysize As Integer = 0
			Dim entryoffs As Integer = 0
			Dim neuronidx As Integer = 0
			Dim neurontype As Integer = 0
			Dim vmean As Double = 0
			Dim vvar As Double = 0
			Dim i As Integer = 0
			Dim n1 As Integer = 0
			Dim n2 As Integer = 0
			Dim desiredsigma As Double = 0
			Dim montecarlocnt As Integer = 0
			Dim ef As Double = 0
			Dim ef2 As Double = 0
			Dim v As Double = 0
			Dim wscale As Double = 0

			hqrnd.hqrndrandomize(r)
			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)
			desiredsigma = 0.5
			montecarlocnt = 20

			'
			' Stage 1:
			' * Network.Weights is filled by standard deviation of weights
			' * default values: sigma=1
			'
			For i = 0 To wcount - 1
				network.weights(i) = 1.0
			Next

			'
			' Stage 2:
			' * assume that input neurons have zero mean and unit standard deviation
			' * assume that constant neurons have zero standard deviation
			' * perform forward pass along neurons
			' * for each non-input non-constant neuron:
			'   * calculate mean and standard deviation of neuron's output
			'     assuming that we know means/deviations of neurons which feed it
			'     and assuming that weights has unit variance and zero mean.
			' * for each nonlinear neuron additionally we perform backward pass:
			'   * scale variances of weights which feed it in such way that neuron's
			'     input has unit standard deviation
			'
			' NOTE: this algorithm assumes that each connection feeds at most one
			'       non-linear neuron. This assumption can be incorrect in upcoming
			'       architectures with strong neurons. However, algorithm should
			'       work smoothly even in this case.
			'
			' During this stage we use Network.RndBuf, which is grouped into NTotal
			' entries, each of them having following format:
			'
			' Buf[Offset+0]        mean value of neuron's output
			' Buf[Offset+1]        standard deviation of neuron's output
			' 
			'
			'
			entrysize = 2
			apserv.rvectorsetlengthatleast(network.rndbuf, entrysize * ntotal)
			For neuronidx = 0 To ntotal - 1
				neurontype = network.structinfo(istart + neuronidx * nfieldwidth + 0)
				entryoffs = entrysize * neuronidx
				If neurontype = -2 Then

					'
					' Input neuron: zero mean, unit variance.
					'
					network.rndbuf(entryoffs + 0) = 0.0
					network.rndbuf(entryoffs + 1) = 1.0
					Continue For
				End If
				If neurontype = -3 Then

					'
					' "-1" neuron: mean=-1, zero variance.
					'
					network.rndbuf(entryoffs + 0) = -1.0
					network.rndbuf(entryoffs + 1) = 0.0
					Continue For
				End If
				If neurontype = -4 Then

					'
					' "0" neuron: mean=0, zero variance.
					'
					network.rndbuf(entryoffs + 0) = 0.0
					network.rndbuf(entryoffs + 1) = 0.0
					Continue For
				End If
				If neurontype = 0 Then

					'
					' Adaptive summator neuron:
					' * calculate its mean and variance.
					' * we assume that weights of this neuron have unit variance and zero mean.
					' * thus, neuron's output is always have zero mean
					' * as for variance, it is a bit more interesting:
					'   * let n[i] is i-th input neuron
					'   * let w[i] is i-th weight
					'   * we assume that n[i] and w[i] are independently distributed
					'   * Var(n0*w0+n1*w1+...) = Var(n0*w0)+Var(n1*w1)+...
					'   * Var(X*Y) = mean(X)^2*Var(Y) + mean(Y)^2*Var(X) + Var(X)*Var(Y)
					'   * mean(w[i])=0, var(w[i])=1
					'   * Var(n[i]*w[i]) = mean(n[i])^2 + Var(n[i])
					'
					n1 = network.structinfo(istart + neuronidx * nfieldwidth + 2)
					n2 = n1 + network.structinfo(istart + neuronidx * nfieldwidth + 1) - 1
					vmean = 0.0
					vvar = 0.0
					For i = n1 To n2
						vvar = vvar + Math.sqr(network.rndbuf(entrysize * i + 0)) + Math.sqr(network.rndbuf(entrysize * i + 1))
					Next
					network.rndbuf(entryoffs + 0) = vmean
					network.rndbuf(entryoffs + 1) = System.Math.sqrt(vvar)
					Continue For
				End If
				If neurontype = -5 Then

					'
					' Linear activation function
					'
					i = network.structinfo(istart + neuronidx * nfieldwidth + 2)
					vmean = network.rndbuf(entrysize * i + 0)
					vvar = Math.sqr(network.rndbuf(entrysize * i + 1))
					If CDbl(vvar) > CDbl(0) Then
						wscale = desiredsigma / System.Math.sqrt(vvar)
					Else
						wscale = 1.0
					End If
					randomizebackwardpass(network, i, wscale)
					network.rndbuf(entryoffs + 0) = vmean * wscale
					network.rndbuf(entryoffs + 1) = desiredsigma
					Continue For
				End If
				If neurontype > 0 Then

					'
					' Nonlinear activation function:
					' * scale its inputs
					' * estimate mean/sigma of its output using Monte-Carlo method
					'   (we simulate different inputs with unit deviation and
					'   sample activation function output on such inputs)
					'
					i = network.structinfo(istart + neuronidx * nfieldwidth + 2)
					vmean = network.rndbuf(entrysize * i + 0)
					vvar = Math.sqr(network.rndbuf(entrysize * i + 1))
					If CDbl(vvar) > CDbl(0) Then
						wscale = desiredsigma / System.Math.sqrt(vvar)
					Else
						wscale = 1.0
					End If
					randomizebackwardpass(network, i, wscale)
					ef = 0.0
					ef2 = 0.0
					vmean = vmean * wscale
					For i = 0 To montecarlocnt - 1
						v = vmean + desiredsigma * hqrnd.hqrndnormal(r)
						ef = ef + v
						ef2 = ef2 + v * v
					Next
					ef = ef / montecarlocnt
					ef2 = ef2 / montecarlocnt
					network.rndbuf(entryoffs + 0) = ef
					network.rndbuf(entryoffs + 1) = System.Math.Max(ef2 - ef * ef, 0.0)
					Continue For
				End If
				alglib.ap.assert(False, "MLPRandomize: unexpected neuron type")
			Next

			'
			' Stage 3: generate weights.
			'
			For i = 0 To wcount - 1
				network.weights(i) = network.weights(i) * hqrnd.hqrndnormal(r)
			Next
		End Sub


		'************************************************************************
'        Randomization of neural network weights and standartisator
'
'          -- ALGLIB --
'             Copyright 10.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlprandomizefull(network As multilayerperceptron)
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim ntype As Integer = 0

			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Process network
			'
			mlprandomize(network)
			For i = 0 To nin - 1
				network.columnmeans(i) = Math.randomreal() - 0.5
				network.columnsigmas(i) = Math.randomreal() + 0.5
			Next
			If Not mlpissoftmax(network) Then
				For i = 0 To nout - 1
					offs = istart + (ntotal - nout + i) * nfieldwidth
					ntype = network.structinfo(offs + 0)
					If ntype = 0 Then

						'
						' Shifts are changed only for linear outputs neurons
						'
						network.columnmeans(nin + i) = 2 * Math.randomreal() - 1
					End If
					If ntype = 0 OrElse ntype = 3 Then

						'
						' Scales are changed only for linear or bounded outputs neurons.
						' Note that scale randomization preserves sign.
						'
						network.columnsigmas(nin + i) = System.Math.Sign(network.columnsigmas(nin + i)) * (1.5 * Math.randomreal() + 0.5)
					End If
				Next
			End If
		End Sub


		'************************************************************************
'        Internal subroutine.
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpinitpreprocessor(network As multilayerperceptron, xy As Double(,), ssize As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim jmax As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim ntype As Integer = 0
			Dim means As Double() = New Double(-1) {}
			Dim sigmas As Double() = New Double(-1) {}
			Dim s As Double = 0

			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Means/Sigmas
			'
			If mlpissoftmax(network) Then
				jmax = nin - 1
			Else
				jmax = nin + nout - 1
			End If
			means = New Double(jmax) {}
			sigmas = New Double(jmax) {}
			For i = 0 To jmax
				means(i) = 0
				sigmas(i) = 0
			Next
			For i = 0 To ssize - 1
				For j = 0 To jmax
					means(j) = means(j) + xy(i, j)
				Next
			Next
			For i = 0 To jmax
				means(i) = means(i) / ssize
			Next
			For i = 0 To ssize - 1
				For j = 0 To jmax
					sigmas(j) = sigmas(j) + Math.sqr(xy(i, j) - means(j))
				Next
			Next
			For i = 0 To jmax
				sigmas(i) = System.Math.sqrt(sigmas(i) / ssize)
			Next

			'
			' Inputs
			'
			For i = 0 To nin - 1
				network.columnmeans(i) = means(i)
				network.columnsigmas(i) = sigmas(i)
				If CDbl(network.columnsigmas(i)) = CDbl(0) Then
					network.columnsigmas(i) = 1
				End If
			Next

			'
			' Outputs
			'
			If Not mlpissoftmax(network) Then
				For i = 0 To nout - 1
					offs = istart + (ntotal - nout + i) * nfieldwidth
					ntype = network.structinfo(offs + 0)

					'
					' Linear outputs
					'
					If ntype = 0 Then
						network.columnmeans(nin + i) = means(nin + i)
						network.columnsigmas(nin + i) = sigmas(nin + i)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If

					'
					' Bounded outputs (half-interval)
					'
					If ntype = 3 Then
						s = means(nin + i) - network.columnmeans(nin + i)
						If CDbl(s) = CDbl(0) Then
							s = System.Math.Sign(network.columnsigmas(nin + i))
						End If
						If CDbl(s) = CDbl(0) Then
							s = 1.0
						End If
						network.columnsigmas(nin + i) = System.Math.Sign(network.columnsigmas(nin + i)) * System.Math.Abs(s)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If
				Next
			End If
		End Sub


		'************************************************************************
'        Internal subroutine.
'        Initialization for preprocessor based on a sample.
'
'        INPUT
'            Network -   initialized neural network;
'            XY      -   sample, given by sparse matrix;
'            SSize   -   sample size.
'
'        OUTPUT
'            Network -   neural network with initialised preprocessor.
'
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpinitpreprocessorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, ssize As Integer)
			Dim jmax As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim ntype As Integer = 0
			Dim means As Double() = New Double(-1) {}
			Dim sigmas As Double() = New Double(-1) {}
			Dim s As Double = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Means/Sigmas
			'
			If mlpissoftmax(network) Then
				jmax = nin - 1
			Else
				jmax = nin + nout - 1
			End If
			means = New Double(jmax) {}
			sigmas = New Double(jmax) {}
			For i = 0 To jmax
				means(i) = 0
				sigmas(i) = 0
			Next
			For i = 0 To ssize - 1
				sparse.sparsegetrow(xy, i, network.xyrow)
				For j = 0 To jmax
					means(j) = means(j) + network.xyrow(j)
				Next
			Next
			For i = 0 To jmax
				means(i) = means(i) / ssize
			Next
			For i = 0 To ssize - 1
				sparse.sparsegetrow(xy, i, network.xyrow)
				For j = 0 To jmax
					sigmas(j) = sigmas(j) + Math.sqr(network.xyrow(j) - means(j))
				Next
			Next
			For i = 0 To jmax
				sigmas(i) = System.Math.sqrt(sigmas(i) / ssize)
			Next

			'
			' Inputs
			'
			For i = 0 To nin - 1
				network.columnmeans(i) = means(i)
				network.columnsigmas(i) = sigmas(i)
				If CDbl(network.columnsigmas(i)) = CDbl(0) Then
					network.columnsigmas(i) = 1
				End If
			Next

			'
			' Outputs
			'
			If Not mlpissoftmax(network) Then
				For i = 0 To nout - 1
					offs = istart + (ntotal - nout + i) * nfieldwidth
					ntype = network.structinfo(offs + 0)

					'
					' Linear outputs
					'
					If ntype = 0 Then
						network.columnmeans(nin + i) = means(nin + i)
						network.columnsigmas(nin + i) = sigmas(nin + i)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If

					'
					' Bounded outputs (half-interval)
					'
					If ntype = 3 Then
						s = means(nin + i) - network.columnmeans(nin + i)
						If CDbl(s) = CDbl(0) Then
							s = System.Math.Sign(network.columnsigmas(nin + i))
						End If
						If CDbl(s) = CDbl(0) Then
							s = 1.0
						End If
						network.columnsigmas(nin + i) = System.Math.Sign(network.columnsigmas(nin + i)) * System.Math.Abs(s)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If
				Next
			End If
		End Sub


		'************************************************************************
'        Internal subroutine.
'        Initialization for preprocessor based on a subsample.
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset; one sample = one row;
'                        first NIn columns contain inputs,
'                        next NOut columns - desired outputs.
'            SetSize -   real size of XY, SetSize>=0;
'            Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                        * Idx[I] stores row index in the original dataset which is
'                          given by XY. Gradient is calculated with respect to rows
'                          whose indexes are stored in Idx[].
'                        * Idx[]  must store correct indexes; this function  throws
'                          an  exception  in  case  incorrect index (less than 0 or
'                          larger than rows(XY)) is given
'                        * Idx[]  may  store  indexes  in  any  order and even with
'                          repetitions.
'            SubsetSize- number of elements in Idx[] array.
'
'        OUTPUT:
'            Network -   neural network with initialised preprocessor.
'            
'        NOTE: when  SubsetSize<0 is used full dataset by call MLPInitPreprocessor
'              function.
'
'          -- ALGLIB --
'             Copyright 23.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpinitpreprocessorsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, idx As Integer(), subsetsize As Integer)
			Dim jmax As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim ntype As Integer = 0
			Dim means As Double() = New Double(-1) {}
			Dim sigmas As Double() = New Double(-1) {}
			Dim s As Double = 0
			Dim npoints As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(setsize >= 0, "MLPInitPreprocessorSubset: SetSize<0")
			If subsetsize < 0 Then
				mlpinitpreprocessor(network, xy, setsize)
				Return
			End If
			alglib.ap.assert(subsetsize <= alglib.ap.len(idx), "MLPInitPreprocessorSubset: SubsetSize>Length(Idx)")
			npoints = setsize
			For i = 0 To subsetsize - 1
				alglib.ap.assert(idx(i) >= 0, "MLPInitPreprocessorSubset: incorrect index of XY row(Idx[I]<0)")
				alglib.ap.assert(idx(i) <= npoints - 1, "MLPInitPreprocessorSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)")
			Next
			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Means/Sigmas
			'
			If mlpissoftmax(network) Then
				jmax = nin - 1
			Else
				jmax = nin + nout - 1
			End If
			means = New Double(jmax) {}
			sigmas = New Double(jmax) {}
			For i = 0 To jmax
				means(i) = 0
				sigmas(i) = 0
			Next
			For i = 0 To subsetsize - 1
				For j = 0 To jmax
					means(j) = means(j) + xy(idx(i), j)
				Next
			Next
			For i = 0 To jmax
				means(i) = means(i) / subsetsize
			Next
			For i = 0 To subsetsize - 1
				For j = 0 To jmax
					sigmas(j) = sigmas(j) + Math.sqr(xy(idx(i), j) - means(j))
				Next
			Next
			For i = 0 To jmax
				sigmas(i) = System.Math.sqrt(sigmas(i) / subsetsize)
			Next

			'
			' Inputs
			'
			For i = 0 To nin - 1
				network.columnmeans(i) = means(i)
				network.columnsigmas(i) = sigmas(i)
				If CDbl(network.columnsigmas(i)) = CDbl(0) Then
					network.columnsigmas(i) = 1
				End If
			Next

			'
			' Outputs
			'
			If Not mlpissoftmax(network) Then
				For i = 0 To nout - 1
					offs = istart + (ntotal - nout + i) * nfieldwidth
					ntype = network.structinfo(offs + 0)

					'
					' Linear outputs
					'
					If ntype = 0 Then
						network.columnmeans(nin + i) = means(nin + i)
						network.columnsigmas(nin + i) = sigmas(nin + i)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If

					'
					' Bounded outputs (half-interval)
					'
					If ntype = 3 Then
						s = means(nin + i) - network.columnmeans(nin + i)
						If CDbl(s) = CDbl(0) Then
							s = System.Math.Sign(network.columnsigmas(nin + i))
						End If
						If CDbl(s) = CDbl(0) Then
							s = 1.0
						End If
						network.columnsigmas(nin + i) = System.Math.Sign(network.columnsigmas(nin + i)) * System.Math.Abs(s)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If
				Next
			End If
		End Sub


		'************************************************************************
'        Internal subroutine.
'        Initialization for preprocessor based on a subsample.
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset, given by sparse matrix;
'                        one sample = one row;
'                        first NIn columns contain inputs,
'                        next NOut columns - desired outputs.
'            SetSize -   real size of XY, SetSize>=0;
'            Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                        * Idx[I] stores row index in the original dataset which is
'                          given by XY. Gradient is calculated with respect to rows
'                          whose indexes are stored in Idx[].
'                        * Idx[]  must store correct indexes; this function  throws
'                          an  exception  in  case  incorrect index (less than 0 or
'                          larger than rows(XY)) is given
'                        * Idx[]  may  store  indexes  in  any  order and even with
'                          repetitions.
'            SubsetSize- number of elements in Idx[] array.
'            
'        OUTPUT:
'            Network -   neural network with initialised preprocessor.
'            
'        NOTE: when SubsetSize<0 is used full dataset by call
'              MLPInitPreprocessorSparse function.
'              
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpinitpreprocessorsparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, idx As Integer(), subsetsize As Integer)
			Dim jmax As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim ntype As Integer = 0
			Dim means As Double() = New Double(-1) {}
			Dim sigmas As Double() = New Double(-1) {}
			Dim s As Double = 0
			Dim npoints As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(setsize >= 0, "MLPInitPreprocessorSparseSubset: SetSize<0")
			If subsetsize < 0 Then
				mlpinitpreprocessorsparse(network, xy, setsize)
				Return
			End If
			alglib.ap.assert(subsetsize <= alglib.ap.len(idx), "MLPInitPreprocessorSparseSubset: SubsetSize>Length(Idx)")
			npoints = setsize
			For i = 0 To subsetsize - 1
				alglib.ap.assert(idx(i) >= 0, "MLPInitPreprocessorSparseSubset: incorrect index of XY row(Idx[I]<0)")
				alglib.ap.assert(idx(i) <= npoints - 1, "MLPInitPreprocessorSparseSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)")
			Next
			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Means/Sigmas
			'
			If mlpissoftmax(network) Then
				jmax = nin - 1
			Else
				jmax = nin + nout - 1
			End If
			means = New Double(jmax) {}
			sigmas = New Double(jmax) {}
			For i = 0 To jmax
				means(i) = 0
				sigmas(i) = 0
			Next
			For i = 0 To subsetsize - 1
				sparse.sparsegetrow(xy, idx(i), network.xyrow)
				For j = 0 To jmax
					means(j) = means(j) + network.xyrow(j)
				Next
			Next
			For i = 0 To jmax
				means(i) = means(i) / subsetsize
			Next
			For i = 0 To subsetsize - 1
				sparse.sparsegetrow(xy, idx(i), network.xyrow)
				For j = 0 To jmax
					sigmas(j) = sigmas(j) + Math.sqr(network.xyrow(j) - means(j))
				Next
			Next
			For i = 0 To jmax
				sigmas(i) = System.Math.sqrt(sigmas(i) / subsetsize)
			Next

			'
			' Inputs
			'
			For i = 0 To nin - 1
				network.columnmeans(i) = means(i)
				network.columnsigmas(i) = sigmas(i)
				If CDbl(network.columnsigmas(i)) = CDbl(0) Then
					network.columnsigmas(i) = 1
				End If
			Next

			'
			' Outputs
			'
			If Not mlpissoftmax(network) Then
				For i = 0 To nout - 1
					offs = istart + (ntotal - nout + i) * nfieldwidth
					ntype = network.structinfo(offs + 0)

					'
					' Linear outputs
					'
					If ntype = 0 Then
						network.columnmeans(nin + i) = means(nin + i)
						network.columnsigmas(nin + i) = sigmas(nin + i)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If

					'
					' Bounded outputs (half-interval)
					'
					If ntype = 3 Then
						s = means(nin + i) - network.columnmeans(nin + i)
						If CDbl(s) = CDbl(0) Then
							s = System.Math.Sign(network.columnsigmas(nin + i))
						End If
						If CDbl(s) = CDbl(0) Then
							s = 1.0
						End If
						network.columnsigmas(nin + i) = System.Math.Sign(network.columnsigmas(nin + i)) * System.Math.Abs(s)
						If CDbl(network.columnsigmas(nin + i)) = CDbl(0) Then
							network.columnsigmas(nin + i) = 1
						End If
					End If
				Next
			End If
		End Sub


		'************************************************************************
'        Returns information about initialized network: number of inputs, outputs,
'        weights.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpproperties(network As multilayerperceptron, ByRef nin As Integer, ByRef nout As Integer, ByRef wcount As Integer)
			nin = 0
			nout = 0
			wcount = 0

			nin = network.structinfo(1)
			nout = network.structinfo(2)
			wcount = network.structinfo(4)
		End Sub


		'************************************************************************
'        Returns number of "internal", low-level neurons in the network (one  which
'        is stored in StructInfo).
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpntotal(network As multilayerperceptron) As Integer
			Dim result As Integer = 0

			result = network.structinfo(3)
			Return result
		End Function


		'************************************************************************
'        Returns number of inputs.
'
'          -- ALGLIB --
'             Copyright 19.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetinputscount(network As multilayerperceptron) As Integer
			Dim result As Integer = 0

			result = network.structinfo(1)
			Return result
		End Function


		'************************************************************************
'        Returns number of outputs.
'
'          -- ALGLIB --
'             Copyright 19.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetoutputscount(network As multilayerperceptron) As Integer
			Dim result As Integer = 0

			result = network.structinfo(2)
			Return result
		End Function


		'************************************************************************
'        Returns number of weights.
'
'          -- ALGLIB --
'             Copyright 19.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetweightscount(network As multilayerperceptron) As Integer
			Dim result As Integer = 0

			result = network.structinfo(4)
			Return result
		End Function


		'************************************************************************
'        Tells whether network is SOFTMAX-normalized (i.e. classifier) or not.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpissoftmax(network As multilayerperceptron) As Boolean
			Dim result As New Boolean()

			result = network.structinfo(6) = 1
			Return result
		End Function


		'************************************************************************
'        This function returns total number of layers (including input, hidden and
'        output layers).
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetlayerscount(network As multilayerperceptron) As Integer
			Dim result As Integer = 0

			result = alglib.ap.len(network.hllayersizes)
			Return result
		End Function


		'************************************************************************
'        This function returns size of K-th layer.
'
'        K=0 corresponds to input layer, K=CNT-1 corresponds to output layer.
'
'        Size of the output layer is always equal to the number of outputs, although
'        when we have softmax-normalized network, last neuron doesn't have any
'        connections - it is just zero.
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetlayersize(network As multilayerperceptron, k As Integer) As Integer
			Dim result As Integer = 0

			alglib.ap.assert(k >= 0 AndAlso k < alglib.ap.len(network.hllayersizes), "MLPGetLayerSize: incorrect layer index")
			result = network.hllayersizes(k)
			Return result
		End Function


		'************************************************************************
'        This function returns offset/scaling coefficients for I-th input of the
'        network.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            I           -   input index
'
'        OUTPUT PARAMETERS:
'            Mean        -   mean term
'            Sigma       -   sigma term, guaranteed to be nonzero.
'
'        I-th input is passed through linear transformation
'            IN[i] = (IN[i]-Mean)/Sigma
'        before feeding to the network
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgetinputscaling(network As multilayerperceptron, i As Integer, ByRef mean As Double, ByRef sigma As Double)
			mean = 0
			sigma = 0

			alglib.ap.assert(i >= 0 AndAlso i < network.hllayersizes(0), "MLPGetInputScaling: incorrect (nonexistent) I")
			mean = network.columnmeans(i)
			sigma = network.columnsigmas(i)
			If CDbl(sigma) = CDbl(0) Then
				sigma = 1
			End If
		End Sub


		'************************************************************************
'        This function returns offset/scaling coefficients for I-th output of the
'        network.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            I           -   input index
'
'        OUTPUT PARAMETERS:
'            Mean        -   mean term
'            Sigma       -   sigma term, guaranteed to be nonzero.
'
'        I-th output is passed through linear transformation
'            OUT[i] = OUT[i]*Sigma+Mean
'        before returning it to user. In case we have SOFTMAX-normalized network,
'        we return (Mean,Sigma)=(0.0,1.0).
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgetoutputscaling(network As multilayerperceptron, i As Integer, ByRef mean As Double, ByRef sigma As Double)
			mean = 0
			sigma = 0

			alglib.ap.assert(i >= 0 AndAlso i < network.hllayersizes(alglib.ap.len(network.hllayersizes) - 1), "MLPGetOutputScaling: incorrect (nonexistent) I")
			If network.structinfo(6) = 1 Then
				mean = 0
				sigma = 1
			Else
				mean = network.columnmeans(network.hllayersizes(0) + i)
				sigma = network.columnsigmas(network.hllayersizes(0) + i)
			End If
		End Sub


		'************************************************************************
'        This function returns information about Ith neuron of Kth layer
'
'        INPUT PARAMETERS:
'            Network     -   network
'            K           -   layer index
'            I           -   neuron index (within layer)
'
'        OUTPUT PARAMETERS:
'            FKind       -   activation function type (used by MLPActivationFunction())
'                            this value is zero for input or linear neurons
'            Threshold   -   also called offset, bias
'                            zero for input neurons
'                            
'        NOTE: this function throws exception if layer or neuron with  given  index
'        do not exists.
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgetneuroninfo(network As multilayerperceptron, k As Integer, i As Integer, ByRef fkind As Integer, ByRef threshold As Double)
			Dim ncnt As Integer = 0
			Dim istart As Integer = 0
			Dim highlevelidx As Integer = 0
			Dim activationoffset As Integer = 0

			fkind = 0
			threshold = 0

			ncnt = alglib.ap.len(network.hlneurons) \ hlnfieldwidth
			istart = network.structinfo(5)

			'
			' search
			'
			network.integerbuf(0) = k
			network.integerbuf(1) = i
			highlevelidx = apserv.recsearch(network.hlneurons, hlnfieldwidth, 2, 0, ncnt, network.integerbuf)
			alglib.ap.assert(highlevelidx >= 0, "MLPGetNeuronInfo: incorrect (nonexistent) layer or neuron index")

			'
			' 1. find offset of the activation function record in the
			'
			If network.hlneurons(highlevelidx * hlnfieldwidth + 2) >= 0 Then
				activationoffset = istart + network.hlneurons(highlevelidx * hlnfieldwidth + 2) * nfieldwidth
				fkind = network.structinfo(activationoffset + 0)
			Else
				fkind = 0
			End If
			If network.hlneurons(highlevelidx * hlnfieldwidth + 3) >= 0 Then
				threshold = network.weights(network.hlneurons(highlevelidx * hlnfieldwidth + 3))
			Else
				threshold = 0
			End If
		End Sub


		'************************************************************************
'        This function returns information about connection from I0-th neuron of
'        K0-th layer to I1-th neuron of K1-th layer.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            K0          -   layer index
'            I0          -   neuron index (within layer)
'            K1          -   layer index
'            I1          -   neuron index (within layer)
'
'        RESULT:
'            connection weight (zero for non-existent connections)
'
'        This function:
'        1. throws exception if layer or neuron with given index do not exists.
'        2. returns zero if neurons exist, but there is no connection between them
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpgetweight(network As multilayerperceptron, k0 As Integer, i0 As Integer, k1 As Integer, i1 As Integer) As Double
			Dim result As Double = 0
			Dim ccnt As Integer = 0
			Dim highlevelidx As Integer = 0

			ccnt = alglib.ap.len(network.hlconnections) \ hlconnfieldwidth

			'
			' check params
			'
			alglib.ap.assert(k0 >= 0 AndAlso k0 < alglib.ap.len(network.hllayersizes), "MLPGetWeight: incorrect (nonexistent) K0")
			alglib.ap.assert(i0 >= 0 AndAlso i0 < network.hllayersizes(k0), "MLPGetWeight: incorrect (nonexistent) I0")
			alglib.ap.assert(k1 >= 0 AndAlso k1 < alglib.ap.len(network.hllayersizes), "MLPGetWeight: incorrect (nonexistent) K1")
			alglib.ap.assert(i1 >= 0 AndAlso i1 < network.hllayersizes(k1), "MLPGetWeight: incorrect (nonexistent) I1")

			'
			' search
			'
			network.integerbuf(0) = k0
			network.integerbuf(1) = i0
			network.integerbuf(2) = k1
			network.integerbuf(3) = i1
			highlevelidx = apserv.recsearch(network.hlconnections, hlconnfieldwidth, 4, 0, ccnt, network.integerbuf)
			If highlevelidx >= 0 Then
				result = network.weights(network.hlconnections(highlevelidx * hlconnfieldwidth + 4))
			Else
				result = 0
			End If
			Return result
		End Function


		'************************************************************************
'        This function sets offset/scaling coefficients for I-th input of the
'        network.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            I           -   input index
'            Mean        -   mean term
'            Sigma       -   sigma term (if zero, will be replaced by 1.0)
'
'        NTE: I-th input is passed through linear transformation
'            IN[i] = (IN[i]-Mean)/Sigma
'        before feeding to the network. This function sets Mean and Sigma.
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetinputscaling(network As multilayerperceptron, i As Integer, mean As Double, sigma As Double)
			alglib.ap.assert(i >= 0 AndAlso i < network.hllayersizes(0), "MLPSetInputScaling: incorrect (nonexistent) I")
			alglib.ap.assert(Math.isfinite(mean), "MLPSetInputScaling: infinite or NAN Mean")
			alglib.ap.assert(Math.isfinite(sigma), "MLPSetInputScaling: infinite or NAN Sigma")
			If CDbl(sigma) = CDbl(0) Then
				sigma = 1
			End If
			network.columnmeans(i) = mean
			network.columnsigmas(i) = sigma
		End Sub


		'************************************************************************
'        This function sets offset/scaling coefficients for I-th output of the
'        network.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            I           -   input index
'            Mean        -   mean term
'            Sigma       -   sigma term (if zero, will be replaced by 1.0)
'
'        OUTPUT PARAMETERS:
'
'        NOTE: I-th output is passed through linear transformation
'            OUT[i] = OUT[i]*Sigma+Mean
'        before returning it to user. This function sets Sigma/Mean. In case we
'        have SOFTMAX-normalized network, you can not set (Sigma,Mean) to anything
'        other than(0.0,1.0) - this function will throw exception.
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetoutputscaling(network As multilayerperceptron, i As Integer, mean As Double, sigma As Double)
			alglib.ap.assert(i >= 0 AndAlso i < network.hllayersizes(alglib.ap.len(network.hllayersizes) - 1), "MLPSetOutputScaling: incorrect (nonexistent) I")
			alglib.ap.assert(Math.isfinite(mean), "MLPSetOutputScaling: infinite or NAN Mean")
			alglib.ap.assert(Math.isfinite(sigma), "MLPSetOutputScaling: infinite or NAN Sigma")
			If network.structinfo(6) = 1 Then
				alglib.ap.assert(CDbl(mean) = CDbl(0), "MLPSetOutputScaling: you can not set non-zero Mean term for classifier network")
				alglib.ap.assert(CDbl(sigma) = CDbl(1), "MLPSetOutputScaling: you can not set non-unit Sigma term for classifier network")
			Else
				If CDbl(sigma) = CDbl(0) Then
					sigma = 1
				End If
				network.columnmeans(network.hllayersizes(0) + i) = mean
				network.columnsigmas(network.hllayersizes(0) + i) = sigma
			End If
		End Sub


		'************************************************************************
'        This function modifies information about Ith neuron of Kth layer
'
'        INPUT PARAMETERS:
'            Network     -   network
'            K           -   layer index
'            I           -   neuron index (within layer)
'            FKind       -   activation function type (used by MLPActivationFunction())
'                            this value must be zero for input neurons
'                            (you can not set activation function for input neurons)
'            Threshold   -   also called offset, bias
'                            this value must be zero for input neurons
'                            (you can not set threshold for input neurons)
'
'        NOTES:
'        1. this function throws exception if layer or neuron with given index do
'           not exists.
'        2. this function also throws exception when you try to set non-linear
'           activation function for input neurons (any kind of network) or for output
'           neurons of classifier network.
'        3. this function throws exception when you try to set non-zero threshold for
'           input neurons (any kind of network).
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetneuroninfo(network As multilayerperceptron, k As Integer, i As Integer, fkind As Integer, threshold As Double)
			Dim ncnt As Integer = 0
			Dim istart As Integer = 0
			Dim highlevelidx As Integer = 0
			Dim activationoffset As Integer = 0

			alglib.ap.assert(Math.isfinite(threshold), "MLPSetNeuronInfo: infinite or NAN Threshold")

			'
			' convenience vars
			'
			ncnt = alglib.ap.len(network.hlneurons) \ hlnfieldwidth
			istart = network.structinfo(5)

			'
			' search
			'
			network.integerbuf(0) = k
			network.integerbuf(1) = i
			highlevelidx = apserv.recsearch(network.hlneurons, hlnfieldwidth, 2, 0, ncnt, network.integerbuf)
			alglib.ap.assert(highlevelidx >= 0, "MLPSetNeuronInfo: incorrect (nonexistent) layer or neuron index")

			'
			' activation function
			'
			If network.hlneurons(highlevelidx * hlnfieldwidth + 2) >= 0 Then
				activationoffset = istart + network.hlneurons(highlevelidx * hlnfieldwidth + 2) * nfieldwidth
				network.structinfo(activationoffset + 0) = fkind
			Else
				alglib.ap.assert(fkind = 0, "MLPSetNeuronInfo: you try to set activation function for neuron which can not have one")
			End If

			'
			' Threshold
			'
			If network.hlneurons(highlevelidx * hlnfieldwidth + 3) >= 0 Then
				network.weights(network.hlneurons(highlevelidx * hlnfieldwidth + 3)) = threshold
			Else
				alglib.ap.assert(CDbl(threshold) = CDbl(0), "MLPSetNeuronInfo: you try to set non-zero threshold for neuron which can not have one")
			End If
		End Sub


		'************************************************************************
'        This function modifies information about connection from I0-th neuron of
'        K0-th layer to I1-th neuron of K1-th layer.
'
'        INPUT PARAMETERS:
'            Network     -   network
'            K0          -   layer index
'            I0          -   neuron index (within layer)
'            K1          -   layer index
'            I1          -   neuron index (within layer)
'            W           -   connection weight (must be zero for non-existent
'                            connections)
'
'        This function:
'        1. throws exception if layer or neuron with given index do not exists.
'        2. throws exception if you try to set non-zero weight for non-existent
'           connection
'
'          -- ALGLIB --
'             Copyright 25.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetweight(network As multilayerperceptron, k0 As Integer, i0 As Integer, k1 As Integer, i1 As Integer, w As Double)
			Dim ccnt As Integer = 0
			Dim highlevelidx As Integer = 0

			ccnt = alglib.ap.len(network.hlconnections) \ hlconnfieldwidth

			'
			' check params
			'
			alglib.ap.assert(k0 >= 0 AndAlso k0 < alglib.ap.len(network.hllayersizes), "MLPSetWeight: incorrect (nonexistent) K0")
			alglib.ap.assert(i0 >= 0 AndAlso i0 < network.hllayersizes(k0), "MLPSetWeight: incorrect (nonexistent) I0")
			alglib.ap.assert(k1 >= 0 AndAlso k1 < alglib.ap.len(network.hllayersizes), "MLPSetWeight: incorrect (nonexistent) K1")
			alglib.ap.assert(i1 >= 0 AndAlso i1 < network.hllayersizes(k1), "MLPSetWeight: incorrect (nonexistent) I1")
			alglib.ap.assert(Math.isfinite(w), "MLPSetWeight: infinite or NAN weight")

			'
			' search
			'
			network.integerbuf(0) = k0
			network.integerbuf(1) = i0
			network.integerbuf(2) = k1
			network.integerbuf(3) = i1
			highlevelidx = apserv.recsearch(network.hlconnections, hlconnfieldwidth, 4, 0, ccnt, network.integerbuf)
			If highlevelidx >= 0 Then
				network.weights(network.hlconnections(highlevelidx * hlconnfieldwidth + 4)) = w
			Else
				alglib.ap.assert(CDbl(w) = CDbl(0), "MLPSetWeight: you try to set non-zero weight for non-existent connection")
			End If
		End Sub


		'************************************************************************
'        Neural network activation function
'
'        INPUT PARAMETERS:
'            NET         -   neuron input
'            K           -   function index (zero for linear function)
'
'        OUTPUT PARAMETERS:
'            F           -   function
'            DF          -   its derivative
'            D2F         -   its second derivative
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpactivationfunction(net As Double, k As Integer, ByRef f As Double, ByRef df As Double, ByRef d2f As Double)
			Dim net2 As Double = 0
			Dim arg As Double = 0
			Dim root As Double = 0
			Dim r As Double = 0

			f = 0
			df = 0
			d2f = 0

			If k = 0 OrElse k = -5 Then
				f = net
				df = 1
				d2f = 0
				Return
			End If
			If k = 1 Then

				'
				' TanH activation function
				'
				If CDbl(System.Math.Abs(net)) < CDbl(100) Then
					f = System.Math.Tanh(net)
				Else
					f = System.Math.Sign(net)
				End If
				df = 1 - f * f
				d2f = -(2 * f * df)
				Return
			End If
			If k = 3 Then

				'
				' EX activation function
				'
				If CDbl(net) >= CDbl(0) Then
					net2 = net * net
					arg = net2 + 1
					root = System.Math.sqrt(arg)
					f = net + root
					r = net / root
					df = 1 + r
					d2f = (root - net * r) / arg
				Else
					f = System.Math.Exp(net)
					df = f
					d2f = f
				End If
				Return
			End If
			If k = 2 Then
				f = System.Math.Exp(-Math.sqr(net))
				df = -(2 * net * f)
				d2f = -(2 * (f + df * net))
				Return
			End If
			f = 0
			df = 0
			d2f = 0
		End Sub


		'************************************************************************
'        Procesing
'
'        INPUT PARAMETERS:
'            Network -   neural network
'            X       -   input vector,  array[0..NIn-1].
'
'        OUTPUT PARAMETERS:
'            Y       -   result. Regression estimate when solving regression  task,
'                        vector of posterior probabilities for classification task.
'
'        See also MLPProcessI
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpprocess(network As multilayerperceptron, x As Double(), ByRef y As Double())
			If alglib.ap.len(y) < network.structinfo(2) Then
				y = New Double(network.structinfo(2) - 1) {}
			End If
			mlpinternalprocessvector(network.structinfo, network.weights, network.columnmeans, network.columnsigmas, network.neurons, network.dfdnet, _
				x, y)
		End Sub


		'************************************************************************
'        'interactive'  variant  of  MLPProcess  for  languages  like  Python which
'        support constructs like "Y = MLPProcess(NN,X)" and interactive mode of the
'        interpreter
'
'        This function allocates new array on each call,  so  it  is  significantly
'        slower than its 'non-interactive' counterpart, but it is  more  convenient
'        when you call it from command line.
'
'          -- ALGLIB --
'             Copyright 21.09.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpprocessi(network As multilayerperceptron, x As Double(), ByRef y As Double())
			y = New Double(-1) {}

			mlpprocess(network, x, y)
		End Sub


		'************************************************************************
'        Error of the neural network on dataset.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore systems.
'          ! Second improvement gives constant speedup (2-3x, depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'            sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPError: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = Math.sqr(network.err.rmserror) * npoints * mlpgetoutputscount(network) / 2
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlperror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlperror(network, xy, npoints)
		End Function


		'************************************************************************
'        Error of the neural network on dataset given by sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore systems.
'          ! Second improvement gives constant speedup (2-3x, depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            NPoints     -   points count, >=0
'
'        RESULT:
'            sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPErrorSparse: XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPErrorSparse: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPErrorSparse: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSparse: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = Math.sqr(network.err.rmserror) * npoints * mlpgetoutputscount(network) / 2
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlperrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlperrorsparse(network, xy, npoints)
		End Function


		'************************************************************************
'        Natural error function for neural network, internal subroutine.
'
'        NOTE: this function is single-threaded. Unlike other  error  function,  it
'        receives no speed-up from being executed in SMP mode.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperrorn(network As multilayerperceptron, xy As Double(,), ssize As Integer) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim e As Double = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			mlpproperties(network, nin, nout, wcount)
			result = 0
			For i = 0 To ssize - 1

				'
				' Process vector
				'
				For i_ = 0 To nin - 1
					network.x(i_) = xy(i, i_)
				Next
				mlpprocess(network, network.x, network.y)

				'
				' Update error function
				'
				If network.structinfo(6) = 0 Then

					'
					' Least squares error function
					'
					i1_ = (nin) - (0)
					For i_ = 0 To nout - 1
						network.y(i_) = network.y(i_) - xy(i, i_ + i1_)
					Next
					e = 0.0
					For i_ = 0 To nout - 1
						e += network.y(i_) * network.y(i_)
					Next
					result = result + e / 2
				Else

					'
					' Cross-entropy error function
					'
					k = CInt(System.Math.Truncate(System.Math.Round(xy(i, nin))))
					If k >= 0 AndAlso k < nout Then
						result = result + safecrossentropy(1, network.y(k))
					End If
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        Classification error of the neural network on dataset.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'            classification error (number of misclassified cases)
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Integer
			Dim result As Integer = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPClsError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPClsError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPClsError: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = CInt(System.Math.Truncate(System.Math.Round(npoints * network.err.relclserror)))
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Integer
			Return mlpclserror(network, xy, npoints)
		End Function


		'************************************************************************
'        Relative classification error on the test set.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'        Percent   of incorrectly   classified  cases.  Works  both  for classifier
'        networks and general purpose networks used as classifiers.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 25.12.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlprelclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPRelClsError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPRelClsError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRelClsError: XY has less than NIn+NOut columns")
				End If
			End If
			If npoints > 0 Then
				result = CDbl(mlpclserror(network, xy, npoints)) / CDbl(npoints)
			Else
				result = 0.0
			End If
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlprelclserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlprelclserror(network, xy, npoints)
		End Function


		'************************************************************************
'        Relative classification error on the test set given by sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. Sparse matrix must use CRS format
'                            for storage.
'            NPoints     -   points count, >=0.
'
'        RESULT:
'        Percent   of incorrectly   classified  cases.  Works  both  for classifier
'        networks and general purpose networks used as classifiers.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 09.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlprelclserrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPRelClsErrorSparse: sparse matrix XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPRelClsErrorSparse: sparse matrix XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPRelClsErrorSparse: sparse matrix XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRelClsErrorSparse: sparse matrix XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.relclserror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlprelclserrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlprelclserrorsparse(network, xy, npoints)
		End Function


		'************************************************************************
'        Average cross-entropy  (in bits  per element) on the test set.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'        CrossEntropy/(NPoints*LN(2)).
'        Zero if network solves regression task.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 08.01.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgce(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPAvgCE: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgCE: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgCE: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgce
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgce(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlpavgce(network, xy, npoints)
		End Function


		'************************************************************************
'        Average  cross-entropy  (in bits  per element)  on the  test set  given by
'        sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            NPoints     -   points count, >=0.
'
'        RESULT:
'        CrossEntropy/(NPoints*LN(2)).
'        Zero if network solves regression task.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 9.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgcesparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPAvgCESparse: sparse matrix XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPAvgCESparse: sparse matrix XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgCESparse: sparse matrix XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgCESparse: sparse matrix XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgce
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgcesparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlpavgcesparse(network, xy, npoints)
		End Function


		'************************************************************************
'        RMS error on the test set given.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'        Root mean  square error. Its meaning for regression task is obvious. As for
'        classification  task,  RMS  error  means  error  when estimating  posterior
'        probabilities.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlprmserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPRMSError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPRMSError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRMSError: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.rmserror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlprmserror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlprmserror(network, xy, npoints)
		End Function


		'************************************************************************
'        RMS error on the test set given by sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            NPoints     -   points count, >=0.
'
'        RESULT:
'        Root mean  square error. Its meaning for regression task is obvious. As for
'        classification  task,  RMS  error  means  error  when estimating  posterior
'        probabilities.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 09.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlprmserrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPRMSErrorSparse: sparse matrix XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPRMSErrorSparse: sparse matrix XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPRMSErrorSparse: sparse matrix XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRMSErrorSparse: sparse matrix XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.rmserror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlprmserrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlprmserrorsparse(network, xy, npoints)
		End Function


		'************************************************************************
'        Average absolute error on the test set.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'        Its meaning for regression task is obvious. As for classification task, it
'        means average error when estimating posterior probabilities.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 11.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPAvgError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgError: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgerror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlpavgerror(network, xy, npoints)
		End Function


		'************************************************************************
'        Average absolute error on the test set given by sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            NPoints     -   points count, >=0.
'
'        RESULT:
'        Its meaning for regression task is obvious. As for classification task, it
'        means average error when estimating posterior probabilities.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 09.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgerrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPAvgErrorSparse: XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPAvgErrorSparse: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgErrorSparse: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgErrorSparse: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgerror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgerrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlpavgerrorsparse(network, xy, npoints)
		End Function


		'************************************************************************
'        Average relative error on the test set.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format;
'            NPoints     -   points count.
'
'        RESULT:
'        Its meaning for regression task is obvious. As for classification task, it
'        means  average  relative  error  when  estimating posterior probability of
'        belonging to the correct class.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 11.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgrelerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= npoints, "MLPAvgRelError: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgRelError: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgRelError: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, xy, network.dummysxy, npoints, 0, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgrelerror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgrelerror(network As multilayerperceptron, xy As Double(,), npoints As Integer) As Double
			Return mlpavgrelerror(network, xy, npoints)
		End Function


		'************************************************************************
'        Average relative error on the test set given by sparse matrix.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network     -   neural network;
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            NPoints     -   points count, >=0.
'
'        RESULT:
'        Its meaning for regression task is obvious. As for classification task, it
'        means  average  relative  error  when  estimating posterior probability of
'        belonging to the correct class.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 09.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpavgrelerrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Dim result As Double = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPAvgRelErrorSparse: XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= npoints, "MLPAvgRelErrorSparse: XY has less than NPoints rows")
			If npoints > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgRelErrorSparse: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgRelErrorSparse: XY has less than NIn+NOut columns")
				End If
			End If
			mlpallerrorsx(network, network.dummydxy, xy, npoints, 1, network.dummyidx, _
				0, npoints, 0, network.buf, network.err)
			result = network.err.avgrelerror
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpavgrelerrorsparse(network As multilayerperceptron, xy As sparse.sparsematrix, npoints As Integer) As Double
			Return mlpavgrelerrorsparse(network, xy, npoints)
		End Function


		'************************************************************************
'        Gradient calculation
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            X       -   input vector, length of array must be at least NIn
'            DesiredY-   desired outputs, length of array must be at least NOut
'            Grad    -   possibly preallocated array. If size of array is smaller
'                        than WCount, it will be reallocated. It is recommended to
'                        reuse previously allocated array to reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'            Grad    -   gradient of E with respect to weights of network, array[WCount]
'            
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgrad(network As multilayerperceptron, x As Double(), desiredy As Double(), ByRef e As Double, ByRef grad As Double())
			Dim i As Integer = 0
			Dim nout As Integer = 0
			Dim ntotal As Integer = 0

			e = 0


			'
			' Alloc
			'
			apserv.rvectorsetlengthatleast(grad, network.structinfo(4))

			'
			' Prepare dError/dOut, internal structures
			'
			mlpprocess(network, x, network.y)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			e = 0
			For i = 0 To ntotal - 1
				network.derror(i) = 0
			Next
			For i = 0 To nout - 1
				network.derror(ntotal - nout + i) = network.y(i) - desiredy(i)
				e = e + Math.sqr(network.y(i) - desiredy(i)) / 2
			Next

			'
			' gradient
			'
			mlpinternalcalculategradient(network, network.neurons, network.weights, network.derror, grad, False)
		End Sub


		'************************************************************************
'        Gradient calculation (natural error function is used)
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            X       -   input vector, length of array must be at least NIn
'            DesiredY-   desired outputs, length of array must be at least NOut
'            Grad    -   possibly preallocated array. If size of array is smaller
'                        than WCount, it will be reallocated. It is recommended to
'                        reuse previously allocated array to reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, sum-of-squares for regression networks,
'                        cross-entropy for classification networks.
'            Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradn(network As multilayerperceptron, x As Double(), desiredy As Double(), ByRef e As Double, ByRef grad As Double())
			Dim s As Double = 0
			Dim i As Integer = 0
			Dim nout As Integer = 0
			Dim ntotal As Integer = 0

			e = 0


			'
			' Alloc
			'
			apserv.rvectorsetlengthatleast(grad, network.structinfo(4))

			'
			' Prepare dError/dOut, internal structures
			'
			mlpprocess(network, x, network.y)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			For i = 0 To ntotal - 1
				network.derror(i) = 0
			Next
			e = 0
			If network.structinfo(6) = 0 Then

				'
				' Regression network, least squares
				'
				For i = 0 To nout - 1
					network.derror(ntotal - nout + i) = network.y(i) - desiredy(i)
					e = e + Math.sqr(network.y(i) - desiredy(i)) / 2
				Next
			Else

				'
				' Classification network, cross-entropy
				'
				s = 0
				For i = 0 To nout - 1
					s = s + desiredy(i)
				Next
				For i = 0 To nout - 1
					network.derror(ntotal - nout + i) = s * network.y(i) - desiredy(i)
					e = e + safecrossentropy(desiredy(i), network.y(i))
				Next
			End If

			'
			' gradient
			'
			mlpinternalcalculategradient(network, network.neurons, network.weights, network.derror, grad, True)
		End Sub


		'************************************************************************
'        Batch gradient calculation for a set of inputs/outputs
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset in dense format; one sample = one row:
'                        * first NIn columns contain inputs,
'                        * for regression problem, next NOut columns store
'                          desired outputs.
'                        * for classification problem, next column (just one!)
'                          stores class number.
'            SSize   -   number of elements in XY
'            Grad    -   possibly preallocated array. If size of array is smaller
'                        than WCount, it will be reallocated. It is recommended to
'                        reuse previously allocated array to reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'            Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim subset0 As Integer = 0
			Dim subset1 As Integer = 0
			Dim subsettype As Integer = 0
			Dim sgrad As smlpgrad = Nothing

			e = 0

			alglib.ap.assert(ssize >= 0, "MLPGradBatchSparse: SSize<0")
			subset0 = 0
			subset1 = ssize
			subsettype = 0
			mlpproperties(network, nin, nout, wcount)
			apserv.rvectorsetlengthatleast(grad, wcount)
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				sgrad.f = 0.0
				For i = 0 To wcount - 1
					sgrad.g(i) = 0.0
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
			mlpgradbatchx(network, xy, network.dummysxy, ssize, 0, network.dummyidx, _
				subset0, subset1, subsettype, network.buf, network.gradbuf)
			e = 0.0
			For i = 0 To wcount - 1
				grad(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				e = e + sgrad.f
				For i = 0 To wcount - 1
					grad(i) = grad(i) + sgrad.g(i)
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpgradbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
			mlpgradbatch(network, xy, ssize, e, grad)
		End Sub


		'************************************************************************
'        Batch gradient calculation for a set  of inputs/outputs  given  by  sparse
'        matrices
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset in sparse format; one sample = one row:
'                        * MATRIX MUST BE STORED IN CRS FORMAT
'                        * first NIn columns contain inputs.
'                        * for regression problem, next NOut columns store
'                          desired outputs.
'                        * for classification problem, next column (just one!)
'                          stores class number.
'            SSize   -   number of elements in XY
'            Grad    -   possibly preallocated array. If size of array is smaller
'                        than WCount, it will be reallocated. It is recommended to
'                        reuse previously allocated array to reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'            Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradbatchsparse(network As multilayerperceptron, xy As sparse.sparsematrix, ssize As Integer, ByRef e As Double, ByRef grad As Double())
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim subset0 As Integer = 0
			Dim subset1 As Integer = 0
			Dim subsettype As Integer = 0
			Dim sgrad As smlpgrad = Nothing

			e = 0

			alglib.ap.assert(ssize >= 0, "MLPGradBatchSparse: SSize<0")
			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPGradBatchSparse: sparse matrix XY must be in CRS format.")
			subset0 = 0
			subset1 = ssize
			subsettype = 0
			mlpproperties(network, nin, nout, wcount)
			apserv.rvectorsetlengthatleast(grad, wcount)
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				sgrad.f = 0.0
				For i = 0 To wcount - 1
					sgrad.g(i) = 0.0
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
			mlpgradbatchx(network, network.dummydxy, xy, ssize, 1, network.dummyidx, _
				subset0, subset1, subsettype, network.buf, network.gradbuf)
			e = 0.0
			For i = 0 To wcount - 1
				grad(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				e = e + sgrad.f
				For i = 0 To wcount - 1
					grad(i) = grad(i) + sgrad.g(i)
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpgradbatchsparse(network As multilayerperceptron, xy As sparse.sparsematrix, ssize As Integer, ByRef e As Double, ByRef grad As Double())
			mlpgradbatchsparse(network, xy, ssize, e, grad)
		End Sub


		'************************************************************************
'        Batch gradient calculation for a subset of dataset
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset in dense format; one sample = one row:
'                        * first NIn columns contain inputs,
'                        * for regression problem, next NOut columns store
'                          desired outputs.
'                        * for classification problem, next column (just one!)
'                          stores class number.
'            SetSize -   real size of XY, SetSize>=0;
'            Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                        * Idx[I] stores row index in the original dataset which is
'                          given by XY. Gradient is calculated with respect to rows
'                          whose indexes are stored in Idx[].
'                        * Idx[]  must store correct indexes; this function  throws
'                          an  exception  in  case  incorrect index (less than 0 or
'                          larger than rows(XY)) is given
'                        * Idx[]  may  store  indexes  in  any  order and even with
'                          repetitions.
'            SubsetSize- number of elements in Idx[] array:
'                        * positive value means that subset given by Idx[] is processed
'                        * zero value results in zero gradient
'                        * negative value means that full dataset is processed
'            Grad      - possibly  preallocated array. If size of array is  smaller
'                        than WCount, it will be reallocated. It is  recommended to
'                        reuse  previously  allocated  array  to  reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E         - error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'            Grad      - gradient  of  E  with  respect   to  weights  of  network,
'                        array[WCount]
'
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradbatchsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
			ByRef grad As Double())
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim npoints As Integer = 0
			Dim subset0 As Integer = 0
			Dim subset1 As Integer = 0
			Dim subsettype As Integer = 0
			Dim sgrad As smlpgrad = Nothing

			e = 0

			alglib.ap.assert(setsize >= 0, "MLPGradBatchSubset: SetSize<0")
			alglib.ap.assert(subsetsize <= alglib.ap.len(idx), "MLPGradBatchSubset: SubsetSize>Length(Idx)")
			npoints = setsize
			If subsetsize < 0 Then
				subset0 = 0
				subset1 = setsize
				subsettype = 0
			Else
				subset0 = 0
				subset1 = subsetsize
				subsettype = 1
				For i = 0 To subsetsize - 1
					alglib.ap.assert(idx(i) >= 0, "MLPGradBatchSubset: incorrect index of XY row(Idx[I]<0)")
					alglib.ap.assert(idx(i) <= npoints - 1, "MLPGradBatchSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)")
				Next
			End If
			mlpproperties(network, nin, nout, wcount)
			apserv.rvectorsetlengthatleast(grad, wcount)
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				sgrad.f = 0.0
				For i = 0 To wcount - 1
					sgrad.g(i) = 0.0
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
			mlpgradbatchx(network, xy, network.dummysxy, setsize, 0, idx, _
				subset0, subset1, subsettype, network.buf, network.gradbuf)
			e = 0.0
			For i = 0 To wcount - 1
				grad(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				e = e + sgrad.f
				For i = 0 To wcount - 1
					grad(i) = grad(i) + sgrad.g(i)
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpgradbatchsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
			ByRef grad As Double())
			mlpgradbatchsubset(network, xy, setsize, idx, subsetsize, e, _
				grad)
		End Sub


		'************************************************************************
'        Batch gradient calculation for a set of inputs/outputs  for  a  subset  of
'        dataset given by set of indexes.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset in sparse format; one sample = one row:
'                        * MATRIX MUST BE STORED IN CRS FORMAT
'                        * first NIn columns contain inputs,
'                        * for regression problem, next NOut columns store
'                          desired outputs.
'                        * for classification problem, next column (just one!)
'                          stores class number.
'            SetSize -   real size of XY, SetSize>=0;
'            Idx     -   subset of SubsetSize elements, array[SubsetSize]:
'                        * Idx[I] stores row index in the original dataset which is
'                          given by XY. Gradient is calculated with respect to rows
'                          whose indexes are stored in Idx[].
'                        * Idx[]  must store correct indexes; this function  throws
'                          an  exception  in  case  incorrect index (less than 0 or
'                          larger than rows(XY)) is given
'                        * Idx[]  may  store  indexes  in  any  order and even with
'                          repetitions.
'            SubsetSize- number of elements in Idx[] array:
'                        * positive value means that subset given by Idx[] is processed
'                        * zero value results in zero gradient
'                        * negative value means that full dataset is processed
'            Grad      - possibly  preallocated array. If size of array is  smaller
'                        than WCount, it will be reallocated. It is  recommended to
'                        reuse  previously  allocated  array  to  reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
'            Grad    -   gradient  of  E  with  respect   to  weights  of  network,
'                        array[WCount]
'
'        NOTE: when  SubsetSize<0 is used full dataset by call MLPGradBatchSparse
'              function.
'            
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradbatchsparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
			ByRef grad As Double())
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim npoints As Integer = 0
			Dim subset0 As Integer = 0
			Dim subset1 As Integer = 0
			Dim subsettype As Integer = 0
			Dim sgrad As smlpgrad = Nothing

			e = 0

			alglib.ap.assert(setsize >= 0, "MLPGradBatchSparseSubset: SetSize<0")
			alglib.ap.assert(subsetsize <= alglib.ap.len(idx), "MLPGradBatchSparseSubset: SubsetSize>Length(Idx)")
			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPGradBatchSparseSubset: sparse matrix XY must be in CRS format.")
			npoints = setsize
			If subsetsize < 0 Then
				subset0 = 0
				subset1 = setsize
				subsettype = 0
			Else
				subset0 = 0
				subset1 = subsetsize
				subsettype = 1
				For i = 0 To subsetsize - 1
					alglib.ap.assert(idx(i) >= 0, "MLPGradBatchSparseSubset: incorrect index of XY row(Idx[I]<0)")
					alglib.ap.assert(idx(i) <= npoints - 1, "MLPGradBatchSparseSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)")
				Next
			End If
			mlpproperties(network, nin, nout, wcount)
			apserv.rvectorsetlengthatleast(grad, wcount)
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				sgrad.f = 0.0
				For i = 0 To wcount - 1
					sgrad.g(i) = 0.0
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
			mlpgradbatchx(network, network.dummydxy, xy, setsize, 1, idx, _
				subset0, subset1, subsettype, network.buf, network.gradbuf)
			e = 0.0
			For i = 0 To wcount - 1
				grad(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_first_recycled(network.gradbuf, sgrad)
			While sgrad IsNot Nothing
				e = e + sgrad.f
				For i = 0 To wcount - 1
					grad(i) = grad(i) + sgrad.g(i)
				Next
				alglib.smp.ae_shared_pool_next_recycled(network.gradbuf, sgrad)
			End While
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpgradbatchsparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, idx As Integer(), subsetsize As Integer, ByRef e As Double, _
			ByRef grad As Double())
			mlpgradbatchsparsesubset(network, xy, setsize, idx, subsetsize, e, _
				grad)
		End Sub


		'************************************************************************
'        Internal function which actually calculates batch gradient for a subset or
'        full dataset, which can be represented in different formats.
'
'        THIS FUNCTION IS NOT INTENDED TO BE USED BY ALGLIB USERS!
'
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradbatchx(network As multilayerperceptron, densexy As Double(,), sparsexy As sparse.sparsematrix, datasetsize As Integer, datasettype As Integer, idx As Integer(), _
			subset0 As Integer, subset1 As Integer, subsettype As Integer, buf As alglib.smp.shared_pool, gradbuf As alglib.smp.shared_pool)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim rowsize As Integer = 0
			Dim srcidx As Integer = 0
			Dim cstart As Integer = 0
			Dim csize As Integer = 0
			Dim j As Integer = 0
			Dim problemcost As Double = 0
			Dim buf2 As hpccores.mlpbuffers = Nothing
			Dim len0 As Integer = 0
			Dim len1 As Integer = 0
			Dim pbuf As hpccores.mlpbuffers = Nothing
			Dim sgrad As smlpgrad = Nothing
			Dim i_ As Integer = 0

			alglib.ap.assert(datasetsize >= 0, "MLPGradBatchX: SetSize<0")
			alglib.ap.assert(datasettype = 0 OrElse datasettype = 1, "MLPGradBatchX: DatasetType is incorrect")
			alglib.ap.assert(subsettype = 0 OrElse subsettype = 1, "MLPGradBatchX: SubsetType is incorrect")

			'
			' Determine network and dataset properties
			'
			mlpproperties(network, nin, nout, wcount)
			If mlpissoftmax(network) Then
				rowsize = nin + 1
			Else
				rowsize = nin + nout
			End If

			'
			' Split problem.
			'
			' Splitting problem allows us to reduce  effect  of  single-precision
			' arithmetics (SSE-optimized version of MLPChunkedGradient uses single
			' precision  internally, but converts them to  double precision after
			' results are exported from HPC buffer to network). Small batches are
			' calculated in single precision, results are  aggregated  in  double
			' precision, and it allows us to avoid accumulation  of  errors  when
			' we process very large batches (tens of thousands of items).
			'
			' NOTE: it is important to use real arithmetics for ProblemCost
			'       because ProblemCost may be larger than MAXINT.
			'
			problemcost = subset1 - subset0
			problemcost = problemcost * wcount
			If subset1 - subset0 >= 2 * microbatchsize AndAlso CDbl(problemcost) > CDbl(gradbasecasecost) Then
				apserv.splitlength(subset1 - subset0, microbatchsize, len0, len1)
				mlpgradbatchx(network, densexy, sparsexy, datasetsize, datasettype, idx, _
					subset0, subset0 + len0, subsettype, buf, gradbuf)
				mlpgradbatchx(network, densexy, sparsexy, datasetsize, datasettype, idx, _
					subset0 + len0, subset1, subsettype, buf, gradbuf)
				Return
			End If

			'
			' Chunked processing
			'
			alglib.smp.ae_shared_pool_retrieve(gradbuf, sgrad)
			alglib.smp.ae_shared_pool_retrieve(buf, pbuf)
			hpccores.hpcpreparechunkedgradient(network.weights, wcount, mlpntotal(network), nin, nout, pbuf)
			cstart = subset0
			While cstart < subset1

				'
				' Determine size of current chunk and copy it to PBuf.XY
				'
				csize = System.Math.Min(subset1, cstart + pbuf.chunksize) - cstart
				For j = 0 To csize - 1
					srcidx = -1
					If subsettype = 0 Then
						srcidx = cstart + j
					End If
					If subsettype = 1 Then
						srcidx = idx(cstart + j)
					End If
					alglib.ap.assert(srcidx >= 0, "MLPGradBatchX: internal error")
					If datasettype = 0 Then
						For i_ = 0 To rowsize - 1
							pbuf.xy(j, i_) = densexy(srcidx, i_)
						Next
					End If
					If datasettype = 1 Then
						sparse.sparsegetrow(sparsexy, srcidx, pbuf.xyrow)
						For i_ = 0 To rowsize - 1
							pbuf.xy(j, i_) = pbuf.xyrow(i_)
						Next
					End If
				Next

				'
				' Process chunk and advance line pointer
				'
				mlpchunkedgradient(network, pbuf.xy, 0, csize, pbuf.batch4buf, pbuf.hpcbuf, _
					sgrad.f, False)
				cstart = cstart + pbuf.chunksize
			End While
			hpccores.hpcfinalizechunkedgradient(pbuf, sgrad.g)
			alglib.smp.ae_shared_pool_recycle(buf, pbuf)
			alglib.smp.ae_shared_pool_recycle(gradbuf, sgrad)
		End Sub


		'************************************************************************
'        Batch gradient calculation for a set of inputs/outputs
'        (natural error function is used)
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   set of inputs/outputs; one sample = one row;
'                        first NIn columns contain inputs,
'                        next NOut columns - desired outputs.
'            SSize   -   number of elements in XY
'            Grad    -   possibly preallocated array. If size of array is smaller
'                        than WCount, it will be reallocated. It is recommended to
'                        reuse previously allocated array to reduce allocation
'                        overhead.
'
'        OUTPUT PARAMETERS:
'            E       -   error function, sum-of-squares for regression networks,
'                        cross-entropy for classification networks.
'            Grad    -   gradient of E with respect to weights of network, array[WCount]
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpgradnbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double())
			Dim i As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim pbuf As hpccores.mlpbuffers = Nothing

			e = 0


			'
			' Alloc
			'
			mlpproperties(network, nin, nout, wcount)
			alglib.smp.ae_shared_pool_retrieve(network.buf, pbuf)
			hpccores.hpcpreparechunkedgradient(network.weights, wcount, mlpntotal(network), nin, nout, pbuf)
			apserv.rvectorsetlengthatleast(grad, wcount)
			For i = 0 To wcount - 1
				grad(i) = 0
			Next
			e = 0
			i = 0
			While i <= ssize - 1
				mlpchunkedgradient(network, xy, i, System.Math.Min(ssize, i + pbuf.chunksize) - i, pbuf.batch4buf, pbuf.hpcbuf, _
					e, True)
				i = i + pbuf.chunksize
			End While
			hpccores.hpcfinalizechunkedgradient(pbuf, grad)
			alglib.smp.ae_shared_pool_recycle(network.buf, pbuf)
		End Sub


		'************************************************************************
'        Batch Hessian calculation (natural error function) using R-algorithm.
'        Internal subroutine.
'
'          -- ALGLIB --
'             Copyright 26.01.2008 by Bochkanov Sergey.
'             
'             Hessian calculation based on R-algorithm described in
'             "Fast Exact Multiplication by the Hessian",
'             B. A. Pearlmutter,
'             Neural Computation, 1994.
'        ************************************************************************

		Public Shared Sub mlphessiannbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double(), ByRef h As Double(,))
			e = 0

			mlphessianbatchinternal(network, xy, ssize, True, e, grad, _
				h)
		End Sub


		'************************************************************************
'        Batch Hessian calculation using R-algorithm.
'        Internal subroutine.
'
'          -- ALGLIB --
'             Copyright 26.01.2008 by Bochkanov Sergey.
'
'             Hessian calculation based on R-algorithm described in
'             "Fast Exact Multiplication by the Hessian",
'             B. A. Pearlmutter,
'             Neural Computation, 1994.
'        ************************************************************************

		Public Shared Sub mlphessianbatch(network As multilayerperceptron, xy As Double(,), ssize As Integer, ByRef e As Double, ByRef grad As Double(), ByRef h As Double(,))
			e = 0

			mlphessianbatchinternal(network, xy, ssize, False, e, grad, _
				h)
		End Sub


		'************************************************************************
'        Internal subroutine, shouldn't be called by user.
'        ************************************************************************

		Public Shared Sub mlpinternalprocessvector(structinfo As Integer(), weights As Double(), columnmeans As Double(), columnsigmas As Double(), ByRef neurons As Double(), ByRef dfdnet As Double(), _
			x As Double(), ByRef y As Double())
			Dim i As Integer = 0
			Dim n1 As Integer = 0
			Dim n2 As Integer = 0
			Dim w1 As Integer = 0
			Dim w2 As Integer = 0
			Dim ntotal As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim istart As Integer = 0
			Dim offs As Integer = 0
			Dim net As Double = 0
			Dim f As Double = 0
			Dim df As Double = 0
			Dim d2f As Double = 0
			Dim mx As Double = 0
			Dim perr As New Boolean()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0


			'
			' Read network geometry
			'
			nin = structinfo(1)
			nout = structinfo(2)
			ntotal = structinfo(3)
			istart = structinfo(5)

			'
			' Inputs standartisation and putting in the network
			'
			For i = 0 To nin - 1
				If CDbl(columnsigmas(i)) <> CDbl(0) Then
					neurons(i) = (x(i) - columnmeans(i)) / columnsigmas(i)
				Else
					neurons(i) = x(i) - columnmeans(i)
				End If
			Next

			'
			' Process network
			'
			For i = 0 To ntotal - 1
				offs = istart + i * nfieldwidth
				If structinfo(offs + 0) > 0 OrElse structinfo(offs + 0) = -5 Then

					'
					' Activation function
					'
					mlpactivationfunction(neurons(structinfo(offs + 2)), structinfo(offs + 0), f, df, d2f)
					neurons(i) = f
					dfdnet(i) = df
					Continue For
				End If
				If structinfo(offs + 0) = 0 Then

					'
					' Adaptive summator
					'
					n1 = structinfo(offs + 2)
					n2 = n1 + structinfo(offs + 1) - 1
					w1 = structinfo(offs + 3)
					w2 = w1 + structinfo(offs + 1) - 1
					i1_ = (n1) - (w1)
					net = 0.0
					For i_ = w1 To w2
						net += weights(i_) * neurons(i_ + i1_)
					Next
					neurons(i) = net
					dfdnet(i) = 1.0
					apserv.touchint(n2)
					Continue For
				End If
				If structinfo(offs + 0) < 0 Then
					perr = True
					If structinfo(offs + 0) = -2 Then

						'
						' input neuron, left unchanged
						'
						perr = False
					End If
					If structinfo(offs + 0) = -3 Then

						'
						' "-1" neuron
						'
						neurons(i) = -1
						perr = False
					End If
					If structinfo(offs + 0) = -4 Then

						'
						' "0" neuron
						'
						neurons(i) = 0
						perr = False
					End If
					alglib.ap.assert(Not perr, "MLPInternalProcessVector: internal error - unknown neuron type!")
					Continue For
				End If
			Next

			'
			' Extract result
			'
			i1_ = (ntotal - nout) - (0)
			For i_ = 0 To nout - 1
				y(i_) = neurons(i_ + i1_)
			Next

			'
			' Softmax post-processing or standardisation if needed
			'
			alglib.ap.assert(structinfo(6) = 0 OrElse structinfo(6) = 1, "MLPInternalProcessVector: unknown normalization type!")
			If structinfo(6) = 1 Then

				'
				' Softmax
				'
				mx = y(0)
				For i = 1 To nout - 1
					mx = System.Math.Max(mx, y(i))
				Next
				net = 0
				For i = 0 To nout - 1
					y(i) = System.Math.Exp(y(i) - mx)
					net = net + y(i)
				Next
				For i = 0 To nout - 1
					y(i) = y(i) / net
				Next
			Else

				'
				' Standardisation
				'
				For i = 0 To nout - 1
					y(i) = y(i) * columnsigmas(nin + i) + columnmeans(nin + i)
				Next
			End If
		End Sub


		'************************************************************************
'        Serializer: allocation
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpalloc(s As alglib.serializer, network As multilayerperceptron)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim fkind As Integer = 0
			Dim threshold As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0

			nin = network.hllayersizes(0)
			nout = network.hllayersizes(alglib.ap.len(network.hllayersizes) - 1)
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			apserv.allocintegerarray(s, network.hllayersizes, -1)
			For i = 1 To alglib.ap.len(network.hllayersizes) - 1
				For j = 0 To network.hllayersizes(i) - 1
					mlpgetneuroninfo(network, i, j, fkind, threshold)
					s.alloc_entry()
					s.alloc_entry()
					For k = 0 To network.hllayersizes(i - 1) - 1
						s.alloc_entry()
					Next
				Next
			Next
			For j = 0 To nin - 1
				mlpgetinputscaling(network, j, v0, v1)
				s.alloc_entry()
				s.alloc_entry()
			Next
			For j = 0 To nout - 1
				mlpgetoutputscaling(network, j, v0, v1)
				s.alloc_entry()
				s.alloc_entry()
			Next
		End Sub


		'************************************************************************
'        Serializer: serialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpserialize(s As alglib.serializer, network As multilayerperceptron)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim fkind As Integer = 0
			Dim threshold As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0

			nin = network.hllayersizes(0)
			nout = network.hllayersizes(alglib.ap.len(network.hllayersizes) - 1)
			s.serialize_int(scodes.getmlpserializationcode())
			s.serialize_int(mlpfirstversion)
			s.serialize_bool(mlpissoftmax(network))
			apserv.serializeintegerarray(s, network.hllayersizes, -1)
			For i = 1 To alglib.ap.len(network.hllayersizes) - 1
				For j = 0 To network.hllayersizes(i) - 1
					mlpgetneuroninfo(network, i, j, fkind, threshold)
					s.serialize_int(fkind)
					s.serialize_double(threshold)
					For k = 0 To network.hllayersizes(i - 1) - 1
						s.serialize_double(mlpgetweight(network, i - 1, k, i, j))
					Next
				Next
			Next
			For j = 0 To nin - 1
				mlpgetinputscaling(network, j, v0, v1)
				s.serialize_double(v0)
				s.serialize_double(v1)
			Next
			For j = 0 To nout - 1
				mlpgetoutputscaling(network, j, v0, v1)
				s.serialize_double(v0)
				s.serialize_double(v1)
			Next
		End Sub


		'************************************************************************
'        Serializer: unserialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpunserialize(s As alglib.serializer, network As multilayerperceptron)
			Dim i0 As Integer = 0
			Dim i1 As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim fkind As Integer = 0
			Dim threshold As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim issoftmax As New Boolean()
			Dim layersizes As Integer() = New Integer(-1) {}


			'
			' check correctness of header
			'
			i0 = s.unserialize_int()
			alglib.ap.assert(i0 = scodes.getmlpserializationcode(), "MLPUnserialize: stream header corrupted")
			i1 = s.unserialize_int()
			alglib.ap.assert(i1 = mlpfirstversion, "MLPUnserialize: stream header corrupted")

			'
			' Create network
			'
			issoftmax = s.unserialize_bool()
			apserv.unserializeintegerarray(s, layersizes)
			alglib.ap.assert((alglib.ap.len(layersizes) = 2 OrElse alglib.ap.len(layersizes) = 3) OrElse alglib.ap.len(layersizes) = 4, "MLPUnserialize: too many hidden layers!")
			nin = layersizes(0)
			nout = layersizes(alglib.ap.len(layersizes) - 1)
			If alglib.ap.len(layersizes) = 2 Then
				If issoftmax Then
					mlpcreatec0(layersizes(0), layersizes(1), network)
				Else
					mlpcreate0(layersizes(0), layersizes(1), network)
				End If
			End If
			If alglib.ap.len(layersizes) = 3 Then
				If issoftmax Then
					mlpcreatec1(layersizes(0), layersizes(1), layersizes(2), network)
				Else
					mlpcreate1(layersizes(0), layersizes(1), layersizes(2), network)
				End If
			End If
			If alglib.ap.len(layersizes) = 4 Then
				If issoftmax Then
					mlpcreatec2(layersizes(0), layersizes(1), layersizes(2), layersizes(3), network)
				Else
					mlpcreate2(layersizes(0), layersizes(1), layersizes(2), layersizes(3), network)
				End If
			End If

			'
			' Load neurons and weights
			'
			For i = 1 To alglib.ap.len(layersizes) - 1
				For j = 0 To layersizes(i) - 1
					fkind = s.unserialize_int()
					threshold = s.unserialize_double()
					mlpsetneuroninfo(network, i, j, fkind, threshold)
					For k = 0 To layersizes(i - 1) - 1
						v0 = s.unserialize_double()
						mlpsetweight(network, i - 1, k, i, j, v0)
					Next
				Next
			Next

			'
			' Load standartizator
			'
			For j = 0 To nin - 1
				v0 = s.unserialize_double()
				v1 = s.unserialize_double()
				mlpsetinputscaling(network, j, v0, v1)
			Next
			For j = 0 To nout - 1
				v0 = s.unserialize_double()
				v1 = s.unserialize_double()
				mlpsetoutputscaling(network, j, v0, v1)
			Next
		End Sub


		'************************************************************************
'        Calculation of all types of errors on subset of dataset.
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset; one sample = one row;
'                        first NIn columns contain inputs,
'                        next NOut columns - desired outputs.
'            SetSize -   real size of XY, SetSize>=0;
'            Subset  -   subset of SubsetSize elements, array[SubsetSize];
'            SubsetSize- number of elements in Subset[] array:
'                        * if SubsetSize>0, rows of XY with indices Subset[0]...
'                          ...Subset[SubsetSize-1] are processed
'                        * if SubsetSize=0, zeros are returned
'                        * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                          array is ignored in this case.
'
'        OUTPUT PARAMETERS:
'            Rep     -   it contains all type of errors.
'
'          -- ALGLIB --
'             Copyright 04.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpallerrorssubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer, rep As modelerrors)
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim idxtype As Integer = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= setsize, "MLPAllErrorsSubset: XY has less than SetSize rows")
			If setsize > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPAllErrorsSubset: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAllErrorsSubset: XY has less than NIn+NOut columns")
				End If
			End If
			If subsetsize >= 0 Then
				idx0 = 0
				idx1 = subsetsize
				idxtype = 1
			Else
				idx0 = 0
				idx1 = setsize
				idxtype = 0
			End If
			mlpallerrorsx(network, xy, network.dummysxy, setsize, 0, subset, _
				idx0, idx1, idxtype, network.buf, rep)
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpallerrorssubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer, rep As modelerrors)
			mlpallerrorssubset(network, xy, setsize, subset, subsetsize, rep)
		End Sub


		'************************************************************************
'        Calculation of all types of errors on subset of dataset.
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network -   network initialized with one of the network creation funcs
'            XY      -   original dataset given by sparse matrix;
'                        one sample = one row;
'                        first NIn columns contain inputs,
'                        next NOut columns - desired outputs.
'            SetSize -   real size of XY, SetSize>=0;
'            Subset  -   subset of SubsetSize elements, array[SubsetSize];
'            SubsetSize- number of elements in Subset[] array:
'                        * if SubsetSize>0, rows of XY with indices Subset[0]...
'                          ...Subset[SubsetSize-1] are processed
'                        * if SubsetSize=0, zeros are returned
'                        * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                          array is ignored in this case.
'
'        OUTPUT PARAMETERS:
'            Rep     -   it contains all type of errors.
'
'
'          -- ALGLIB --
'             Copyright 04.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpallerrorssparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer, rep As modelerrors)
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim idxtype As Integer = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPAllErrorsSparseSubset: XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= setsize, "MLPAllErrorsSparseSubset: XY has less than SetSize rows")
			If setsize > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAllErrorsSparseSubset: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAllErrorsSparseSubset: XY has less than NIn+NOut columns")
				End If
			End If
			If subsetsize >= 0 Then
				idx0 = 0
				idx1 = subsetsize
				idxtype = 1
			Else
				idx0 = 0
				idx1 = setsize
				idxtype = 0
			End If
			mlpallerrorsx(network, network.dummydxy, xy, setsize, 1, subset, _
				idx0, idx1, idxtype, network.buf, rep)
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpallerrorssparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer, rep As modelerrors)
			mlpallerrorssparsesubset(network, xy, setsize, subset, subsetsize, rep)
		End Sub


		'************************************************************************
'        Error of the neural network on subset of dataset.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network   -     neural network;
'            XY        -     training  set,  see  below  for  information  on   the
'                            training set format;
'            SetSize   -     real size of XY, SetSize>=0;
'            Subset    -     subset of SubsetSize elements, array[SubsetSize];
'            SubsetSize-     number of elements in Subset[] array:
'                            * if SubsetSize>0, rows of XY with indices Subset[0]...
'                              ...Subset[SubsetSize-1] are processed
'                            * if SubsetSize=0, zeros are returned
'                            * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                              array is ignored in this case.
'
'        RESULT:
'            sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 04.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperrorsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer) As Double
			Dim result As Double = 0
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim idxtype As Integer = 0

			alglib.ap.assert(alglib.ap.rows(xy) >= setsize, "MLPErrorSubset: XY has less than SetSize rows")
			If setsize > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + 1, "MLPErrorSubset: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(alglib.ap.cols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSubset: XY has less than NIn+NOut columns")
				End If
			End If
			If subsetsize >= 0 Then
				idx0 = 0
				idx1 = subsetsize
				idxtype = 1
			Else
				idx0 = 0
				idx1 = setsize
				idxtype = 0
			End If
			mlpallerrorsx(network, xy, network.dummysxy, setsize, 0, subset, _
				idx0, idx1, idxtype, network.buf, network.err)
			result = Math.sqr(network.err.rmserror) * (idx1 - idx0) * mlpgetoutputscount(network) / 2
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlperrorsubset(network As multilayerperceptron, xy As Double(,), setsize As Integer, subset As Integer(), subsetsize As Integer) As Double
			Return mlperrorsubset(network, xy, setsize, subset, subsetsize)
		End Function


		'************************************************************************
'        Error of the neural network on subset of sparse dataset.
'
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support 
'          !
'          ! First improvement gives close-to-linear speedup on multicore  systems.
'          ! Second improvement gives constant speedup (2-3x depending on your CPU)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'
'        INPUT PARAMETERS:
'            Network   -     neural network;
'            XY        -     training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.  Sparse  matrix  must  use  CRS  format for
'                            storage.
'            SetSize   -     real size of XY, SetSize>=0;
'                            it is used when SubsetSize<0;
'            Subset    -     subset of SubsetSize elements, array[SubsetSize];
'            SubsetSize-     number of elements in Subset[] array:
'                            * if SubsetSize>0, rows of XY with indices Subset[0]...
'                              ...Subset[SubsetSize-1] are processed
'                            * if SubsetSize=0, zeros are returned
'                            * if SubsetSize<0, entire dataset is  processed;  Subset[]
'                              array is ignored in this case.
'
'        RESULT:
'            sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        dataset format is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'
'          -- ALGLIB --
'             Copyright 04.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperrorsparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer) As Double
			Dim result As Double = 0
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim idxtype As Integer = 0

			alglib.ap.assert(sparse.sparseiscrs(xy), "MLPErrorSparseSubset: XY is not in CRS format.")
			alglib.ap.assert(sparse.sparsegetnrows(xy) >= setsize, "MLPErrorSparseSubset: XY has less than SetSize rows")
			If setsize > 0 Then
				If mlpissoftmax(network) Then
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPErrorSparseSubset: XY has less than NIn+1 columns")
				Else
					alglib.ap.assert(sparse.sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSparseSubset: XY has less than NIn+NOut columns")
				End If
			End If
			If subsetsize >= 0 Then
				idx0 = 0
				idx1 = subsetsize
				idxtype = 1
			Else
				idx0 = 0
				idx1 = setsize
				idxtype = 0
			End If
			mlpallerrorsx(network, network.dummydxy, xy, setsize, 1, subset, _
				idx0, idx1, idxtype, network.buf, network.err)
			result = Math.sqr(network.err.rmserror) * (idx1 - idx0) * mlpgetoutputscount(network) / 2
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlperrorsparsesubset(network As multilayerperceptron, xy As sparse.sparsematrix, setsize As Integer, subset As Integer(), subsetsize As Integer) As Double
			Return mlperrorsparsesubset(network, xy, setsize, subset, subsetsize)
		End Function


		'************************************************************************
'        Calculation of all types of errors at once for a subset or  full  dataset,
'        which can be represented in different formats.
'
'        THIS INTERNAL FUNCTION IS NOT INTENDED TO BE USED BY ALGLIB USERS!
'
'          -- ALGLIB --
'             Copyright 26.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpallerrorsx(network As multilayerperceptron, densexy As Double(,), sparsexy As sparse.sparsematrix, datasetsize As Integer, datasettype As Integer, idx As Integer(), _
			subset0 As Integer, subset1 As Integer, subsettype As Integer, buf As alglib.smp.shared_pool, rep As modelerrors)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim rowsize As Integer = 0
			Dim iscls As New Boolean()
			Dim srcidx As Integer = 0
			Dim cstart As Integer = 0
			Dim csize As Integer = 0
			Dim j As Integer = 0
			Dim pbuf As hpccores.mlpbuffers = Nothing
			Dim len0 As Integer = 0
			Dim len1 As Integer = 0
			Dim rep0 As New modelerrors()
			Dim rep1 As New modelerrors()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(datasetsize >= 0, "MLPAllErrorsX: SetSize<0")
			alglib.ap.assert(datasettype = 0 OrElse datasettype = 1, "MLPAllErrorsX: DatasetType is incorrect")
			alglib.ap.assert(subsettype = 0 OrElse subsettype = 1, "MLPAllErrorsX: SubsetType is incorrect")

			'
			' Determine network properties
			'
			mlpproperties(network, nin, nout, wcount)
			iscls = mlpissoftmax(network)

			'
			' Split problem.
			'
			' Splitting problem allows us to reduce  effect  of  single-precision
			' arithmetics (SSE-optimized version of MLPChunkedProcess uses single
			' precision  internally, but converts them to  double precision after
			' results are exported from HPC buffer to network). Small batches are
			' calculated in single precision, results are  aggregated  in  double
			' precision, and it allows us to avoid accumulation  of  errors  when
			' we process very large batches (tens of thousands of items).
			'
			' NOTE: it is important to use real arithmetics for ProblemCost
			'       because ProblemCost may be larger than MAXINT.
			'
			If subset1 - subset0 >= 2 * microbatchsize AndAlso CDbl(apserv.inttoreal(subset1 - subset0) * apserv.inttoreal(wcount)) > CDbl(gradbasecasecost) Then
				apserv.splitlength(subset1 - subset0, microbatchsize, len0, len1)
				mlpallerrorsx(network, densexy, sparsexy, datasetsize, datasettype, idx, _
					subset0, subset0 + len0, subsettype, buf, rep0)
				mlpallerrorsx(network, densexy, sparsexy, datasetsize, datasettype, idx, _
					subset0 + len0, subset1, subsettype, buf, rep1)
				rep.relclserror = (len0 * rep0.relclserror + len1 * rep1.relclserror) / (len0 + len1)
				rep.avgce = (len0 * rep0.avgce + len1 * rep1.avgce) / (len0 + len1)
				rep.rmserror = System.Math.sqrt((len0 * Math.sqr(rep0.rmserror) + len1 * Math.sqr(rep1.rmserror)) / (len0 + len1))
				rep.avgerror = (len0 * rep0.avgerror + len1 * rep1.avgerror) / (len0 + len1)
				rep.avgrelerror = (len0 * rep0.avgrelerror + len1 * rep1.avgrelerror) / (len0 + len1)
				Return
			End If

			'
			' Retrieve and prepare
			'
			alglib.smp.ae_shared_pool_retrieve(buf, pbuf)
			If iscls Then
				rowsize = nin + 1
				bdss.dserrallocate(nout, pbuf.tmp0)
			Else
				rowsize = nin + nout
				bdss.dserrallocate(-nout, pbuf.tmp0)
			End If

			'
			' Processing
			'
			hpccores.hpcpreparechunkedgradient(network.weights, wcount, mlpntotal(network), nin, nout, pbuf)
			cstart = subset0
			While cstart < subset1

				'
				' Determine size of current chunk and copy it to PBuf.XY
				'
				csize = System.Math.Min(subset1, cstart + pbuf.chunksize) - cstart
				For j = 0 To csize - 1
					srcidx = -1
					If subsettype = 0 Then
						srcidx = cstart + j
					End If
					If subsettype = 1 Then
						srcidx = idx(cstart + j)
					End If
					alglib.ap.assert(srcidx >= 0, "MLPAllErrorsX: internal error")
					If datasettype = 0 Then
						For i_ = 0 To rowsize - 1
							pbuf.xy(j, i_) = densexy(srcidx, i_)
						Next
					End If
					If datasettype = 1 Then
						sparse.sparsegetrow(sparsexy, srcidx, pbuf.xyrow)
						For i_ = 0 To rowsize - 1
							pbuf.xy(j, i_) = pbuf.xyrow(i_)
						Next
					End If
				Next

				'
				' Unpack XY and process (temporary code, to be replaced by chunked processing)
				'
				For j = 0 To csize - 1
					For i_ = 0 To rowsize - 1
						pbuf.xy2(j, i_) = pbuf.xy(j, i_)
					Next
				Next
				mlpchunkedprocess(network, pbuf.xy2, 0, csize, pbuf.batch4buf, pbuf.hpcbuf)
				For j = 0 To csize - 1
					For i_ = 0 To nin - 1
						pbuf.x(i_) = pbuf.xy2(j, i_)
					Next
					i1_ = (nin) - (0)
					For i_ = 0 To nout - 1
						pbuf.y(i_) = pbuf.xy2(j, i_ + i1_)
					Next
					If iscls Then
						pbuf.desiredy(0) = pbuf.xy(j, nin)
					Else
						i1_ = (nin) - (0)
						For i_ = 0 To nout - 1
							pbuf.desiredy(i_) = pbuf.xy(j, i_ + i1_)
						Next
					End If
					bdss.dserraccumulate(pbuf.tmp0, pbuf.y, pbuf.desiredy)
				Next

				'
				' Process chunk and advance line pointer
				'
				cstart = cstart + pbuf.chunksize
			End While
			bdss.dserrfinish(pbuf.tmp0)
			rep.relclserror = pbuf.tmp0(0)
			rep.avgce = pbuf.tmp0(1) / System.Math.Log(2)
			rep.rmserror = pbuf.tmp0(2)
			rep.avgerror = pbuf.tmp0(3)
			rep.avgrelerror = pbuf.tmp0(4)

			'
			' Recycle
			'
			alglib.smp.ae_shared_pool_recycle(buf, pbuf)
		End Sub


		'************************************************************************
'        Internal subroutine: adding new input layer to network
'        ************************************************************************

		Private Shared Sub addinputlayer(ncount As Integer, ByRef lsizes As Integer(), ByRef ltypes As Integer(), ByRef lconnfirst As Integer(), ByRef lconnlast As Integer(), ByRef lastproc As Integer)
			lsizes(0) = ncount
			ltypes(0) = -2
			lconnfirst(0) = 0
			lconnlast(0) = 0
			lastproc = 0
		End Sub


		'************************************************************************
'        Internal subroutine: adding new summator layer to network
'        ************************************************************************

		Private Shared Sub addbiasedsummatorlayer(ncount As Integer, ByRef lsizes As Integer(), ByRef ltypes As Integer(), ByRef lconnfirst As Integer(), ByRef lconnlast As Integer(), ByRef lastproc As Integer)
			lsizes(lastproc + 1) = 1
			ltypes(lastproc + 1) = -3
			lconnfirst(lastproc + 1) = 0
			lconnlast(lastproc + 1) = 0
			lsizes(lastproc + 2) = ncount
			ltypes(lastproc + 2) = 0
			lconnfirst(lastproc + 2) = lastproc
			lconnlast(lastproc + 2) = lastproc + 1
			lastproc = lastproc + 2
		End Sub


		'************************************************************************
'        Internal subroutine: adding new summator layer to network
'        ************************************************************************

		Private Shared Sub addactivationlayer(functype As Integer, ByRef lsizes As Integer(), ByRef ltypes As Integer(), ByRef lconnfirst As Integer(), ByRef lconnlast As Integer(), ByRef lastproc As Integer)
			alglib.ap.assert(functype > 0 OrElse functype = -5, "AddActivationLayer: incorrect function type")
			lsizes(lastproc + 1) = lsizes(lastproc)
			ltypes(lastproc + 1) = functype
			lconnfirst(lastproc + 1) = lastproc
			lconnlast(lastproc + 1) = lastproc
			lastproc = lastproc + 1
		End Sub


		'************************************************************************
'        Internal subroutine: adding new zero layer to network
'        ************************************************************************

		Private Shared Sub addzerolayer(ByRef lsizes As Integer(), ByRef ltypes As Integer(), ByRef lconnfirst As Integer(), ByRef lconnlast As Integer(), ByRef lastproc As Integer)
			lsizes(lastproc + 1) = 1
			ltypes(lastproc + 1) = -4
			lconnfirst(lastproc + 1) = 0
			lconnlast(lastproc + 1) = 0
			lastproc = lastproc + 1
		End Sub


		'************************************************************************
'        This routine adds input layer to the high-level description of the network.
'
'        It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
'        these  arrays  have  enough  place  to  store  data.  It accepts following
'        parameters:
'            Network     -   network
'            ConnIdx     -   index of the first free entry in the HLConnections
'            NeuroIdx    -   index of the first free entry in the HLNeurons
'            StructInfoIdx-  index of the first entry in the low level description
'                            of the current layer (in the StructInfo array)
'            NIn         -   number of inputs
'                            
'        It modified Network and indices.
'        ************************************************************************

		Private Shared Sub hladdinputlayer(network As multilayerperceptron, ByRef connidx As Integer, ByRef neuroidx As Integer, ByRef structinfoidx As Integer, nin As Integer)
			Dim i As Integer = 0
			Dim offs As Integer = 0

			offs = hlnfieldwidth * neuroidx
			For i = 0 To nin - 1
				network.hlneurons(offs + 0) = 0
				network.hlneurons(offs + 1) = i
				network.hlneurons(offs + 2) = -1
				network.hlneurons(offs + 3) = -1
				offs = offs + hlnfieldwidth
			Next
			neuroidx = neuroidx + nin
			structinfoidx = structinfoidx + nin
		End Sub


		'************************************************************************
'        This routine adds output layer to the high-level description of
'        the network.
'
'        It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
'        these  arrays  have  enough  place  to  store  data.  It accepts following
'        parameters:
'            Network     -   network
'            ConnIdx     -   index of the first free entry in the HLConnections
'            NeuroIdx    -   index of the first free entry in the HLNeurons
'            StructInfoIdx-  index of the first entry in the low level description
'                            of the current layer (in the StructInfo array)
'            WeightsIdx  -   index of the first entry in the Weights array which
'                            corresponds to the current layer
'            K           -   current layer index
'            NPrev       -   number of neurons in the previous layer
'            NOut        -   number of outputs
'            IsCls       -   is it classifier network?
'            IsLinear    -   is it network with linear output?
'
'        It modified Network and ConnIdx/NeuroIdx/StructInfoIdx/WeightsIdx.
'        ************************************************************************

		Private Shared Sub hladdoutputlayer(network As multilayerperceptron, ByRef connidx As Integer, ByRef neuroidx As Integer, ByRef structinfoidx As Integer, ByRef weightsidx As Integer, k As Integer, _
			nprev As Integer, nout As Integer, iscls As Boolean, islinearout As Boolean)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim neurooffs As Integer = 0
			Dim connoffs As Integer = 0

			alglib.ap.assert((iscls AndAlso islinearout) OrElse Not iscls, "HLAddOutputLayer: internal error")
			neurooffs = hlnfieldwidth * neuroidx
			connoffs = hlconnfieldwidth * connidx
			If Not iscls Then

				'
				' Regression network
				'
				For i = 0 To nout - 1
					network.hlneurons(neurooffs + 0) = k
					network.hlneurons(neurooffs + 1) = i
					network.hlneurons(neurooffs + 2) = structinfoidx + 1 + nout + i
					network.hlneurons(neurooffs + 3) = weightsidx + nprev + (nprev + 1) * i
					neurooffs = neurooffs + hlnfieldwidth
				Next
				For i = 0 To nprev - 1
					For j = 0 To nout - 1
						network.hlconnections(connoffs + 0) = k - 1
						network.hlconnections(connoffs + 1) = i
						network.hlconnections(connoffs + 2) = k
						network.hlconnections(connoffs + 3) = j
						network.hlconnections(connoffs + 4) = weightsidx + i + j * (nprev + 1)
						connoffs = connoffs + hlconnfieldwidth
					Next
				Next
				connidx = connidx + nprev * nout
				neuroidx = neuroidx + nout
				structinfoidx = structinfoidx + 2 * nout + 1
				weightsidx = weightsidx + nout * (nprev + 1)
			Else

				'
				' Classification network
				'
				For i = 0 To nout - 2
					network.hlneurons(neurooffs + 0) = k
					network.hlneurons(neurooffs + 1) = i
					network.hlneurons(neurooffs + 2) = -1
					network.hlneurons(neurooffs + 3) = weightsidx + nprev + (nprev + 1) * i
					neurooffs = neurooffs + hlnfieldwidth
				Next
				network.hlneurons(neurooffs + 0) = k
				network.hlneurons(neurooffs + 1) = i
				network.hlneurons(neurooffs + 2) = -1
				network.hlneurons(neurooffs + 3) = -1
				For i = 0 To nprev - 1
					For j = 0 To nout - 2
						network.hlconnections(connoffs + 0) = k - 1
						network.hlconnections(connoffs + 1) = i
						network.hlconnections(connoffs + 2) = k
						network.hlconnections(connoffs + 3) = j
						network.hlconnections(connoffs + 4) = weightsidx + i + j * (nprev + 1)
						connoffs = connoffs + hlconnfieldwidth
					Next
				Next
				connidx = connidx + nprev * (nout - 1)
				neuroidx = neuroidx + nout
				structinfoidx = structinfoidx + nout + 2
				weightsidx = weightsidx + (nout - 1) * (nprev + 1)
			End If
		End Sub


		'************************************************************************
'        This routine adds hidden layer to the high-level description of
'        the network.
'
'        It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
'        these  arrays  have  enough  place  to  store  data.  It accepts following
'        parameters:
'            Network     -   network
'            ConnIdx     -   index of the first free entry in the HLConnections
'            NeuroIdx    -   index of the first free entry in the HLNeurons
'            StructInfoIdx-  index of the first entry in the low level description
'                            of the current layer (in the StructInfo array)
'            WeightsIdx  -   index of the first entry in the Weights array which
'                            corresponds to the current layer
'            K           -   current layer index
'            NPrev       -   number of neurons in the previous layer
'            NCur        -   number of neurons in the current layer
'
'        It modified Network and ConnIdx/NeuroIdx/StructInfoIdx/WeightsIdx.
'        ************************************************************************

		Private Shared Sub hladdhiddenlayer(network As multilayerperceptron, ByRef connidx As Integer, ByRef neuroidx As Integer, ByRef structinfoidx As Integer, ByRef weightsidx As Integer, k As Integer, _
			nprev As Integer, ncur As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim neurooffs As Integer = 0
			Dim connoffs As Integer = 0

			neurooffs = hlnfieldwidth * neuroidx
			connoffs = hlconnfieldwidth * connidx
			For i = 0 To ncur - 1
				network.hlneurons(neurooffs + 0) = k
				network.hlneurons(neurooffs + 1) = i
				network.hlneurons(neurooffs + 2) = structinfoidx + 1 + ncur + i
				network.hlneurons(neurooffs + 3) = weightsidx + nprev + (nprev + 1) * i
				neurooffs = neurooffs + hlnfieldwidth
			Next
			For i = 0 To nprev - 1
				For j = 0 To ncur - 1
					network.hlconnections(connoffs + 0) = k - 1
					network.hlconnections(connoffs + 1) = i
					network.hlconnections(connoffs + 2) = k
					network.hlconnections(connoffs + 3) = j
					network.hlconnections(connoffs + 4) = weightsidx + i + j * (nprev + 1)
					connoffs = connoffs + hlconnfieldwidth
				Next
			Next
			connidx = connidx + nprev * ncur
			neuroidx = neuroidx + ncur
			structinfoidx = structinfoidx + 2 * ncur + 1
			weightsidx = weightsidx + ncur * (nprev + 1)
		End Sub


		'************************************************************************
'        This function fills high level information about network created using
'        internal MLPCreate() function.
'
'        This function does NOT examine StructInfo for low level information, it
'        just expects that network has following structure:
'
'            input neuron            \
'            ...                      | input layer
'            input neuron            /
'            
'            "-1" neuron             \
'            biased summator          |
'            ...                      |
'            biased summator          | hidden layer(s), if there are exists any
'            activation function      |
'            ...                      |
'            activation function     /
'            
'            "-1" neuron            \
'            biased summator         | output layer:
'            ...                     |
'            biased summator         | * we have NOut summators/activators for regression networks
'            activation function     | * we have only NOut-1 summators and no activators for classifiers
'            ...                     | * we have "0" neuron only when we have classifier
'            activation function     |
'            "0" neuron              /
'
'
'          -- ALGLIB --
'             Copyright 30.03.2008 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub fillhighlevelinformation(network As multilayerperceptron, nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, iscls As Boolean, _
			islinearout As Boolean)
			Dim idxweights As Integer = 0
			Dim idxstruct As Integer = 0
			Dim idxneuro As Integer = 0
			Dim idxconn As Integer = 0

			alglib.ap.assert((iscls AndAlso islinearout) OrElse Not iscls, "FillHighLevelInformation: internal error")

			'
			' Preparations common to all types of networks
			'
			idxweights = 0
			idxneuro = 0
			idxstruct = 0
			idxconn = 0
			network.hlnetworktype = 0

			'
			' network without hidden layers
			'
			If nhid1 = 0 Then
				network.hllayersizes = New Integer(1) {}
				network.hllayersizes(0) = nin
				network.hllayersizes(1) = nout
				If Not iscls Then
					network.hlconnections = New Integer(hlconnfieldwidth * nin * nout - 1) {}
					network.hlneurons = New Integer(hlnfieldwidth * (nin + nout) - 1) {}
					network.hlnormtype = 0
				Else
					network.hlconnections = New Integer(hlconnfieldwidth * nin * (nout - 1) - 1) {}
					network.hlneurons = New Integer(hlnfieldwidth * (nin + nout) - 1) {}
					network.hlnormtype = 1
				End If
				hladdinputlayer(network, idxconn, idxneuro, idxstruct, nin)
				hladdoutputlayer(network, idxconn, idxneuro, idxstruct, idxweights, 1, _
					nin, nout, iscls, islinearout)
				Return
			End If

			'
			' network with one hidden layers
			'
			If nhid2 = 0 Then
				network.hllayersizes = New Integer(2) {}
				network.hllayersizes(0) = nin
				network.hllayersizes(1) = nhid1
				network.hllayersizes(2) = nout
				If Not iscls Then
					network.hlconnections = New Integer(hlconnfieldwidth * (nin * nhid1 + nhid1 * nout) - 1) {}
					network.hlneurons = New Integer(hlnfieldwidth * (nin + nhid1 + nout) - 1) {}
					network.hlnormtype = 0
				Else
					network.hlconnections = New Integer(hlconnfieldwidth * (nin * nhid1 + nhid1 * (nout - 1)) - 1) {}
					network.hlneurons = New Integer(hlnfieldwidth * (nin + nhid1 + nout) - 1) {}
					network.hlnormtype = 1
				End If
				hladdinputlayer(network, idxconn, idxneuro, idxstruct, nin)
				hladdhiddenlayer(network, idxconn, idxneuro, idxstruct, idxweights, 1, _
					nin, nhid1)
				hladdoutputlayer(network, idxconn, idxneuro, idxstruct, idxweights, 2, _
					nhid1, nout, iscls, islinearout)
				Return
			End If

			'
			' Two hidden layers
			'
			network.hllayersizes = New Integer(3) {}
			network.hllayersizes(0) = nin
			network.hllayersizes(1) = nhid1
			network.hllayersizes(2) = nhid2
			network.hllayersizes(3) = nout
			If Not iscls Then
				network.hlconnections = New Integer(hlconnfieldwidth * (nin * nhid1 + nhid1 * nhid2 + nhid2 * nout) - 1) {}
				network.hlneurons = New Integer(hlnfieldwidth * (nin + nhid1 + nhid2 + nout) - 1) {}
				network.hlnormtype = 0
			Else
				network.hlconnections = New Integer(hlconnfieldwidth * (nin * nhid1 + nhid1 * nhid2 + nhid2 * (nout - 1)) - 1) {}
				network.hlneurons = New Integer(hlnfieldwidth * (nin + nhid1 + nhid2 + nout) - 1) {}
				network.hlnormtype = 1
			End If
			hladdinputlayer(network, idxconn, idxneuro, idxstruct, nin)
			hladdhiddenlayer(network, idxconn, idxneuro, idxstruct, idxweights, 1, _
				nin, nhid1)
			hladdhiddenlayer(network, idxconn, idxneuro, idxstruct, idxweights, 2, _
				nhid1, nhid2)
			hladdoutputlayer(network, idxconn, idxneuro, idxstruct, idxweights, 3, _
				nhid2, nout, iscls, islinearout)
		End Sub


		'************************************************************************
'        Internal subroutine.
'
'          -- ALGLIB --
'             Copyright 04.11.2007 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mlpcreate(nin As Integer, nout As Integer, lsizes As Integer(), ltypes As Integer(), lconnfirst As Integer(), lconnlast As Integer(), _
			layerscount As Integer, isclsnet As Boolean, network As multilayerperceptron)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ssize As Integer = 0
			Dim ntotal As Integer = 0
			Dim wcount As Integer = 0
			Dim offs As Integer = 0
			Dim nprocessed As Integer = 0
			Dim wallocated As Integer = 0
			Dim localtemp As Integer() = New Integer(-1) {}
			Dim lnfirst As Integer() = New Integer(-1) {}
			Dim lnsyn As Integer() = New Integer(-1) {}
			Dim buf As New hpccores.mlpbuffers()
			Dim sgrad As New smlpgrad()


			'
			' Check
			'
			alglib.ap.assert(layerscount > 0, "MLPCreate: wrong parameters!")
			alglib.ap.assert(ltypes(0) = -2, "MLPCreate: wrong LTypes[0] (must be -2)!")
			For i = 0 To layerscount - 1
				alglib.ap.assert(lsizes(i) > 0, "MLPCreate: wrong LSizes!")
				alglib.ap.assert(lconnfirst(i) >= 0 AndAlso (lconnfirst(i) < i OrElse i = 0), "MLPCreate: wrong LConnFirst!")
				alglib.ap.assert(lconnlast(i) >= lconnfirst(i) AndAlso (lconnlast(i) < i OrElse i = 0), "MLPCreate: wrong LConnLast!")
			Next

			'
			' Build network geometry
			'
			lnfirst = New Integer(layerscount - 1) {}
			lnsyn = New Integer(layerscount - 1) {}
			ntotal = 0
			wcount = 0
			For i = 0 To layerscount - 1

				'
				' Analyze connections.
				' This code must throw an assertion in case of unknown LTypes[I]
				'
				lnsyn(i) = -1
				If ltypes(i) >= 0 OrElse ltypes(i) = -5 Then
					lnsyn(i) = 0
					For j = lconnfirst(i) To lconnlast(i)
						lnsyn(i) = lnsyn(i) + lsizes(j)
					Next
				Else
					If (ltypes(i) = -2 OrElse ltypes(i) = -3) OrElse ltypes(i) = -4 Then
						lnsyn(i) = 0
					End If
				End If
				alglib.ap.assert(lnsyn(i) >= 0, "MLPCreate: internal error #0!")

				'
				' Other info
				'
				lnfirst(i) = ntotal
				ntotal = ntotal + lsizes(i)
				If ltypes(i) = 0 Then
					wcount = wcount + lnsyn(i) * lsizes(i)
				End If
			Next
			ssize = 7 + ntotal * nfieldwidth

			'
			' Allocate
			'
			network.structinfo = New Integer(ssize - 1) {}
			network.weights = New Double(wcount - 1) {}
			If isclsnet Then
				network.columnmeans = New Double(nin - 1) {}
				network.columnsigmas = New Double(nin - 1) {}
			Else
				network.columnmeans = New Double(nin + nout - 1) {}
				network.columnsigmas = New Double(nin + nout - 1) {}
			End If
			network.neurons = New Double(ntotal - 1) {}
			network.nwbuf = New Double(System.Math.Max(wcount, 2 * nout) - 1) {}
			network.integerbuf = New Integer(3) {}
			network.dfdnet = New Double(ntotal - 1) {}
			network.x = New Double(nin - 1) {}
			network.y = New Double(nout - 1) {}
			network.derror = New Double(ntotal - 1) {}

			'
			' Fill structure: global info
			'
			network.structinfo(0) = ssize
			network.structinfo(1) = nin
			network.structinfo(2) = nout
			network.structinfo(3) = ntotal
			network.structinfo(4) = wcount
			network.structinfo(5) = 7
			If isclsnet Then
				network.structinfo(6) = 1
			Else
				network.structinfo(6) = 0
			End If

			'
			' Fill structure: neuron connections
			'
			nprocessed = 0
			wallocated = 0
			For i = 0 To layerscount - 1
				For j = 0 To lsizes(i) - 1
					offs = network.structinfo(5) + nprocessed * nfieldwidth
					network.structinfo(offs + 0) = ltypes(i)
					If ltypes(i) = 0 Then

						'
						' Adaptive summator:
						' * connections with weights to previous neurons
						'
						network.structinfo(offs + 1) = lnsyn(i)
						network.structinfo(offs + 2) = lnfirst(lconnfirst(i))
						network.structinfo(offs + 3) = wallocated
						wallocated = wallocated + lnsyn(i)
						nprocessed = nprocessed + 1
					End If
					If ltypes(i) > 0 OrElse ltypes(i) = -5 Then

						'
						' Activation layer:
						' * each neuron connected to one (only one) of previous neurons.
						' * no weights
						'
						network.structinfo(offs + 1) = 1
						network.structinfo(offs + 2) = lnfirst(lconnfirst(i)) + j
						network.structinfo(offs + 3) = -1
						nprocessed = nprocessed + 1
					End If
					If (ltypes(i) = -2 OrElse ltypes(i) = -3) OrElse ltypes(i) = -4 Then
						nprocessed = nprocessed + 1
					End If
				Next
			Next
			alglib.ap.assert(wallocated = wcount, "MLPCreate: internal error #1!")
			alglib.ap.assert(nprocessed = ntotal, "MLPCreate: internal error #2!")

			'
			' Fill weights by small random values
			' Initialize means and sigmas
			'
			For i = 0 To nin - 1
				network.columnmeans(i) = 0
				network.columnsigmas(i) = 1
			Next
			If Not isclsnet Then
				For i = 0 To nout - 1
					network.columnmeans(nin + i) = 0
					network.columnsigmas(nin + i) = 1
				Next
			End If
			mlprandomize(network)

			'
			' Seed buffers
			'
			alglib.smp.ae_shared_pool_set_seed(network.buf, buf)
			sgrad.g = New Double(wcount - 1) {}
			sgrad.f = 0.0
			For i = 0 To wcount - 1
				sgrad.g(i) = 0.0
			Next
			alglib.smp.ae_shared_pool_set_seed(network.gradbuf, sgrad)
		End Sub


		'************************************************************************
'        Internal subroutine for Hessian calculation.
'
'        WARNING! Unspeakable math far beyong human capabilities :)
'        ************************************************************************

		Private Shared Sub mlphessianbatchinternal(network As multilayerperceptron, xy As Double(,), ssize As Integer, naturalerr As Boolean, ByRef e As Double, ByRef grad As Double(), _
			ByRef h As Double(,))
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim kl As Integer = 0
			Dim offs As Integer = 0
			Dim n1 As Integer = 0
			Dim n2 As Integer = 0
			Dim w1 As Integer = 0
			Dim w2 As Integer = 0
			Dim s As Double = 0
			Dim t As Double = 0
			Dim v As Double = 0
			Dim et As Double = 0
			Dim bflag As New Boolean()
			Dim f As Double = 0
			Dim df As Double = 0
			Dim d2f As Double = 0
			Dim deidyj As Double = 0
			Dim mx As Double = 0
			Dim q As Double = 0
			Dim z As Double = 0
			Dim s2 As Double = 0
			Dim expi As Double = 0
			Dim expj As Double = 0
			Dim x As Double() = New Double(-1) {}
			Dim desiredy As Double() = New Double(-1) {}
			Dim gt As Double() = New Double(-1) {}
			Dim zeros As Double() = New Double(-1) {}
			Dim rx As Double(,) = New Double(-1, -1) {}
			Dim ry As Double(,) = New Double(-1, -1) {}
			Dim rdx As Double(,) = New Double(-1, -1) {}
			Dim rdy As Double(,) = New Double(-1, -1) {}
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			e = 0

			mlpproperties(network, nin, nout, wcount)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Prepare
			'
			x = New Double(nin - 1) {}
			desiredy = New Double(nout - 1) {}
			zeros = New Double(wcount - 1) {}
			gt = New Double(wcount - 1) {}
			rx = New Double(ntotal + nout - 1, wcount - 1) {}
			ry = New Double(ntotal + nout - 1, wcount - 1) {}
			rdx = New Double(ntotal + nout - 1, wcount - 1) {}
			rdy = New Double(ntotal + nout - 1, wcount - 1) {}
			e = 0
			For i = 0 To wcount - 1
				zeros(i) = 0
			Next
			For i_ = 0 To wcount - 1
				grad(i_) = zeros(i_)
			Next
			For i = 0 To wcount - 1
				For i_ = 0 To wcount - 1
					h(i, i_) = zeros(i_)
				Next
			Next

			'
			' Process
			'
			For k = 0 To ssize - 1

				'
				' Process vector with MLPGradN.
				' Now Neurons, DFDNET and DError contains results of the last run.
				'
				For i_ = 0 To nin - 1
					x(i_) = xy(k, i_)
				Next
				If mlpissoftmax(network) Then

					'
					' class labels outputs
					'
					kl = CInt(System.Math.Truncate(System.Math.Round(xy(k, nin))))
					For i = 0 To nout - 1
						If i = kl Then
							desiredy(i) = 1
						Else
							desiredy(i) = 0
						End If
					Next
				Else

					'
					' real outputs
					'
					i1_ = (nin) - (0)
					For i_ = 0 To nout - 1
						desiredy(i_) = xy(k, i_ + i1_)
					Next
				End If
				If naturalerr Then
					mlpgradn(network, x, desiredy, et, gt)
				Else
					mlpgrad(network, x, desiredy, et, gt)
				End If

				'
				' grad, error
				'
				e = e + et
				For i_ = 0 To wcount - 1
					grad(i_) = grad(i_) + gt(i_)
				Next

				'
				' Hessian.
				' Forward pass of the R-algorithm
				'
				For i = 0 To ntotal - 1
					offs = istart + i * nfieldwidth
					For i_ = 0 To wcount - 1
						rx(i, i_) = zeros(i_)
					Next
					For i_ = 0 To wcount - 1
						ry(i, i_) = zeros(i_)
					Next
					If network.structinfo(offs + 0) > 0 OrElse network.structinfo(offs + 0) = -5 Then

						'
						' Activation function
						'
						n1 = network.structinfo(offs + 2)
						For i_ = 0 To wcount - 1
							rx(i, i_) = ry(n1, i_)
						Next
						v = network.dfdnet(i)
						For i_ = 0 To wcount - 1
							ry(i, i_) = v * rx(i, i_)
						Next
						Continue For
					End If
					If network.structinfo(offs + 0) = 0 Then

						'
						' Adaptive summator
						'
						n1 = network.structinfo(offs + 2)
						n2 = n1 + network.structinfo(offs + 1) - 1
						w1 = network.structinfo(offs + 3)
						w2 = w1 + network.structinfo(offs + 1) - 1
						For j = n1 To n2
							v = network.weights(w1 + j - n1)
							For i_ = 0 To wcount - 1
								rx(i, i_) = rx(i, i_) + v * ry(j, i_)
							Next
							rx(i, w1 + j - n1) = rx(i, w1 + j - n1) + network.neurons(j)
						Next
						For i_ = 0 To wcount - 1
							ry(i, i_) = rx(i, i_)
						Next
						Continue For
					End If
					If network.structinfo(offs + 0) < 0 Then
						bflag = True
						If network.structinfo(offs + 0) = -2 Then

							'
							' input neuron, left unchanged
							'
							bflag = False
						End If
						If network.structinfo(offs + 0) = -3 Then

							'
							' "-1" neuron, left unchanged
							'
							bflag = False
						End If
						If network.structinfo(offs + 0) = -4 Then

							'
							' "0" neuron, left unchanged
							'
							bflag = False
						End If
						alglib.ap.assert(Not bflag, "MLPHessianNBatch: internal error - unknown neuron type!")
						Continue For
					End If
				Next

				'
				' Hessian. Backward pass of the R-algorithm.
				'
				' Stage 1. Initialize RDY
				'
				For i = 0 To ntotal + nout - 1
					For i_ = 0 To wcount - 1
						rdy(i, i_) = zeros(i_)
					Next
				Next
				If network.structinfo(6) = 0 Then

					'
					' Standardisation.
					'
					' In context of the Hessian calculation standardisation
					' is considered as additional layer with weightless
					' activation function:
					'
					' F(NET) := Sigma*NET
					'
					' So we add one more layer to forward pass, and
					' make forward/backward pass through this layer.
					'
					For i = 0 To nout - 1
						n1 = ntotal - nout + i
						n2 = ntotal + i

						'
						' Forward pass from N1 to N2
						'
						For i_ = 0 To wcount - 1
							rx(n2, i_) = ry(n1, i_)
						Next
						v = network.columnsigmas(nin + i)
						For i_ = 0 To wcount - 1
							ry(n2, i_) = v * rx(n2, i_)
						Next

						'
						' Initialization of RDY
						'
						For i_ = 0 To wcount - 1
							rdy(n2, i_) = ry(n2, i_)
						Next

						'
						' Backward pass from N2 to N1:
						' 1. Calculate R(dE/dX).
						' 2. No R(dE/dWij) is needed since weight of activation neuron
						'    is fixed to 1. So we can update R(dE/dY) for
						'    the connected neuron (note that Vij=0, Wij=1)
						'
						df = network.columnsigmas(nin + i)
						For i_ = 0 To wcount - 1
							rdx(n2, i_) = df * rdy(n2, i_)
						Next
						For i_ = 0 To wcount - 1
							rdy(n1, i_) = rdy(n1, i_) + rdx(n2, i_)
						Next
					Next
				Else

					'
					' Softmax.
					'
					' Initialize RDY using generalized expression for ei'(yi)
					' (see expression (9) from p. 5 of "Fast Exact Multiplication by the Hessian").
					'
					' When we are working with softmax network, generalized
					' expression for ei'(yi) is used because softmax
					' normalization leads to ei, which depends on all y's
					'
					If naturalerr Then

						'
						' softmax + cross-entropy.
						' We have:
						'
						' S = sum(exp(yk)),
						' ei = sum(trn)*exp(yi)/S-trn_i
						'
						' j=i:   d(ei)/d(yj) = T*exp(yi)*(S-exp(yi))/S^2
						' j<>i:  d(ei)/d(yj) = -T*exp(yi)*exp(yj)/S^2
						'
						t = 0
						For i = 0 To nout - 1
							t = t + desiredy(i)
						Next
						mx = network.neurons(ntotal - nout)
						For i = 0 To nout - 1
							mx = System.Math.Max(mx, network.neurons(ntotal - nout + i))
						Next
						s = 0
						For i = 0 To nout - 1
							network.nwbuf(i) = System.Math.Exp(network.neurons(ntotal - nout + i) - mx)
							s = s + network.nwbuf(i)
						Next
						For i = 0 To nout - 1
							For j = 0 To nout - 1
								If j = i Then
									deidyj = t * network.nwbuf(i) * (s - network.nwbuf(i)) / Math.sqr(s)
									For i_ = 0 To wcount - 1
										rdy(ntotal - nout + i, i_) = rdy(ntotal - nout + i, i_) + deidyj * ry(ntotal - nout + i, i_)
									Next
								Else
									deidyj = -(t * network.nwbuf(i) * network.nwbuf(j) / Math.sqr(s))
									For i_ = 0 To wcount - 1
										rdy(ntotal - nout + i, i_) = rdy(ntotal - nout + i, i_) + deidyj * ry(ntotal - nout + j, i_)
									Next
								End If
							Next
						Next
					Else

						'
						' For a softmax + squared error we have expression
						' far beyond human imagination so we dont even try
						' to comment on it. Just enjoy the code...
						'
						' P.S. That's why "natural error" is called "natural" -
						' compact beatiful expressions, fast code....
						'
						mx = network.neurons(ntotal - nout)
						For i = 0 To nout - 1
							mx = System.Math.Max(mx, network.neurons(ntotal - nout + i))
						Next
						s = 0
						s2 = 0
						For i = 0 To nout - 1
							network.nwbuf(i) = System.Math.Exp(network.neurons(ntotal - nout + i) - mx)
							s = s + network.nwbuf(i)
							s2 = s2 + Math.sqr(network.nwbuf(i))
						Next
						q = 0
						For i = 0 To nout - 1
							q = q + (network.y(i) - desiredy(i)) * network.nwbuf(i)
						Next
						For i = 0 To nout - 1
							z = -q + (network.y(i) - desiredy(i)) * s
							expi = network.nwbuf(i)
							For j = 0 To nout - 1
								expj = network.nwbuf(j)
								If j = i Then
									deidyj = expi / Math.sqr(s) * ((z + expi) * (s - 2 * expi) / s + expi * s2 / Math.sqr(s))
								Else
									deidyj = expi * expj / Math.sqr(s) * (s2 / Math.sqr(s) - 2 * z / s - (expi + expj) / s + (network.y(i) - desiredy(i)) - (network.y(j) - desiredy(j)))
								End If
								For i_ = 0 To wcount - 1
									rdy(ntotal - nout + i, i_) = rdy(ntotal - nout + i, i_) + deidyj * ry(ntotal - nout + j, i_)
								Next
							Next
						Next
					End If
				End If

				'
				' Hessian. Backward pass of the R-algorithm
				'
				' Stage 2. Process.
				'
				For i = ntotal - 1 To 0 Step -1

					'
					' Possible variants:
					' 1. Activation function
					' 2. Adaptive summator
					' 3. Special neuron
					'
					offs = istart + i * nfieldwidth
					If network.structinfo(offs + 0) > 0 OrElse network.structinfo(offs + 0) = -5 Then
						n1 = network.structinfo(offs + 2)

						'
						' First, calculate R(dE/dX).
						'
						mlpactivationfunction(network.neurons(n1), network.structinfo(offs + 0), f, df, d2f)
						v = d2f * network.derror(i)
						For i_ = 0 To wcount - 1
							rdx(i, i_) = df * rdy(i, i_)
						Next
						For i_ = 0 To wcount - 1
							rdx(i, i_) = rdx(i, i_) + v * rx(i, i_)
						Next

						'
						' No R(dE/dWij) is needed since weight of activation neuron
						' is fixed to 1.
						'
						' So we can update R(dE/dY) for the connected neuron.
						' (note that Vij=0, Wij=1)
						'
						For i_ = 0 To wcount - 1
							rdy(n1, i_) = rdy(n1, i_) + rdx(i, i_)
						Next
						Continue For
					End If
					If network.structinfo(offs + 0) = 0 Then

						'
						' Adaptive summator
						'
						n1 = network.structinfo(offs + 2)
						n2 = n1 + network.structinfo(offs + 1) - 1
						w1 = network.structinfo(offs + 3)
						w2 = w1 + network.structinfo(offs + 1) - 1

						'
						' First, calculate R(dE/dX).
						'
						For i_ = 0 To wcount - 1
							rdx(i, i_) = rdy(i, i_)
						Next

						'
						' Then, calculate R(dE/dWij)
						'
						For j = w1 To w2
							v = network.neurons(n1 + j - w1)
							For i_ = 0 To wcount - 1
								h(j, i_) = h(j, i_) + v * rdx(i, i_)
							Next
							v = network.derror(i)
							For i_ = 0 To wcount - 1
								h(j, i_) = h(j, i_) + v * ry(n1 + j - w1, i_)
							Next
						Next

						'
						' And finally, update R(dE/dY) for connected neurons.
						'
						For j = w1 To w2
							v = network.weights(j)
							For i_ = 0 To wcount - 1
								rdy(n1 + j - w1, i_) = rdy(n1 + j - w1, i_) + v * rdx(i, i_)
							Next
							rdy(n1 + j - w1, j) = rdy(n1 + j - w1, j) + network.derror(i)
						Next
						Continue For
					End If
					If network.structinfo(offs + 0) < 0 Then
						bflag = False
						If (network.structinfo(offs + 0) = -2 OrElse network.structinfo(offs + 0) = -3) OrElse network.structinfo(offs + 0) = -4 Then

							'
							' Special neuron type, no back-propagation required
							'
							bflag = True
						End If
						alglib.ap.assert(bflag, "MLPHessianNBatch: unknown neuron type!")
						Continue For
					End If
				Next
			Next
		End Sub


		'************************************************************************
'        Internal subroutine
'
'        Network must be processed by MLPProcess on X
'        ************************************************************************

		Private Shared Sub mlpinternalcalculategradient(network As multilayerperceptron, neurons As Double(), weights As Double(), ByRef derror As Double(), ByRef grad As Double(), naturalerrorfunc As Boolean)
			Dim i As Integer = 0
			Dim n1 As Integer = 0
			Dim n2 As Integer = 0
			Dim w1 As Integer = 0
			Dim w2 As Integer = 0
			Dim ntotal As Integer = 0
			Dim istart As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim offs As Integer = 0
			Dim dedf As Double = 0
			Dim dfdnet As Double = 0
			Dim v As Double = 0
			Dim fown As Double = 0
			Dim deown As Double = 0
			Dim net As Double = 0
			Dim mx As Double = 0
			Dim bflag As New Boolean()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0


			'
			' Read network geometry
			'
			nin = network.structinfo(1)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)

			'
			' Pre-processing of dError/dOut:
			' from dError/dOut(normalized) to dError/dOut(non-normalized)
			'
			alglib.ap.assert(network.structinfo(6) = 0 OrElse network.structinfo(6) = 1, "MLPInternalCalculateGradient: unknown normalization type!")
			If network.structinfo(6) = 1 Then

				'
				' Softmax
				'
				If Not naturalerrorfunc Then
					mx = network.neurons(ntotal - nout)
					For i = 0 To nout - 1
						mx = System.Math.Max(mx, network.neurons(ntotal - nout + i))
					Next
					net = 0
					For i = 0 To nout - 1
						network.nwbuf(i) = System.Math.Exp(network.neurons(ntotal - nout + i) - mx)
						net = net + network.nwbuf(i)
					Next
					i1_ = (0) - (ntotal - nout)
					v = 0.0
					For i_ = ntotal - nout To ntotal - 1
						v += network.derror(i_) * network.nwbuf(i_ + i1_)
					Next
					For i = 0 To nout - 1
						fown = network.nwbuf(i)
						deown = network.derror(ntotal - nout + i)
						network.nwbuf(nout + i) = (-v + deown * fown + deown * (net - fown)) * fown / Math.sqr(net)
					Next
					For i = 0 To nout - 1
						network.derror(ntotal - nout + i) = network.nwbuf(nout + i)
					Next
				End If
			Else

				'
				' Un-standardisation
				'
				For i = 0 To nout - 1
					network.derror(ntotal - nout + i) = network.derror(ntotal - nout + i) * network.columnsigmas(nin + i)
				Next
			End If

			'
			' Backpropagation
			'
			For i = ntotal - 1 To 0 Step -1

				'
				' Extract info
				'
				offs = istart + i * nfieldwidth
				If network.structinfo(offs + 0) > 0 OrElse network.structinfo(offs + 0) = -5 Then

					'
					' Activation function
					'
					dedf = network.derror(i)
					dfdnet = network.dfdnet(i)
					derror(network.structinfo(offs + 2)) = derror(network.structinfo(offs + 2)) + dedf * dfdnet
					Continue For
				End If
				If network.structinfo(offs + 0) = 0 Then

					'
					' Adaptive summator
					'
					n1 = network.structinfo(offs + 2)
					n2 = n1 + network.structinfo(offs + 1) - 1
					w1 = network.structinfo(offs + 3)
					w2 = w1 + network.structinfo(offs + 1) - 1
					dedf = network.derror(i)
					dfdnet = 1.0
					v = dedf * dfdnet
					i1_ = (n1) - (w1)
					For i_ = w1 To w2
						grad(i_) = v * neurons(i_ + i1_)
					Next
					i1_ = (w1) - (n1)
					For i_ = n1 To n2
						derror(i_) = derror(i_) + v * weights(i_ + i1_)
					Next
					Continue For
				End If
				If network.structinfo(offs + 0) < 0 Then
					bflag = False
					If (network.structinfo(offs + 0) = -2 OrElse network.structinfo(offs + 0) = -3) OrElse network.structinfo(offs + 0) = -4 Then

						'
						' Special neuron type, no back-propagation required
						'
						bflag = True
					End If
					alglib.ap.assert(bflag, "MLPInternalCalculateGradient: unknown neuron type!")
					Continue For
				End If
			Next
		End Sub


		Private Shared Sub mlpchunkedgradient(network As multilayerperceptron, xy As Double(,), cstart As Integer, csize As Integer, batch4buf As Double(), hpcbuf As Double(), _
			ByRef e As Double, naturalerrorfunc As Boolean)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim kl As Integer = 0
			Dim ntotal As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim offs As Integer = 0
			Dim f As Double = 0
			Dim df As Double = 0
			Dim d2f As Double = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim s As Double = 0
			Dim fown As Double = 0
			Dim deown As Double = 0
			Dim bflag As New Boolean()
			Dim istart As Integer = 0
			Dim entrysize As Integer = 0
			Dim dfoffs As Integer = 0
			Dim derroroffs As Integer = 0
			Dim entryoffs As Integer = 0
			Dim neuronidx As Integer = 0
			Dim srcentryoffs As Integer = 0
			Dim srcneuronidx As Integer = 0
			Dim srcweightidx As Integer = 0
			Dim neurontype As Integer = 0
			Dim nweights As Integer = 0
			Dim offs0 As Integer = 0
			Dim offs1 As Integer = 0
			Dim offs2 As Integer = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim v2 As Double = 0
			Dim v3 As Double = 0
			Dim s0 As Double = 0
			Dim s1 As Double = 0
			Dim s2 As Double = 0
			Dim s3 As Double = 0
			Dim chunksize As Integer = 0

			chunksize = 4
			alglib.ap.assert(csize <= chunksize, "MLPChunkedGradient: internal error (CSize>ChunkSize)")

			'
			' Try to use HPC core, if possible
			'
			If hpccores.hpcchunkedgradient(network.weights, network.structinfo, network.columnmeans, network.columnsigmas, xy, cstart, _
				csize, batch4buf, hpcbuf, e, naturalerrorfunc) Then
				Return
			End If

			'
			' Read network geometry, prepare data
			'
			nin = network.structinfo(1)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)
			entrysize = 12
			dfoffs = 4
			derroroffs = 8

			'
			' Fill Batch4Buf by zeros.
			'
			' THIS STAGE IS VERY IMPORTANT!
			'
			' We fill all components of entry - neuron values, dF/dNET, dError/dF.
			' It allows us to easily handle  situations  when  CSize<ChunkSize  by
			' simply  working  with  ALL  components  of  Batch4Buf,  without ever
			' looking at CSize. The idea is that dError/dF for  absent  components
			' will be initialized by zeros - and won't be  rewritten  by  non-zero
			' values during backpropagation.
			'
			For i = 0 To entrysize * ntotal - 1
				batch4buf(i) = 0
			Next

			'
			' Forward pass:
			' 1. Load data into Batch4Buf. If CSize<ChunkSize, data are padded by zeros.
			' 2. Perform forward pass through network
			'
			For i = 0 To nin - 1
				entryoffs = entrysize * i
				For j = 0 To csize - 1
					If CDbl(network.columnsigmas(i)) <> CDbl(0) Then
						batch4buf(entryoffs + j) = (xy(cstart + j, i) - network.columnmeans(i)) / network.columnsigmas(i)
					Else
						batch4buf(entryoffs + j) = xy(cstart + j, i) - network.columnmeans(i)
					End If
				Next
			Next
			For neuronidx = 0 To ntotal - 1
				entryoffs = entrysize * neuronidx
				offs = istart + neuronidx * nfieldwidth
				neurontype = network.structinfo(offs + 0)
				If neurontype > 0 OrElse neurontype = -5 Then

					'
					' "activation function" neuron, which takes value of neuron SrcNeuronIdx
					' and applies activation function to it.
					'
					' This neuron has no weights and no tunable parameters.
					'
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					mlpactivationfunction(batch4buf(srcentryoffs + 0), neurontype, f, df, d2f)
					batch4buf(entryoffs + 0) = f
					batch4buf(entryoffs + 0 + dfoffs) = df
					mlpactivationfunction(batch4buf(srcentryoffs + 1), neurontype, f, df, d2f)
					batch4buf(entryoffs + 1) = f
					batch4buf(entryoffs + 1 + dfoffs) = df
					mlpactivationfunction(batch4buf(srcentryoffs + 2), neurontype, f, df, d2f)
					batch4buf(entryoffs + 2) = f
					batch4buf(entryoffs + 2 + dfoffs) = df
					mlpactivationfunction(batch4buf(srcentryoffs + 3), neurontype, f, df, d2f)
					batch4buf(entryoffs + 3) = f
					batch4buf(entryoffs + 3 + dfoffs) = df
					Continue For
				End If
				If neurontype = 0 Then

					'
					' "adaptive summator" neuron, whose output is a weighted sum of inputs.
					' It has weights, but has no activation function.
					'
					nweights = network.structinfo(offs + 1)
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					srcweightidx = network.structinfo(offs + 3)
					v0 = 0
					v1 = 0
					v2 = 0
					v3 = 0
					For j = 0 To nweights - 1
						v = network.weights(srcweightidx)
						srcweightidx = srcweightidx + 1
						v0 = v0 + v * batch4buf(srcentryoffs + 0)
						v1 = v1 + v * batch4buf(srcentryoffs + 1)
						v2 = v2 + v * batch4buf(srcentryoffs + 2)
						v3 = v3 + v * batch4buf(srcentryoffs + 3)
						srcentryoffs = srcentryoffs + entrysize
					Next
					batch4buf(entryoffs + 0) = v0
					batch4buf(entryoffs + 1) = v1
					batch4buf(entryoffs + 2) = v2
					batch4buf(entryoffs + 3) = v3
					batch4buf(entryoffs + 0 + dfoffs) = 1
					batch4buf(entryoffs + 1 + dfoffs) = 1
					batch4buf(entryoffs + 2 + dfoffs) = 1
					batch4buf(entryoffs + 3 + dfoffs) = 1
					Continue For
				End If
				If neurontype < 0 Then
					bflag = False
					If neurontype = -2 Then

						'
						' Input neuron, left unchanged
						'
						bflag = True
					End If
					If neurontype = -3 Then

						'
						' "-1" neuron
						'
						batch4buf(entryoffs + 0) = -1
						batch4buf(entryoffs + 1) = -1
						batch4buf(entryoffs + 2) = -1
						batch4buf(entryoffs + 3) = -1
						batch4buf(entryoffs + 0 + dfoffs) = 0
						batch4buf(entryoffs + 1 + dfoffs) = 0
						batch4buf(entryoffs + 2 + dfoffs) = 0
						batch4buf(entryoffs + 3 + dfoffs) = 0
						bflag = True
					End If
					If neurontype = -4 Then

						'
						' "0" neuron
						'
						batch4buf(entryoffs + 0) = 0
						batch4buf(entryoffs + 1) = 0
						batch4buf(entryoffs + 2) = 0
						batch4buf(entryoffs + 3) = 0
						batch4buf(entryoffs + 0 + dfoffs) = 0
						batch4buf(entryoffs + 1 + dfoffs) = 0
						batch4buf(entryoffs + 2 + dfoffs) = 0
						batch4buf(entryoffs + 3 + dfoffs) = 0
						bflag = True
					End If
					alglib.ap.assert(bflag, "MLPChunkedGradient: internal error - unknown neuron type!")
					Continue For
				End If
			Next

			'
			' Intermediate phase between forward and backward passes.
			'
			' For regression networks:
			' * forward pass is completely done (no additional post-processing is
			'   needed).
			' * before starting backward pass, we have to  calculate  dError/dOut
			'   for output neurons. We also update error at this phase.
			'
			' For classification networks:
			' * in addition to forward pass we  apply  SOFTMAX  normalization  to
			'   output neurons.
			' * after applying normalization, we have to  calculate  dError/dOut,
			'   which is calculated in two steps:
			'   * first, we calculate derivative of error with respect to SOFTMAX
			'     normalized outputs (normalized dError)
			'   * then,  we calculate derivative of error with respect to  values
			'     of outputs BEFORE normalization was applied to them
			'
			alglib.ap.assert(network.structinfo(6) = 0 OrElse network.structinfo(6) = 1, "MLPChunkedGradient: unknown normalization type!")
			If network.structinfo(6) = 1 Then

				'
				' SOFTMAX-normalized network.
				'
				' First,  calculate (V0,V1,V2,V3)  -  component-wise  maximum
				' of output neurons. This vector of maximum  values  will  be
				' used for normalization  of  outputs  prior  to  calculating
				' exponentials.
				'
				' NOTE: the only purpose of this stage is to prevent overflow
				'       during calculation of exponentials.  With  this stage
				'       we  make  sure  that  all exponentials are calculated
				'       with non-positive argument. If you load (0,0,0,0)  to
				'       (V0,V1,V2,V3), your program will continue  working  -
				'       although with less robustness.
				'
				entryoffs = entrysize * (ntotal - nout)
				v0 = batch4buf(entryoffs + 0)
				v1 = batch4buf(entryoffs + 1)
				v2 = batch4buf(entryoffs + 2)
				v3 = batch4buf(entryoffs + 3)
				entryoffs = entryoffs + entrysize
				For i = 1 To nout - 1
					v = batch4buf(entryoffs + 0)
					If v > v0 Then
						v0 = v
					End If
					v = batch4buf(entryoffs + 1)
					If v > v1 Then
						v1 = v
					End If
					v = batch4buf(entryoffs + 2)
					If v > v2 Then
						v2 = v
					End If
					v = batch4buf(entryoffs + 3)
					If v > v3 Then
						v3 = v
					End If
					entryoffs = entryoffs + entrysize
				Next

				'
				' Then,  calculate exponentials and place them to part of the
				' array which  is  located  past  the  last  entry.  We  also
				' calculate sum of exponentials which will be stored past the
				' exponentials.
				'
				entryoffs = entrysize * (ntotal - nout)
				offs0 = entrysize * ntotal
				s0 = 0
				s1 = 0
				s2 = 0
				s3 = 0
				For i = 0 To nout - 1
					v = System.Math.Exp(batch4buf(entryoffs + 0) - v0)
					s0 = s0 + v
					batch4buf(offs0 + 0) = v
					v = System.Math.Exp(batch4buf(entryoffs + 1) - v1)
					s1 = s1 + v
					batch4buf(offs0 + 1) = v
					v = System.Math.Exp(batch4buf(entryoffs + 2) - v2)
					s2 = s2 + v
					batch4buf(offs0 + 2) = v
					v = System.Math.Exp(batch4buf(entryoffs + 3) - v3)
					s3 = s3 + v
					batch4buf(offs0 + 3) = v
					entryoffs = entryoffs + entrysize
					offs0 = offs0 + chunksize
				Next
				offs0 = entrysize * ntotal + 2 * nout * chunksize
				batch4buf(offs0 + 0) = s0
				batch4buf(offs0 + 1) = s1
				batch4buf(offs0 + 2) = s2
				batch4buf(offs0 + 3) = s3

				'
				' Now we have:
				' * Batch4Buf[0...EntrySize*NTotal-1] stores:
				'   * NTotal*ChunkSize neuron output values (SOFTMAX normalization
				'     was not applied to these values),
				'   * NTotal*ChunkSize values of dF/dNET (derivative of neuron
				'     output with respect to its input)
				'   * NTotal*ChunkSize zeros in the elements which correspond to
				'     dError/dOut (derivative of error with respect to neuron output).
				' * Batch4Buf[EntrySize*NTotal...EntrySize*NTotal+ChunkSize*NOut-1] -
				'   stores exponentials of last NOut neurons.
				' * Batch4Buf[EntrySize*NTotal+ChunkSize*NOut-1...EntrySize*NTotal+ChunkSize*2*NOut-1]
				'   - can be used for temporary calculations
				' * Batch4Buf[EntrySize*NTotal+ChunkSize*2*NOut...EntrySize*NTotal+ChunkSize*2*NOut+ChunkSize-1]
				'   - stores sum-of-exponentials
				'
				' Block below calculates derivatives of error function with respect 
				' to non-SOFTMAX-normalized output values of last NOut neurons.
				'
				' It is quite complicated; we do not describe algebra behind it,
				' but if you want you may check it yourself :)
				'
				If naturalerrorfunc Then

					'
					' Calculate  derivative  of  error  with respect to values of
					' output  neurons  PRIOR TO SOFTMAX NORMALIZATION. Because we
					' use natural error function (cross-entropy), we  can  do  so
					' very easy.
					'
					offs0 = entrysize * ntotal + 2 * nout * chunksize
					For k = 0 To csize - 1
						s = batch4buf(offs0 + k)
						kl = CInt(System.Math.Truncate(System.Math.Round(xy(cstart + k, nin))))
						offs1 = (ntotal - nout) * entrysize + derroroffs + k
						offs2 = entrysize * ntotal + k
						For i = 0 To nout - 1
							If i = kl Then
								v = 1
							Else
								v = 0
							End If
							vv = batch4buf(offs2)
							batch4buf(offs1) = vv / s - v
							e = e + safecrossentropy(v, vv / s)
							offs1 = offs1 + entrysize
							offs2 = offs2 + chunksize
						Next
					Next
				Else

					'
					' SOFTMAX normalization makes things very difficult.
					' Sorry, we do not dare to describe this esoteric math
					' in details.
					'
					offs0 = entrysize * ntotal + chunksize * 2 * nout
					For k = 0 To csize - 1
						s = batch4buf(offs0 + k)
						kl = CInt(System.Math.Truncate(System.Math.Round(xy(cstart + k, nin))))
						vv = 0
						offs1 = entrysize * ntotal + k
						offs2 = entrysize * ntotal + nout * chunksize + k
						For i = 0 To nout - 1
							fown = batch4buf(offs1)
							If i = kl Then
								deown = fown / s - 1
							Else
								deown = fown / s
							End If
							batch4buf(offs2) = deown
							vv = vv + deown * fown
							e = e + deown * deown / 2
							offs1 = offs1 + chunksize
							offs2 = offs2 + chunksize
						Next
						offs1 = entrysize * ntotal + k
						offs2 = entrysize * ntotal + nout * chunksize + k
						For i = 0 To nout - 1
							fown = batch4buf(offs1)
							deown = batch4buf(offs2)
							batch4buf((ntotal - nout + i) * entrysize + derroroffs + k) = (-vv + deown * fown + deown * (s - fown)) * fown / Math.sqr(s)
							offs1 = offs1 + chunksize
							offs2 = offs2 + chunksize
						Next
					Next
				End If
			Else

				'
				' Regression network with sum-of-squares function.
				'
				' For each NOut of last neurons:
				' * calculate difference between actual and desired output
				' * calculate dError/dOut for this neuron (proportional to difference)
				' * store in in last 4 components of entry (these values are used
				'   to start backpropagation)
				' * update error
				'
				For i = 0 To nout - 1
					v0 = network.columnsigmas(nin + i)
					v1 = network.columnmeans(nin + i)
					entryoffs = entrysize * (ntotal - nout + i)
					offs0 = entryoffs
					offs1 = entryoffs + derroroffs
					For j = 0 To csize - 1
						v = batch4buf(offs0 + j) * v0 + v1 - xy(cstart + j, nin + i)
						batch4buf(offs1 + j) = v * v0
						e = e + v * v / 2
					Next
				Next
			End If

			'
			' Backpropagation
			'
			For neuronidx = ntotal - 1 To 0 Step -1
				entryoffs = entrysize * neuronidx
				offs = istart + neuronidx * nfieldwidth
				neurontype = network.structinfo(offs + 0)
				If neurontype > 0 OrElse neurontype = -5 Then

					'
					' Activation function
					'
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					offs0 = srcentryoffs + derroroffs
					offs1 = entryoffs + derroroffs
					offs2 = entryoffs + dfoffs
					batch4buf(offs0 + 0) = batch4buf(offs0 + 0) + batch4buf(offs1 + 0) * batch4buf(offs2 + 0)
					batch4buf(offs0 + 1) = batch4buf(offs0 + 1) + batch4buf(offs1 + 1) * batch4buf(offs2 + 1)
					batch4buf(offs0 + 2) = batch4buf(offs0 + 2) + batch4buf(offs1 + 2) * batch4buf(offs2 + 2)
					batch4buf(offs0 + 3) = batch4buf(offs0 + 3) + batch4buf(offs1 + 3) * batch4buf(offs2 + 3)
					Continue For
				End If
				If neurontype = 0 Then

					'
					' Adaptive summator
					'
					nweights = network.structinfo(offs + 1)
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					srcweightidx = network.structinfo(offs + 3)
					v0 = batch4buf(entryoffs + derroroffs + 0)
					v1 = batch4buf(entryoffs + derroroffs + 1)
					v2 = batch4buf(entryoffs + derroroffs + 2)
					v3 = batch4buf(entryoffs + derroroffs + 3)
					For j = 0 To nweights - 1
						offs0 = srcentryoffs
						offs1 = srcentryoffs + derroroffs
						v = network.weights(srcweightidx)
						hpcbuf(srcweightidx) = hpcbuf(srcweightidx) + batch4buf(offs0 + 0) * v0 + batch4buf(offs0 + 1) * v1 + batch4buf(offs0 + 2) * v2 + batch4buf(offs0 + 3) * v3
						batch4buf(offs1 + 0) = batch4buf(offs1 + 0) + v * v0
						batch4buf(offs1 + 1) = batch4buf(offs1 + 1) + v * v1
						batch4buf(offs1 + 2) = batch4buf(offs1 + 2) + v * v2
						batch4buf(offs1 + 3) = batch4buf(offs1 + 3) + v * v3
						srcentryoffs = srcentryoffs + entrysize
						srcweightidx = srcweightidx + 1
					Next
					Continue For
				End If
				If neurontype < 0 Then
					bflag = False
					If (neurontype = -2 OrElse neurontype = -3) OrElse neurontype = -4 Then

						'
						' Special neuron type, no back-propagation required
						'
						bflag = True
					End If
					alglib.ap.assert(bflag, "MLPInternalCalculateGradient: unknown neuron type!")
					Continue For
				End If
			Next
		End Sub


		Private Shared Sub mlpchunkedprocess(network As multilayerperceptron, xy As Double(,), cstart As Integer, csize As Integer, batch4buf As Double(), hpcbuf As Double())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ntotal As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim offs As Integer = 0
			Dim f As Double = 0
			Dim df As Double = 0
			Dim d2f As Double = 0
			Dim v As Double = 0
			Dim bflag As New Boolean()
			Dim istart As Integer = 0
			Dim entrysize As Integer = 0
			Dim entryoffs As Integer = 0
			Dim neuronidx As Integer = 0
			Dim srcentryoffs As Integer = 0
			Dim srcneuronidx As Integer = 0
			Dim srcweightidx As Integer = 0
			Dim neurontype As Integer = 0
			Dim nweights As Integer = 0
			Dim offs0 As Integer = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim v2 As Double = 0
			Dim v3 As Double = 0
			Dim s0 As Double = 0
			Dim s1 As Double = 0
			Dim s2 As Double = 0
			Dim s3 As Double = 0
			Dim chunksize As Integer = 0

			chunksize = 4
			alglib.ap.assert(csize <= chunksize, "MLPChunkedProcess: internal error (CSize>ChunkSize)")

			'
			' Try to use HPC core, if possible
			'
			If hpccores.hpcchunkedprocess(network.weights, network.structinfo, network.columnmeans, network.columnsigmas, xy, cstart, _
				csize, batch4buf, hpcbuf) Then
				Return
			End If

			'
			' Read network geometry, prepare data
			'
			nin = network.structinfo(1)
			nout = network.structinfo(2)
			ntotal = network.structinfo(3)
			istart = network.structinfo(5)
			entrysize = 4

			'
			' Fill Batch4Buf by zeros.
			'
			' THIS STAGE IS VERY IMPORTANT!
			'
			' We fill all components of entry - neuron values, dF/dNET, dError/dF.
			' It allows us to easily handle  situations  when  CSize<ChunkSize  by
			' simply  working  with  ALL  components  of  Batch4Buf,  without ever
			' looking at CSize.
			'
			For i = 0 To entrysize * ntotal - 1
				batch4buf(i) = 0
			Next

			'
			' Forward pass:
			' 1. Load data into Batch4Buf. If CSize<ChunkSize, data are padded by zeros.
			' 2. Perform forward pass through network
			'
			For i = 0 To nin - 1
				entryoffs = entrysize * i
				For j = 0 To csize - 1
					If CDbl(network.columnsigmas(i)) <> CDbl(0) Then
						batch4buf(entryoffs + j) = (xy(cstart + j, i) - network.columnmeans(i)) / network.columnsigmas(i)
					Else
						batch4buf(entryoffs + j) = xy(cstart + j, i) - network.columnmeans(i)
					End If
				Next
			Next
			For neuronidx = 0 To ntotal - 1
				entryoffs = entrysize * neuronidx
				offs = istart + neuronidx * nfieldwidth
				neurontype = network.structinfo(offs + 0)
				If neurontype > 0 OrElse neurontype = -5 Then

					'
					' "activation function" neuron, which takes value of neuron SrcNeuronIdx
					' and applies activation function to it.
					'
					' This neuron has no weights and no tunable parameters.
					'
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					mlpactivationfunction(batch4buf(srcentryoffs + 0), neurontype, f, df, d2f)
					batch4buf(entryoffs + 0) = f
					mlpactivationfunction(batch4buf(srcentryoffs + 1), neurontype, f, df, d2f)
					batch4buf(entryoffs + 1) = f
					mlpactivationfunction(batch4buf(srcentryoffs + 2), neurontype, f, df, d2f)
					batch4buf(entryoffs + 2) = f
					mlpactivationfunction(batch4buf(srcentryoffs + 3), neurontype, f, df, d2f)
					batch4buf(entryoffs + 3) = f
					Continue For
				End If
				If neurontype = 0 Then

					'
					' "adaptive summator" neuron, whose output is a weighted sum of inputs.
					' It has weights, but has no activation function.
					'
					nweights = network.structinfo(offs + 1)
					srcneuronidx = network.structinfo(offs + 2)
					srcentryoffs = entrysize * srcneuronidx
					srcweightidx = network.structinfo(offs + 3)
					v0 = 0
					v1 = 0
					v2 = 0
					v3 = 0
					For j = 0 To nweights - 1
						v = network.weights(srcweightidx)
						srcweightidx = srcweightidx + 1
						v0 = v0 + v * batch4buf(srcentryoffs + 0)
						v1 = v1 + v * batch4buf(srcentryoffs + 1)
						v2 = v2 + v * batch4buf(srcentryoffs + 2)
						v3 = v3 + v * batch4buf(srcentryoffs + 3)
						srcentryoffs = srcentryoffs + entrysize
					Next
					batch4buf(entryoffs + 0) = v0
					batch4buf(entryoffs + 1) = v1
					batch4buf(entryoffs + 2) = v2
					batch4buf(entryoffs + 3) = v3
					Continue For
				End If
				If neurontype < 0 Then
					bflag = False
					If neurontype = -2 Then

						'
						' Input neuron, left unchanged
						'
						bflag = True
					End If
					If neurontype = -3 Then

						'
						' "-1" neuron
						'
						batch4buf(entryoffs + 0) = -1
						batch4buf(entryoffs + 1) = -1
						batch4buf(entryoffs + 2) = -1
						batch4buf(entryoffs + 3) = -1
						bflag = True
					End If
					If neurontype = -4 Then

						'
						' "0" neuron
						'
						batch4buf(entryoffs + 0) = 0
						batch4buf(entryoffs + 1) = 0
						batch4buf(entryoffs + 2) = 0
						batch4buf(entryoffs + 3) = 0
						bflag = True
					End If
					alglib.ap.assert(bflag, "MLPChunkedProcess: internal error - unknown neuron type!")
					Continue For
				End If
			Next

			'
			' SOFTMAX normalization or scaling.
			'
			alglib.ap.assert(network.structinfo(6) = 0 OrElse network.structinfo(6) = 1, "MLPChunkedProcess: unknown normalization type!")
			If network.structinfo(6) = 1 Then

				'
				' SOFTMAX-normalized network.
				'
				' First,  calculate (V0,V1,V2,V3)  -  component-wise  maximum
				' of output neurons. This vector of maximum  values  will  be
				' used for normalization  of  outputs  prior  to  calculating
				' exponentials.
				'
				' NOTE: the only purpose of this stage is to prevent overflow
				'       during calculation of exponentials.  With  this stage
				'       we  make  sure  that  all exponentials are calculated
				'       with non-positive argument. If you load (0,0,0,0)  to
				'       (V0,V1,V2,V3), your program will continue  working  -
				'       although with less robustness.
				'
				entryoffs = entrysize * (ntotal - nout)
				v0 = batch4buf(entryoffs + 0)
				v1 = batch4buf(entryoffs + 1)
				v2 = batch4buf(entryoffs + 2)
				v3 = batch4buf(entryoffs + 3)
				entryoffs = entryoffs + entrysize
				For i = 1 To nout - 1
					v = batch4buf(entryoffs + 0)
					If v > v0 Then
						v0 = v
					End If
					v = batch4buf(entryoffs + 1)
					If v > v1 Then
						v1 = v
					End If
					v = batch4buf(entryoffs + 2)
					If v > v2 Then
						v2 = v
					End If
					v = batch4buf(entryoffs + 3)
					If v > v3 Then
						v3 = v
					End If
					entryoffs = entryoffs + entrysize
				Next

				'
				' Then,  calculate exponentials and place them to part of the
				' array which  is  located  past  the  last  entry.  We  also
				' calculate sum of exponentials.
				'
				entryoffs = entrysize * (ntotal - nout)
				offs0 = entrysize * ntotal
				s0 = 0
				s1 = 0
				s2 = 0
				s3 = 0
				For i = 0 To nout - 1
					v = System.Math.Exp(batch4buf(entryoffs + 0) - v0)
					s0 = s0 + v
					batch4buf(offs0 + 0) = v
					v = System.Math.Exp(batch4buf(entryoffs + 1) - v1)
					s1 = s1 + v
					batch4buf(offs0 + 1) = v
					v = System.Math.Exp(batch4buf(entryoffs + 2) - v2)
					s2 = s2 + v
					batch4buf(offs0 + 2) = v
					v = System.Math.Exp(batch4buf(entryoffs + 3) - v3)
					s3 = s3 + v
					batch4buf(offs0 + 3) = v
					entryoffs = entryoffs + entrysize
					offs0 = offs0 + chunksize
				Next

				'
				' Write SOFTMAX-normalized values to the output array.
				'
				offs0 = entrysize * ntotal
				For i = 0 To nout - 1
					If csize > 0 Then
						xy(cstart + 0, nin + i) = batch4buf(offs0 + 0) / s0
					End If
					If csize > 1 Then
						xy(cstart + 1, nin + i) = batch4buf(offs0 + 1) / s1
					End If
					If csize > 2 Then
						xy(cstart + 2, nin + i) = batch4buf(offs0 + 2) / s2
					End If
					If csize > 3 Then
						xy(cstart + 3, nin + i) = batch4buf(offs0 + 3) / s3
					End If
					offs0 = offs0 + chunksize
				Next
			Else

				'
				' Regression network with sum-of-squares function.
				'
				' For each NOut of last neurons:
				' * calculate difference between actual and desired output
				' * calculate dError/dOut for this neuron (proportional to difference)
				' * store in in last 4 components of entry (these values are used
				'   to start backpropagation)
				' * update error
				'
				For i = 0 To nout - 1
					v0 = network.columnsigmas(nin + i)
					v1 = network.columnmeans(nin + i)
					entryoffs = entrysize * (ntotal - nout + i)
					For j = 0 To csize - 1
						xy(cstart + j, nin + i) = batch4buf(entryoffs + j) * v0 + v1
					Next
				Next
			End If
		End Sub


		'************************************************************************
'        Returns T*Ln(T/Z), guarded against overflow/underflow.
'        Internal subroutine.
'        ************************************************************************

		Private Shared Function safecrossentropy(t As Double, z As Double) As Double
			Dim result As Double = 0
			Dim r As Double = 0

			If CDbl(t) = CDbl(0) Then
				result = 0
			Else
				If CDbl(System.Math.Abs(z)) > CDbl(1) Then

					'
					' Shouldn't be the case with softmax,
					' but we just want to be sure.
					'
					If CDbl(t / z) = CDbl(0) Then
						r = Math.minrealnumber
					Else
						r = t / z
					End If
				Else

					'
					' Normal case
					'
					If CDbl(z) = CDbl(0) OrElse CDbl(System.Math.Abs(t)) >= CDbl(Math.maxrealnumber * System.Math.Abs(z)) Then
						r = Math.maxrealnumber
					Else
						r = t / z
					End If
				End If
				result = t * System.Math.Log(r)
			End If
			Return result
		End Function


		'************************************************************************
'        This function performs backward pass of neural network randimization:
'        * it assumes that Network.Weights stores standard deviation of weights
'          (weights are not generated yet, only their deviations are present)
'        * it sets deviations of weights which feed NeuronIdx-th neuron to specified value
'        * it recursively passes to deeper neuron and modifies their weights
'        * it stops after encountering nonlinear neurons, linear activation function,
'          input neurons, "0" and "-1" neurons
'
'          -- ALGLIB --
'             Copyright 27.06.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub randomizebackwardpass(network As multilayerperceptron, neuronidx As Integer, v As Double)
			Dim istart As Integer = 0
			Dim neurontype As Integer = 0
			Dim n1 As Integer = 0
			Dim n2 As Integer = 0
			Dim w1 As Integer = 0
			Dim w2 As Integer = 0
			Dim offs As Integer = 0
			Dim i As Integer = 0

			istart = network.structinfo(5)
			neurontype = network.structinfo(istart + neuronidx * nfieldwidth + 0)
			If neurontype = -2 Then

				'
				' Input neuron - stop
				'
				Return
			End If
			If neurontype = -3 Then

				'
				' "-1" neuron: stop
				'
				Return
			End If
			If neurontype = -4 Then

				'
				' "0" neuron: stop
				'
				Return
			End If
			If neurontype = 0 Then

				'
				' Adaptive summator neuron:
				' * modify deviations of its weights
				' * recursively call this function for its inputs
				'
				offs = istart + neuronidx * nfieldwidth
				n1 = network.structinfo(offs + 2)
				n2 = n1 + network.structinfo(offs + 1) - 1
				w1 = network.structinfo(offs + 3)
				w2 = w1 + network.structinfo(offs + 1) - 1
				For i = w1 To w2
					network.weights(i) = v
				Next
				For i = n1 To n2
					randomizebackwardpass(network, i, v)
				Next
				Return
			End If
			If neurontype = -5 Then

				'
				' Linear activation function: stop
				'
				Return
			End If
			If neurontype > 0 Then

				'
				' Nonlinear activation function: stop
				'
				Return
			End If
			alglib.ap.assert(False, "RandomizeBackwardPass: unexpected neuron type")
		End Sub


	End Class
	Public Class logit
		Public Class logitmodel
			Inherits apobject
			Public w As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				w = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New logitmodel()
				_result.w = DirectCast(w.Clone(), Double())
				Return _result
			End Function
		End Class


		Public Class logitmcstate
			Inherits apobject
			Public brackt As Boolean
			Public stage1 As Boolean
			Public infoc As Integer
			Public dg As Double
			Public dgm As Double
			Public dginit As Double
			Public dgtest As Double
			Public dgx As Double
			Public dgxm As Double
			Public dgy As Double
			Public dgym As Double
			Public finit As Double
			Public ftest1 As Double
			Public fm As Double
			Public fx As Double
			Public fxm As Double
			Public fy As Double
			Public fym As Double
			Public stx As Double
			Public sty As Double
			Public stmin As Double
			Public stmax As Double
			Public width As Double
			Public width1 As Double
			Public xtrapf As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New logitmcstate()
				_result.brackt = brackt
				_result.stage1 = stage1
				_result.infoc = infoc
				_result.dg = dg
				_result.dgm = dgm
				_result.dginit = dginit
				_result.dgtest = dgtest
				_result.dgx = dgx
				_result.dgxm = dgxm
				_result.dgy = dgy
				_result.dgym = dgym
				_result.finit = finit
				_result.ftest1 = ftest1
				_result.fm = fm
				_result.fx = fx
				_result.fxm = fxm
				_result.fy = fy
				_result.fym = fym
				_result.stx = stx
				_result.sty = sty
				_result.stmin = stmin
				_result.stmax = stmax
				_result.width = width
				_result.width1 = width1
				_result.xtrapf = xtrapf
				Return _result
			End Function
		End Class


		'************************************************************************
'        MNLReport structure contains information about training process:
'        * NGrad     -   number of gradient calculations
'        * NHess     -   number of Hessian calculations
'        ************************************************************************

		Public Class mnlreport
			Inherits apobject
			Public ngrad As Integer
			Public nhess As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mnlreport()
				_result.ngrad = ngrad
				_result.nhess = nhess
				Return _result
			End Function
		End Class




		Public Const xtol As Double = 100 * Math.machineepsilon
		Public Const ftol As Double = 0.0001
		Public Const gtol As Double = 0.3
		Public Const maxfev As Integer = 20
		Public Const stpmin As Double = 0.01
		Public Const stpmax As Double = 100000.0
		Public Const logitvnum As Integer = 6


		'************************************************************************
'        This subroutine trains logit model.
'
'        INPUT PARAMETERS:
'            XY          -   training set, array[0..NPoints-1,0..NVars]
'                            First NVars columns store values of independent
'                            variables, next column stores number of class (from 0
'                            to NClasses-1) which dataset element belongs to. Fractional
'                            values are rounded to nearest integer.
'            NPoints     -   training set size, NPoints>=1
'            NVars       -   number of independent variables, NVars>=1
'            NClasses    -   number of classes, NClasses>=2
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<NVars+2, NVars<1, NClasses<2).
'                            *  1, if task has been solved
'            LM          -   model built
'            Rep         -   training report
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnltrainh(xy As Double(,), npoints As Integer, nvars As Integer, nclasses As Integer, ByRef info As Integer, lm As logitmodel, _
			rep As mnlreport)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim ssize As Integer = 0
			Dim allsame As New Boolean()
			Dim offs As Integer = 0
			Dim decay As Double = 0
			Dim v As Double = 0
			Dim s As Double = 0
			Dim network As New mlpbase.multilayerperceptron()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim e As Double = 0
			Dim g As Double() = New Double(-1) {}
			Dim h As Double(,) = New Double(-1, -1) {}
			Dim spd As New Boolean()
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim wbase As Double() = New Double(-1) {}
			Dim wstep As Double = 0
			Dim wdir As Double() = New Double(-1) {}
			Dim work As Double() = New Double(-1) {}
			Dim mcstage As Integer = 0
			Dim mcstate As New logitmcstate()
			Dim mcinfo As Integer = 0
			Dim mcnfev As Integer = 0
			Dim solverinfo As Integer = 0
			Dim solverrep As New densesolver.densesolverreport()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			info = 0

			decay = 0.001

			'
			' Test for inputs
			'
			If (npoints < nvars + 2 OrElse nvars < 1) OrElse nclasses < 2 Then
				info = -1
				Return
			End If
			For i = 0 To npoints - 1
				If CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) >= nclasses Then
					info = -2
					Return
				End If
			Next
			info = 1

			'
			' Initialize data
			'
			rep.ngrad = 0
			rep.nhess = 0

			'
			' Allocate array
			'
			offs = 5
			ssize = 5 + (nvars + 1) * (nclasses - 1) + nclasses
			lm.w = New Double(ssize - 1) {}
			lm.w(0) = ssize
			lm.w(1) = logitvnum
			lm.w(2) = nvars
			lm.w(3) = nclasses
			lm.w(4) = offs

			'
			' Degenerate case: all outputs are equal
			'
			allsame = True
			For i = 1 To npoints - 1
				If CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) <> CInt(System.Math.Truncate(System.Math.Round(xy(i - 1, nvars)))) Then
					allsame = False
				End If
			Next
			If allsame Then
				For i = 0 To (nvars + 1) * (nclasses - 1) - 1
					lm.w(offs + i) = 0
				Next
				v = -(2 * System.Math.Log(Math.minrealnumber))
				k = CInt(System.Math.Truncate(System.Math.Round(xy(0, nvars))))
				If k = nclasses - 1 Then
					For i = 0 To nclasses - 2
						lm.w(offs + i * (nvars + 1) + nvars) = -v
					Next
				Else
					For i = 0 To nclasses - 2
						If i = k Then
							lm.w(offs + i * (nvars + 1) + nvars) = v
						Else
							lm.w(offs + i * (nvars + 1) + nvars) = 0
						End If
					Next
				End If
				Return
			End If

			'
			' General case.
			' Prepare task and network. Allocate space.
			'
			mlpbase.mlpcreatec0(nvars, nclasses, network)
			mlpbase.mlpinitpreprocessor(network, xy, npoints)
			mlpbase.mlpproperties(network, nin, nout, wcount)
			For i = 0 To wcount - 1
				network.weights(i) = (2 * Math.randomreal() - 1) / nvars
			Next
			g = New Double(wcount - 1) {}
			h = New Double(wcount - 1, wcount - 1) {}
			wbase = New Double(wcount - 1) {}
			wdir = New Double(wcount - 1) {}
			work = New Double(wcount - 1) {}

			'
			' First stage: optimize in gradient direction.
			'
			For k = 0 To wcount \ 3 + 10

				'
				' Calculate gradient in starting point
				'
				mlpbase.mlpgradnbatch(network, xy, npoints, e, g)
				v = 0.0
				For i_ = 0 To wcount - 1
					v += network.weights(i_) * network.weights(i_)
				Next
				e = e + 0.5 * decay * v
				For i_ = 0 To wcount - 1
					g(i_) = g(i_) + decay * network.weights(i_)
				Next
				rep.ngrad = rep.ngrad + 1

				'
				' Setup optimization scheme
				'
				For i_ = 0 To wcount - 1
					wdir(i_) = -g(i_)
				Next
				v = 0.0
				For i_ = 0 To wcount - 1
					v += wdir(i_) * wdir(i_)
				Next
				wstep = System.Math.sqrt(v)
				v = 1 / System.Math.sqrt(v)
				For i_ = 0 To wcount - 1
					wdir(i_) = v * wdir(i_)
				Next
				mcstage = 0
				mnlmcsrch(wcount, network.weights, e, g, wdir, wstep, _
					mcinfo, mcnfev, work, mcstate, mcstage)
				While mcstage <> 0
					mlpbase.mlpgradnbatch(network, xy, npoints, e, g)
					v = 0.0
					For i_ = 0 To wcount - 1
						v += network.weights(i_) * network.weights(i_)
					Next
					e = e + 0.5 * decay * v
					For i_ = 0 To wcount - 1
						g(i_) = g(i_) + decay * network.weights(i_)
					Next
					rep.ngrad = rep.ngrad + 1
					mnlmcsrch(wcount, network.weights, e, g, wdir, wstep, _
						mcinfo, mcnfev, work, mcstate, mcstage)
				End While
			Next

			'
			' Second stage: use Hessian when we are close to the minimum
			'
			While True

				'
				' Calculate and update E/G/H
				'
				mlpbase.mlphessiannbatch(network, xy, npoints, e, g, h)
				v = 0.0
				For i_ = 0 To wcount - 1
					v += network.weights(i_) * network.weights(i_)
				Next
				e = e + 0.5 * decay * v
				For i_ = 0 To wcount - 1
					g(i_) = g(i_) + decay * network.weights(i_)
				Next
				For k = 0 To wcount - 1
					h(k, k) = h(k, k) + decay
				Next
				rep.nhess = rep.nhess + 1

				'
				' Select step direction
				' NOTE: it is important to use lower-triangle Cholesky
				' factorization since it is much faster than higher-triangle version.
				'
				spd = trfac.spdmatrixcholesky(h, wcount, False)
				densesolver.spdmatrixcholeskysolve(h, wcount, False, g, solverinfo, solverrep, _
					wdir)
				spd = solverinfo > 0
				If spd Then

					'
					' H is positive definite.
					' Step in Newton direction.
					'
					For i_ = 0 To wcount - 1
						wdir(i_) = -1 * wdir(i_)
					Next
					spd = True
				Else

					'
					' H is indefinite.
					' Step in gradient direction.
					'
					For i_ = 0 To wcount - 1
						wdir(i_) = -g(i_)
					Next
					spd = False
				End If

				'
				' Optimize in WDir direction
				'
				v = 0.0
				For i_ = 0 To wcount - 1
					v += wdir(i_) * wdir(i_)
				Next
				wstep = System.Math.sqrt(v)
				v = 1 / System.Math.sqrt(v)
				For i_ = 0 To wcount - 1
					wdir(i_) = v * wdir(i_)
				Next
				mcstage = 0
				mnlmcsrch(wcount, network.weights, e, g, wdir, wstep, _
					mcinfo, mcnfev, work, mcstate, mcstage)
				While mcstage <> 0
					mlpbase.mlpgradnbatch(network, xy, npoints, e, g)
					v = 0.0
					For i_ = 0 To wcount - 1
						v += network.weights(i_) * network.weights(i_)
					Next
					e = e + 0.5 * decay * v
					For i_ = 0 To wcount - 1
						g(i_) = g(i_) + decay * network.weights(i_)
					Next
					rep.ngrad = rep.ngrad + 1
					mnlmcsrch(wcount, network.weights, e, g, wdir, wstep, _
						mcinfo, mcnfev, work, mcstate, mcstage)
				End While
				If spd AndAlso ((mcinfo = 2 OrElse mcinfo = 4) OrElse mcinfo = 6) Then
					Exit While
				End If
			End While

			'
			' Convert from NN format to MNL format
			'
			i1_ = (0) - (offs)
			For i_ = offs To offs + wcount - 1
				lm.w(i_) = network.weights(i_ + i1_)
			Next
			For k = 0 To nvars - 1
				For i = 0 To nclasses - 2
					s = network.columnsigmas(k)
					If CDbl(s) = CDbl(0) Then
						s = 1
					End If
					j = offs + (nvars + 1) * i
					v = lm.w(j + k)
					lm.w(j + k) = v / s
					lm.w(j + nvars) = lm.w(j + nvars) + v * network.columnmeans(k) / s
				Next
			Next
			For k = 0 To nclasses - 2
				lm.w(offs + (nvars + 1) * k + nvars) = -lm.w(offs + (nvars + 1) * k + nvars)
			Next
		End Sub


		'************************************************************************
'        Procesing
'
'        INPUT PARAMETERS:
'            LM      -   logit model, passed by non-constant reference
'                        (some fields of structure are used as temporaries
'                        when calculating model output).
'            X       -   input vector,  array[0..NVars-1].
'            Y       -   (possibly) preallocated buffer; if size of Y is less than
'                        NClasses, it will be reallocated.If it is large enough, it
'                        is NOT reallocated, so we can save some time on reallocation.
'
'        OUTPUT PARAMETERS:
'            Y       -   result, array[0..NClasses-1]
'                        Vector of posterior probabilities for classification task.
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnlprocess(lm As logitmodel, x As Double(), ByRef y As Double())
			Dim nvars As Integer = 0
			Dim nclasses As Integer = 0
			Dim offs As Integer = 0
			Dim i As Integer = 0
			Dim i1 As Integer = 0
			Dim s As Double = 0

			alglib.ap.assert(CDbl(lm.w(1)) = CDbl(logitvnum), "MNLProcess: unexpected model version")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(4))))
			mnliexp(lm.w, x)
			s = 0
			i1 = offs + (nvars + 1) * (nclasses - 1)
			For i = i1 To i1 + nclasses - 1
				s = s + lm.w(i)
			Next
			If alglib.ap.len(y) < nclasses Then
				y = New Double(nclasses - 1) {}
			End If
			For i = 0 To nclasses - 1
				y(i) = lm.w(i1 + i) / s
			Next
		End Sub


		'************************************************************************
'        'interactive'  variant  of  MNLProcess  for  languages  like  Python which
'        support constructs like "Y = MNLProcess(LM,X)" and interactive mode of the
'        interpreter
'
'        This function allocates new array on each call,  so  it  is  significantly
'        slower than its 'non-interactive' counterpart, but it is  more  convenient
'        when you call it from command line.
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnlprocessi(lm As logitmodel, x As Double(), ByRef y As Double())
			y = New Double(-1) {}

			mnlprocess(lm, x, y)
		End Sub


		'************************************************************************
'        Unpacks coefficients of logit model. Logit model have form:
'
'            P(class=i) = S(i) / (S(0) + S(1) + ... +S(M-1))
'                  S(i) = Exp(A[i,0]*X[0] + ... + A[i,N-1]*X[N-1] + A[i,N]), when i<M-1
'                S(M-1) = 1
'
'        INPUT PARAMETERS:
'            LM          -   logit model in ALGLIB format
'
'        OUTPUT PARAMETERS:
'            V           -   coefficients, array[0..NClasses-2,0..NVars]
'            NVars       -   number of independent variables
'            NClasses    -   number of classes
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnlunpack(lm As logitmodel, ByRef a As Double(,), ByRef nvars As Integer, ByRef nclasses As Integer)
			Dim offs As Integer = 0
			Dim i As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			a = New Double(-1, -1) {}
			nvars = 0
			nclasses = 0

			alglib.ap.assert(CDbl(lm.w(1)) = CDbl(logitvnum), "MNLUnpack: unexpected model version")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			offs = CInt(System.Math.Truncate(System.Math.Round(lm.w(4))))
			a = New Double(nclasses - 2, nvars) {}
			For i = 0 To nclasses - 2
				i1_ = (offs + i * (nvars + 1)) - (0)
				For i_ = 0 To nvars
					a(i, i_) = lm.w(i_ + i1_)
				Next
			Next
		End Sub


		'************************************************************************
'        "Packs" coefficients and creates logit model in ALGLIB format (MNLUnpack
'        reversed).
'
'        INPUT PARAMETERS:
'            A           -   model (see MNLUnpack)
'            NVars       -   number of independent variables
'            NClasses    -   number of classes
'
'        OUTPUT PARAMETERS:
'            LM          -   logit model.
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnlpack(a As Double(,), nvars As Integer, nclasses As Integer, lm As logitmodel)
			Dim offs As Integer = 0
			Dim i As Integer = 0
			Dim ssize As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			offs = 5
			ssize = 5 + (nvars + 1) * (nclasses - 1) + nclasses
			lm.w = New Double(ssize - 1) {}
			lm.w(0) = ssize
			lm.w(1) = logitvnum
			lm.w(2) = nvars
			lm.w(3) = nclasses
			lm.w(4) = offs
			For i = 0 To nclasses - 2
				i1_ = (0) - (offs + i * (nvars + 1))
				For i_ = offs + i * (nvars + 1) To offs + i * (nvars + 1) + nvars
					lm.w(i_) = a(i, i_ + i1_)
				Next
			Next
		End Sub


		'************************************************************************
'        Copying of LogitModel strucure
'
'        INPUT PARAMETERS:
'            LM1 -   original
'
'        OUTPUT PARAMETERS:
'            LM2 -   copy
'
'          -- ALGLIB --
'             Copyright 15.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mnlcopy(lm1 As logitmodel, lm2 As logitmodel)
			Dim k As Integer = 0
			Dim i_ As Integer = 0

			k = CInt(System.Math.Truncate(System.Math.Round(lm1.w(0))))
			lm2.w = New Double(k - 1) {}
			For i_ = 0 To k - 1
				lm2.w(i_) = lm1.w(i_)
			Next
		End Sub


		'************************************************************************
'        Average cross-entropy (in bits per element) on the test set
'
'        INPUT PARAMETERS:
'            LM      -   logit model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            CrossEntropy/(NPoints*ln(2)).
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlavgce(lm As logitmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim nvars As Integer = 0
			Dim nclasses As Integer = 0
			Dim i As Integer = 0
			Dim workx As Double() = New Double(-1) {}
			Dim worky As Double() = New Double(-1) {}
			Dim i_ As Integer = 0

			alglib.ap.assert(CDbl(lm.w(1)) = CDbl(logitvnum), "MNLClsError: unexpected model version")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			workx = New Double(nvars - 1) {}
			worky = New Double(nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1
				alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) >= 0 AndAlso CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) < nclasses, "MNLAvgCE: incorrect class number!")

				'
				' Process
				'
				For i_ = 0 To nvars - 1
					workx(i_) = xy(i, i_)
				Next
				mnlprocess(lm, workx, worky)
				If CDbl(worky(CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))))) > CDbl(0) Then
					result = result - System.Math.Log(worky(CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars))))))
				Else
					result = result - System.Math.Log(Math.minrealnumber)
				End If
			Next
			result = result / (npoints * System.Math.Log(2))
			Return result
		End Function


		'************************************************************************
'        Relative classification error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   logit model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            percent of incorrectly classified cases.
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlrelclserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0

			result = CDbl(mnlclserror(lm, xy, npoints)) / CDbl(npoints)
			Return result
		End Function


		'************************************************************************
'        RMS error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   logit model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            root mean square error (error when estimating posterior probabilities).
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlrmserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim relcls As Double = 0
			Dim avgce As Double = 0
			Dim rms As Double = 0
			Dim avg As Double = 0
			Dim avgrel As Double = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = logitvnum, "MNLRMSError: Incorrect MNL version!")
			mnlallerrors(lm, xy, npoints, relcls, avgce, rms, _
				avg, avgrel)
			result = rms
			Return result
		End Function


		'************************************************************************
'        Average error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   logit model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            average error (error when estimating posterior probabilities).
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlavgerror(lm As logitmodel, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim relcls As Double = 0
			Dim avgce As Double = 0
			Dim rms As Double = 0
			Dim avg As Double = 0
			Dim avgrel As Double = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = logitvnum, "MNLRMSError: Incorrect MNL version!")
			mnlallerrors(lm, xy, npoints, relcls, avgce, rms, _
				avg, avgrel)
			result = avg
			Return result
		End Function


		'************************************************************************
'        Average relative error on the test set
'
'        INPUT PARAMETERS:
'            LM      -   logit model
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            average relative error (error when estimating posterior probabilities).
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlavgrelerror(lm As logitmodel, xy As Double(,), ssize As Integer) As Double
			Dim result As Double = 0
			Dim relcls As Double = 0
			Dim avgce As Double = 0
			Dim rms As Double = 0
			Dim avg As Double = 0
			Dim avgrel As Double = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = logitvnum, "MNLRMSError: Incorrect MNL version!")
			mnlallerrors(lm, xy, ssize, relcls, avgce, rms, _
				avg, avgrel)
			result = avgrel
			Return result
		End Function


		'************************************************************************
'        Classification error on test set = MNLRelClsError*NPoints
'
'          -- ALGLIB --
'             Copyright 10.09.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mnlclserror(lm As logitmodel, xy As Double(,), npoints As Integer) As Integer
			Dim result As Integer = 0
			Dim nvars As Integer = 0
			Dim nclasses As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim workx As Double() = New Double(-1) {}
			Dim worky As Double() = New Double(-1) {}
			Dim nmax As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(CDbl(lm.w(1)) = CDbl(logitvnum), "MNLClsError: unexpected model version")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			workx = New Double(nvars - 1) {}
			worky = New Double(nclasses - 1) {}
			result = 0
			For i = 0 To npoints - 1

				'
				' Process
				'
				For i_ = 0 To nvars - 1
					workx(i_) = xy(i, i_)
				Next
				mnlprocess(lm, workx, worky)

				'
				' Logit version of the answer
				'
				nmax = 0
				For j = 0 To nclasses - 1
					If CDbl(worky(j)) > CDbl(worky(nmax)) Then
						nmax = j
					End If
				Next

				'
				' compare
				'
				If nmax <> CInt(System.Math.Truncate(System.Math.Round(xy(i, nvars)))) Then
					result = result + 1
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        Internal subroutine. Places exponents of the anti-overflow shifted
'        internal linear outputs into the service part of the W array.
'        ************************************************************************

		Private Shared Sub mnliexp(ByRef w As Double(), x As Double())
			Dim nvars As Integer = 0
			Dim nclasses As Integer = 0
			Dim offs As Integer = 0
			Dim i As Integer = 0
			Dim i1 As Integer = 0
			Dim v As Double = 0
			Dim mx As Double = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(CDbl(w(1)) = CDbl(logitvnum), "LOGIT: unexpected model version")
			nvars = CInt(System.Math.Truncate(System.Math.Round(w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(w(3))))
			offs = CInt(System.Math.Truncate(System.Math.Round(w(4))))
			i1 = offs + (nvars + 1) * (nclasses - 1)
			For i = 0 To nclasses - 2
				i1_ = (0) - (offs + i * (nvars + 1))
				v = 0.0
				For i_ = offs + i * (nvars + 1) To offs + i * (nvars + 1) + nvars - 1
					v += w(i_) * x(i_ + i1_)
				Next
				w(i1 + i) = v + w(offs + i * (nvars + 1) + nvars)
			Next
			w(i1 + nclasses - 1) = 0
			mx = 0
			For i = i1 To i1 + nclasses - 1
				mx = System.Math.Max(mx, w(i))
			Next
			For i = i1 To i1 + nclasses - 1
				w(i) = System.Math.Exp(w(i) - mx)
			Next
		End Sub


		'************************************************************************
'        Calculation of all types of errors
'
'          -- ALGLIB --
'             Copyright 30.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mnlallerrors(lm As logitmodel, xy As Double(,), npoints As Integer, ByRef relcls As Double, ByRef avgce As Double, ByRef rms As Double, _
			ByRef avg As Double, ByRef avgrel As Double)
			Dim nvars As Integer = 0
			Dim nclasses As Integer = 0
			Dim i As Integer = 0
			Dim buf As Double() = New Double(-1) {}
			Dim workx As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim dy As Double() = New Double(-1) {}
			Dim i_ As Integer = 0

			relcls = 0
			avgce = 0
			rms = 0
			avg = 0
			avgrel = 0

			alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(lm.w(1)))) = logitvnum, "MNL unit: Incorrect MNL version!")
			nvars = CInt(System.Math.Truncate(System.Math.Round(lm.w(2))))
			nclasses = CInt(System.Math.Truncate(System.Math.Round(lm.w(3))))
			workx = New Double(nvars - 1) {}
			y = New Double(nclasses - 1) {}
			dy = New Double(0) {}
			bdss.dserrallocate(nclasses, buf)
			For i = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					workx(i_) = xy(i, i_)
				Next
				mnlprocess(lm, workx, y)
				dy(0) = xy(i, nvars)
				bdss.dserraccumulate(buf, y, dy)
			Next
			bdss.dserrfinish(buf)
			relcls = buf(0)
			avgce = buf(1)
			rms = buf(2)
			avg = buf(3)
			avgrel = buf(4)
		End Sub


		'************************************************************************
'        THE  PURPOSE  OF  MCSRCH  IS  TO  FIND A STEP WHICH SATISFIES A SUFFICIENT
'        DECREASE CONDITION AND A CURVATURE CONDITION.
'
'        AT EACH STAGE THE SUBROUTINE  UPDATES  AN  INTERVAL  OF  UNCERTAINTY  WITH
'        ENDPOINTS  STX  AND  STY.  THE INTERVAL OF UNCERTAINTY IS INITIALLY CHOSEN
'        SO THAT IT CONTAINS A MINIMIZER OF THE MODIFIED FUNCTION
'
'            F(X+STP*S) - F(X) - FTOL*STP*(GRADF(X)'S).
'
'        IF  A STEP  IS OBTAINED FOR  WHICH THE MODIFIED FUNCTION HAS A NONPOSITIVE
'        FUNCTION  VALUE  AND  NONNEGATIVE  DERIVATIVE,   THEN   THE   INTERVAL  OF
'        UNCERTAINTY IS CHOSEN SO THAT IT CONTAINS A MINIMIZER OF F(X+STP*S).
'
'        THE  ALGORITHM  IS  DESIGNED TO FIND A STEP WHICH SATISFIES THE SUFFICIENT
'        DECREASE CONDITION
'
'            F(X+STP*S) .LE. F(X) + FTOL*STP*(GRADF(X)'S),
'
'        AND THE CURVATURE CONDITION
'
'            ABS(GRADF(X+STP*S)'S)) .LE. GTOL*ABS(GRADF(X)'S).
'
'        IF  FTOL  IS  LESS  THAN GTOL AND IF, FOR EXAMPLE, THE FUNCTION IS BOUNDED
'        BELOW,  THEN  THERE  IS  ALWAYS  A  STEP  WHICH SATISFIES BOTH CONDITIONS.
'        IF  NO  STEP  CAN BE FOUND  WHICH  SATISFIES  BOTH  CONDITIONS,  THEN  THE
'        ALGORITHM  USUALLY STOPS  WHEN  ROUNDING ERRORS  PREVENT FURTHER PROGRESS.
'        IN THIS CASE STP ONLY SATISFIES THE SUFFICIENT DECREASE CONDITION.
'
'        PARAMETERS DESCRIPRION
'
'        N IS A POSITIVE INTEGER INPUT VARIABLE SET TO THE NUMBER OF VARIABLES.
'
'        X IS  AN  ARRAY  OF  LENGTH N. ON INPUT IT MUST CONTAIN THE BASE POINT FOR
'        THE LINE SEARCH. ON OUTPUT IT CONTAINS X+STP*S.
'
'        F IS  A  VARIABLE. ON INPUT IT MUST CONTAIN THE VALUE OF F AT X. ON OUTPUT
'        IT CONTAINS THE VALUE OF F AT X + STP*S.
'
'        G IS AN ARRAY OF LENGTH N. ON INPUT IT MUST CONTAIN THE GRADIENT OF F AT X.
'        ON OUTPUT IT CONTAINS THE GRADIENT OF F AT X + STP*S.
'
'        S IS AN INPUT ARRAY OF LENGTH N WHICH SPECIFIES THE SEARCH DIRECTION.
'
'        STP  IS  A NONNEGATIVE VARIABLE. ON INPUT STP CONTAINS AN INITIAL ESTIMATE
'        OF A SATISFACTORY STEP. ON OUTPUT STP CONTAINS THE FINAL ESTIMATE.
'
'        FTOL AND GTOL ARE NONNEGATIVE INPUT VARIABLES. TERMINATION OCCURS WHEN THE
'        SUFFICIENT DECREASE CONDITION AND THE DIRECTIONAL DERIVATIVE CONDITION ARE
'        SATISFIED.
'
'        XTOL IS A NONNEGATIVE INPUT VARIABLE. TERMINATION OCCURS WHEN THE RELATIVE
'        WIDTH OF THE INTERVAL OF UNCERTAINTY IS AT MOST XTOL.
'
'        STPMIN AND STPMAX ARE NONNEGATIVE INPUT VARIABLES WHICH SPECIFY LOWER  AND
'        UPPER BOUNDS FOR THE STEP.
'
'        MAXFEV IS A POSITIVE INTEGER INPUT VARIABLE. TERMINATION OCCURS WHEN THE
'        NUMBER OF CALLS TO FCN IS AT LEAST MAXFEV BY THE END OF AN ITERATION.
'
'        INFO IS AN INTEGER OUTPUT VARIABLE SET AS FOLLOWS:
'            INFO = 0  IMPROPER INPUT PARAMETERS.
'
'            INFO = 1  THE SUFFICIENT DECREASE CONDITION AND THE
'                      DIRECTIONAL DERIVATIVE CONDITION HOLD.
'
'            INFO = 2  RELATIVE WIDTH OF THE INTERVAL OF UNCERTAINTY
'                      IS AT MOST XTOL.
'
'            INFO = 3  NUMBER OF CALLS TO FCN HAS REACHED MAXFEV.
'
'            INFO = 4  THE STEP IS AT THE LOWER BOUND STPMIN.
'
'            INFO = 5  THE STEP IS AT THE UPPER BOUND STPMAX.
'
'            INFO = 6  ROUNDING ERRORS PREVENT FURTHER PROGRESS.
'                      THERE MAY NOT BE A STEP WHICH SATISFIES THE
'                      SUFFICIENT DECREASE AND CURVATURE CONDITIONS.
'                      TOLERANCES MAY BE TOO SMALL.
'
'        NFEV IS AN INTEGER OUTPUT VARIABLE SET TO THE NUMBER OF CALLS TO FCN.
'
'        WA IS A WORK ARRAY OF LENGTH N.
'
'        ARGONNE NATIONAL LABORATORY. MINPACK PROJECT. JUNE 1983
'        JORGE J. MORE', DAVID J. THUENTE
'        ************************************************************************

		Private Shared Sub mnlmcsrch(n As Integer, ByRef x As Double(), ByRef f As Double, ByRef g As Double(), s As Double(), ByRef stp As Double, _
			ByRef info As Integer, ByRef nfev As Integer, ByRef wa As Double(), state As logitmcstate, ByRef stage As Integer)
			Dim v As Double = 0
			Dim p5 As Double = 0
			Dim p66 As Double = 0
			Dim zero As Double = 0
			Dim i_ As Integer = 0


			'
			' init
			'
			p5 = 0.5
			p66 = 0.66
			state.xtrapf = 4.0
			zero = 0

			'
			' Main cycle
			'
			While True
				If stage = 0 Then

					'
					' NEXT
					'
					stage = 2
					Continue While
				End If
				If stage = 2 Then
					state.infoc = 1
					info = 0

					'
					'     CHECK THE INPUT PARAMETERS FOR ERRORS.
					'
					If ((((((n <= 0 OrElse CDbl(stp) <= CDbl(0)) OrElse CDbl(ftol) < CDbl(0)) OrElse CDbl(gtol) < CDbl(zero)) OrElse CDbl(xtol) < CDbl(zero)) OrElse CDbl(stpmin) < CDbl(zero)) OrElse CDbl(stpmax) < CDbl(stpmin)) OrElse maxfev <= 0 Then
						stage = 0
						Return
					End If

					'
					'     COMPUTE THE INITIAL GRADIENT IN THE SEARCH DIRECTION
					'     AND CHECK THAT S IS A DESCENT DIRECTION.
					'
					v = 0.0
					For i_ = 0 To n - 1
						v += g(i_) * s(i_)
					Next
					state.dginit = v
					If CDbl(state.dginit) >= CDbl(0) Then
						stage = 0
						Return
					End If

					'
					'     INITIALIZE LOCAL VARIABLES.
					'
					state.brackt = False
					state.stage1 = True
					nfev = 0
					state.finit = f
					state.dgtest = ftol * state.dginit
					state.width = stpmax - stpmin
					state.width1 = state.width / p5
					For i_ = 0 To n - 1
						wa(i_) = x(i_)
					Next

					'
					'     THE VARIABLES STX, FX, DGX CONTAIN THE VALUES OF THE STEP,
					'     FUNCTION, AND DIRECTIONAL DERIVATIVE AT THE BEST STEP.
					'     THE VARIABLES STY, FY, DGY CONTAIN THE VALUE OF THE STEP,
					'     FUNCTION, AND DERIVATIVE AT THE OTHER ENDPOINT OF
					'     THE INTERVAL OF UNCERTAINTY.
					'     THE VARIABLES STP, F, DG CONTAIN THE VALUES OF THE STEP,
					'     FUNCTION, AND DERIVATIVE AT THE CURRENT STEP.
					'
					state.stx = 0
					state.fx = state.finit
					state.dgx = state.dginit
					state.sty = 0
					state.fy = state.finit
					state.dgy = state.dginit

					'
					' NEXT
					'
					stage = 3
					Continue While
				End If
				If stage = 3 Then

					'
					'     START OF ITERATION.
					'
					'     SET THE MINIMUM AND MAXIMUM STEPS TO CORRESPOND
					'     TO THE PRESENT INTERVAL OF UNCERTAINTY.
					'
					If state.brackt Then
						If CDbl(state.stx) < CDbl(state.sty) Then
							state.stmin = state.stx
							state.stmax = state.sty
						Else
							state.stmin = state.sty
							state.stmax = state.stx
						End If
					Else
						state.stmin = state.stx
						state.stmax = stp + state.xtrapf * (stp - state.stx)
					End If

					'
					'        FORCE THE STEP TO BE WITHIN THE BOUNDS STPMAX AND STPMIN.
					'
					If CDbl(stp) > CDbl(stpmax) Then
						stp = stpmax
					End If
					If CDbl(stp) < CDbl(stpmin) Then
						stp = stpmin
					End If

					'
					'        IF AN UNUSUAL TERMINATION IS TO OCCUR THEN LET
					'        STP BE THE LOWEST POINT OBTAINED SO FAR.
					'
					If (((state.brackt AndAlso (CDbl(stp) <= CDbl(state.stmin) OrElse CDbl(stp) >= CDbl(state.stmax))) OrElse nfev >= maxfev - 1) OrElse state.infoc = 0) OrElse (state.brackt AndAlso CDbl(state.stmax - state.stmin) <= CDbl(xtol * state.stmax)) Then
						stp = state.stx
					End If

					'
					'        EVALUATE THE FUNCTION AND GRADIENT AT STP
					'        AND COMPUTE THE DIRECTIONAL DERIVATIVE.
					'
					For i_ = 0 To n - 1
						x(i_) = wa(i_)
					Next
					For i_ = 0 To n - 1
						x(i_) = x(i_) + stp * s(i_)
					Next

					'
					' NEXT
					'
					stage = 4
					Return
				End If
				If stage = 4 Then
					info = 0
					nfev = nfev + 1
					v = 0.0
					For i_ = 0 To n - 1
						v += g(i_) * s(i_)
					Next
					state.dg = v
					state.ftest1 = state.finit + stp * state.dgtest

					'
					'        TEST FOR CONVERGENCE.
					'
					If (state.brackt AndAlso (CDbl(stp) <= CDbl(state.stmin) OrElse CDbl(stp) >= CDbl(state.stmax))) OrElse state.infoc = 0 Then
						info = 6
					End If
					If (CDbl(stp) = CDbl(stpmax) AndAlso CDbl(f) <= CDbl(state.ftest1)) AndAlso CDbl(state.dg) <= CDbl(state.dgtest) Then
						info = 5
					End If
					If CDbl(stp) = CDbl(stpmin) AndAlso (CDbl(f) > CDbl(state.ftest1) OrElse CDbl(state.dg) >= CDbl(state.dgtest)) Then
						info = 4
					End If
					If nfev >= maxfev Then
						info = 3
					End If
					If state.brackt AndAlso CDbl(state.stmax - state.stmin) <= CDbl(xtol * state.stmax) Then
						info = 2
					End If
					If CDbl(f) <= CDbl(state.ftest1) AndAlso CDbl(System.Math.Abs(state.dg)) <= CDbl(-(gtol * state.dginit)) Then
						info = 1
					End If

					'
					'        CHECK FOR TERMINATION.
					'
					If info <> 0 Then
						stage = 0
						Return
					End If

					'
					'        IN THE FIRST STAGE WE SEEK A STEP FOR WHICH THE MODIFIED
					'        FUNCTION HAS A NONPOSITIVE VALUE AND NONNEGATIVE DERIVATIVE.
					'
					If (state.stage1 AndAlso CDbl(f) <= CDbl(state.ftest1)) AndAlso CDbl(state.dg) >= CDbl(System.Math.Min(ftol, gtol) * state.dginit) Then
						state.stage1 = False
					End If

					'
					'        A MODIFIED FUNCTION IS USED TO PREDICT THE STEP ONLY IF
					'        WE HAVE NOT OBTAINED A STEP FOR WHICH THE MODIFIED
					'        FUNCTION HAS A NONPOSITIVE FUNCTION VALUE AND NONNEGATIVE
					'        DERIVATIVE, AND IF A LOWER FUNCTION VALUE HAS BEEN
					'        OBTAINED BUT THE DECREASE IS NOT SUFFICIENT.
					'
					If (state.stage1 AndAlso CDbl(f) <= CDbl(state.fx)) AndAlso CDbl(f) > CDbl(state.ftest1) Then

						'
						'           DEFINE THE MODIFIED FUNCTION AND DERIVATIVE VALUES.
						'
						state.fm = f - stp * state.dgtest
						state.fxm = state.fx - state.stx * state.dgtest
						state.fym = state.fy - state.sty * state.dgtest
						state.dgm = state.dg - state.dgtest
						state.dgxm = state.dgx - state.dgtest
						state.dgym = state.dgy - state.dgtest

						'
						'           CALL CSTEP TO UPDATE THE INTERVAL OF UNCERTAINTY
						'           AND TO COMPUTE THE NEW STEP.
						'
						mnlmcstep(state.stx, state.fxm, state.dgxm, state.sty, state.fym, state.dgym, _
							stp, state.fm, state.dgm, state.brackt, state.stmin, state.stmax, _
							state.infoc)

						'
						'           RESET THE FUNCTION AND GRADIENT VALUES FOR F.
						'
						state.fx = state.fxm + state.stx * state.dgtest
						state.fy = state.fym + state.sty * state.dgtest
						state.dgx = state.dgxm + state.dgtest
						state.dgy = state.dgym + state.dgtest
					Else

						'
						'           CALL MCSTEP TO UPDATE THE INTERVAL OF UNCERTAINTY
						'           AND TO COMPUTE THE NEW STEP.
						'
						mnlmcstep(state.stx, state.fx, state.dgx, state.sty, state.fy, state.dgy, _
							stp, f, state.dg, state.brackt, state.stmin, state.stmax, _
							state.infoc)
					End If

					'
					'        FORCE A SUFFICIENT DECREASE IN THE SIZE OF THE
					'        INTERVAL OF UNCERTAINTY.
					'
					If state.brackt Then
						If CDbl(System.Math.Abs(state.sty - state.stx)) >= CDbl(p66 * state.width1) Then
							stp = state.stx + p5 * (state.sty - state.stx)
						End If
						state.width1 = state.width
						state.width = System.Math.Abs(state.sty - state.stx)
					End If

					'
					'  NEXT.
					'
					stage = 3
					Continue While
				End If
			End While
		End Sub


		Private Shared Sub mnlmcstep(ByRef stx As Double, ByRef fx As Double, ByRef dx As Double, ByRef sty As Double, ByRef fy As Double, ByRef dy As Double, _
			ByRef stp As Double, fp As Double, dp As Double, ByRef brackt As Boolean, stmin As Double, stmax As Double, _
			ByRef info As Integer)
			Dim bound As New Boolean()
			Dim gamma As Double = 0
			Dim p As Double = 0
			Dim q As Double = 0
			Dim r As Double = 0
			Dim s As Double = 0
			Dim sgnd As Double = 0
			Dim stpc As Double = 0
			Dim stpf As Double = 0
			Dim stpq As Double = 0
			Dim theta As Double = 0

			info = 0

			'
			'     CHECK THE INPUT PARAMETERS FOR ERRORS.
			'
			If ((brackt AndAlso (CDbl(stp) <= CDbl(System.Math.Min(stx, sty)) OrElse CDbl(stp) >= CDbl(System.Math.Max(stx, sty)))) OrElse CDbl(dx * (stp - stx)) >= CDbl(0)) OrElse CDbl(stmax) < CDbl(stmin) Then
				Return
			End If

			'
			'     DETERMINE IF THE DERIVATIVES HAVE OPPOSITE SIGN.
			'
			sgnd = dp * (dx / System.Math.Abs(dx))

			'
			'     FIRST CASE. A HIGHER FUNCTION VALUE.
			'     THE MINIMUM IS BRACKETED. IF THE CUBIC STEP IS CLOSER
			'     TO STX THAN THE QUADRATIC STEP, THE CUBIC STEP IS TAKEN,
			'     ELSE THE AVERAGE OF THE CUBIC AND QUADRATIC STEPS IS TAKEN.
			'
			If CDbl(fp) > CDbl(fx) Then
				info = 1
				bound = True
				theta = 3 * (fx - fp) / (stp - stx) + dx + dp
				s = System.Math.Max(System.Math.Abs(theta), System.Math.Max(System.Math.Abs(dx), System.Math.Abs(dp)))
				gamma = s * System.Math.sqrt(Math.sqr(theta / s) - dx / s * (dp / s))
				If CDbl(stp) < CDbl(stx) Then
					gamma = -gamma
				End If
				p = gamma - dx + theta
				q = gamma - dx + gamma + dp
				r = p / q
				stpc = stx + r * (stp - stx)
				stpq = stx + dx / ((fx - fp) / (stp - stx) + dx) / 2 * (stp - stx)
				If CDbl(System.Math.Abs(stpc - stx)) < CDbl(System.Math.Abs(stpq - stx)) Then
					stpf = stpc
				Else
					stpf = stpc + (stpq - stpc) / 2
				End If
				brackt = True
			Else
				If CDbl(sgnd) < CDbl(0) Then

					'
					'     SECOND CASE. A LOWER FUNCTION VALUE AND DERIVATIVES OF
					'     OPPOSITE SIGN. THE MINIMUM IS BRACKETED. IF THE CUBIC
					'     STEP IS CLOSER TO STX THAN THE QUADRATIC (SECANT) STEP,
					'     THE CUBIC STEP IS TAKEN, ELSE THE QUADRATIC STEP IS TAKEN.
					'
					info = 2
					bound = False
					theta = 3 * (fx - fp) / (stp - stx) + dx + dp
					s = System.Math.Max(System.Math.Abs(theta), System.Math.Max(System.Math.Abs(dx), System.Math.Abs(dp)))
					gamma = s * System.Math.sqrt(Math.sqr(theta / s) - dx / s * (dp / s))
					If CDbl(stp) > CDbl(stx) Then
						gamma = -gamma
					End If
					p = gamma - dp + theta
					q = gamma - dp + gamma + dx
					r = p / q
					stpc = stp + r * (stx - stp)
					stpq = stp + dp / (dp - dx) * (stx - stp)
					If CDbl(System.Math.Abs(stpc - stp)) > CDbl(System.Math.Abs(stpq - stp)) Then
						stpf = stpc
					Else
						stpf = stpq
					End If
					brackt = True
				Else
					If CDbl(System.Math.Abs(dp)) < CDbl(System.Math.Abs(dx)) Then

						'
						'     THIRD CASE. A LOWER FUNCTION VALUE, DERIVATIVES OF THE
						'     SAME SIGN, AND THE MAGNITUDE OF THE DERIVATIVE DECREASES.
						'     THE CUBIC STEP IS ONLY USED IF THE CUBIC TENDS TO INFINITY
						'     IN THE DIRECTION OF THE STEP OR IF THE MINIMUM OF THE CUBIC
						'     IS BEYOND STP. OTHERWISE THE CUBIC STEP IS DEFINED TO BE
						'     EITHER STPMIN OR STPMAX. THE QUADRATIC (SECANT) STEP IS ALSO
						'     COMPUTED AND IF THE MINIMUM IS BRACKETED THEN THE THE STEP
						'     CLOSEST TO STX IS TAKEN, ELSE THE STEP FARTHEST AWAY IS TAKEN.
						'
						info = 3
						bound = True
						theta = 3 * (fx - fp) / (stp - stx) + dx + dp
						s = System.Math.Max(System.Math.Abs(theta), System.Math.Max(System.Math.Abs(dx), System.Math.Abs(dp)))

						'
						'        THE CASE GAMMA = 0 ONLY ARISES IF THE CUBIC DOES NOT TEND
						'        TO INFINITY IN THE DIRECTION OF THE STEP.
						'
						gamma = s * System.Math.sqrt(System.Math.Max(0, Math.sqr(theta / s) - dx / s * (dp / s)))
						If CDbl(stp) > CDbl(stx) Then
							gamma = -gamma
						End If
						p = gamma - dp + theta
						q = gamma + (dx - dp) + gamma
						r = p / q
						If CDbl(r) < CDbl(0) AndAlso CDbl(gamma) <> CDbl(0) Then
							stpc = stp + r * (stx - stp)
						Else
							If CDbl(stp) > CDbl(stx) Then
								stpc = stmax
							Else
								stpc = stmin
							End If
						End If
						stpq = stp + dp / (dp - dx) * (stx - stp)
						If brackt Then
							If CDbl(System.Math.Abs(stp - stpc)) < CDbl(System.Math.Abs(stp - stpq)) Then
								stpf = stpc
							Else
								stpf = stpq
							End If
						Else
							If CDbl(System.Math.Abs(stp - stpc)) > CDbl(System.Math.Abs(stp - stpq)) Then
								stpf = stpc
							Else
								stpf = stpq
							End If
						End If
					Else

						'
						'     FOURTH CASE. A LOWER FUNCTION VALUE, DERIVATIVES OF THE
						'     SAME SIGN, AND THE MAGNITUDE OF THE DERIVATIVE DOES
						'     NOT DECREASE. IF THE MINIMUM IS NOT BRACKETED, THE STEP
						'     IS EITHER STPMIN OR STPMAX, ELSE THE CUBIC STEP IS TAKEN.
						'
						info = 4
						bound = False
						If brackt Then
							theta = 3 * (fp - fy) / (sty - stp) + dy + dp
							s = System.Math.Max(System.Math.Abs(theta), System.Math.Max(System.Math.Abs(dy), System.Math.Abs(dp)))
							gamma = s * System.Math.sqrt(Math.sqr(theta / s) - dy / s * (dp / s))
							If CDbl(stp) > CDbl(sty) Then
								gamma = -gamma
							End If
							p = gamma - dp + theta
							q = gamma - dp + gamma + dy
							r = p / q
							stpc = stp + r * (sty - stp)
							stpf = stpc
						Else
							If CDbl(stp) > CDbl(stx) Then
								stpf = stmax
							Else
								stpf = stmin
							End If
						End If
					End If
				End If
			End If

			'
			'     UPDATE THE INTERVAL OF UNCERTAINTY. THIS UPDATE DOES NOT
			'     DEPEND ON THE NEW STEP OR THE CASE ANALYSIS ABOVE.
			'
			If CDbl(fp) > CDbl(fx) Then
				sty = stp
				fy = fp
				dy = dp
			Else
				If CDbl(sgnd) < CDbl(0.0) Then
					sty = stx
					fy = fx
					dy = dx
				End If
				stx = stp
				fx = fp
				dx = dp
			End If

			'
			'     COMPUTE THE NEW STEP AND SAFEGUARD IT.
			'
			stpf = System.Math.Min(stmax, stpf)
			stpf = System.Math.Max(stmin, stpf)
			stp = stpf
			If brackt AndAlso bound Then
				If CDbl(sty) > CDbl(stx) Then
					stp = System.Math.Min(stx + 0.66 * (sty - stx), stp)
				Else
					stp = System.Math.Max(stx + 0.66 * (sty - stx), stp)
				End If
			End If
		End Sub


	End Class
	Public Class mcpd
		'************************************************************************
'        This structure is a MCPD (Markov Chains for Population Data) solver.
'
'        You should use ALGLIB functions in order to work with this object.
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Class mcpdstate
			Inherits apobject
			Public n As Integer
			Public states As Integer()
			Public npairs As Integer
			Public data As Double(,)
			Public ec As Double(,)
			Public bndl As Double(,)
			Public bndu As Double(,)
			Public c As Double(,)
			Public ct As Integer()
			Public ccnt As Integer
			Public pw As Double()
			Public priorp As Double(,)
			Public regterm As Double
			Public bs As minbleic.minbleicstate
			Public repinneriterationscount As Integer
			Public repouteriterationscount As Integer
			Public repnfev As Integer
			Public repterminationtype As Integer
			Public br As minbleic.minbleicreport
			Public tmpp As Double()
			Public effectivew As Double()
			Public effectivebndl As Double()
			Public effectivebndu As Double()
			Public effectivec As Double(,)
			Public effectivect As Integer()
			Public h As Double()
			Public p As Double(,)
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				states = New Integer(-1) {}
				data = New Double(-1, -1) {}
				ec = New Double(-1, -1) {}
				bndl = New Double(-1, -1) {}
				bndu = New Double(-1, -1) {}
				c = New Double(-1, -1) {}
				ct = New Integer(-1) {}
				pw = New Double(-1) {}
				priorp = New Double(-1, -1) {}
				bs = New minbleic.minbleicstate()
				br = New minbleic.minbleicreport()
				tmpp = New Double(-1) {}
				effectivew = New Double(-1) {}
				effectivebndl = New Double(-1) {}
				effectivebndu = New Double(-1) {}
				effectivec = New Double(-1, -1) {}
				effectivect = New Integer(-1) {}
				h = New Double(-1) {}
				p = New Double(-1, -1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mcpdstate()
				_result.n = n
				_result.states = DirectCast(states.Clone(), Integer())
				_result.npairs = npairs
				_result.data = DirectCast(data.Clone(), Double(,))
				_result.ec = DirectCast(ec.Clone(), Double(,))
				_result.bndl = DirectCast(bndl.Clone(), Double(,))
				_result.bndu = DirectCast(bndu.Clone(), Double(,))
				_result.c = DirectCast(c.Clone(), Double(,))
				_result.ct = DirectCast(ct.Clone(), Integer())
				_result.ccnt = ccnt
				_result.pw = DirectCast(pw.Clone(), Double())
				_result.priorp = DirectCast(priorp.Clone(), Double(,))
				_result.regterm = regterm
				_result.bs = DirectCast(bs.make_copy(), minbleic.minbleicstate)
				_result.repinneriterationscount = repinneriterationscount
				_result.repouteriterationscount = repouteriterationscount
				_result.repnfev = repnfev
				_result.repterminationtype = repterminationtype
				_result.br = DirectCast(br.make_copy(), minbleic.minbleicreport)
				_result.tmpp = DirectCast(tmpp.Clone(), Double())
				_result.effectivew = DirectCast(effectivew.Clone(), Double())
				_result.effectivebndl = DirectCast(effectivebndl.Clone(), Double())
				_result.effectivebndu = DirectCast(effectivebndu.Clone(), Double())
				_result.effectivec = DirectCast(effectivec.Clone(), Double(,))
				_result.effectivect = DirectCast(effectivect.Clone(), Integer())
				_result.h = DirectCast(h.Clone(), Double())
				_result.p = DirectCast(p.Clone(), Double(,))
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure is a MCPD training report:
'            InnerIterationsCount    -   number of inner iterations of the
'                                        underlying optimization algorithm
'            OuterIterationsCount    -   number of outer iterations of the
'                                        underlying optimization algorithm
'            NFEV                    -   number of merit function evaluations
'            TerminationType         -   termination type
'                                        (same as for MinBLEIC optimizer, positive
'                                        values denote success, negative ones -
'                                        failure)
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Class mcpdreport
			Inherits apobject
			Public inneriterationscount As Integer
			Public outeriterationscount As Integer
			Public nfev As Integer
			Public terminationtype As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mcpdreport()
				_result.inneriterationscount = inneriterationscount
				_result.outeriterationscount = outeriterationscount
				_result.nfev = nfev
				_result.terminationtype = terminationtype
				Return _result
			End Function
		End Class




		Public Const xtol As Double = 1E-08


		'************************************************************************
'        DESCRIPTION:
'
'        This function creates MCPD (Markov Chains for Population Data) solver.
'
'        This  solver  can  be  used  to find transition matrix P for N-dimensional
'        prediction  problem  where transition from X[i] to X[i+1] is  modelled  as
'            X[i+1] = P*X[i]
'        where X[i] and X[i+1] are N-dimensional population vectors (components  of
'        each X are non-negative), and P is a N*N transition matrix (elements of  P
'        are non-negative, each column sums to 1.0).
'
'        Such models arise when when:
'        * there is some population of individuals
'        * individuals can have different states
'        * individuals can transit from one state to another
'        * population size is constant, i.e. there is no new individuals and no one
'          leaves population
'        * you want to model transitions of individuals from one state into another
'
'        USAGE:
'
'        Here we give very brief outline of the MCPD. We strongly recommend you  to
'        read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
'        on data analysis which is available at http://www.alglib.net/dataanalysis/
'
'        1. User initializes algorithm state with MCPDCreate() call
'
'        2. User  adds  one  or  more  tracks -  sequences of states which describe
'           evolution of a system being modelled from different starting conditions
'
'        3. User may add optional boundary, equality  and/or  linear constraints on
'           the coefficients of P by calling one of the following functions:
'           * MCPDSetEC() to set equality constraints
'           * MCPDSetBC() to set bound constraints
'           * MCPDSetLC() to set linear constraints
'
'        4. Optionally,  user  may  set  custom  weights  for prediction errors (by
'           default, algorithm assigns non-equal, automatically chosen weights  for
'           errors in the prediction of different components of X). It can be  done
'           with a call of MCPDSetPredictionWeights() function.
'
'        5. User calls MCPDSolve() function which takes algorithm  state and
'           pointer (delegate, etc.) to callback function which calculates F/G.
'
'        6. User calls MCPDResults() to get solution
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>=1
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdcreate(n As Integer, s As mcpdstate)
			alglib.ap.assert(n >= 1, "MCPDCreate: N<1")
			mcpdinit(n, -1, -1, s)
		End Sub


		'************************************************************************
'        DESCRIPTION:
'
'        This function is a specialized version of MCPDCreate()  function,  and  we
'        recommend  you  to read comments for this function for general information
'        about MCPD solver.
'
'        This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'        for "Entry-state" model,  i.e. model  where transition from X[i] to X[i+1]
'        is modelled as
'            X[i+1] = P*X[i]
'        where
'            X[i] and X[i+1] are N-dimensional state vectors
'            P is a N*N transition matrix
'        and  one  selected component of X[] is called "entry" state and is treated
'        in a special way:
'            system state always transits from "entry" state to some another state
'            system state can not transit from any state into "entry" state
'        Such conditions basically mean that row of P which corresponds to  "entry"
'        state is zero.
'
'        Such models arise when:
'        * there is some population of individuals
'        * individuals can have different states
'        * individuals can transit from one state to another
'        * population size is NOT constant -  at every moment of time there is some
'          (unpredictable) amount of "new" individuals, which can transit into  one
'          of the states at the next turn, but still no one leaves population
'        * you want to model transitions of individuals from one state into another
'        * but you do NOT want to predict amount of "new"  individuals  because  it
'          does not depends on individuals already present (hence  system  can  not
'          transit INTO entry state - it can only transit FROM it).
'
'        This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'        http://www.alglib.net/dataanalysis/ for more data).
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>=2
'            EntryState- index of entry state, in 0..N-1
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdcreateentry(n As Integer, entrystate As Integer, s As mcpdstate)
			alglib.ap.assert(n >= 2, "MCPDCreateEntry: N<2")
			alglib.ap.assert(entrystate >= 0, "MCPDCreateEntry: EntryState<0")
			alglib.ap.assert(entrystate < n, "MCPDCreateEntry: EntryState>=N")
			mcpdinit(n, entrystate, -1, s)
		End Sub


		'************************************************************************
'        DESCRIPTION:
'
'        This function is a specialized version of MCPDCreate()  function,  and  we
'        recommend  you  to read comments for this function for general information
'        about MCPD solver.
'
'        This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'        for "Exit-state" model,  i.e. model  where  transition from X[i] to X[i+1]
'        is modelled as
'            X[i+1] = P*X[i]
'        where
'            X[i] and X[i+1] are N-dimensional state vectors
'            P is a N*N transition matrix
'        and  one  selected component of X[] is called "exit"  state and is treated
'        in a special way:
'            system state can transit from any state into "exit" state
'            system state can not transit from "exit" state into any other state
'            transition operator discards "exit" state (makes it zero at each turn)
'        Such  conditions  basically  mean  that  column  of P which corresponds to
'        "exit" state is zero. Multiplication by such P may decrease sum of  vector
'        components.
'
'        Such models arise when:
'        * there is some population of individuals
'        * individuals can have different states
'        * individuals can transit from one state to another
'        * population size is NOT constant - individuals can move into "exit" state
'          and leave population at the next turn, but there are no new individuals
'        * amount of individuals which leave population can be predicted
'        * you want to model transitions of individuals from one state into another
'          (including transitions into the "exit" state)
'
'        This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'        http://www.alglib.net/dataanalysis/ for more data).
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>=2
'            ExitState-  index of exit state, in 0..N-1
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdcreateexit(n As Integer, exitstate As Integer, s As mcpdstate)
			alglib.ap.assert(n >= 2, "MCPDCreateExit: N<2")
			alglib.ap.assert(exitstate >= 0, "MCPDCreateExit: ExitState<0")
			alglib.ap.assert(exitstate < n, "MCPDCreateExit: ExitState>=N")
			mcpdinit(n, -1, exitstate, s)
		End Sub


		'************************************************************************
'        DESCRIPTION:
'
'        This function is a specialized version of MCPDCreate()  function,  and  we
'        recommend  you  to read comments for this function for general information
'        about MCPD solver.
'
'        This  function  creates  MCPD (Markov Chains for Population  Data)  solver
'        for "Entry-Exit-states" model, i.e. model where  transition  from  X[i] to
'        X[i+1] is modelled as
'            X[i+1] = P*X[i]
'        where
'            X[i] and X[i+1] are N-dimensional state vectors
'            P is a N*N transition matrix
'        one selected component of X[] is called "entry" state and is treated in  a
'        special way:
'            system state always transits from "entry" state to some another state
'            system state can not transit from any state into "entry" state
'        and another one component of X[] is called "exit" state and is treated  in
'        a special way too:
'            system state can transit from any state into "exit" state
'            system state can not transit from "exit" state into any other state
'            transition operator discards "exit" state (makes it zero at each turn)
'        Such conditions basically mean that:
'            row of P which corresponds to "entry" state is zero
'            column of P which corresponds to "exit" state is zero
'        Multiplication by such P may decrease sum of vector components.
'
'        Such models arise when:
'        * there is some population of individuals
'        * individuals can have different states
'        * individuals can transit from one state to another
'        * population size is NOT constant
'        * at every moment of time there is some (unpredictable)  amount  of  "new"
'          individuals, which can transit into one of the states at the next turn
'        * some  individuals  can  move  (predictably)  into "exit" state and leave
'          population at the next turn
'        * you want to model transitions of individuals from one state into another,
'          including transitions from the "entry" state and into the "exit" state.
'        * but you do NOT want to predict amount of "new"  individuals  because  it
'          does not depends on individuals already present (hence  system  can  not
'          transit INTO entry state - it can only transit FROM it).
'
'        This model is discussed  in  more  details  in  the ALGLIB User Guide (see
'        http://www.alglib.net/dataanalysis/ for more data).
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>=2
'            EntryState- index of entry state, in 0..N-1
'            ExitState-  index of exit state, in 0..N-1
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdcreateentryexit(n As Integer, entrystate As Integer, exitstate As Integer, s As mcpdstate)
			alglib.ap.assert(n >= 2, "MCPDCreateEntryExit: N<2")
			alglib.ap.assert(entrystate >= 0, "MCPDCreateEntryExit: EntryState<0")
			alglib.ap.assert(entrystate < n, "MCPDCreateEntryExit: EntryState>=N")
			alglib.ap.assert(exitstate >= 0, "MCPDCreateEntryExit: ExitState<0")
			alglib.ap.assert(exitstate < n, "MCPDCreateEntryExit: ExitState>=N")
			alglib.ap.assert(entrystate <> exitstate, "MCPDCreateEntryExit: EntryState=ExitState")
			mcpdinit(n, entrystate, exitstate, s)
		End Sub


		'************************************************************************
'        This  function  is  used to add a track - sequence of system states at the
'        different moments of its evolution.
'
'        You  may  add  one  or several tracks to the MCPD solver. In case you have
'        several tracks, they won't overwrite each other. For example,  if you pass
'        two tracks, A1-A2-A3 (system at t=A+1, t=A+2 and t=A+3) and B1-B2-B3, then
'        solver will try to model transitions from t=A+1 to t=A+2, t=A+2 to  t=A+3,
'        t=B+1 to t=B+2, t=B+2 to t=B+3. But it WONT mix these two tracks - i.e. it
'        wont try to model transition from t=A+3 to t=B+1.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            XY      -   track, array[K,N]:
'                        * I-th row is a state at t=I
'                        * elements of XY must be non-negative (exception will be
'                          thrown on negative elements)
'            K       -   number of points in a track
'                        * if given, only leading K rows of XY are used
'                        * if not given, automatically determined from size of XY
'
'        NOTES:
'
'        1. Track may contain either proportional or population data:
'           * with proportional data all rows of XY must sum to 1.0, i.e. we have
'             proportions instead of absolute population values
'           * with population data rows of XY contain population counts and generally
'             do not sum to 1.0 (although they still must be non-negative)
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdaddtrack(s As mcpdstate, xy As Double(,), k As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0
			Dim s0 As Double = 0
			Dim s1 As Double = 0

			n = s.n
			alglib.ap.assert(k >= 0, "MCPDAddTrack: K<0")
			alglib.ap.assert(alglib.ap.cols(xy) >= n, "MCPDAddTrack: Cols(XY)<N")
			alglib.ap.assert(alglib.ap.rows(xy) >= k, "MCPDAddTrack: Rows(XY)<K")
			alglib.ap.assert(apserv.apservisfinitematrix(xy, k, n), "MCPDAddTrack: XY contains infinite or NaN elements")
			For i = 0 To k - 1
				For j = 0 To n - 1
					alglib.ap.assert(CDbl(xy(i, j)) >= CDbl(0), "MCPDAddTrack: XY contains negative elements")
				Next
			Next
			If k < 2 Then
				Return
			End If
			If alglib.ap.rows(s.data) < s.npairs + k - 1 Then
				apserv.rmatrixresize(s.data, System.Math.Max(2 * alglib.ap.rows(s.data), s.npairs + k - 1), 2 * n)
			End If
			For i = 0 To k - 2
				s0 = 0
				s1 = 0
				For j = 0 To n - 1
					If s.states(j) >= 0 Then
						s0 = s0 + xy(i, j)
					End If
					If s.states(j) <= 0 Then
						s1 = s1 + xy(i + 1, j)
					End If
				Next
				If CDbl(s0) > CDbl(0) AndAlso CDbl(s1) > CDbl(0) Then
					For j = 0 To n - 1
						If s.states(j) >= 0 Then
							s.data(s.npairs, j) = xy(i, j) / s0
						Else
							s.data(s.npairs, j) = 0.0
						End If
						If s.states(j) <= 0 Then
							s.data(s.npairs, n + j) = xy(i + 1, j) / s1
						Else
							s.data(s.npairs, n + j) = 0.0
						End If
					Next
					s.npairs = s.npairs + 1
				End If
			Next
		End Sub


		'************************************************************************
'        This function is used to add equality constraints on the elements  of  the
'        transition matrix P.
'
'        MCPD solver has four types of constraints which can be placed on P:
'        * user-specified equality constraints (optional)
'        * user-specified bound constraints (optional)
'        * user-specified general linear constraints (optional)
'        * basic constraints (always present):
'          * non-negativity: P[i,j]>=0
'          * consistency: every column of P sums to 1.0
'
'        Final  constraints  which  are  passed  to  the  underlying  optimizer are
'        calculated  as  intersection  of all present constraints. For example, you
'        may specify boundary constraint on P[0,0] and equality one:
'            0.1<=P[0,0]<=0.9
'            P[0,0]=0.5
'        Such  combination  of  constraints  will  be  silently  reduced  to  their
'        intersection, which is P[0,0]=0.5.
'
'        This  function  can  be  used  to  place equality constraints on arbitrary
'        subset of elements of P. Set of constraints is specified by EC, which  may
'        contain either NAN's or finite numbers from [0,1]. NAN denotes absence  of
'        constraint, finite number denotes equality constraint on specific  element
'        of P.
'
'        You can also  use  MCPDAddEC()  function  which  allows  to  ADD  equality
'        constraint  for  one  element  of P without changing constraints for other
'        elements.
'
'        These functions (MCPDSetEC and MCPDAddEC) interact as follows:
'        * there is internal matrix of equality constraints which is stored in  the
'          MCPD solver
'        * MCPDSetEC() replaces this matrix by another one (SET)
'        * MCPDAddEC() modifies one element of this matrix and  leaves  other  ones
'          unchanged (ADD)
'        * thus  MCPDAddEC()  call  preserves  all  modifications  done by previous
'          calls,  while  MCPDSetEC()  completely discards all changes  done to the
'          equality constraints.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            EC      -   equality constraints, array[N,N]. Elements of  EC  can  be
'                        either NAN's or finite  numbers from  [0,1].  NAN  denotes
'                        absence  of  constraints,  while  finite  value    denotes
'                        equality constraint on the corresponding element of P.
'
'        NOTES:
'
'        1. infinite values of EC will lead to exception being thrown. Values  less
'        than 0.0 or greater than 1.0 will lead to error code being returned  after
'        call to MCPDSolve().
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsetec(s As mcpdstate, ec As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0

			n = s.n
			alglib.ap.assert(alglib.ap.cols(ec) >= n, "MCPDSetEC: Cols(EC)<N")
			alglib.ap.assert(alglib.ap.rows(ec) >= n, "MCPDSetEC: Rows(EC)<N")
			For i = 0 To n - 1
				For j = 0 To n - 1
					alglib.ap.assert(Math.isfinite(ec(i, j)) OrElse [Double].IsNaN(ec(i, j)), "MCPDSetEC: EC containts infinite elements")
					s.ec(i, j) = ec(i, j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function is used to add equality constraints on the elements  of  the
'        transition matrix P.
'
'        MCPD solver has four types of constraints which can be placed on P:
'        * user-specified equality constraints (optional)
'        * user-specified bound constraints (optional)
'        * user-specified general linear constraints (optional)
'        * basic constraints (always present):
'          * non-negativity: P[i,j]>=0
'          * consistency: every column of P sums to 1.0
'
'        Final  constraints  which  are  passed  to  the  underlying  optimizer are
'        calculated  as  intersection  of all present constraints. For example, you
'        may specify boundary constraint on P[0,0] and equality one:
'            0.1<=P[0,0]<=0.9
'            P[0,0]=0.5
'        Such  combination  of  constraints  will  be  silently  reduced  to  their
'        intersection, which is P[0,0]=0.5.
'
'        This function can be used to ADD equality constraint for one element of  P
'        without changing constraints for other elements.
'
'        You  can  also  use  MCPDSetEC()  function  which  allows  you  to specify
'        arbitrary set of equality constraints in one call.
'
'        These functions (MCPDSetEC and MCPDAddEC) interact as follows:
'        * there is internal matrix of equality constraints which is stored in the
'          MCPD solver
'        * MCPDSetEC() replaces this matrix by another one (SET)
'        * MCPDAddEC() modifies one element of this matrix and leaves  other  ones
'          unchanged (ADD)
'        * thus  MCPDAddEC()  call  preserves  all  modifications done by previous
'          calls,  while  MCPDSetEC()  completely discards all changes done to the
'          equality constraints.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            I       -   row index of element being constrained
'            J       -   column index of element being constrained
'            C       -   value (constraint for P[I,J]).  Can  be  either  NAN  (no
'                        constraint) or finite value from [0,1].
'                        
'        NOTES:
'
'        1. infinite values of C  will lead to exception being thrown. Values  less
'        than 0.0 or greater than 1.0 will lead to error code being returned  after
'        call to MCPDSolve().
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdaddec(s As mcpdstate, i As Integer, j As Integer, c As Double)
			alglib.ap.assert(i >= 0, "MCPDAddEC: I<0")
			alglib.ap.assert(i < s.n, "MCPDAddEC: I>=N")
			alglib.ap.assert(j >= 0, "MCPDAddEC: J<0")
			alglib.ap.assert(j < s.n, "MCPDAddEC: J>=N")
			alglib.ap.assert([Double].IsNaN(c) OrElse Math.isfinite(c), "MCPDAddEC: C is not finite number or NAN")
			s.ec(i, j) = c
		End Sub


		'************************************************************************
'        This function is used to add bound constraints  on  the  elements  of  the
'        transition matrix P.
'
'        MCPD solver has four types of constraints which can be placed on P:
'        * user-specified equality constraints (optional)
'        * user-specified bound constraints (optional)
'        * user-specified general linear constraints (optional)
'        * basic constraints (always present):
'          * non-negativity: P[i,j]>=0
'          * consistency: every column of P sums to 1.0
'
'        Final  constraints  which  are  passed  to  the  underlying  optimizer are
'        calculated  as  intersection  of all present constraints. For example, you
'        may specify boundary constraint on P[0,0] and equality one:
'            0.1<=P[0,0]<=0.9
'            P[0,0]=0.5
'        Such  combination  of  constraints  will  be  silently  reduced  to  their
'        intersection, which is P[0,0]=0.5.
'
'        This  function  can  be  used  to  place bound   constraints  on arbitrary
'        subset  of  elements  of  P.  Set of constraints is specified by BndL/BndU
'        matrices, which may contain arbitrary combination  of  finite  numbers  or
'        infinities (like -INF<x<=0.5 or 0.1<=x<+INF).
'
'        You can also use MCPDAddBC() function which allows to ADD bound constraint
'        for one element of P without changing constraints for other elements.
'
'        These functions (MCPDSetBC and MCPDAddBC) interact as follows:
'        * there is internal matrix of bound constraints which is stored in the
'          MCPD solver
'        * MCPDSetBC() replaces this matrix by another one (SET)
'        * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
'          unchanged (ADD)
'        * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
'          calls,  while  MCPDSetBC()  completely discards all changes  done to the
'          equality constraints.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            BndL    -   lower bounds constraints, array[N,N]. Elements of BndL can
'                        be finite numbers or -INF.
'            BndU    -   upper bounds constraints, array[N,N]. Elements of BndU can
'                        be finite numbers or +INF.
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsetbc(s As mcpdstate, bndl As Double(,), bndu As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0

			n = s.n
			alglib.ap.assert(alglib.ap.cols(bndl) >= n, "MCPDSetBC: Cols(BndL)<N")
			alglib.ap.assert(alglib.ap.rows(bndl) >= n, "MCPDSetBC: Rows(BndL)<N")
			alglib.ap.assert(alglib.ap.cols(bndu) >= n, "MCPDSetBC: Cols(BndU)<N")
			alglib.ap.assert(alglib.ap.rows(bndu) >= n, "MCPDSetBC: Rows(BndU)<N")
			For i = 0 To n - 1
				For j = 0 To n - 1
					alglib.ap.assert(Math.isfinite(bndl(i, j)) OrElse [Double].IsNegativeInfinity(bndl(i, j)), "MCPDSetBC: BndL containts NAN or +INF")
					alglib.ap.assert(Math.isfinite(bndu(i, j)) OrElse [Double].IsPositiveInfinity(bndu(i, j)), "MCPDSetBC: BndU containts NAN or -INF")
					s.bndl(i, j) = bndl(i, j)
					s.bndu(i, j) = bndu(i, j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function is used to add bound constraints  on  the  elements  of  the
'        transition matrix P.
'
'        MCPD solver has four types of constraints which can be placed on P:
'        * user-specified equality constraints (optional)
'        * user-specified bound constraints (optional)
'        * user-specified general linear constraints (optional)
'        * basic constraints (always present):
'          * non-negativity: P[i,j]>=0
'          * consistency: every column of P sums to 1.0
'
'        Final  constraints  which  are  passed  to  the  underlying  optimizer are
'        calculated  as  intersection  of all present constraints. For example, you
'        may specify boundary constraint on P[0,0] and equality one:
'            0.1<=P[0,0]<=0.9
'            P[0,0]=0.5
'        Such  combination  of  constraints  will  be  silently  reduced  to  their
'        intersection, which is P[0,0]=0.5.
'
'        This  function  can  be  used to ADD bound constraint for one element of P
'        without changing constraints for other elements.
'
'        You  can  also  use  MCPDSetBC()  function  which  allows to  place  bound
'        constraints  on arbitrary subset of elements of P.   Set of constraints is
'        specified  by  BndL/BndU matrices, which may contain arbitrary combination
'        of finite numbers or infinities (like -INF<x<=0.5 or 0.1<=x<+INF).
'
'        These functions (MCPDSetBC and MCPDAddBC) interact as follows:
'        * there is internal matrix of bound constraints which is stored in the
'          MCPD solver
'        * MCPDSetBC() replaces this matrix by another one (SET)
'        * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
'          unchanged (ADD)
'        * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
'          calls,  while  MCPDSetBC()  completely discards all changes  done to the
'          equality constraints.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            I       -   row index of element being constrained
'            J       -   column index of element being constrained
'            BndL    -   lower bound
'            BndU    -   upper bound
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdaddbc(s As mcpdstate, i As Integer, j As Integer, bndl As Double, bndu As Double)
			alglib.ap.assert(i >= 0, "MCPDAddBC: I<0")
			alglib.ap.assert(i < s.n, "MCPDAddBC: I>=N")
			alglib.ap.assert(j >= 0, "MCPDAddBC: J<0")
			alglib.ap.assert(j < s.n, "MCPDAddBC: J>=N")
			alglib.ap.assert(Math.isfinite(bndl) OrElse [Double].IsNegativeInfinity(bndl), "MCPDAddBC: BndL is NAN or +INF")
			alglib.ap.assert(Math.isfinite(bndu) OrElse [Double].IsPositiveInfinity(bndu), "MCPDAddBC: BndU is NAN or -INF")
			s.bndl(i, j) = bndl
			s.bndu(i, j) = bndu
		End Sub


		'************************************************************************
'        This function is used to set linear equality/inequality constraints on the
'        elements of the transition matrix P.
'
'        This function can be used to set one or several general linear constraints
'        on the elements of P. Two types of constraints are supported:
'        * equality constraints
'        * inequality constraints (both less-or-equal and greater-or-equal)
'
'        Coefficients  of  constraints  are  specified  by  matrix  C (one  of  the
'        parameters).  One  row  of  C  corresponds  to  one  constraint.   Because
'        transition  matrix P has N*N elements,  we  need  N*N columns to store all
'        coefficients  (they  are  stored row by row), and one more column to store
'        right part - hence C has N*N+1 columns.  Constraint  kind is stored in the
'        CT array.
'
'        Thus, I-th linear constraint is
'            P[0,0]*C[I,0] + P[0,1]*C[I,1] + .. + P[0,N-1]*C[I,N-1] +
'                + P[1,0]*C[I,N] + P[1,1]*C[I,N+1] + ... +
'                + P[N-1,N-1]*C[I,N*N-1]  ?=?  C[I,N*N]
'        where ?=? can be either "=" (CT[i]=0), "<=" (CT[i]<0) or ">=" (CT[i]>0).
'
'        Your constraint may involve only some subset of P (less than N*N elements).
'        For example it can be something like
'            P[0,0] + P[0,1] = 0.5
'        In this case you still should pass matrix  with N*N+1 columns, but all its
'        elements (except for C[0,0], C[0,1] and C[0,N*N-1]) will be zero.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            C       -   array[K,N*N+1] - coefficients of constraints
'                        (see above for complete description)
'            CT      -   array[K] - constraint types
'                        (see above for complete description)
'            K       -   number of equality/inequality constraints, K>=0:
'                        * if given, only leading K elements of C/CT are used
'                        * if not given, automatically determined from sizes of C/CT
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsetlc(s As mcpdstate, c As Double(,), ct As Integer(), k As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0

			n = s.n
			alglib.ap.assert(alglib.ap.cols(c) >= n * n + 1, "MCPDSetLC: Cols(C)<N*N+1")
			alglib.ap.assert(alglib.ap.rows(c) >= k, "MCPDSetLC: Rows(C)<K")
			alglib.ap.assert(alglib.ap.len(ct) >= k, "MCPDSetLC: Len(CT)<K")
			alglib.ap.assert(apserv.apservisfinitematrix(c, k, n * n + 1), "MCPDSetLC: C contains infinite or NaN values!")
			apserv.rmatrixsetlengthatleast(s.c, k, n * n + 1)
			apserv.ivectorsetlengthatleast(s.ct, k)
			For i = 0 To k - 1
				For j = 0 To n * n
					s.c(i, j) = c(i, j)
				Next
				s.ct(i) = ct(i)
			Next
			s.ccnt = k
		End Sub


		'************************************************************************
'        This function allows to  tune  amount  of  Tikhonov  regularization  being
'        applied to your problem.
'
'        By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
'        small non-zero value,  P is transition matrix, prior_P is identity matrix,
'        ||X||^2 is a sum of squared elements of X.
'
'        This  function  allows  you to change coefficient r. You can  also  change
'        prior values with MCPDSetPrior() function.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            V       -   regularization  coefficient, finite non-negative value. It
'                        is  not  recommended  to specify zero value unless you are
'                        pretty sure that you want it.
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsettikhonovregularizer(s As mcpdstate, v As Double)
			alglib.ap.assert(Math.isfinite(v), "MCPDSetTikhonovRegularizer: V is infinite or NAN")
			alglib.ap.assert(CDbl(v) >= CDbl(0.0), "MCPDSetTikhonovRegularizer: V is less than zero")
			s.regterm = v
		End Sub


		'************************************************************************
'        This  function  allows to set prior values used for regularization of your
'        problem.
'
'        By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
'        small non-zero value,  P is transition matrix, prior_P is identity matrix,
'        ||X||^2 is a sum of squared elements of X.
'
'        This  function  allows  you to change prior values prior_P. You  can  also
'        change r with MCPDSetTikhonovRegularizer() function.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            PP      -   array[N,N], matrix of prior values:
'                        1. elements must be real numbers from [0,1]
'                        2. columns must sum to 1.0.
'                        First property is checked (exception is thrown otherwise),
'                        while second one is not checked/enforced.
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsetprior(s As mcpdstate, pp As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0

			pp = DirectCast(pp.Clone(), Double(,))

			n = s.n
			alglib.ap.assert(alglib.ap.cols(pp) >= n, "MCPDSetPrior: Cols(PP)<N")
			alglib.ap.assert(alglib.ap.rows(pp) >= n, "MCPDSetPrior: Rows(PP)<K")
			For i = 0 To n - 1
				For j = 0 To n - 1
					alglib.ap.assert(Math.isfinite(pp(i, j)), "MCPDSetPrior: PP containts infinite elements")
					alglib.ap.assert(CDbl(pp(i, j)) >= CDbl(0.0) AndAlso CDbl(pp(i, j)) <= CDbl(1.0), "MCPDSetPrior: PP[i,j] is less than 0.0 or greater than 1.0")
					s.priorp(i, j) = pp(i, j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function is used to change prediction weights
'
'        MCPD solver scales prediction errors as follows
'            Error(P) = ||W*(y-P*x)||^2
'        where
'            x is a system state at time t
'            y is a system state at time t+1
'            P is a transition matrix
'            W is a diagonal scaling matrix
'
'        By default, weights are chosen in order  to  minimize  relative prediction
'        error instead of absolute one. For example, if one component of  state  is
'        about 0.5 in magnitude and another one is about 0.05, then algorithm  will
'        make corresponding weights equal to 2.0 and 20.0.
'
'        INPUT PARAMETERS:
'            S       -   solver
'            PW      -   array[N], weights:
'                        * must be non-negative values (exception will be thrown otherwise)
'                        * zero values will be replaced by automatically chosen values
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsetpredictionweights(s As mcpdstate, pw As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0

			n = s.n
			alglib.ap.assert(alglib.ap.len(pw) >= n, "MCPDSetPredictionWeights: Length(PW)<N")
			For i = 0 To n - 1
				alglib.ap.assert(Math.isfinite(pw(i)), "MCPDSetPredictionWeights: PW containts infinite or NAN elements")
				alglib.ap.assert(CDbl(pw(i)) >= CDbl(0), "MCPDSetPredictionWeights: PW containts negative elements")
				s.pw(i) = pw(i)
			Next
		End Sub


		'************************************************************************
'        This function is used to start solution of the MCPD problem.
'
'        After return from this function, you can use MCPDResults() to get solution
'        and completion code.
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdsolve(s As mcpdstate)
			Dim n As Integer = 0
			Dim npairs As Integer = 0
			Dim ccnt As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim k2 As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			n = s.n
			npairs = s.npairs

			'
			' init fields of S
			'
			s.repterminationtype = 0
			s.repinneriterationscount = 0
			s.repouteriterationscount = 0
			s.repnfev = 0
			For k = 0 To n - 1
				For k2 = 0 To n - 1
					s.p(k, k2) = [Double].NaN
				Next
			Next

			'
			' Generate "effective" weights for prediction and calculate preconditioner
			'
			For i = 0 To n - 1
				If CDbl(s.pw(i)) = CDbl(0) Then
					v = 0
					k = 0
					For j = 0 To npairs - 1
						If CDbl(s.data(j, n + i)) <> CDbl(0) Then
							v = v + s.data(j, n + i)
							k = k + 1
						End If
					Next
					If k <> 0 Then
						s.effectivew(i) = k / v
					Else
						s.effectivew(i) = 1.0
					End If
				Else
					s.effectivew(i) = s.pw(i)
				End If
			Next
			For i = 0 To n - 1
				For j = 0 To n - 1
					s.h(i * n + j) = 2 * s.regterm
				Next
			Next
			For k = 0 To npairs - 1
				For i = 0 To n - 1
					For j = 0 To n - 1
						s.h(i * n + j) = s.h(i * n + j) + 2 * Math.sqr(s.effectivew(i)) * Math.sqr(s.data(k, j))
					Next
				Next
			Next
			For i = 0 To n - 1
				For j = 0 To n - 1
					If CDbl(s.h(i * n + j)) = CDbl(0) Then
						s.h(i * n + j) = 1
					End If
				Next
			Next

			'
			' Generate "effective" BndL/BndU
			'
			For i = 0 To n - 1
				For j = 0 To n - 1

					'
					' Set default boundary constraints.
					' Lower bound is always zero, upper bound is calculated
					' with respect to entry/exit states.
					'
					s.effectivebndl(i * n + j) = 0.0
					If s.states(i) > 0 OrElse s.states(j) < 0 Then
						s.effectivebndu(i * n + j) = 0.0
					Else
						s.effectivebndu(i * n + j) = 1.0
					End If

					'
					' Calculate intersection of the default and user-specified bound constraints.
					' This code checks consistency of such combination.
					'
					If Math.isfinite(s.bndl(i, j)) AndAlso CDbl(s.bndl(i, j)) > CDbl(s.effectivebndl(i * n + j)) Then
						s.effectivebndl(i * n + j) = s.bndl(i, j)
					End If
					If Math.isfinite(s.bndu(i, j)) AndAlso CDbl(s.bndu(i, j)) < CDbl(s.effectivebndu(i * n + j)) Then
						s.effectivebndu(i * n + j) = s.bndu(i, j)
					End If
					If CDbl(s.effectivebndl(i * n + j)) > CDbl(s.effectivebndu(i * n + j)) Then
						s.repterminationtype = -3
						Return
					End If

					'
					' Calculate intersection of the effective bound constraints
					' and user-specified equality constraints.
					' This code checks consistency of such combination.
					'
					If Math.isfinite(s.ec(i, j)) Then
						If CDbl(s.ec(i, j)) < CDbl(s.effectivebndl(i * n + j)) OrElse CDbl(s.ec(i, j)) > CDbl(s.effectivebndu(i * n + j)) Then
							s.repterminationtype = -3
							Return
						End If
						s.effectivebndl(i * n + j) = s.ec(i, j)
						s.effectivebndu(i * n + j) = s.ec(i, j)
					End If
				Next
			Next

			'
			' Generate linear constraints:
			' * "default" sums-to-one constraints (not generated for "exit" states)
			'
			apserv.rmatrixsetlengthatleast(s.effectivec, s.ccnt + n, n * n + 1)
			apserv.ivectorsetlengthatleast(s.effectivect, s.ccnt + n)
			ccnt = s.ccnt
			For i = 0 To s.ccnt - 1
				For j = 0 To n * n
					s.effectivec(i, j) = s.c(i, j)
				Next
				s.effectivect(i) = s.ct(i)
			Next
			For i = 0 To n - 1
				If s.states(i) >= 0 Then
					For k = 0 To n * n - 1
						s.effectivec(ccnt, k) = 0
					Next
					For k = 0 To n - 1
						s.effectivec(ccnt, k * n + i) = 1
					Next
					s.effectivec(ccnt, n * n) = 1.0
					s.effectivect(ccnt) = 0
					ccnt = ccnt + 1
				End If
			Next

			'
			' create optimizer
			'
			For i = 0 To n - 1
				For j = 0 To n - 1
					s.tmpp(i * n + j) = CDbl(1) / CDbl(n)
				Next
			Next
			minbleic.minbleicrestartfrom(s.bs, s.tmpp)
			minbleic.minbleicsetbc(s.bs, s.effectivebndl, s.effectivebndu)
			minbleic.minbleicsetlc(s.bs, s.effectivec, s.effectivect, ccnt)
			minbleic.minbleicsetcond(s.bs, 0.0, 0.0, xtol, 0)
			minbleic.minbleicsetprecdiag(s.bs, s.h)

			'
			' solve problem
			'
			While minbleic.minbleiciteration(s.bs)
				alglib.ap.assert(s.bs.needfg, "MCPDSolve: internal error")
				If s.bs.needfg Then

					'
					' Calculate regularization term
					'
					s.bs.f = 0.0
					vv = s.regterm
					For i = 0 To n - 1
						For j = 0 To n - 1
							s.bs.f = s.bs.f + vv * Math.sqr(s.bs.x(i * n + j) - s.priorp(i, j))
							s.bs.g(i * n + j) = 2 * vv * (s.bs.x(i * n + j) - s.priorp(i, j))
						Next
					Next

					'
					' calculate prediction error/gradient for K-th pair
					'
					For k = 0 To npairs - 1
						For i = 0 To n - 1
							i1_ = (0) - (i * n)
							v = 0.0
							For i_ = i * n To i * n + n - 1
								v += s.bs.x(i_) * s.data(k, i_ + i1_)
							Next
							vv = s.effectivew(i)
							s.bs.f = s.bs.f + Math.sqr(vv * (v - s.data(k, n + i)))
							For j = 0 To n - 1
								s.bs.g(i * n + j) = s.bs.g(i * n + j) + 2 * vv * vv * (v - s.data(k, n + i)) * s.data(k, j)
							Next
						Next
					Next

					'
					' continue
					'
					Continue While
				End If
			End While
			minbleic.minbleicresultsbuf(s.bs, s.tmpp, s.br)
			For i = 0 To n - 1
				For j = 0 To n - 1
					s.p(i, j) = s.tmpp(i * n + j)
				Next
			Next
			s.repterminationtype = s.br.terminationtype
			s.repinneriterationscount = s.br.inneriterationscount
			s.repouteriterationscount = s.br.outeriterationscount
			s.repnfev = s.br.nfev
		End Sub


		'************************************************************************
'        MCPD results
'
'        INPUT PARAMETERS:
'            State   -   algorithm state
'
'        OUTPUT PARAMETERS:
'            P       -   array[N,N], transition matrix
'            Rep     -   optimization report. You should check Rep.TerminationType
'                        in  order  to  distinguish  successful  termination  from
'                        unsuccessful one. Speaking short, positive values  denote
'                        success, negative ones are failures.
'                        More information about fields of this  structure  can  be
'                        found in the comments on MCPDReport datatype.
'
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mcpdresults(s As mcpdstate, ByRef p As Double(,), rep As mcpdreport)
			Dim i As Integer = 0
			Dim j As Integer = 0

			p = New Double(-1, -1) {}

			p = New Double(s.n - 1, s.n - 1) {}
			For i = 0 To s.n - 1
				For j = 0 To s.n - 1
					p(i, j) = s.p(i, j)
				Next
			Next
			rep.terminationtype = s.repterminationtype
			rep.inneriterationscount = s.repinneriterationscount
			rep.outeriterationscount = s.repouteriterationscount
			rep.nfev = s.repnfev
		End Sub


		'************************************************************************
'        Internal initialization function
'
'          -- ALGLIB --
'             Copyright 23.05.2010 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mcpdinit(n As Integer, entrystate As Integer, exitstate As Integer, s As mcpdstate)
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(n >= 1, "MCPDCreate: N<1")
			s.n = n
			s.states = New Integer(n - 1) {}
			For i = 0 To n - 1
				s.states(i) = 0
			Next
			If entrystate >= 0 Then
				s.states(entrystate) = 1
			End If
			If exitstate >= 0 Then
				s.states(exitstate) = -1
			End If
			s.npairs = 0
			s.regterm = 1E-08
			s.ccnt = 0
			s.p = New Double(n - 1, n - 1) {}
			s.ec = New Double(n - 1, n - 1) {}
			s.bndl = New Double(n - 1, n - 1) {}
			s.bndu = New Double(n - 1, n - 1) {}
			s.pw = New Double(n - 1) {}
			s.priorp = New Double(n - 1, n - 1) {}
			s.tmpp = New Double(n * n - 1) {}
			s.effectivew = New Double(n - 1) {}
			s.effectivebndl = New Double(n * n - 1) {}
			s.effectivebndu = New Double(n * n - 1) {}
			s.h = New Double(n * n - 1) {}
			For i = 0 To n - 1
				For j = 0 To n - 1
					s.p(i, j) = 0.0
					s.priorp(i, j) = 0.0
					s.bndl(i, j) = [Double].NegativeInfinity
					s.bndu(i, j) = [Double].PositiveInfinity
					s.ec(i, j) = [Double].NaN
				Next
				s.pw(i) = 0.0
				s.priorp(i, i) = 1.0
			Next
			s.data = New Double(0, 2 * n - 1) {}
			For i = 0 To 2 * n - 1
				s.data(0, i) = 0.0
			Next
			For i = 0 To n * n - 1
				s.tmpp(i) = 0.0
			Next
			minbleic.minbleiccreate(n * n, s.tmpp, s.bs)
		End Sub


	End Class
	Public Class mlpe
		'************************************************************************
'        Neural networks ensemble
'        ************************************************************************

		Public Class mlpensemble
			Inherits apobject
			Public ensemblesize As Integer
			Public weights As Double()
			Public columnmeans As Double()
			Public columnsigmas As Double()
			Public network As mlpbase.multilayerperceptron
			Public y As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				weights = New Double(-1) {}
				columnmeans = New Double(-1) {}
				columnsigmas = New Double(-1) {}
				network = New mlpbase.multilayerperceptron()
				y = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlpensemble()
				_result.ensemblesize = ensemblesize
				_result.weights = DirectCast(weights.Clone(), Double())
				_result.columnmeans = DirectCast(columnmeans.Clone(), Double())
				_result.columnsigmas = DirectCast(columnsigmas.Clone(), Double())
				_result.network = DirectCast(network.make_copy(), mlpbase.multilayerperceptron)
				_result.y = DirectCast(y.Clone(), Double())
				Return _result
			End Function
		End Class




		Public Const mlpefirstversion As Integer = 1


		'************************************************************************
'        Like MLPCreate0, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreate0(nin As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreate0(nin, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreate1, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreate1(nin As Integer, nhid As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreate1(nin, nhid, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreate2, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreate2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreate2(nin, nhid1, nhid2, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateB0, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreateb0(nin As Integer, nout As Integer, b As Double, d As Double, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreateb0(nin, nout, b, d, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateB1, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreateb1(nin As Integer, nhid As Integer, nout As Integer, b As Double, d As Double, ensemblesize As Integer, _
			ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreateb1(nin, nhid, nout, b, d, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateB2, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreateb2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, b As Double, d As Double, _
			ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreateb2(nin, nhid1, nhid2, nout, b, d, _
				net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateR0, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreater0(nin As Integer, nout As Integer, a As Double, b As Double, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreater0(nin, nout, a, b, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateR1, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreater1(nin As Integer, nhid As Integer, nout As Integer, a As Double, b As Double, ensemblesize As Integer, _
			ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreater1(nin, nhid, nout, a, b, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateR2, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreater2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, a As Double, b As Double, _
			ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreater2(nin, nhid1, nhid2, nout, a, b, _
				net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateC0, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreatec0(nin As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreatec0(nin, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateC1, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreatec1(nin As Integer, nhid As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreatec1(nin, nhid, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Like MLPCreateC2, but for ensembles.
'
'          -- ALGLIB --
'             Copyright 18.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreatec2(nin As Integer, nhid1 As Integer, nhid2 As Integer, nout As Integer, ensemblesize As Integer, ensemble As mlpensemble)
			Dim net As New mlpbase.multilayerperceptron()

			mlpbase.mlpcreatec2(nin, nhid1, nhid2, nout, net)
			mlpecreatefromnetwork(net, ensemblesize, ensemble)
		End Sub


		'************************************************************************
'        Creates ensemble from network. Only network geometry is copied.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecreatefromnetwork(network As mlpbase.multilayerperceptron, ensemblesize As Integer, ensemble As mlpensemble)
			Dim i As Integer = 0
			Dim ccount As Integer = 0
			Dim wcount As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			alglib.ap.assert(ensemblesize > 0, "MLPECreate: incorrect ensemble size!")

			'
			' Copy network
			'
			mlpbase.mlpcopy(network, ensemble.network)

			'
			' network properties
			'
			If mlpbase.mlpissoftmax(network) Then
				ccount = mlpbase.mlpgetinputscount(ensemble.network)
			Else
				ccount = mlpbase.mlpgetinputscount(ensemble.network) + mlpbase.mlpgetoutputscount(ensemble.network)
			End If
			wcount = mlpbase.mlpgetweightscount(ensemble.network)
			ensemble.ensemblesize = ensemblesize

			'
			' weights, means, sigmas
			'
			ensemble.weights = New Double(ensemblesize * wcount - 1) {}
			ensemble.columnmeans = New Double(ensemblesize * ccount - 1) {}
			ensemble.columnsigmas = New Double(ensemblesize * ccount - 1) {}
			For i = 0 To ensemblesize * wcount - 1
				ensemble.weights(i) = Math.randomreal() - 0.5
			Next
			For i = 0 To ensemblesize - 1
				i1_ = (0) - (i * ccount)
				For i_ = i * ccount To (i + 1) * ccount - 1
					ensemble.columnmeans(i_) = network.columnmeans(i_ + i1_)
				Next
				i1_ = (0) - (i * ccount)
				For i_ = i * ccount To (i + 1) * ccount - 1
					ensemble.columnsigmas(i_) = network.columnsigmas(i_ + i1_)
				Next
			Next

			'
			' temporaries, internal buffers
			'
			ensemble.y = New Double(mlpbase.mlpgetoutputscount(ensemble.network) - 1) {}
		End Sub


		'************************************************************************
'        Copying of MLPEnsemble strucure
'
'        INPUT PARAMETERS:
'            Ensemble1 -   original
'
'        OUTPUT PARAMETERS:
'            Ensemble2 -   copy
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpecopy(ensemble1 As mlpensemble, ensemble2 As mlpensemble)
			Dim ccount As Integer = 0
			Dim wcount As Integer = 0
			Dim i_ As Integer = 0


			'
			' Unload info
			'
			If mlpbase.mlpissoftmax(ensemble1.network) Then
				ccount = mlpbase.mlpgetinputscount(ensemble1.network)
			Else
				ccount = mlpbase.mlpgetinputscount(ensemble1.network) + mlpbase.mlpgetoutputscount(ensemble1.network)
			End If
			wcount = mlpbase.mlpgetweightscount(ensemble1.network)

			'
			' Allocate space
			'
			ensemble2.weights = New Double(ensemble1.ensemblesize * wcount - 1) {}
			ensemble2.columnmeans = New Double(ensemble1.ensemblesize * ccount - 1) {}
			ensemble2.columnsigmas = New Double(ensemble1.ensemblesize * ccount - 1) {}
			ensemble2.y = New Double(mlpbase.mlpgetoutputscount(ensemble1.network) - 1) {}

			'
			' Copy
			'
			ensemble2.ensemblesize = ensemble1.ensemblesize
			For i_ = 0 To ensemble1.ensemblesize * wcount - 1
				ensemble2.weights(i_) = ensemble1.weights(i_)
			Next
			For i_ = 0 To ensemble1.ensemblesize * ccount - 1
				ensemble2.columnmeans(i_) = ensemble1.columnmeans(i_)
			Next
			For i_ = 0 To ensemble1.ensemblesize * ccount - 1
				ensemble2.columnsigmas(i_) = ensemble1.columnsigmas(i_)
			Next
			mlpbase.mlpcopy(ensemble1.network, ensemble2.network)
		End Sub


		'************************************************************************
'        Randomization of MLP ensemble
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlperandomize(ensemble As mlpensemble)
			Dim i As Integer = 0
			Dim wcount As Integer = 0

			wcount = mlpbase.mlpgetweightscount(ensemble.network)
			For i = 0 To ensemble.ensemblesize * wcount - 1
				ensemble.weights(i) = Math.randomreal() - 0.5
			Next
		End Sub


		'************************************************************************
'        Return ensemble properties (number of inputs and outputs).
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeproperties(ensemble As mlpensemble, ByRef nin As Integer, ByRef nout As Integer)
			nin = 0
			nout = 0

			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
		End Sub


		'************************************************************************
'        Return normalization type (whether ensemble is SOFTMAX-normalized or not).
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpeissoftmax(ensemble As mlpensemble) As Boolean
			Dim result As New Boolean()

			result = mlpbase.mlpissoftmax(ensemble.network)
			Return result
		End Function


		'************************************************************************
'        Procesing
'
'        INPUT PARAMETERS:
'            Ensemble-   neural networks ensemble
'            X       -   input vector,  array[0..NIn-1].
'            Y       -   (possibly) preallocated buffer; if size of Y is less than
'                        NOut, it will be reallocated. If it is large enough, it
'                        is NOT reallocated, so we can save some time on reallocation.
'
'
'        OUTPUT PARAMETERS:
'            Y       -   result. Regression estimate when solving regression  task,
'                        vector of posterior probabilities for classification task.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeprocess(ensemble As mlpensemble, x As Double(), ByRef y As Double())
			Dim i As Integer = 0
			Dim es As Integer = 0
			Dim wc As Integer = 0
			Dim cc As Integer = 0
			Dim v As Double = 0
			Dim nout As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			If alglib.ap.len(y) < mlpbase.mlpgetoutputscount(ensemble.network) Then
				y = New Double(mlpbase.mlpgetoutputscount(ensemble.network) - 1) {}
			End If
			es = ensemble.ensemblesize
			wc = mlpbase.mlpgetweightscount(ensemble.network)
			If mlpbase.mlpissoftmax(ensemble.network) Then
				cc = mlpbase.mlpgetinputscount(ensemble.network)
			Else
				cc = mlpbase.mlpgetinputscount(ensemble.network) + mlpbase.mlpgetoutputscount(ensemble.network)
			End If
			v = CDbl(1) / CDbl(es)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			For i = 0 To nout - 1
				y(i) = 0
			Next
			For i = 0 To es - 1
				i1_ = (i * wc) - (0)
				For i_ = 0 To wc - 1
					ensemble.network.weights(i_) = ensemble.weights(i_ + i1_)
				Next
				i1_ = (i * cc) - (0)
				For i_ = 0 To cc - 1
					ensemble.network.columnmeans(i_) = ensemble.columnmeans(i_ + i1_)
				Next
				i1_ = (i * cc) - (0)
				For i_ = 0 To cc - 1
					ensemble.network.columnsigmas(i_) = ensemble.columnsigmas(i_ + i1_)
				Next
				mlpbase.mlpprocess(ensemble.network, x, ensemble.y)
				For i_ = 0 To nout - 1
					y(i_) = y(i_) + v * ensemble.y(i_)
				Next
			Next
		End Sub


		'************************************************************************
'        'interactive'  variant  of  MLPEProcess  for  languages  like Python which
'        support constructs like "Y = MLPEProcess(LM,X)" and interactive mode of the
'        interpreter
'
'        This function allocates new array on each call,  so  it  is  significantly
'        slower than its 'non-interactive' counterpart, but it is  more  convenient
'        when you call it from command line.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeprocessi(ensemble As mlpensemble, x As Double(), ByRef y As Double())
			y = New Double(-1) {}

			mlpeprocess(ensemble, x, y)
		End Sub


		'************************************************************************
'        Calculation of all types of errors
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeallerrorsx(ensemble As mlpensemble, densexy As Double(,), sparsexy As sparse.sparsematrix, datasetsize As Integer, datasettype As Integer, idx As Integer(), _
			subset0 As Integer, subset1 As Integer, subsettype As Integer, buf As alglib.smp.shared_pool, rep As mlpbase.modelerrors)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim iscls As New Boolean()
			Dim srcidx As Integer = 0
			Dim pbuf As hpccores.mlpbuffers = Nothing
			Dim rep0 As New mlpbase.modelerrors()
			Dim rep1 As New mlpbase.modelerrors()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0


			'
			' Get network information
			'
			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			iscls = mlpbase.mlpissoftmax(ensemble.network)

			'
			' Retrieve buffer, prepare, process data, recycle buffer
			'
			alglib.smp.ae_shared_pool_retrieve(buf, pbuf)
			If iscls Then
				bdss.dserrallocate(nout, pbuf.tmp0)
			Else
				bdss.dserrallocate(-nout, pbuf.tmp0)
			End If
			apserv.rvectorsetlengthatleast(pbuf.x, nin)
			apserv.rvectorsetlengthatleast(pbuf.y, nout)
			apserv.rvectorsetlengthatleast(pbuf.desiredy, nout)
			For i = subset0 To subset1 - 1
				srcidx = -1
				If subsettype = 0 Then
					srcidx = i
				End If
				If subsettype = 1 Then
					srcidx = idx(i)
				End If
				alglib.ap.assert(srcidx >= 0, "MLPEAllErrorsX: internal error")
				If datasettype = 0 Then
					For i_ = 0 To nin - 1
						pbuf.x(i_) = densexy(srcidx, i_)
					Next
				End If
				If datasettype = 1 Then
					sparse.sparsegetrow(sparsexy, srcidx, pbuf.x)
				End If
				mlpeprocess(ensemble, pbuf.x, pbuf.y)
				If mlpbase.mlpissoftmax(ensemble.network) Then
					If datasettype = 0 Then
						pbuf.desiredy(0) = densexy(srcidx, nin)
					End If
					If datasettype = 1 Then
						pbuf.desiredy(0) = sparse.sparseget(sparsexy, srcidx, nin)
					End If
				Else
					If datasettype = 0 Then
						i1_ = (nin) - (0)
						For i_ = 0 To nout - 1
							pbuf.desiredy(i_) = densexy(srcidx, i_ + i1_)
						Next
					End If
					If datasettype = 1 Then
						For j = 0 To nout - 1
							pbuf.desiredy(j) = sparse.sparseget(sparsexy, srcidx, nin + j)
						Next
					End If
				End If
				bdss.dserraccumulate(pbuf.tmp0, pbuf.y, pbuf.desiredy)
			Next
			bdss.dserrfinish(pbuf.tmp0)
			rep.relclserror = pbuf.tmp0(0)
			rep.avgce = pbuf.tmp0(1) / System.Math.Log(2)
			rep.rmserror = pbuf.tmp0(2)
			rep.avgerror = pbuf.tmp0(3)
			rep.avgrelerror = pbuf.tmp0(4)
			alglib.smp.ae_shared_pool_recycle(buf, pbuf)
		End Sub


		'************************************************************************
'        Calculation of all types of errors on dataset given by sparse matrix
'
'          -- ALGLIB --
'             Copyright 10.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeallerrorssparse(ensemble As mlpensemble, xy As sparse.sparsematrix, npoints As Integer, ByRef relcls As Double, ByRef avgce As Double, ByRef rms As Double, _
			ByRef avg As Double, ByRef avgrel As Double)
			Dim i As Integer = 0
			Dim buf As Double() = New Double(-1) {}
			Dim workx As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim dy As Double() = New Double(-1) {}
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			relcls = 0
			avgce = 0
			rms = 0
			avg = 0
			avgrel = 0

			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			If mlpbase.mlpissoftmax(ensemble.network) Then
				dy = New Double(0) {}
				bdss.dserrallocate(nout, buf)
			Else
				dy = New Double(nout - 1) {}
				bdss.dserrallocate(-nout, buf)
			End If
			For i = 0 To npoints - 1
				sparse.sparsegetrow(xy, i, workx)
				mlpeprocess(ensemble, workx, y)
				If mlpbase.mlpissoftmax(ensemble.network) Then
					dy(0) = workx(nin)
				Else
					i1_ = (nin) - (0)
					For i_ = 0 To nout - 1
						dy(i_) = workx(i_ + i1_)
					Next
				End If
				bdss.dserraccumulate(buf, y, dy)
			Next
			bdss.dserrfinish(buf)
			relcls = buf(0)
			avgce = buf(1)
			rms = buf(2)
			avg = buf(3)
			avgrel = buf(4)
		End Sub


		'************************************************************************
'        Relative classification error on the test set
'
'        INPUT PARAMETERS:
'            Ensemble-   ensemble
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            percent of incorrectly classified cases.
'            Works both for classifier betwork and for regression networks which
'        are used as classifiers.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlperelclserror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim rep As New mlpbase.modelerrors()

			mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, rep)
			result = rep.relclserror
			Return result
		End Function


		'************************************************************************
'        Average cross-entropy (in bits per element) on the test set
'
'        INPUT PARAMETERS:
'            Ensemble-   ensemble
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            CrossEntropy/(NPoints*LN(2)).
'            Zero if ensemble solves regression task.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpeavgce(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim rep As New mlpbase.modelerrors()

			mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, rep)
			result = rep.avgce
			Return result
		End Function


		'************************************************************************
'        RMS error on the test set
'
'        INPUT PARAMETERS:
'            Ensemble-   ensemble
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            root mean square error.
'            Its meaning for regression task is obvious. As for classification task
'        RMS error means error when estimating posterior probabilities.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpermserror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim rep As New mlpbase.modelerrors()

			mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, rep)
			result = rep.rmserror
			Return result
		End Function


		'************************************************************************
'        Average error on the test set
'
'        INPUT PARAMETERS:
'            Ensemble-   ensemble
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            Its meaning for regression task is obvious. As for classification task
'        it means average error when estimating posterior probabilities.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpeavgerror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim rep As New mlpbase.modelerrors()

			mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, rep)
			result = rep.avgerror
			Return result
		End Function


		'************************************************************************
'        Average relative error on the test set
'
'        INPUT PARAMETERS:
'            Ensemble-   ensemble
'            XY      -   test set
'            NPoints -   test set size
'
'        RESULT:
'            Its meaning for regression task is obvious. As for classification task
'        it means average relative error when estimating posterior probabilities.
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpeavgrelerror(ensemble As mlpensemble, xy As Double(,), npoints As Integer) As Double
			Dim result As Double = 0
			Dim rep As New mlpbase.modelerrors()

			mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, rep)
			result = rep.avgrelerror
			Return result
		End Function


		'************************************************************************
'        Serializer: allocation
'
'          -- ALGLIB --
'             Copyright 19.10.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpealloc(s As alglib.serializer, ensemble As mlpensemble)
			s.alloc_entry()
			s.alloc_entry()
			s.alloc_entry()
			apserv.allocrealarray(s, ensemble.weights, -1)
			apserv.allocrealarray(s, ensemble.columnmeans, -1)
			apserv.allocrealarray(s, ensemble.columnsigmas, -1)
			mlpbase.mlpalloc(s, ensemble.network)
		End Sub


		'************************************************************************
'        Serializer: serialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeserialize(s As alglib.serializer, ensemble As mlpensemble)
			s.serialize_int(scodes.getmlpeserializationcode())
			s.serialize_int(mlpefirstversion)
			s.serialize_int(ensemble.ensemblesize)
			apserv.serializerealarray(s, ensemble.weights, -1)
			apserv.serializerealarray(s, ensemble.columnmeans, -1)
			apserv.serializerealarray(s, ensemble.columnsigmas, -1)
			mlpbase.mlpserialize(s, ensemble.network)
		End Sub


		'************************************************************************
'        Serializer: unserialization
'
'          -- ALGLIB --
'             Copyright 14.03.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpeunserialize(s As alglib.serializer, ensemble As mlpensemble)
			Dim i0 As Integer = 0
			Dim i1 As Integer = 0


			'
			' check correctness of header
			'
			i0 = s.unserialize_int()
			alglib.ap.assert(i0 = scodes.getmlpeserializationcode(), "MLPEUnserialize: stream header corrupted")
			i1 = s.unserialize_int()
			alglib.ap.assert(i1 = mlpefirstversion, "MLPEUnserialize: stream header corrupted")

			'
			' Create network
			'
			ensemble.ensemblesize = s.unserialize_int()
			apserv.unserializerealarray(s, ensemble.weights)
			apserv.unserializerealarray(s, ensemble.columnmeans)
			apserv.unserializerealarray(s, ensemble.columnsigmas)
			mlpbase.mlpunserialize(s, ensemble.network)

			'
			' Allocate termoraries
			'
			ensemble.y = New Double(mlpbase.mlpgetoutputscount(ensemble.network) - 1) {}
		End Sub


	End Class
	Public Class mlptrain
		'************************************************************************
'        Training report:
'            * RelCLSError   -   fraction of misclassified cases.
'            * AvgCE         -   acerage cross-entropy
'            * RMSError      -   root-mean-square error
'            * AvgError      -   average error
'            * AvgRelError   -   average relative error
'            * NGrad         -   number of gradient calculations
'            * NHess         -   number of Hessian calculations
'            * NCholesky     -   number of Cholesky decompositions
'            
'        NOTE 1: RelCLSError/AvgCE are zero on regression problems.
'
'        NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
'                errors in prediction of posterior probabilities
'        ************************************************************************

		Public Class mlpreport
			Inherits apobject
			Public relclserror As Double
			Public avgce As Double
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public ngrad As Integer
			Public nhess As Integer
			Public ncholesky As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlpreport()
				_result.relclserror = relclserror
				_result.avgce = avgce
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				_result.ngrad = ngrad
				_result.nhess = nhess
				_result.ncholesky = ncholesky
				Return _result
			End Function
		End Class


		'************************************************************************
'        Cross-validation estimates of generalization error
'        ************************************************************************

		Public Class mlpcvreport
			Inherits apobject
			Public relclserror As Double
			Public avgce As Double
			Public rmserror As Double
			Public avgerror As Double
			Public avgrelerror As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlpcvreport()
				_result.relclserror = relclserror
				_result.avgce = avgce
				_result.rmserror = rmserror
				_result.avgerror = avgerror
				_result.avgrelerror = avgrelerror
				Return _result
			End Function
		End Class


		'************************************************************************
'        Temporary data structures used by following functions:
'        * TrainNetworkX
'        * StartTrainingX
'        * ContinueTrainingX
'
'        This structure contains:
'        * network being trained
'        * fully initialized LBFGS optimizer (we have to call MinLBFGSRestartFrom()
'          before using it; usually it is done by StartTrainingX() function).
'        * additional temporary arrays
'          
'        This structure should be initialized with InitMLPTrnSession() call.
'        ************************************************************************

		Public Class smlptrnsession
			Inherits apobject
			Public bestparameters As Double()
			Public bestrmserror As Double
			Public randomizenetwork As Boolean
			Public network As mlpbase.multilayerperceptron
			Public optimizer As minlbfgs.minlbfgsstate
			Public optimizerrep As minlbfgs.minlbfgsreport
			Public wbuf0 As Double()
			Public wbuf1 As Double()
			Public allminibatches As Integer()
			Public currentminibatch As Integer()
			Public rstate As rcommstate
			Public algoused As Integer
			Public minibatchsize As Integer
			Public generator As hqrnd.hqrndstate
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				bestparameters = New Double(-1) {}
				network = New mlpbase.multilayerperceptron()
				optimizer = New minlbfgs.minlbfgsstate()
				optimizerrep = New minlbfgs.minlbfgsreport()
				wbuf0 = New Double(-1) {}
				wbuf1 = New Double(-1) {}
				allminibatches = New Integer(-1) {}
				currentminibatch = New Integer(-1) {}
				rstate = New rcommstate()
				generator = New hqrnd.hqrndstate()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New smlptrnsession()
				_result.bestparameters = DirectCast(bestparameters.Clone(), Double())
				_result.bestrmserror = bestrmserror
				_result.randomizenetwork = randomizenetwork
				_result.network = DirectCast(network.make_copy(), mlpbase.multilayerperceptron)
				_result.optimizer = DirectCast(optimizer.make_copy(), minlbfgs.minlbfgsstate)
				_result.optimizerrep = DirectCast(optimizerrep.make_copy(), minlbfgs.minlbfgsreport)
				_result.wbuf0 = DirectCast(wbuf0.Clone(), Double())
				_result.wbuf1 = DirectCast(wbuf1.Clone(), Double())
				_result.allminibatches = DirectCast(allminibatches.Clone(), Integer())
				_result.currentminibatch = DirectCast(currentminibatch.Clone(), Integer())
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.algoused = algoused
				_result.minibatchsize = minibatchsize
				_result.generator = DirectCast(generator.make_copy(), hqrnd.hqrndstate)
				Return _result
			End Function
		End Class


		'************************************************************************
'        Temporary data structures used by following functions:
'        * TrainEnsembleX
'
'        This structure contains:
'        * two arrays which can be used to store training and validation subsets
'        * sessions for MLP training
'
'        This structure should be initialized with InitMLPETrnSession() call.
'        ************************************************************************

		Public Class mlpetrnsession
			Inherits apobject
			Public trnsubset As Integer()
			Public valsubset As Integer()
			Public mlpsessions As alglib.smp.shared_pool
			Public mlprep As mlpreport
			Public network As mlpbase.multilayerperceptron
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				trnsubset = New Integer(-1) {}
				valsubset = New Integer(-1) {}
				mlpsessions = New alglib.smp.shared_pool()
				mlprep = New mlpreport()
				network = New mlpbase.multilayerperceptron()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlpetrnsession()
				_result.trnsubset = DirectCast(trnsubset.Clone(), Integer())
				_result.valsubset = DirectCast(valsubset.Clone(), Integer())
				_result.mlpsessions = DirectCast(mlpsessions.make_copy(), alglib.smp.shared_pool)
				_result.mlprep = DirectCast(mlprep.make_copy(), mlpreport)
				_result.network = DirectCast(network.make_copy(), mlpbase.multilayerperceptron)
				Return _result
			End Function
		End Class


		'************************************************************************
'        Trainer object for neural network.
'
'        You should not try to access fields of this object directly -  use  ALGLIB
'        functions to work with this object.
'        ************************************************************************

		Public Class mlptrainer
			Inherits apobject
			Public nin As Integer
			Public nout As Integer
			Public rcpar As Boolean
			Public lbfgsfactor As Integer
			Public decay As Double
			Public wstep As Double
			Public maxits As Integer
			Public datatype As Integer
			Public npoints As Integer
			Public densexy As Double(,)
			Public sparsexy As sparse.sparsematrix
			Public session As smlptrnsession
			Public ngradbatch As Integer
			Public subset As Integer()
			Public subsetsize As Integer
			Public valsubset As Integer()
			Public valsubsetsize As Integer
			Public algokind As Integer
			Public minibatchsize As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				densexy = New Double(-1, -1) {}
				sparsexy = New sparse.sparsematrix()
				session = New smlptrnsession()
				subset = New Integer(-1) {}
				valsubset = New Integer(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlptrainer()
				_result.nin = nin
				_result.nout = nout
				_result.rcpar = rcpar
				_result.lbfgsfactor = lbfgsfactor
				_result.decay = decay
				_result.wstep = wstep
				_result.maxits = maxits
				_result.datatype = datatype
				_result.npoints = npoints
				_result.densexy = DirectCast(densexy.Clone(), Double(,))
				_result.sparsexy = DirectCast(sparsexy.make_copy(), sparse.sparsematrix)
				_result.session = DirectCast(session.make_copy(), smlptrnsession)
				_result.ngradbatch = ngradbatch
				_result.subset = DirectCast(subset.Clone(), Integer())
				_result.subsetsize = subsetsize
				_result.valsubset = DirectCast(valsubset.Clone(), Integer())
				_result.valsubsetsize = valsubsetsize
				_result.algokind = algokind
				_result.minibatchsize = minibatchsize
				Return _result
			End Function
		End Class


		'************************************************************************
'        Internal record for parallelization function MLPFoldCV.
'        ************************************************************************

		Public Class mlpparallelizationcv
			Inherits apobject
			Public network As mlpbase.multilayerperceptron
			Public rep As mlpreport
			Public subset As Integer()
			Public subsetsize As Integer
			Public xyrow As Double()
			Public y As Double()
			Public ngrad As Integer
			Public trnpool As alglib.smp.shared_pool
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				network = New mlpbase.multilayerperceptron()
				rep = New mlpreport()
				subset = New Integer(-1) {}
				xyrow = New Double(-1) {}
				y = New Double(-1) {}
				trnpool = New alglib.smp.shared_pool()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mlpparallelizationcv()
				_result.network = DirectCast(network.make_copy(), mlpbase.multilayerperceptron)
				_result.rep = DirectCast(rep.make_copy(), mlpreport)
				_result.subset = DirectCast(subset.Clone(), Integer())
				_result.subsetsize = subsetsize
				_result.xyrow = DirectCast(xyrow.Clone(), Double())
				_result.y = DirectCast(y.Clone(), Double())
				_result.ngrad = ngrad
				_result.trnpool = DirectCast(trnpool.make_copy(), alglib.smp.shared_pool)
				Return _result
			End Function
		End Class




		Public Const mindecay As Double = 0.001
		Public Const defaultlbfgsfactor As Integer = 6


		'************************************************************************
'        Neural network training  using  modified  Levenberg-Marquardt  with  exact
'        Hessian calculation and regularization. Subroutine trains  neural  network
'        with restarts from random positions. Algorithm is well  suited  for  small
'        and medium scale problems (hundreds of weights).
'
'        INPUT PARAMETERS:
'            Network     -   neural network with initialized geometry
'            XY          -   training set
'            NPoints     -   training set size
'            Decay       -   weight decay constant, >=0.001
'                            Decay term 'Decay*||Weights||^2' is added to error
'                            function.
'                            If you don't know what Decay to choose, use 0.001.
'            Restarts    -   number of restarts from random position, >0.
'                            If you don't know what Restarts to choose, use 2.
'
'        OUTPUT PARAMETERS:
'            Network     -   trained neural network.
'            Info        -   return code:
'                            * -9, if internal matrix inverse subroutine failed
'                            * -2, if there is a point with class number
'                                  outside of [0..NOut-1].
'                            * -1, if wrong parameters specified
'                                  (NPoints<0, Restarts<1).
'                            *  2, if task has been solved.
'            Rep         -   training report
'
'          -- ALGLIB --
'             Copyright 10.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlptrainlm(network As mlpbase.multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
			rep As mlpreport)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim lmsteptol As Double = 0
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim e As Double = 0
			Dim enew As Double = 0
			Dim xnorm2 As Double = 0
			Dim stepnorm As Double = 0
			Dim g As Double() = New Double(-1) {}
			Dim d As Double() = New Double(-1) {}
			Dim h As Double(,) = New Double(-1, -1) {}
			Dim hmod As Double(,) = New Double(-1, -1) {}
			Dim z As Double(,) = New Double(-1, -1) {}
			Dim spd As New Boolean()
			Dim nu As Double = 0
			Dim lambdav As Double = 0
			Dim lambdaup As Double = 0
			Dim lambdadown As Double = 0
			Dim internalrep As New minlbfgs.minlbfgsreport()
			Dim state As New minlbfgs.minlbfgsstate()
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim wbase As Double() = New Double(-1) {}
			Dim wdir As Double() = New Double(-1) {}
			Dim wt As Double() = New Double(-1) {}
			Dim wx As Double() = New Double(-1) {}
			Dim pass As Integer = 0
			Dim wbest As Double() = New Double(-1) {}
			Dim ebest As Double = 0
			Dim invinfo As Integer = 0
			Dim invrep As New matinv.matinvreport()
			Dim solverinfo As Integer = 0
			Dim solverrep As New densesolver.densesolverreport()
			Dim i_ As Integer = 0

			info = 0

			mlpbase.mlpproperties(network, nin, nout, wcount)
			lambdaup = 10
			lambdadown = 0.3
			lmsteptol = 0.001

			'
			' Test for inputs
			'
			If npoints <= 0 OrElse restarts < 1 Then
				info = -1
				Return
			End If
			If mlpbase.mlpissoftmax(network) Then
				For i = 0 To npoints - 1
					If CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
			End If
			decay = System.Math.Max(decay, mindecay)
			info = 2

			'
			' Initialize data
			'
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0

			'
			' General case.
			' Prepare task and network. Allocate space.
			'
			mlpbase.mlpinitpreprocessor(network, xy, npoints)
			g = New Double(wcount - 1) {}
			h = New Double(wcount - 1, wcount - 1) {}
			hmod = New Double(wcount - 1, wcount - 1) {}
			wbase = New Double(wcount - 1) {}
			wdir = New Double(wcount - 1) {}
			wbest = New Double(wcount - 1) {}
			wt = New Double(wcount - 1) {}
			wx = New Double(wcount - 1) {}
			ebest = Math.maxrealnumber

			'
			' Multiple passes
			'
			For pass = 1 To restarts

				'
				' Initialize weights
				'
				mlpbase.mlprandomize(network)

				'
				' First stage of the hybrid algorithm: LBFGS
				'
				For i_ = 0 To wcount - 1
					wbase(i_) = network.weights(i_)
				Next
				minlbfgs.minlbfgscreate(wcount, System.Math.Min(wcount, 5), wbase, state)
				minlbfgs.minlbfgssetcond(state, 0, 0, 0, System.Math.Max(25, wcount))
				While minlbfgs.minlbfgsiteration(state)

					'
					' gradient
					'
					For i_ = 0 To wcount - 1
						network.weights(i_) = state.x(i_)
					Next
					mlpbase.mlpgradbatch(network, xy, npoints, state.f, state.g)

					'
					' weight decay
					'
					v = 0.0
					For i_ = 0 To wcount - 1
						v += network.weights(i_) * network.weights(i_)
					Next
					state.f = state.f + 0.5 * decay * v
					For i_ = 0 To wcount - 1
						state.g(i_) = state.g(i_) + decay * network.weights(i_)
					Next

					'
					' next iteration
					'
					rep.ngrad = rep.ngrad + 1
				End While
				minlbfgs.minlbfgsresults(state, wbase, internalrep)
				For i_ = 0 To wcount - 1
					network.weights(i_) = wbase(i_)
				Next

				'
				' Second stage of the hybrid algorithm: LM
				'
				' Initialize H with identity matrix,
				' G with gradient,
				' E with regularized error.
				'
				mlpbase.mlphessianbatch(network, xy, npoints, e, g, h)
				v = 0.0
				For i_ = 0 To wcount - 1
					v += network.weights(i_) * network.weights(i_)
				Next
				e = e + 0.5 * decay * v
				For i_ = 0 To wcount - 1
					g(i_) = g(i_) + decay * network.weights(i_)
				Next
				For k = 0 To wcount - 1
					h(k, k) = h(k, k) + decay
				Next
				rep.nhess = rep.nhess + 1
				lambdav = 0.001
				nu = 2
				While True

					'
					' 1. HMod = H+lambda*I
					' 2. Try to solve (H+Lambda*I)*dx = -g.
					'    Increase lambda if left part is not positive definite.
					'
					For i = 0 To wcount - 1
						For i_ = 0 To wcount - 1
							hmod(i, i_) = h(i, i_)
						Next
						hmod(i, i) = hmod(i, i) + lambdav
					Next
					spd = trfac.spdmatrixcholesky(hmod, wcount, True)
					rep.ncholesky = rep.ncholesky + 1
					If Not spd Then
						lambdav = lambdav * lambdaup * nu
						nu = nu * 2
						Continue While
					End If
					densesolver.spdmatrixcholeskysolve(hmod, wcount, True, g, solverinfo, solverrep, _
						wdir)
					If solverinfo < 0 Then
						lambdav = lambdav * lambdaup * nu
						nu = nu * 2
						Continue While
					End If
					For i_ = 0 To wcount - 1
						wdir(i_) = -1 * wdir(i_)
					Next

					'
					' Lambda found.
					' 1. Save old w in WBase
					' 1. Test some stopping criterions
					' 2. If error(w+wdir)>error(w), increase lambda
					'
					For i_ = 0 To wcount - 1
						network.weights(i_) = network.weights(i_) + wdir(i_)
					Next
					xnorm2 = 0.0
					For i_ = 0 To wcount - 1
						xnorm2 += network.weights(i_) * network.weights(i_)
					Next
					stepnorm = 0.0
					For i_ = 0 To wcount - 1
						stepnorm += wdir(i_) * wdir(i_)
					Next
					stepnorm = System.Math.sqrt(stepnorm)
					enew = mlpbase.mlperror(network, xy, npoints) + 0.5 * decay * xnorm2
					If CDbl(stepnorm) < CDbl(lmsteptol * (1 + System.Math.sqrt(xnorm2))) Then
						Exit While
					End If
					If CDbl(enew) > CDbl(e) Then
						lambdav = lambdav * lambdaup * nu
						nu = nu * 2
						Continue While
					End If

					'
					' Optimize using inv(cholesky(H)) as preconditioner
					'
					matinv.rmatrixtrinverse(hmod, wcount, True, False, invinfo, invrep)
					If invinfo <= 0 Then

						'
						' if matrix can't be inverted then exit with errors
						' TODO: make WCount steps in direction suggested by HMod
						'
						info = -9
						Return
					End If
					For i_ = 0 To wcount - 1
						wbase(i_) = network.weights(i_)
					Next
					For i = 0 To wcount - 1
						wt(i) = 0
					Next
					minlbfgs.minlbfgscreatex(wcount, wcount, wt, 1, 0.0, state)
					minlbfgs.minlbfgssetcond(state, 0, 0, 0, 5)
					While minlbfgs.minlbfgsiteration(state)

						'
						' gradient
						'
						For i = 0 To wcount - 1
							v = 0.0
							For i_ = i To wcount - 1
								v += state.x(i_) * hmod(i, i_)
							Next
							network.weights(i) = wbase(i) + v
						Next
						mlpbase.mlpgradbatch(network, xy, npoints, state.f, g)
						For i = 0 To wcount - 1
							state.g(i) = 0
						Next
						For i = 0 To wcount - 1
							v = g(i)
							For i_ = i To wcount - 1
								state.g(i_) = state.g(i_) + v * hmod(i, i_)
							Next
						Next

						'
						' weight decay
						' grad(x'*x) = A'*(x0+A*t)
						'
						v = 0.0
						For i_ = 0 To wcount - 1
							v += network.weights(i_) * network.weights(i_)
						Next
						state.f = state.f + 0.5 * decay * v
						For i = 0 To wcount - 1
							v = decay * network.weights(i)
							For i_ = i To wcount - 1
								state.g(i_) = state.g(i_) + v * hmod(i, i_)
							Next
						Next

						'
						' next iteration
						'
						rep.ngrad = rep.ngrad + 1
					End While
					minlbfgs.minlbfgsresults(state, wt, internalrep)

					'
					' Accept new position.
					' Calculate Hessian
					'
					For i = 0 To wcount - 1
						v = 0.0
						For i_ = i To wcount - 1
							v += wt(i_) * hmod(i, i_)
						Next
						network.weights(i) = wbase(i) + v
					Next
					mlpbase.mlphessianbatch(network, xy, npoints, e, g, h)
					v = 0.0
					For i_ = 0 To wcount - 1
						v += network.weights(i_) * network.weights(i_)
					Next
					e = e + 0.5 * decay * v
					For i_ = 0 To wcount - 1
						g(i_) = g(i_) + decay * network.weights(i_)
					Next
					For k = 0 To wcount - 1
						h(k, k) = h(k, k) + decay
					Next
					rep.nhess = rep.nhess + 1

					'
					' Update lambda
					'
					lambdav = lambdav * lambdadown
					nu = 2
				End While

				'
				' update WBest
				'
				v = 0.0
				For i_ = 0 To wcount - 1
					v += network.weights(i_) * network.weights(i_)
				Next
				e = 0.5 * decay * v + mlpbase.mlperror(network, xy, npoints)
				If CDbl(e) < CDbl(ebest) Then
					ebest = e
					For i_ = 0 To wcount - 1
						wbest(i_) = network.weights(i_)
					Next
				End If
			Next

			'
			' copy WBest to output
			'
			For i_ = 0 To wcount - 1
				network.weights(i_) = wbest(i_)
			Next
		End Sub


		'************************************************************************
'        Neural  network  training  using  L-BFGS  algorithm  with  regularization.
'        Subroutine  trains  neural  network  with  restarts from random positions.
'        Algorithm  is  well  suited  for  problems  of  any dimensionality (memory
'        requirements and step complexity are linear by weights number).
'
'        INPUT PARAMETERS:
'            Network     -   neural network with initialized geometry
'            XY          -   training set
'            NPoints     -   training set size
'            Decay       -   weight decay constant, >=0.001
'                            Decay term 'Decay*||Weights||^2' is added to error
'                            function.
'                            If you don't know what Decay to choose, use 0.001.
'            Restarts    -   number of restarts from random position, >0.
'                            If you don't know what Restarts to choose, use 2.
'            WStep       -   stopping criterion. Algorithm stops if  step  size  is
'                            less than WStep. Recommended value - 0.01.  Zero  step
'                            size means stopping after MaxIts iterations.
'            MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
'                            iterations (NOT gradient  calculations).  Zero  MaxIts
'                            means stopping when step is sufficiently small.
'
'        OUTPUT PARAMETERS:
'            Network     -   trained neural network.
'            Info        -   return code:
'                            * -8, if both WStep=0 and MaxIts=0
'                            * -2, if there is a point with class number
'                                  outside of [0..NOut-1].
'                            * -1, if wrong parameters specified
'                                  (NPoints<0, Restarts<1).
'                            *  2, if task has been solved.
'            Rep         -   training report
'
'          -- ALGLIB --
'             Copyright 09.12.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlptrainlbfgs(network As mlpbase.multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
			maxits As Integer, ByRef info As Integer, rep As mlpreport)
			Dim i As Integer = 0
			Dim pass As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim w As Double() = New Double(-1) {}
			Dim wbest As Double() = New Double(-1) {}
			Dim e As Double = 0
			Dim v As Double = 0
			Dim ebest As Double = 0
			Dim internalrep As New minlbfgs.minlbfgsreport()
			Dim state As New minlbfgs.minlbfgsstate()
			Dim i_ As Integer = 0

			info = 0


			'
			' Test inputs, parse flags, read network geometry
			'
			If CDbl(wstep) = CDbl(0) AndAlso maxits = 0 Then
				info = -8
				Return
			End If
			If ((npoints <= 0 OrElse restarts < 1) OrElse CDbl(wstep) < CDbl(0)) OrElse maxits < 0 Then
				info = -1
				Return
			End If
			mlpbase.mlpproperties(network, nin, nout, wcount)
			If mlpbase.mlpissoftmax(network) Then
				For i = 0 To npoints - 1
					If CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
			End If
			decay = System.Math.Max(decay, mindecay)
			info = 2

			'
			' Prepare
			'
			mlpbase.mlpinitpreprocessor(network, xy, npoints)
			w = New Double(wcount - 1) {}
			wbest = New Double(wcount - 1) {}
			ebest = Math.maxrealnumber

			'
			' Multiple starts
			'
			rep.ncholesky = 0
			rep.nhess = 0
			rep.ngrad = 0
			For pass = 1 To restarts

				'
				' Process
				'
				mlpbase.mlprandomize(network)
				For i_ = 0 To wcount - 1
					w(i_) = network.weights(i_)
				Next
				minlbfgs.minlbfgscreate(wcount, System.Math.Min(wcount, 10), w, state)
				minlbfgs.minlbfgssetcond(state, 0.0, 0.0, wstep, maxits)
				While minlbfgs.minlbfgsiteration(state)
					For i_ = 0 To wcount - 1
						network.weights(i_) = state.x(i_)
					Next
					mlpbase.mlpgradnbatch(network, xy, npoints, state.f, state.g)
					v = 0.0
					For i_ = 0 To wcount - 1
						v += network.weights(i_) * network.weights(i_)
					Next
					state.f = state.f + 0.5 * decay * v
					For i_ = 0 To wcount - 1
						state.g(i_) = state.g(i_) + decay * network.weights(i_)
					Next
					rep.ngrad = rep.ngrad + 1
				End While
				minlbfgs.minlbfgsresults(state, w, internalrep)
				For i_ = 0 To wcount - 1
					network.weights(i_) = w(i_)
				Next

				'
				' Compare with best
				'
				v = 0.0
				For i_ = 0 To wcount - 1
					v += network.weights(i_) * network.weights(i_)
				Next
				e = mlpbase.mlperrorn(network, xy, npoints) + 0.5 * decay * v
				If CDbl(e) < CDbl(ebest) Then
					For i_ = 0 To wcount - 1
						wbest(i_) = network.weights(i_)
					Next
					ebest = e
				End If
			Next

			'
			' The best network
			'
			For i_ = 0 To wcount - 1
				network.weights(i_) = wbest(i_)
			Next
		End Sub


		'************************************************************************
'        Neural network training using early stopping (base algorithm - L-BFGS with
'        regularization).
'
'        INPUT PARAMETERS:
'            Network     -   neural network with initialized geometry
'            TrnXY       -   training set
'            TrnSize     -   training set size, TrnSize>0
'            ValXY       -   validation set
'            ValSize     -   validation set size, ValSize>0
'            Decay       -   weight decay constant, >=0.001
'                            Decay term 'Decay*||Weights||^2' is added to error
'                            function.
'                            If you don't know what Decay to choose, use 0.001.
'            Restarts    -   number of restarts, either:
'                            * strictly positive number - algorithm make specified
'                              number of restarts from random position.
'                            * -1, in which case algorithm makes exactly one run
'                              from the initial state of the network (no randomization).
'                            If you don't know what Restarts to choose, choose one
'                            one the following:
'                            * -1 (deterministic start)
'                            * +1 (one random restart)
'                            * +5 (moderate amount of random restarts)
'
'        OUTPUT PARAMETERS:
'            Network     -   trained neural network.
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NOut-1].
'                            * -1, if wrong parameters specified
'                                  (NPoints<0, Restarts<1, ...).
'                            *  2, task has been solved, stopping  criterion  met -
'                                  sufficiently small step size.  Not expected  (we
'                                  use  EARLY  stopping)  but  possible  and not an
'                                  error.
'                            *  6, task has been solved, stopping  criterion  met -
'                                  increasing of validation set error.
'            Rep         -   training report
'
'        NOTE:
'
'        Algorithm stops if validation set error increases for  a  long  enough  or
'        step size is small enought  (there  are  task  where  validation  set  may
'        decrease for eternity). In any case solution returned corresponds  to  the
'        minimum of validation set error.
'
'          -- ALGLIB --
'             Copyright 10.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlptraines(network As mlpbase.multilayerperceptron, trnxy As Double(,), trnsize As Integer, valxy As Double(,), valsize As Integer, decay As Double, _
			restarts As Integer, ByRef info As Integer, rep As mlpreport)
			Dim i As Integer = 0
			Dim pass As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim w As Double() = New Double(-1) {}
			Dim wbest As Double() = New Double(-1) {}
			Dim e As Double = 0
			Dim v As Double = 0
			Dim ebest As Double = 0
			Dim wfinal As Double() = New Double(-1) {}
			Dim efinal As Double = 0
			Dim itcnt As Integer = 0
			Dim itbest As Integer = 0
			Dim internalrep As New minlbfgs.minlbfgsreport()
			Dim state As New minlbfgs.minlbfgsstate()
			Dim wstep As Double = 0
			Dim needrandomization As New Boolean()
			Dim i_ As Integer = 0

			info = 0

			wstep = 0.001

			'
			' Test inputs, parse flags, read network geometry
			'
			If ((trnsize <= 0 OrElse valsize <= 0) OrElse (restarts < 1 AndAlso restarts <> -1)) OrElse CDbl(decay) < CDbl(0) Then
				info = -1
				Return
			End If
			If restarts = -1 Then
				needrandomization = False
				restarts = 1
			Else
				needrandomization = True
			End If
			mlpbase.mlpproperties(network, nin, nout, wcount)
			If mlpbase.mlpissoftmax(network) Then
				For i = 0 To trnsize - 1
					If CInt(System.Math.Truncate(System.Math.Round(trnxy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(trnxy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
				For i = 0 To valsize - 1
					If CInt(System.Math.Truncate(System.Math.Round(valxy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(valxy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
			End If
			info = 2

			'
			' Prepare
			'
			mlpbase.mlpinitpreprocessor(network, trnxy, trnsize)
			w = New Double(wcount - 1) {}
			wbest = New Double(wcount - 1) {}
			wfinal = New Double(wcount - 1) {}
			efinal = Math.maxrealnumber
			For i = 0 To wcount - 1
				wfinal(i) = 0
			Next

			'
			' Multiple starts
			'
			rep.ncholesky = 0
			rep.nhess = 0
			rep.ngrad = 0
			For pass = 1 To restarts

				'
				' Process
				'
				If needrandomization Then
					mlpbase.mlprandomize(network)
				End If
				ebest = mlpbase.mlperror(network, valxy, valsize)
				For i_ = 0 To wcount - 1
					wbest(i_) = network.weights(i_)
				Next
				itbest = 0
				itcnt = 0
				For i_ = 0 To wcount - 1
					w(i_) = network.weights(i_)
				Next
				minlbfgs.minlbfgscreate(wcount, System.Math.Min(wcount, 10), w, state)
				minlbfgs.minlbfgssetcond(state, 0.0, 0.0, wstep, 0)
				minlbfgs.minlbfgssetxrep(state, True)
				While minlbfgs.minlbfgsiteration(state)

					'
					' Calculate gradient
					'
					If state.needfg Then
						For i_ = 0 To wcount - 1
							network.weights(i_) = state.x(i_)
						Next
						mlpbase.mlpgradnbatch(network, trnxy, trnsize, state.f, state.g)
						v = 0.0
						For i_ = 0 To wcount - 1
							v += network.weights(i_) * network.weights(i_)
						Next
						state.f = state.f + 0.5 * decay * v
						For i_ = 0 To wcount - 1
							state.g(i_) = state.g(i_) + decay * network.weights(i_)
						Next
						rep.ngrad = rep.ngrad + 1
					End If

					'
					' Validation set
					'
					If state.xupdated Then
						For i_ = 0 To wcount - 1
							network.weights(i_) = state.x(i_)
						Next
						e = mlpbase.mlperror(network, valxy, valsize)
						If CDbl(e) < CDbl(ebest) Then
							ebest = e
							For i_ = 0 To wcount - 1
								wbest(i_) = network.weights(i_)
							Next
							itbest = itcnt
						End If
						If itcnt > 30 AndAlso CDbl(itcnt) > CDbl(1.5 * itbest) Then
							info = 6
							Exit While
						End If
						itcnt = itcnt + 1
					End If
				End While
				minlbfgs.minlbfgsresults(state, w, internalrep)

				'
				' Compare with final answer
				'
				If CDbl(ebest) < CDbl(efinal) Then
					For i_ = 0 To wcount - 1
						wfinal(i_) = wbest(i_)
					Next
					efinal = ebest
				End If
			Next

			'
			' The best network
			'
			For i_ = 0 To wcount - 1
				network.weights(i_) = wfinal(i_)
			Next
		End Sub


		'************************************************************************
'        Cross-validation estimate of generalization error.
'
'        Base algorithm - L-BFGS.
'
'        INPUT PARAMETERS:
'            Network     -   neural network with initialized geometry.   Network is
'                            not changed during cross-validation -  it is used only
'                            as a representative of its architecture.
'            XY          -   training set.
'            SSize       -   training set size
'            Decay       -   weight  decay, same as in MLPTrainLBFGS
'            Restarts    -   number of restarts, >0.
'                            restarts are counted for each partition separately, so
'                            total number of restarts will be Restarts*FoldsCount.
'            WStep       -   stopping criterion, same as in MLPTrainLBFGS
'            MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
'            FoldsCount  -   number of folds in k-fold cross-validation,
'                            2<=FoldsCount<=SSize.
'                            recommended value: 10.
'
'        OUTPUT PARAMETERS:
'            Info        -   return code, same as in MLPTrainLBFGS
'            Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
'            CVRep       -   generalization error estimates
'
'          -- ALGLIB --
'             Copyright 09.12.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpkfoldcvlbfgs(network As mlpbase.multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
			maxits As Integer, foldscount As Integer, ByRef info As Integer, rep As mlpreport, cvrep As mlpcvreport)
			info = 0

			mlpkfoldcvgeneral(network, xy, npoints, decay, restarts, foldscount, _
				False, wstep, maxits, info, rep, cvrep)
		End Sub


		'************************************************************************
'        Cross-validation estimate of generalization error.
'
'        Base algorithm - Levenberg-Marquardt.
'
'        INPUT PARAMETERS:
'            Network     -   neural network with initialized geometry.   Network is
'                            not changed during cross-validation -  it is used only
'                            as a representative of its architecture.
'            XY          -   training set.
'            SSize       -   training set size
'            Decay       -   weight  decay, same as in MLPTrainLBFGS
'            Restarts    -   number of restarts, >0.
'                            restarts are counted for each partition separately, so
'                            total number of restarts will be Restarts*FoldsCount.
'            FoldsCount  -   number of folds in k-fold cross-validation,
'                            2<=FoldsCount<=SSize.
'                            recommended value: 10.
'
'        OUTPUT PARAMETERS:
'            Info        -   return code, same as in MLPTrainLBFGS
'            Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
'            CVRep       -   generalization error estimates
'
'          -- ALGLIB --
'             Copyright 09.12.2007 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpkfoldcvlm(network As mlpbase.multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, foldscount As Integer, _
			ByRef info As Integer, rep As mlpreport, cvrep As mlpcvreport)
			info = 0

			mlpkfoldcvgeneral(network, xy, npoints, decay, restarts, foldscount, _
				True, 0.0, 0, info, rep, cvrep)
		End Sub


		'************************************************************************
'        This function estimates generalization error using cross-validation on the
'        current dataset with current training settings.
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support (C++ computational core)
'          !
'          ! Second improvement gives constant  speedup (2-3X).  First  improvement
'          ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'          ! operations can be executed in parallel:
'          ! * FoldsCount cross-validation rounds (always)
'          ! * NRestarts training sessions performed within each of
'          !   cross-validation rounds (if NRestarts>1)
'          ! * gradient calculation over large dataset (if dataset is large enough)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            Network     -   neural network. It must have same number of inputs and
'                            output/classes as was specified during creation of the
'                            trainer object. Network is not changed  during  cross-
'                            validation and is not trained - it  is  used  only  as
'                            representative of its architecture. I.e., we  estimate
'                            generalization properties of  ARCHITECTURE,  not  some
'                            specific network.
'            NRestarts   -   number of restarts, >=0:
'                            * NRestarts>0  means  that  for  each cross-validation
'                              round   specified  number   of  random  restarts  is
'                              performed,  with  best  network  being  chosen after
'                              training.
'                            * NRestarts=0 is same as NRestarts=1
'            FoldsCount  -   number of folds in k-fold cross-validation:
'                            * 2<=FoldsCount<=size of dataset
'                            * recommended value: 10.
'                            * values larger than dataset size will be silently
'                              truncated down to dataset size
'
'        OUTPUT PARAMETERS:
'            Rep         -   structure which contains cross-validation estimates:
'                            * Rep.RelCLSError - fraction of misclassified cases.
'                            * Rep.AvgCE - acerage cross-entropy
'                            * Rep.RMSError - root-mean-square error
'                            * Rep.AvgError - average error
'                            * Rep.AvgRelError - average relative error
'                            
'        NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'              or subset with only one point  was  given,  zeros  are  returned  as
'              estimates.
'
'        NOTE: this method performs FoldsCount cross-validation  rounds,  each  one
'              with NRestarts random starts.  Thus,  FoldsCount*NRestarts  networks
'              are trained in total.
'
'        NOTE: Rep.RelCLSError/Rep.AvgCE are zero on regression problems.
'
'        NOTE: on classification problems Rep.RMSError/Rep.AvgError/Rep.AvgRelError
'              contain errors in prediction of posterior probabilities.
'                
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpkfoldcv(s As mlptrainer, network As mlpbase.multilayerperceptron, nrestarts As Integer, foldscount As Integer, rep As mlpreport)
			Dim pooldatacv As New alglib.smp.shared_pool()
			Dim datacv As New mlpparallelizationcv()
			Dim sdatacv As mlpparallelizationcv = Nothing
			Dim cvy As Double(,) = New Double(-1, -1) {}
			Dim folds As Integer() = New Integer(-1) {}
			Dim buf As Double() = New Double(-1) {}
			Dim dy As Double() = New Double(-1) {}
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim rowsize As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim rs As New hqrnd.hqrndstate()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			If Not mlpbase.mlpissoftmax(network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPKFoldCV: type of input network is not similar to network type in trainer object")
			alglib.ap.assert(s.npoints >= 0, "MLPKFoldCV: possible trainer S is not initialized(S.NPoints<0)")
			mlpbase.mlpproperties(network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPKFoldCV:  number of inputs in trainer is not equal to number of inputs in network")
			alglib.ap.assert(s.nout = nout, "MLPKFoldCV:  number of outputs in trainer is not equal to number of outputs in network")
			alglib.ap.assert(nrestarts >= 0, "MLPKFoldCV: NRestarts<0")
			alglib.ap.assert(foldscount >= 2, "MLPKFoldCV: FoldsCount<2")
			If foldscount > s.npoints Then
				foldscount = s.npoints
			End If
			rep.relclserror = 0
			rep.avgce = 0
			rep.rmserror = 0
			rep.avgerror = 0
			rep.avgrelerror = 0
			hqrnd.hqrndrandomize(rs)
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0
			If s.npoints = 0 OrElse s.npoints = 1 Then
				Return
			End If

			'
			' Read network geometry, test parameters
			'
			If s.rcpar Then
				rowsize = nin + nout
				dy = New Double(nout - 1) {}
				bdss.dserrallocate(-nout, buf)
			Else
				rowsize = nin + 1
				dy = New Double(0) {}
				bdss.dserrallocate(nout, buf)
			End If

			'
			' Folds
			'
			folds = New Integer(s.npoints - 1) {}
			For i = 0 To s.npoints - 1
				folds(i) = i * foldscount \ s.npoints
			Next
			For i = 0 To s.npoints - 2
				j = i + hqrnd.hqrnduniformi(rs, s.npoints - i)
				If j <> i Then
					k = folds(i)
					folds(i) = folds(j)
					folds(j) = k
				End If
			Next
			cvy = New Double(s.npoints - 1, nout - 1) {}

			'
			' Initialize SEED-value for shared pool
			'
			datacv.ngrad = 0
			mlpbase.mlpcopy(network, datacv.network)
			datacv.subset = New Integer(s.npoints - 1) {}
			datacv.xyrow = New Double(rowsize - 1) {}
			datacv.y = New Double(nout - 1) {}

			'
			' Create shared pool
			'
			alglib.smp.ae_shared_pool_set_seed(pooldatacv, datacv)

			'
			' Parallelization
			'
			mthreadcv(s, rowsize, nrestarts, folds, 0, foldscount, _
				cvy, pooldatacv)

			'
			' Calculate value for NGrad
			'
			alglib.smp.ae_shared_pool_first_recycled(pooldatacv, sdatacv)
			While sdatacv IsNot Nothing
				rep.ngrad = rep.ngrad + sdatacv.ngrad
				alglib.smp.ae_shared_pool_next_recycled(pooldatacv, sdatacv)
			End While

			'
			' Connect of results and calculate cross-validation error
			'
			For i = 0 To s.npoints - 1
				If s.datatype = 0 Then
					For i_ = 0 To rowsize - 1
						datacv.xyrow(i_) = s.densexy(i, i_)
					Next
				End If
				If s.datatype = 1 Then
					sparse.sparsegetrow(s.sparsexy, i, datacv.xyrow)
				End If
				For i_ = 0 To nout - 1
					datacv.y(i_) = cvy(i, i_)
				Next
				If s.rcpar Then
					i1_ = (nin) - (0)
					For i_ = 0 To nout - 1
						dy(i_) = datacv.xyrow(i_ + i1_)
					Next
				Else
					dy(0) = datacv.xyrow(nin)
				End If
				bdss.dserraccumulate(buf, datacv.y, dy)
			Next
			bdss.dserrfinish(buf)
			rep.relclserror = buf(0)
			rep.avgce = buf(1)
			rep.rmserror = buf(2)
			rep.avgerror = buf(3)
			rep.avgrelerror = buf(4)
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlpkfoldcv(s As mlptrainer, network As mlpbase.multilayerperceptron, nrestarts As Integer, foldscount As Integer, rep As mlpreport)
			mlpkfoldcv(s, network, nrestarts, foldscount, rep)
		End Sub


		'************************************************************************
'        Creation of the network trainer object for regression networks
'
'        INPUT PARAMETERS:
'            NIn         -   number of inputs, NIn>=1
'            NOut        -   number of outputs, NOut>=1
'
'        OUTPUT PARAMETERS:
'            S           -   neural network trainer object.
'                            This structure can be used to train any regression
'                            network with NIn inputs and NOut outputs.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreatetrainer(nin As Integer, nout As Integer, s As mlptrainer)
			alglib.ap.assert(nin >= 1, "MLPCreateTrainer: NIn<1.")
			alglib.ap.assert(nout >= 1, "MLPCreateTrainer: NOut<1.")
			s.nin = nin
			s.nout = nout
			s.rcpar = True
			s.lbfgsfactor = defaultlbfgsfactor
			s.decay = 1E-06
			mlpsetcond(s, 0, 0)
			s.datatype = 0
			s.npoints = 0
			mlpsetalgobatch(s)
		End Sub


		'************************************************************************
'        Creation of the network trainer object for classification networks
'
'        INPUT PARAMETERS:
'            NIn         -   number of inputs, NIn>=1
'            NClasses    -   number of classes, NClasses>=2
'
'        OUTPUT PARAMETERS:
'            S           -   neural network trainer object.
'                            This structure can be used to train any classification
'                            network with NIn inputs and NOut outputs.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpcreatetrainercls(nin As Integer, nclasses As Integer, s As mlptrainer)
			alglib.ap.assert(nin >= 1, "MLPCreateTrainerCls: NIn<1.")
			alglib.ap.assert(nclasses >= 2, "MLPCreateTrainerCls: NClasses<2.")
			s.nin = nin
			s.nout = nclasses
			s.rcpar = False
			s.lbfgsfactor = defaultlbfgsfactor
			s.decay = 1E-06
			mlpsetcond(s, 0, 0)
			s.datatype = 0
			s.npoints = 0
			mlpsetalgobatch(s)
		End Sub


		'************************************************************************
'        This function sets "current dataset" of the trainer object to  one  passed
'        by user.
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed.
'            NPoints     -   points count, >=0.
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        datasetformat is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetdataset(s As mlptrainer, xy As Double(,), npoints As Integer)
			Dim ndim As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(s.nin >= 1, "MLPSetDataset: possible parameter S is not initialized or spoiled(S.NIn<=0).")
			alglib.ap.assert(npoints >= 0, "MLPSetDataset: NPoint<0")
			alglib.ap.assert(npoints <= alglib.ap.rows(xy), "MLPSetDataset: invalid size of matrix XY(NPoint more then rows of matrix XY)")
			s.datatype = 0
			s.npoints = npoints
			If npoints = 0 Then
				Return
			End If
			If s.rcpar Then
				alglib.ap.assert(s.nout >= 1, "MLPSetDataset: possible parameter S is not initialized or is spoiled(NOut<1 for regression).")
				ndim = s.nin + s.nout
				alglib.ap.assert(ndim <= alglib.ap.cols(xy), "MLPSetDataset: invalid size of matrix XY(too few columns in matrix XY).")
				alglib.ap.assert(apserv.apservisfinitematrix(xy, npoints, ndim), "MLPSetDataset: parameter XY contains Infinite or NaN.")
			Else
				alglib.ap.assert(s.nout >= 2, "MLPSetDataset: possible parameter S is not initialized or is spoiled(NClasses<2 for classifier).")
				ndim = s.nin + 1
				alglib.ap.assert(ndim <= alglib.ap.cols(xy), "MLPSetDataset: invalid size of matrix XY(too few columns in matrix XY).")
				alglib.ap.assert(apserv.apservisfinitematrix(xy, npoints, ndim), "MLPSetDataset: parameter XY contains Infinite or NaN.")
				For i = 0 To npoints - 1
					alglib.ap.assert(CInt(System.Math.Truncate(System.Math.Round(xy(i, s.nin)))) >= 0 AndAlso CInt(System.Math.Truncate(System.Math.Round(xy(i, s.nin)))) < s.nout, "MLPSetDataset: invalid parameter XY(in classifier used nonexistent class number: either XY[.,NIn]<0 or XY[.,NIn]>=NClasses).")
				Next
			End If
			apserv.rmatrixsetlengthatleast(s.densexy, npoints, ndim)
			For i = 0 To npoints - 1
				For j = 0 To ndim - 1
					s.densexy(i, j) = xy(i, j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function sets "current dataset" of the trainer object to  one  passed
'        by user (sparse matrix is used to store dataset).
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            XY          -   training  set,  see  below  for  information  on   the
'                            training set format. This function checks  correctness
'                            of  the  dataset  (no  NANs/INFs,  class  numbers  are
'                            correct) and throws exception when  incorrect  dataset
'                            is passed. Any  sparse  storage  format  can be  used:
'                            Hash-table, CRS...
'            NPoints     -   points count, >=0
'
'        DATASET FORMAT:
'
'        This  function  uses  two  different  dataset formats - one for regression
'        networks, another one for classification networks.
'
'        For regression networks with NIn inputs and NOut outputs following dataset
'        format is used:
'        * dataset is given by NPoints*(NIn+NOut) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, next NOut columns are outputs
'
'        For classification networks with NIn inputs and NClasses clases  following
'        datasetformat is used:
'        * dataset is given by NPoints*(NIn+1) matrix
'        * each row corresponds to one example
'        * first NIn columns are inputs, last column stores class number (from 0 to
'          NClasses-1).
'          
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetsparsedataset(s As mlptrainer, xy As sparse.sparsematrix, npoints As Integer)
			Dim v As Double = 0
			Dim t0 As Integer = 0
			Dim t1 As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0


			'
			' Check correctness of the data
			'
			alglib.ap.assert(s.nin > 0, "MLPSetSparseDataset: possible parameter S is not initialized or spoiled(S.NIn<=0).")
			alglib.ap.assert(npoints >= 0, "MLPSetSparseDataset: NPoint<0")
			alglib.ap.assert(npoints <= sparse.sparsegetnrows(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(NPoint more then rows of matrix XY)")
			If npoints > 0 Then
				t0 = 0
				t1 = 0
				If s.rcpar Then
					alglib.ap.assert(s.nout >= 1, "MLPSetSparseDataset: possible parameter S is not initialized or is spoiled(NOut<1 for regression).")
					alglib.ap.assert(s.nin + s.nout <= sparse.sparsegetncols(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(too few columns in sparse matrix XY).")
					While sparse.sparseenumerate(xy, t0, t1, i, j, v)
						If i < npoints AndAlso j < s.nin + s.nout Then
							alglib.ap.assert(Math.isfinite(v), "MLPSetSparseDataset: sparse matrix XY contains Infinite or NaN.")
						End If
					End While
				Else
					alglib.ap.assert(s.nout >= 2, "MLPSetSparseDataset: possible parameter S is not initialized or is spoiled(NClasses<2 for classifier).")
					alglib.ap.assert(s.nin + 1 <= sparse.sparsegetncols(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(too few columns in sparse matrix XY).")
					While sparse.sparseenumerate(xy, t0, t1, i, j, v)
						If i < npoints AndAlso j <= s.nin Then
							If j <> s.nin Then
								alglib.ap.assert(Math.isfinite(v), "MLPSetSparseDataset: sparse matrix XY contains Infinite or NaN.")
							Else
								alglib.ap.assert((Math.isfinite(v) AndAlso CInt(System.Math.Truncate(System.Math.Round(v))) >= 0) AndAlso CInt(System.Math.Truncate(System.Math.Round(v))) < s.nout, "MLPSetSparseDataset: invalid sparse matrix XY(in classifier used nonexistent class number: either XY[.,NIn]<0 or XY[.,NIn]>=NClasses).")
							End If
						End If
					End While
				End If
			End If

			'
			' Set dataset
			'
			s.datatype = 1
			s.npoints = npoints
			sparse.sparsecopytocrs(xy, s.sparsexy)
		End Sub


		'************************************************************************
'        This function sets weight decay coefficient which is used for training.
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            Decay       -   weight  decay  coefficient,  >=0.  Weight  decay  term
'                            'Decay*||Weights||^2' is added to error  function.  If
'                            you don't know what Decay to choose, use 1.0E-3.
'                            Weight decay can be set to zero,  in this case network
'                            is trained without weight decay.
'
'        NOTE: by default network uses some small nonzero value for weight decay.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetdecay(s As mlptrainer, decay As Double)
			alglib.ap.assert(Math.isfinite(decay), "MLPSetDecay: parameter Decay contains Infinite or NaN.")
			alglib.ap.assert(CDbl(decay) >= CDbl(0), "MLPSetDecay: Decay<0.")
			s.decay = decay
		End Sub


		'************************************************************************
'        This function sets stopping criteria for the optimizer.
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            WStep       -   stopping criterion. Algorithm stops if  step  size  is
'                            less than WStep. Recommended value - 0.01.  Zero  step
'                            size means stopping after MaxIts iterations.
'                            WStep>=0.
'            MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
'                            epochs (full passes over entire dataset).  Zero MaxIts
'                            means stopping when step is sufficiently small.
'                            MaxIts>=0.
'
'        NOTE: by default, WStep=0.005 and MaxIts=0 are used. These values are also
'              used when MLPSetCond() is called with WStep=0 and MaxIts=0.
'              
'        NOTE: these stopping criteria are used for all kinds of neural training  -
'              from "conventional" networks to early stopping ensembles. When  used
'              for "conventional" networks, they are  used  as  the  only  stopping
'              criteria. When combined with early stopping, they used as ADDITIONAL
'              stopping criteria which can terminate early stopping algorithm.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetcond(s As mlptrainer, wstep As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(wstep), "MLPSetCond: parameter WStep contains Infinite or NaN.")
			alglib.ap.assert(CDbl(wstep) >= CDbl(0), "MLPSetCond: WStep<0.")
			alglib.ap.assert(maxits >= 0, "MLPSetCond: MaxIts<0.")
			If CDbl(wstep) <> CDbl(0) OrElse maxits <> 0 Then
				s.wstep = wstep
				s.maxits = maxits
			Else
				s.wstep = 0.005
				s.maxits = 0
			End If
		End Sub


		'************************************************************************
'        This function sets training algorithm: batch training using L-BFGS will be
'        used.
'
'        This algorithm:
'        * the most robust for small-scale problems, but may be too slow for  large
'          scale ones.
'        * perfoms full pass through the dataset before performing step
'        * uses conditions specified by MLPSetCond() for stopping
'        * is default one used by trainer object
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpsetalgobatch(s As mlptrainer)
			s.algokind = 0
		End Sub


		'************************************************************************
'        This function trains neural network passed to this function, using current
'        dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
'        and current training settings. Training  from  NRestarts  random  starting
'        positions is performed, best network is chosen.
'
'        Training is performed using current training algorithm.
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support (C++ computational core)
'          !
'          ! Second improvement gives constant  speedup (2-3X).  First  improvement
'          ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'          ! operations can be executed in parallel:
'          ! * NRestarts training sessions performed within each of
'          !   cross-validation rounds (if NRestarts>1)
'          ! * gradient calculation over large dataset (if dataset is large enough)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            Network     -   neural network. It must have same number of inputs and
'                            output/classes as was specified during creation of the
'                            trainer object.
'            NRestarts   -   number of restarts, >=0:
'                            * NRestarts>0 means that specified  number  of  random
'                              restarts are performed, best network is chosen after
'                              training
'                            * NRestarts=0 means that current state of the  network
'                              is used for training.
'
'        OUTPUT PARAMETERS:
'            Network     -   trained network
'
'        NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'              network  is  filled  by zero  values.  Same  behavior  for functions
'              MLPStartTraining and MLPContinueTraining.
'
'        NOTE: this method uses sum-of-squares error function for training.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlptrainnetwork(s As mlptrainer, network As mlpbase.multilayerperceptron, nrestarts As Integer, rep As mlpreport)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim trnpool As New alglib.smp.shared_pool()

			alglib.ap.assert(s.npoints >= 0, "MLPTrainNetwork: parameter S is not initialized or is spoiled(S.NPoints<0)")
			If Not mlpbase.mlpissoftmax(network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPTrainNetwork: type of input network is not similar to network type in trainer object")
			mlpbase.mlpproperties(network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPTrainNetwork: number of inputs in trainer is not equal to number of inputs in network")
			alglib.ap.assert(s.nout = nout, "MLPTrainNetwork: number of outputs in trainer is not equal to number of outputs in network")
			alglib.ap.assert(nrestarts >= 0, "MLPTrainNetwork: NRestarts<0.")

			'
			' Train
			'
			mlptrainnetworkx(s, nrestarts, -1, s.subset, -1, s.subset, _
				0, network, rep, True, trnpool)
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlptrainnetwork(s As mlptrainer, network As mlpbase.multilayerperceptron, nrestarts As Integer, rep As mlpreport)
			mlptrainnetwork(s, network, nrestarts, rep)
		End Sub


		'************************************************************************
'        IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
'                   not recommend you to use it unless you are pretty sure that you
'                   need ability to monitor training progress.
'
'        This function performs step-by-step training of the neural  network.  Here
'        "step-by-step" means that training  starts  with  MLPStartTraining() call,
'        and then user subsequently calls MLPContinueTraining() to perform one more
'        iteration of the training.
'
'        After call to this function trainer object remembers network and  is ready
'        to  train  it.  However,  no  training  is  performed  until first call to 
'        MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
'        will advance training progress one iteration further.
'
'        EXAMPLE:
'            >
'            > ...initialize network and trainer object....
'            >
'            > MLPStartTraining(Trainer, Network, True)
'            > while MLPContinueTraining(Trainer, Network) do
'            >     ...visualize training progress...
'            >
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            Network     -   neural network. It must have same number of inputs and
'                            output/classes as was specified during creation of the
'                            trainer object.
'            RandomStart -   randomize network before training or not:
'                            * True  means  that  network  is  randomized  and  its
'                              initial state (one which was passed to  the  trainer
'                              object) is lost.
'                            * False  means  that  training  is  started  from  the
'                              current state of the network
'                            
'        OUTPUT PARAMETERS:
'            Network     -   neural network which is ready to training (weights are
'                            initialized, preprocessor is initialized using current
'                            training set)
'
'        NOTE: this method uses sum-of-squares error function for training.
'
'        NOTE: it is expected that trainer object settings are NOT  changed  during
'              step-by-step training, i.e. no  one  changes  stopping  criteria  or
'              training set during training. It is possible and there is no defense
'              against  such  actions,  but  algorithm  behavior  in  such cases is
'              undefined and can be unpredictable.
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpstarttraining(s As mlptrainer, network As mlpbase.multilayerperceptron, randomstart As Boolean)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0

			alglib.ap.assert(s.npoints >= 0, "MLPStartTraining: parameter S is not initialized or is spoiled(S.NPoints<0)")
			If Not mlpbase.mlpissoftmax(network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPStartTraining: type of input network is not similar to network type in trainer object")
			mlpbase.mlpproperties(network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPStartTraining: number of inputs in trainer is not equal to number of inputs in the network.")
			alglib.ap.assert(s.nout = nout, "MLPStartTraining: number of outputs in trainer is not equal to number of outputs in the network.")

			'
			' Initialize temporaries
			'
			initmlptrnsession(network, randomstart, s, s.session)

			'
			' Train network
			'
			mlpstarttrainingx(s, randomstart, -1, s.subset, -1, s.session)

			'
			' Update network
			'
			mlpbase.mlpcopytunableparameters(s.session.network, network)
		End Sub


		'************************************************************************
'        IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
'                   not recommend you to use it unless you are pretty sure that you
'                   need ability to monitor training progress.
'                   
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support (C++ computational core)
'          !
'          ! Second improvement gives constant  speedup (2-3X).  First  improvement
'          ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'          ! operations can be executed in parallel:
'          ! * gradient calculation over large dataset (if dataset is large enough)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'        This function performs step-by-step training of the neural  network.  Here
'        "step-by-step" means that training starts  with  MLPStartTraining()  call,
'        and then user subsequently calls MLPContinueTraining() to perform one more
'        iteration of the training.
'
'        This  function  performs  one  more  iteration of the training and returns
'        either True (training continues) or False (training stopped). In case True
'        was returned, Network weights are updated according to the  current  state
'        of the optimization progress. In case False was  returned,  no  additional
'        updates is performed (previous update of  the  network weights moved us to
'        the final point, and no additional updates is needed).
'
'        EXAMPLE:
'            >
'            > [initialize network and trainer object]
'            >
'            > MLPStartTraining(Trainer, Network, True)
'            > while MLPContinueTraining(Trainer, Network) do
'            >     [visualize training progress]
'            >
'
'        INPUT PARAMETERS:
'            S           -   trainer object
'            Network     -   neural  network  structure,  which  is  used to  store
'                            current state of the training process.
'                            
'        OUTPUT PARAMETERS:
'            Network     -   weights of the neural network  are  rewritten  by  the
'                            current approximation.
'
'        NOTE: this method uses sum-of-squares error function for training.
'
'        NOTE: it is expected that trainer object settings are NOT  changed  during
'              step-by-step training, i.e. no  one  changes  stopping  criteria  or
'              training set during training. It is possible and there is no defense
'              against  such  actions,  but  algorithm  behavior  in  such cases is
'              undefined and can be unpredictable.
'              
'        NOTE: It  is  expected that Network is the same one which  was  passed  to
'              MLPStartTraining() function.  However,  THIS  function  checks  only
'              following:
'              * that number of network inputs is consistent with trainer object
'                settings
'              * that number of network outputs/classes is consistent with  trainer
'                object settings
'              * that number of network weights is the same as number of weights in
'                the network passed to MLPStartTraining() function
'              Exception is thrown when these conditions are violated.
'              
'              It is also expected that you do not change state of the  network  on
'              your own - the only party who has right to change network during its
'              training is a trainer object. Any attempt to interfere with  trainer
'              may lead to unpredictable results.
'              
'
'          -- ALGLIB --
'             Copyright 23.07.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mlpcontinuetraining(s As mlptrainer, network As mlpbase.multilayerperceptron) As Boolean
			Dim result As New Boolean()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(s.npoints >= 0, "MLPContinueTraining: parameter S is not initialized or is spoiled(S.NPoints<0)")
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			If Not mlpbase.mlpissoftmax(network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPContinueTraining: type of input network is not similar to network type in trainer object.")
			mlpbase.mlpproperties(network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPContinueTraining: number of inputs in trainer is not equal to number of inputs in the network.")
			alglib.ap.assert(s.nout = nout, "MLPContinueTraining: number of outputs in trainer is not equal to number of outputs in the network.")
			result = mlpcontinuetrainingx(s, s.subset, -1, s.ngradbatch, s.session)
			If result Then
				For i_ = 0 To wcount - 1
					network.weights(i_) = s.session.network.weights(i_)
				Next
			End If
			Return result
		End Function


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Function _pexec_mlpcontinuetraining(s As mlptrainer, network As mlpbase.multilayerperceptron) As Boolean
			Return mlpcontinuetraining(s, network)
		End Function


		'************************************************************************
'        Training neural networks ensemble using  bootstrap  aggregating (bagging).
'        Modified Levenberg-Marquardt algorithm is used as base training method.
'
'        INPUT PARAMETERS:
'            Ensemble    -   model with initialized geometry
'            XY          -   training set
'            NPoints     -   training set size
'            Decay       -   weight decay coefficient, >=0.001
'            Restarts    -   restarts, >0.
'
'        OUTPUT PARAMETERS:
'            Ensemble    -   trained model
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<0, Restarts<1).
'                            *  2, if task has been solved.
'            Rep         -   training report.
'            OOBErrors   -   out-of-bag generalization error estimate
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpebagginglm(ensemble As mlpe.mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
			rep As mlpreport, ooberrors As mlpcvreport)
			info = 0

			mlpebagginginternal(ensemble, xy, npoints, decay, restarts, 0.0, _
				0, True, info, rep, ooberrors)
		End Sub


		'************************************************************************
'        Training neural networks ensemble using  bootstrap  aggregating (bagging).
'        L-BFGS algorithm is used as base training method.
'
'        INPUT PARAMETERS:
'            Ensemble    -   model with initialized geometry
'            XY          -   training set
'            NPoints     -   training set size
'            Decay       -   weight decay coefficient, >=0.001
'            Restarts    -   restarts, >0.
'            WStep       -   stopping criterion, same as in MLPTrainLBFGS
'            MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
'
'        OUTPUT PARAMETERS:
'            Ensemble    -   trained model
'            Info        -   return code:
'                            * -8, if both WStep=0 and MaxIts=0
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<0, Restarts<1).
'                            *  2, if task has been solved.
'            Rep         -   training report.
'            OOBErrors   -   out-of-bag generalization error estimate
'
'          -- ALGLIB --
'             Copyright 17.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpebagginglbfgs(ensemble As mlpe.mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
			maxits As Integer, ByRef info As Integer, rep As mlpreport, ooberrors As mlpcvreport)
			info = 0

			mlpebagginginternal(ensemble, xy, npoints, decay, restarts, wstep, _
				maxits, False, info, rep, ooberrors)
		End Sub


		'************************************************************************
'        Training neural networks ensemble using early stopping.
'
'        INPUT PARAMETERS:
'            Ensemble    -   model with initialized geometry
'            XY          -   training set
'            NPoints     -   training set size
'            Decay       -   weight decay coefficient, >=0.001
'            Restarts    -   restarts, >0.
'
'        OUTPUT PARAMETERS:
'            Ensemble    -   trained model
'            Info        -   return code:
'                            * -2, if there is a point with class number
'                                  outside of [0..NClasses-1].
'                            * -1, if incorrect parameters was passed
'                                  (NPoints<0, Restarts<1).
'                            *  6, if task has been solved.
'            Rep         -   training report.
'            OOBErrors   -   out-of-bag generalization error estimate
'
'          -- ALGLIB --
'             Copyright 10.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlpetraines(ensemble As mlpe.mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, ByRef info As Integer, _
			rep As mlpreport)
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim ccount As Integer = 0
			Dim pcount As Integer = 0
			Dim trnxy As Double(,) = New Double(-1, -1) {}
			Dim valxy As Double(,) = New Double(-1, -1) {}
			Dim trnsize As Integer = 0
			Dim valsize As Integer = 0
			Dim tmpinfo As Integer = 0
			Dim tmprep As New mlpreport()
			Dim moderr As New mlpbase.modelerrors()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			info = 0

			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			wcount = mlpbase.mlpgetweightscount(ensemble.network)
			If (npoints < 2 OrElse restarts < 1) OrElse CDbl(decay) < CDbl(0) Then
				info = -1
				Return
			End If
			If mlpbase.mlpissoftmax(ensemble.network) Then
				For i = 0 To npoints - 1
					If CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
			End If
			info = 6

			'
			' allocate
			'
			If mlpbase.mlpissoftmax(ensemble.network) Then
				ccount = nin + 1
				pcount = nin
			Else
				ccount = nin + nout
				pcount = nin + nout
			End If
			trnxy = New Double(npoints - 1, ccount - 1) {}
			valxy = New Double(npoints - 1, ccount - 1) {}
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0

			'
			' train networks
			'
			For k = 0 To ensemble.ensemblesize - 1

				'
				' Split set
				'
				Do
					trnsize = 0
					valsize = 0
					For i = 0 To npoints - 1
						If CDbl(Math.randomreal()) < CDbl(0.66) Then

							'
							' Assign sample to training set
							'
							For i_ = 0 To ccount - 1
								trnxy(trnsize, i_) = xy(i, i_)
							Next
							trnsize = trnsize + 1
						Else

							'
							' Assign sample to validation set
							'
							For i_ = 0 To ccount - 1
								valxy(valsize, i_) = xy(i, i_)
							Next
							valsize = valsize + 1
						End If
					Next
				Loop While Not (trnsize <> 0 AndAlso valsize <> 0)

				'
				' Train
				'
				mlptraines(ensemble.network, trnxy, trnsize, valxy, valsize, decay, _
					restarts, tmpinfo, tmprep)
				If tmpinfo < 0 Then
					info = tmpinfo
					Return
				End If

				'
				' save results
				'
				i1_ = (0) - (k * wcount)
				For i_ = k * wcount To (k + 1) * wcount - 1
					ensemble.weights(i_) = ensemble.network.weights(i_ + i1_)
				Next
				i1_ = (0) - (k * pcount)
				For i_ = k * pcount To (k + 1) * pcount - 1
					ensemble.columnmeans(i_) = ensemble.network.columnmeans(i_ + i1_)
				Next
				i1_ = (0) - (k * pcount)
				For i_ = k * pcount To (k + 1) * pcount - 1
					ensemble.columnsigmas(i_) = ensemble.network.columnsigmas(i_ + i1_)
				Next
				rep.ngrad = rep.ngrad + tmprep.ngrad
				rep.nhess = rep.nhess + tmprep.nhess
				rep.ncholesky = rep.ncholesky + tmprep.ncholesky
			Next
			mlpe.mlpeallerrorsx(ensemble, xy, ensemble.network.dummysxy, npoints, 0, ensemble.network.dummyidx, _
				0, npoints, 0, ensemble.network.buf, moderr)
			rep.relclserror = moderr.relclserror
			rep.avgce = moderr.avgce
			rep.rmserror = moderr.rmserror
			rep.avgerror = moderr.avgerror
			rep.avgrelerror = moderr.avgrelerror
		End Sub


		'************************************************************************
'        This function trains neural network ensemble passed to this function using
'        current dataset and early stopping training algorithm. Each early stopping
'        round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
'        training rounds is performed in total).
'
'        FOR USERS OF COMMERCIAL EDITION:
'
'          ! Commercial version of ALGLIB includes two  important  improvements  of
'          ! this function:
'          ! * multicore support (C++ and C# computational cores)
'          ! * SSE support (C++ computational core)
'          !
'          ! Second improvement gives constant  speedup (2-3X).  First  improvement
'          ! gives  close-to-linear  speedup  on   multicore   systems.   Following
'          ! operations can be executed in parallel:
'          ! * EnsembleSize  training  sessions  performed  for  each  of  ensemble
'          !   members (always parallelized)
'          ! * NRestarts  training  sessions  performed  within  each  of  training
'          !   sessions (if NRestarts>1)
'          ! * gradient calculation over large dataset (if dataset is large enough)
'          !
'          ! In order to use multicore features you have to:
'          ! * use commercial version of ALGLIB
'          ! * call  this  function  with  "smp_"  prefix,  which  indicates  that
'          !   multicore code will be used (for multicore support)
'          !
'          ! In order to use SSE features you have to:
'          ! * use commercial version of ALGLIB on Intel processors
'          ! * use C++ computational core
'          !
'          ! This note is given for users of commercial edition; if  you  use  GPL
'          ! edition, you still will be able to call smp-version of this function,
'          ! but all computations will be done serially.
'          !
'          ! We recommend you to carefully read ALGLIB Reference  Manual,  section
'          ! called 'SMP support', before using parallel version of this function.
'
'        INPUT PARAMETERS:
'            S           -   trainer object;
'            Ensemble    -   neural network ensemble. It must have same  number  of
'                            inputs and outputs/classes  as  was  specified  during
'                            creation of the trainer object.
'            NRestarts   -   number of restarts, >=0:
'                            * NRestarts>0 means that specified  number  of  random
'                              restarts are performed during each ES round;
'                            * NRestarts=0 is silently replaced by 1.
'
'        OUTPUT PARAMETERS:
'            Ensemble    -   trained ensemble;
'            Rep         -   it contains all type of errors.
'            
'        NOTE: this training method uses BOTH early stopping and weight decay!  So,
'              you should select weight decay before starting training just as  you
'              select it before training "conventional" networks.
'
'        NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
'              or  single-point  dataset  was  passed,  ensemble  is filled by zero
'              values.
'
'        NOTE: this method uses sum-of-squares error function for training.
'
'          -- ALGLIB --
'             Copyright 22.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mlptrainensemblees(s As mlptrainer, ensemble As mlpe.mlpensemble, nrestarts As Integer, rep As mlpreport)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim esessions As New alglib.smp.shared_pool()
			Dim sgrad As New apserv.sinteger()
			Dim tmprep As New mlpbase.modelerrors()

			alglib.ap.assert(s.npoints >= 0, "MLPTrainEnsembleES: parameter S is not initialized or is spoiled(S.NPoints<0)")
			If Not mlpe.mlpeissoftmax(ensemble) Then
				ntype = 0
			Else
				ntype = 1
			End If
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPTrainEnsembleES: internal error - type of input network is not similar to network type in trainer object")
			nin = mlpbase.mlpgetinputscount(ensemble.network)
			alglib.ap.assert(s.nin = nin, "MLPTrainEnsembleES: number of inputs in trainer is not equal to number of inputs in ensemble network")
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			alglib.ap.assert(s.nout = nout, "MLPTrainEnsembleES: number of outputs in trainer is not equal to number of outputs in ensemble network")
			alglib.ap.assert(nrestarts >= 0, "MLPTrainEnsembleES: NRestarts<0.")

			'
			' Initialize parameter Rep
			'
			rep.relclserror = 0
			rep.avgce = 0
			rep.rmserror = 0
			rep.avgerror = 0
			rep.avgrelerror = 0
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0

			'
			' Allocate
			'
			apserv.ivectorsetlengthatleast(s.subset, s.npoints)
			apserv.ivectorsetlengthatleast(s.valsubset, s.npoints)

			'
			' Start training
			'
			' NOTE: ESessions is not initialized because MLPTrainEnsembleX
			'       needs uninitialized pool.
			'
			sgrad.val = 0
			mlptrainensemblex(s, ensemble, 0, ensemble.ensemblesize, nrestarts, 0, _
				sgrad, True, esessions)
			rep.ngrad = sgrad.val

			'
			' Calculate errors.
			'
			If s.datatype = 0 Then
				mlpe.mlpeallerrorsx(ensemble, s.densexy, s.sparsexy, s.npoints, 0, ensemble.network.dummyidx, _
					0, s.npoints, 0, ensemble.network.buf, tmprep)
			End If
			If s.datatype = 1 Then
				mlpe.mlpeallerrorsx(ensemble, s.densexy, s.sparsexy, s.npoints, 1, ensemble.network.dummyidx, _
					0, s.npoints, 0, ensemble.network.buf, tmprep)
			End If
			rep.relclserror = tmprep.relclserror
			rep.avgce = tmprep.avgce
			rep.rmserror = tmprep.rmserror
			rep.avgerror = tmprep.avgerror
			rep.avgrelerror = tmprep.avgrelerror
		End Sub


		'************************************************************************
'        Single-threaded stub. HPC ALGLIB replaces it by multithreaded code.
'        ************************************************************************

		Public Shared Sub _pexec_mlptrainensemblees(s As mlptrainer, ensemble As mlpe.mlpensemble, nrestarts As Integer, rep As mlpreport)
			mlptrainensemblees(s, ensemble, nrestarts, rep)
		End Sub


		'************************************************************************
'        Internal cross-validation subroutine
'        ************************************************************************

		Private Shared Sub mlpkfoldcvgeneral(n As mlpbase.multilayerperceptron, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, foldscount As Integer, _
			lmalgorithm As Boolean, wstep As Double, maxits As Integer, ByRef info As Integer, rep As mlpreport, cvrep As mlpcvreport)
			Dim i As Integer = 0
			Dim fold As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim network As New mlpbase.multilayerperceptron()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim rowlen As Integer = 0
			Dim wcount As Integer = 0
			Dim nclasses As Integer = 0
			Dim tssize As Integer = 0
			Dim cvssize As Integer = 0
			Dim cvset As Double(,) = New Double(-1, -1) {}
			Dim testset As Double(,) = New Double(-1, -1) {}
			Dim folds As Integer() = New Integer(-1) {}
			Dim relcnt As Integer = 0
			Dim internalrep As New mlpreport()
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim i_ As Integer = 0

			info = 0


			'
			' Read network geometry, test parameters
			'
			mlpbase.mlpproperties(n, nin, nout, wcount)
			If mlpbase.mlpissoftmax(n) Then
				nclasses = nout
				rowlen = nin + 1
			Else
				nclasses = -nout
				rowlen = nin + nout
			End If
			If (npoints <= 0 OrElse foldscount < 2) OrElse foldscount > npoints Then
				info = -1
				Return
			End If
			mlpbase.mlpcopy(n, network)

			'
			' K-fold out cross-validation.
			' First, estimate generalization error
			'
			testset = New Double(npoints - 1, rowlen - 1) {}
			cvset = New Double(npoints - 1, rowlen - 1) {}
			x = New Double(nin - 1) {}
			y = New Double(nout - 1) {}
			mlpkfoldsplit(xy, npoints, nclasses, foldscount, False, folds)
			cvrep.relclserror = 0
			cvrep.avgce = 0
			cvrep.rmserror = 0
			cvrep.avgerror = 0
			cvrep.avgrelerror = 0
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0
			relcnt = 0
			For fold = 0 To foldscount - 1

				'
				' Separate set
				'
				tssize = 0
				cvssize = 0
				For i = 0 To npoints - 1
					If folds(i) = fold Then
						For i_ = 0 To rowlen - 1
							testset(tssize, i_) = xy(i, i_)
						Next
						tssize = tssize + 1
					Else
						For i_ = 0 To rowlen - 1
							cvset(cvssize, i_) = xy(i, i_)
						Next
						cvssize = cvssize + 1
					End If
				Next

				'
				' Train on CV training set
				'
				If lmalgorithm Then
					mlptrainlm(network, cvset, cvssize, decay, restarts, info, _
						internalrep)
				Else
					mlptrainlbfgs(network, cvset, cvssize, decay, restarts, wstep, _
						maxits, info, internalrep)
				End If
				If info < 0 Then
					cvrep.relclserror = 0
					cvrep.avgce = 0
					cvrep.rmserror = 0
					cvrep.avgerror = 0
					cvrep.avgrelerror = 0
					Return
				End If
				rep.ngrad = rep.ngrad + internalrep.ngrad
				rep.nhess = rep.nhess + internalrep.nhess
				rep.ncholesky = rep.ncholesky + internalrep.ncholesky

				'
				' Estimate error using CV test set
				'
				If mlpbase.mlpissoftmax(network) Then

					'
					' classification-only code
					'
					cvrep.relclserror = cvrep.relclserror + mlpbase.mlpclserror(network, testset, tssize)
					cvrep.avgce = cvrep.avgce + mlpbase.mlperrorn(network, testset, tssize)
				End If
				For i = 0 To tssize - 1
					For i_ = 0 To nin - 1
						x(i_) = testset(i, i_)
					Next
					mlpbase.mlpprocess(network, x, y)
					If mlpbase.mlpissoftmax(network) Then

						'
						' Classification-specific code
						'
						k = CInt(System.Math.Truncate(System.Math.Round(testset(i, nin))))
						For j = 0 To nout - 1
							If j = k Then
								cvrep.rmserror = cvrep.rmserror + Math.sqr(y(j) - 1)
								cvrep.avgerror = cvrep.avgerror + System.Math.Abs(y(j) - 1)
								cvrep.avgrelerror = cvrep.avgrelerror + System.Math.Abs(y(j) - 1)
								relcnt = relcnt + 1
							Else
								cvrep.rmserror = cvrep.rmserror + Math.sqr(y(j))
								cvrep.avgerror = cvrep.avgerror + System.Math.Abs(y(j))
							End If
						Next
					Else

						'
						' Regression-specific code
						'
						For j = 0 To nout - 1
							cvrep.rmserror = cvrep.rmserror + Math.sqr(y(j) - testset(i, nin + j))
							cvrep.avgerror = cvrep.avgerror + System.Math.Abs(y(j) - testset(i, nin + j))
							If CDbl(testset(i, nin + j)) <> CDbl(0) Then
								cvrep.avgrelerror = cvrep.avgrelerror + System.Math.Abs((y(j) - testset(i, nin + j)) / testset(i, nin + j))
								relcnt = relcnt + 1
							End If
						Next
					End If
				Next
			Next
			If mlpbase.mlpissoftmax(network) Then
				cvrep.relclserror = cvrep.relclserror / npoints
				cvrep.avgce = cvrep.avgce / (System.Math.Log(2) * npoints)
			End If
			cvrep.rmserror = System.Math.sqrt(cvrep.rmserror / (npoints * nout))
			cvrep.avgerror = cvrep.avgerror / (npoints * nout)
			If relcnt > 0 Then
				cvrep.avgrelerror = cvrep.avgrelerror / relcnt
			End If
			info = 1
		End Sub


		'************************************************************************
'        Subroutine prepares K-fold split of the training set.
'
'        NOTES:
'            "NClasses>0" means that we have classification task.
'            "NClasses<0" means regression task with -NClasses real outputs.
'        ************************************************************************

		Private Shared Sub mlpkfoldsplit(xy As Double(,), npoints As Integer, nclasses As Integer, foldscount As Integer, stratifiedsplits As Boolean, ByRef folds As Integer())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim rs As New hqrnd.hqrndstate()

			folds = New Integer(-1) {}


			'
			' test parameters
			'
			alglib.ap.assert(npoints > 0, "MLPKFoldSplit: wrong NPoints!")
			alglib.ap.assert(nclasses > 1 OrElse nclasses < 0, "MLPKFoldSplit: wrong NClasses!")
			alglib.ap.assert(foldscount >= 2 AndAlso foldscount <= npoints, "MLPKFoldSplit: wrong FoldsCount!")
			alglib.ap.assert(Not stratifiedsplits, "MLPKFoldSplit: stratified splits are not supported!")

			'
			' Folds
			'
			hqrnd.hqrndrandomize(rs)
			folds = New Integer(npoints - 1) {}
			For i = 0 To npoints - 1
				folds(i) = i * foldscount \ npoints
			Next
			For i = 0 To npoints - 2
				j = i + hqrnd.hqrnduniformi(rs, npoints - i)
				If j <> i Then
					k = folds(i)
					folds(i) = folds(j)
					folds(j) = k
				End If
			Next
		End Sub


		'************************************************************************
'        Internal subroutine for parallelization function MLPFoldCV.
'
'
'        INPUT PARAMETERS:
'            S         -   trainer object;
'            RowSize   -   row size(eitherNIn+NOut or NIn+1);
'            NRestarts -   number of restarts(>=0);
'            Folds     -   cross-validation set;
'            Fold      -   the number of first cross-validation(>=0);
'            DFold     -   the number of second cross-validation(>=Fold+1);
'            CVY       -   parameter which stores  the result is returned by network,
'                          training on I-th cross-validation set.
'                          It has to be preallocated.
'            PoolDataCV-   parameter for parallelization.
'            
'        NOTE: There are no checks on the parameters correctness.
'
'          -- ALGLIB --
'             Copyright 25.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mthreadcv(s As mlptrainer, rowsize As Integer, nrestarts As Integer, folds As Integer(), fold As Integer, dfold As Integer, _
			cvy As Double(,), pooldatacv As alglib.smp.shared_pool)
			Dim datacv As mlpparallelizationcv = Nothing
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			If fold = dfold - 1 Then

				'
				' Separate set
				'
				alglib.smp.ae_shared_pool_retrieve(pooldatacv, datacv)
				datacv.subsetsize = 0
				For i = 0 To s.npoints - 1
					If folds(i) <> fold Then
						datacv.subset(datacv.subsetsize) = i
						datacv.subsetsize = datacv.subsetsize + 1
					End If
				Next

				'
				' Train on CV training set
				'
				mlptrainnetworkx(s, nrestarts, -1, datacv.subset, datacv.subsetsize, datacv.subset, _
					0, datacv.network, datacv.rep, True, datacv.trnpool)
				datacv.ngrad = datacv.ngrad + datacv.rep.ngrad

				'
				' Estimate error using CV test set
				'
				For i = 0 To s.npoints - 1
					If folds(i) = fold Then
						If s.datatype = 0 Then
							For i_ = 0 To rowsize - 1
								datacv.xyrow(i_) = s.densexy(i, i_)
							Next
						End If
						If s.datatype = 1 Then
							sparse.sparsegetrow(s.sparsexy, i, datacv.xyrow)
						End If
						mlpbase.mlpprocess(datacv.network, datacv.xyrow, datacv.y)
						For i_ = 0 To s.nout - 1
							cvy(i, i_) = datacv.y(i_)
						Next
					End If
				Next
				alglib.smp.ae_shared_pool_recycle(pooldatacv, datacv)
			Else
				alglib.ap.assert(fold < dfold - 1, "MThreadCV: internal error(Fold>DFold-1).")
				mthreadcv(s, rowsize, nrestarts, folds, fold, (fold + dfold) \ 2, _
					cvy, pooldatacv)
				mthreadcv(s, rowsize, nrestarts, folds, (fold + dfold) \ 2, dfold, _
					cvy, pooldatacv)
			End If
		End Sub


		'************************************************************************
'        This function trains neural network passed to this function, using current
'        dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
'        and current training settings. Training  from  NRestarts  random  starting
'        positions is performed, best network is chosen.
'
'        This function is inteded to be used internally. It may be used in  several
'        settings:
'        * training with ValSubsetSize=0, corresponds  to  "normal"  training  with
'          termination  criteria  based on S.MaxIts (steps count) and S.WStep (step
'          size). Training sample is given by TrnSubset/TrnSubsetSize.
'        * training with ValSubsetSize>0, corresponds to  early  stopping  training
'          with additional MaxIts/WStep stopping criteria. Training sample is given
'          by TrnSubset/TrnSubsetSize, validation sample  is  given  by  ValSubset/
'          ValSubsetSize.
'
'          -- ALGLIB --
'             Copyright 13.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mlptrainnetworkx(s As mlptrainer, nrestarts As Integer, algokind As Integer, trnsubset As Integer(), trnsubsetsize As Integer, valsubset As Integer(), _
			valsubsetsize As Integer, network As mlpbase.multilayerperceptron, rep As mlpreport, isrootcall As Boolean, sessions As alglib.smp.shared_pool)
			Dim modrep As New mlpbase.modelerrors()
			Dim eval As Double = 0
			Dim ebest As Double = 0
			Dim ngradbatch As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim pcount As Integer = 0
			Dim itbest As Integer = 0
			Dim itcnt As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim rndstart As New Boolean()
			Dim i As Integer = 0
			Dim nr0 As Integer = 0
			Dim nr1 As Integer = 0
			Dim rep0 As New mlpreport()
			Dim rep1 As New mlpreport()
			Dim randomizenetwork As New Boolean()
			Dim bestrmserror As Double = 0
			Dim psession As smlptrnsession = Nothing
			Dim i_ As Integer = 0

			mlpbase.mlpproperties(network, nin, nout, wcount)

			'
			' Process root call
			'
			If isrootcall Then

				'
				' Check correctness of parameters
				'
				alglib.ap.assert(algokind = 0 OrElse algokind = -1, "MLPTrainNetworkX: unexpected AlgoKind")
				alglib.ap.assert(s.npoints >= 0, "MLPTrainNetworkX: internal error - parameter S is not initialized or is spoiled(S.NPoints<0)")
				If s.rcpar Then
					ttype = 0
				Else
					ttype = 1
				End If
				If Not mlpbase.mlpissoftmax(network) Then
					ntype = 0
				Else
					ntype = 1
				End If
				alglib.ap.assert(ntype = ttype, "MLPTrainNetworkX: internal error - type of the training network is not similar to network type in trainer object")
				alglib.ap.assert(s.nin = nin, "MLPTrainNetworkX: internal error - number of inputs in trainer is not equal to number of inputs in the training network.")
				alglib.ap.assert(s.nout = nout, "MLPTrainNetworkX: internal error - number of outputs in trainer is not equal to number of outputs in the training network.")
				alglib.ap.assert(nrestarts >= 0, "MLPTrainNetworkX: internal error - NRestarts<0.")
				alglib.ap.assert(alglib.ap.len(trnsubset) >= trnsubsetsize, "MLPTrainNetworkX: internal error - parameter TrnSubsetSize more than input subset size(Length(TrnSubset)<TrnSubsetSize)")
				For i = 0 To trnsubsetsize - 1
					alglib.ap.assert(trnsubset(i) >= 0 AndAlso trnsubset(i) <= s.npoints - 1, "MLPTrainNetworkX: internal error - parameter TrnSubset contains incorrect index(TrnSubset[I]<0 or TrnSubset[I]>S.NPoints-1)")
				Next
				alglib.ap.assert(alglib.ap.len(valsubset) >= valsubsetsize, "MLPTrainNetworkX: internal error - parameter ValSubsetSize more than input subset size(Length(ValSubset)<ValSubsetSize)")
				For i = 0 To valsubsetsize - 1
					alglib.ap.assert(valsubset(i) >= 0 AndAlso valsubset(i) <= s.npoints - 1, "MLPTrainNetworkX: internal error - parameter ValSubset contains incorrect index(ValSubset[I]<0 or ValSubset[I]>S.NPoints-1)")
				Next

				'
				' Train
				'
				randomizenetwork = nrestarts > 0
				initmlptrnsessions(network, randomizenetwork, s, sessions)
				mlptrainnetworkx(s, nrestarts, algokind, trnsubset, trnsubsetsize, valsubset, _
					valsubsetsize, network, rep, False, sessions)

				'
				' Choose best network
				'
				bestrmserror = Math.maxrealnumber
				alglib.smp.ae_shared_pool_first_recycled(sessions, psession)
				While psession IsNot Nothing
					If CDbl(psession.bestrmserror) < CDbl(bestrmserror) Then
						mlpbase.mlpimporttunableparameters(network, psession.bestparameters)
						bestrmserror = psession.bestrmserror
					End If
					alglib.smp.ae_shared_pool_next_recycled(sessions, psession)
				End While

				'
				' Calculate errors
				'
				If s.datatype = 0 Then
					mlpbase.mlpallerrorssubset(network, s.densexy, s.npoints, trnsubset, trnsubsetsize, modrep)
				End If
				If s.datatype = 1 Then
					mlpbase.mlpallerrorssparsesubset(network, s.sparsexy, s.npoints, trnsubset, trnsubsetsize, modrep)
				End If
				rep.relclserror = modrep.relclserror
				rep.avgce = modrep.avgce
				rep.rmserror = modrep.rmserror
				rep.avgerror = modrep.avgerror
				rep.avgrelerror = modrep.avgrelerror

				'
				' Done
				'
				Return
			End If

			'
			' Split problem, if we have more than 1 restart
			'
			If nrestarts >= 2 Then

				'
				' Divide problem with NRestarts into two: NR0 and NR1.
				'
				nr0 = nrestarts \ 2
				nr1 = nrestarts - nr0
				mlptrainnetworkx(s, nr0, algokind, trnsubset, trnsubsetsize, valsubset, _
					valsubsetsize, network, rep0, False, sessions)
				mlptrainnetworkx(s, nr1, algokind, trnsubset, trnsubsetsize, valsubset, _
					valsubsetsize, network, rep1, False, sessions)

				'
				' Aggregate results
				'
				rep.ngrad = rep0.ngrad + rep1.ngrad
				rep.nhess = rep0.nhess + rep1.nhess
				rep.ncholesky = rep0.ncholesky + rep1.ncholesky

				'
				' Done :)
				'
				Return
			End If

			'
			' Execution with NRestarts=1 or NRestarts=0:
			' * NRestarts=1 means that network is restarted from random position
			' * NRestarts=0 means that network is not randomized
			'
			alglib.ap.assert(nrestarts = 0 OrElse nrestarts = 1, "MLPTrainNetworkX: internal error")
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0
			alglib.smp.ae_shared_pool_retrieve(sessions, psession)
			If ((s.datatype = 0 OrElse s.datatype = 1) AndAlso s.npoints > 0) AndAlso trnsubsetsize <> 0 Then

				'
				' Train network using combination of early stopping and step-size
				' and step-count based criteria. Network state with best value of
				' validation set error is stored in WBuf0. When validation set is
				' zero, most recent state of network is stored.
				'
				rndstart = nrestarts <> 0
				ngradbatch = 0
				eval = 0
				ebest = 0
				itbest = 0
				itcnt = 0
				mlpstarttrainingx(s, rndstart, algokind, trnsubset, trnsubsetsize, psession)
				If s.datatype = 0 Then
					ebest = mlpbase.mlperrorsubset(psession.network, s.densexy, s.npoints, valsubset, valsubsetsize)
				End If
				If s.datatype = 1 Then
					ebest = mlpbase.mlperrorsparsesubset(psession.network, s.sparsexy, s.npoints, valsubset, valsubsetsize)
				End If
				For i_ = 0 To wcount - 1
					psession.wbuf0(i_) = psession.network.weights(i_)
				Next
				While mlpcontinuetrainingx(s, trnsubset, trnsubsetsize, ngradbatch, psession)
					If s.datatype = 0 Then
						eval = mlpbase.mlperrorsubset(psession.network, s.densexy, s.npoints, valsubset, valsubsetsize)
					End If
					If s.datatype = 1 Then
						eval = mlpbase.mlperrorsparsesubset(psession.network, s.sparsexy, s.npoints, valsubset, valsubsetsize)
					End If
					If CDbl(eval) <= CDbl(ebest) OrElse valsubsetsize = 0 Then
						For i_ = 0 To wcount - 1
							psession.wbuf0(i_) = psession.network.weights(i_)
						Next
						ebest = eval
						itbest = itcnt
					End If
					If itcnt > 30 AndAlso CDbl(itcnt) > CDbl(1.5 * itbest) Then
						Exit While
					End If
					itcnt = itcnt + 1
				End While
				For i_ = 0 To wcount - 1
					psession.network.weights(i_) = psession.wbuf0(i_)
				Next
				rep.ngrad = ngradbatch
			Else
				For i = 0 To wcount - 1
					psession.network.weights(i) = 0
				Next
			End If

			'
			' Evaluate network performance and update PSession.BestParameters/BestRMSError
			' (if needed).
			'
			If s.datatype = 0 Then
				mlpbase.mlpallerrorssubset(psession.network, s.densexy, s.npoints, trnsubset, trnsubsetsize, modrep)
			End If
			If s.datatype = 1 Then
				mlpbase.mlpallerrorssparsesubset(psession.network, s.sparsexy, s.npoints, trnsubset, trnsubsetsize, modrep)
			End If
			If CDbl(modrep.rmserror) < CDbl(psession.bestrmserror) Then
				mlpbase.mlpexporttunableparameters(psession.network, psession.bestparameters, pcount)
				psession.bestrmserror = modrep.rmserror
			End If

			'
			' Move session back to pool
			'
			alglib.smp.ae_shared_pool_recycle(sessions, psession)
		End Sub


		'************************************************************************
'        This function trains neural network ensemble passed to this function using
'        current dataset and early stopping training algorithm. Each early stopping
'        round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
'        training rounds is performed in total).
'
'
'          -- ALGLIB --
'             Copyright 22.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mlptrainensemblex(s As mlptrainer, ensemble As mlpe.mlpensemble, idx0 As Integer, idx1 As Integer, nrestarts As Integer, trainingmethod As Integer, _
			ngrad As apserv.sinteger, isrootcall As Boolean, esessions As alglib.smp.shared_pool)
			Dim pcount As Integer = 0
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim trnsubsetsize As Integer = 0
			Dim valsubsetsize As Integer = 0
			Dim k0 As Integer = 0
			Dim ngrad0 As New apserv.sinteger()
			Dim ngrad1 As New apserv.sinteger()
			Dim psession As mlpetrnsession = Nothing
			Dim rs As New hqrnd.hqrndstate()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			wcount = mlpbase.mlpgetweightscount(ensemble.network)
			If mlpbase.mlpissoftmax(ensemble.network) Then
				pcount = nin
			Else
				pcount = nin + nout
			End If
			If nrestarts <= 0 Then
				nrestarts = 1
			End If

			'
			' Handle degenerate case
			'
			If s.npoints < 2 Then
				For i = idx0 To idx1 - 1
					For j = 0 To wcount - 1
						ensemble.weights(i * wcount + j) = 0.0
					Next
					For j = 0 To pcount - 1
						ensemble.columnmeans(i * pcount + j) = 0.0
						ensemble.columnsigmas(i * pcount + j) = 1.0
					Next
				Next
				Return
			End If

			'
			' Process root call
			'
			If isrootcall Then

				'
				' Prepare:
				' * prepare MLPETrnSessions
				' * fill ensemble by zeros (helps to detect errors)
				'
				initmlpetrnsessions(ensemble.network, s, esessions)
				For i = idx0 To idx1 - 1
					For j = 0 To wcount - 1
						ensemble.weights(i * wcount + j) = 0.0
					Next
					For j = 0 To pcount - 1
						ensemble.columnmeans(i * pcount + j) = 0.0
						ensemble.columnsigmas(i * pcount + j) = 0.0
					Next
				Next

				'
				' Train in non-root mode and exit
				'
				mlptrainensemblex(s, ensemble, idx0, idx1, nrestarts, trainingmethod, _
					ngrad, False, esessions)
				Return
			End If

			'
			' Split problem
			'
			If idx1 - idx0 >= 2 Then
				k0 = (idx1 - idx0) \ 2
				ngrad0.val = 0
				ngrad1.val = 0
				mlptrainensemblex(s, ensemble, idx0, idx0 + k0, nrestarts, trainingmethod, _
					ngrad0, False, esessions)
				mlptrainensemblex(s, ensemble, idx0 + k0, idx1, nrestarts, trainingmethod, _
					ngrad1, False, esessions)
				ngrad.val = ngrad0.val + ngrad1.val
				Return
			End If

			'
			' Retrieve and prepare session
			'
			alglib.smp.ae_shared_pool_retrieve(esessions, psession)

			'
			' Train
			'
			hqrnd.hqrndrandomize(rs)
			For k = idx0 To idx1 - 1

				'
				' Split set
				'
				trnsubsetsize = 0
				valsubsetsize = 0
				If trainingmethod = 0 Then
					Do
						trnsubsetsize = 0
						valsubsetsize = 0
						For i = 0 To s.npoints - 1
							If CDbl(Math.randomreal()) < CDbl(0.66) Then

								'
								' Assign sample to training set
								'
								psession.trnsubset(trnsubsetsize) = i
								trnsubsetsize = trnsubsetsize + 1
							Else

								'
								' Assign sample to validation set
								'
								psession.valsubset(valsubsetsize) = i
								valsubsetsize = valsubsetsize + 1
							End If
						Next
					Loop While Not (trnsubsetsize <> 0 AndAlso valsubsetsize <> 0)
				End If
				If trainingmethod = 1 Then
					valsubsetsize = 0
					trnsubsetsize = s.npoints
					For i = 0 To s.npoints - 1
						psession.trnsubset(i) = hqrnd.hqrnduniformi(rs, s.npoints)
					Next
				End If

				'
				' Train
				'
				mlptrainnetworkx(s, nrestarts, -1, psession.trnsubset, trnsubsetsize, psession.valsubset, _
					valsubsetsize, psession.network, psession.mlprep, True, psession.mlpsessions)
				ngrad.val = ngrad.val + psession.mlprep.ngrad

				'
				' Save results
				'
				i1_ = (0) - (k * wcount)
				For i_ = k * wcount To (k + 1) * wcount - 1
					ensemble.weights(i_) = psession.network.weights(i_ + i1_)
				Next
				i1_ = (0) - (k * pcount)
				For i_ = k * pcount To (k + 1) * pcount - 1
					ensemble.columnmeans(i_) = psession.network.columnmeans(i_ + i1_)
				Next
				i1_ = (0) - (k * pcount)
				For i_ = k * pcount To (k + 1) * pcount - 1
					ensemble.columnsigmas(i_) = psession.network.columnsigmas(i_ + i1_)
				Next
			Next

			'
			' Recycle session
			'
			alglib.smp.ae_shared_pool_recycle(esessions, psession)
		End Sub


		'************************************************************************
'        This function performs step-by-step training of the neural  network.  Here
'        "step-by-step" means that training  starts  with  MLPStartTrainingX  call,
'        and then user subsequently calls MLPContinueTrainingX  to perform one more
'        iteration of the training.
'
'        After call to this function trainer object remembers network and  is ready
'        to  train  it.  However,  no  training  is  performed  until first call to 
'        MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
'        will advance traing progress one iteration further.
'
'
'          -- ALGLIB --
'             Copyright 13.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mlpstarttrainingx(s As mlptrainer, randomstart As Boolean, algokind As Integer, subset As Integer(), subsetsize As Integer, session As smlptrnsession)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim i As Integer = 0


			'
			' Check parameters
			'
			alglib.ap.assert(s.npoints >= 0, "MLPStartTrainingX: internal error - parameter S is not initialized or is spoiled(S.NPoints<0)")
			alglib.ap.assert(algokind = 0 OrElse algokind = -1, "MLPStartTrainingX: unexpected AlgoKind")
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			If Not mlpbase.mlpissoftmax(session.network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPStartTrainingX: internal error - type of the resulting network is not similar to network type in trainer object")
			mlpbase.mlpproperties(session.network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPStartTrainingX: number of inputs in trainer is not equal to number of inputs in the network.")
			alglib.ap.assert(s.nout = nout, "MLPStartTrainingX: number of outputs in trainer is not equal to number of outputs in the network.")
			alglib.ap.assert(alglib.ap.len(subset) >= subsetsize, "MLPStartTrainingX: internal error - parameter SubsetSize more than input subset size(Length(Subset)<SubsetSize)")
			For i = 0 To subsetsize - 1
				alglib.ap.assert(subset(i) >= 0 AndAlso subset(i) <= s.npoints - 1, "MLPStartTrainingX: internal error - parameter Subset contains incorrect index(Subset[I]<0 or Subset[I]>S.NPoints-1)")
			Next

			'
			' Prepare session
			'
			minlbfgs.minlbfgssetcond(session.optimizer, 0.0, 0.0, s.wstep, s.maxits)
			If s.npoints > 0 AndAlso subsetsize <> 0 Then
				If randomstart Then
					mlpbase.mlprandomize(session.network)
				End If
				minlbfgs.minlbfgsrestartfrom(session.optimizer, session.network.weights)
			Else
				For i = 0 To wcount - 1
					session.network.weights(i) = 0
				Next
			End If
			If algokind = -1 Then
				session.algoused = s.algokind
				If s.algokind = 1 Then
					session.minibatchsize = s.minibatchsize
				End If
			Else
				session.algoused = 0
			End If
			hqrnd.hqrndrandomize(session.generator)
			session.rstate.ia = New Integer(15) {}
			session.rstate.ra = New Double(1) {}
			session.rstate.stage = -1
		End Sub


		'************************************************************************
'        This function performs step-by-step training of the neural  network.  Here
'        "step-by-step" means  that training starts  with  MLPStartTrainingX  call,
'        and then user subsequently calls MLPContinueTrainingX  to perform one more
'        iteration of the training.
'
'        This  function  performs  one  more  iteration of the training and returns
'        either True (training continues) or False (training stopped). In case True
'        was returned, Network weights are updated according to the  current  state
'        of the optimization progress. In case False was  returned,  no  additional
'        updates is performed (previous update of  the  network weights moved us to
'        the final point, and no additional updates is needed).
'
'        EXAMPLE:
'            >
'            > [initialize network and trainer object]
'            >
'            > MLPStartTraining(Trainer, Network, True)
'            > while MLPContinueTraining(Trainer, Network) do
'            >     [visualize training progress]
'            >
'
'
'          -- ALGLIB --
'             Copyright 13.08.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function mlpcontinuetrainingx(s As mlptrainer, subset As Integer(), subsetsize As Integer, ByRef ngradbatch As Integer, session As smlptrnsession) As Boolean
			Dim result As New Boolean()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim twcount As Integer = 0
			Dim ntype As Integer = 0
			Dim ttype As Integer = 0
			Dim decay As Double = 0
			Dim v As Double = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim trnsetsize As Integer = 0
			Dim epoch As Integer = 0
			Dim minibatchcount As Integer = 0
			Dim minibatchidx As Integer = 0
			Dim cursize As Integer = 0
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If session.rstate.stage >= 0 Then
				nin = session.rstate.ia(0)
				nout = session.rstate.ia(1)
				wcount = session.rstate.ia(2)
				twcount = session.rstate.ia(3)
				ntype = session.rstate.ia(4)
				ttype = session.rstate.ia(5)
				i = session.rstate.ia(6)
				j = session.rstate.ia(7)
				k = session.rstate.ia(8)
				trnsetsize = session.rstate.ia(9)
				epoch = session.rstate.ia(10)
				minibatchcount = session.rstate.ia(11)
				minibatchidx = session.rstate.ia(12)
				cursize = session.rstate.ia(13)
				idx0 = session.rstate.ia(14)
				idx1 = session.rstate.ia(15)
				decay = session.rstate.ra(0)
				v = session.rstate.ra(1)
			Else
				nin = -983
				nout = -989
				wcount = -834
				twcount = 900
				ntype = -287
				ttype = 364
				i = 214
				j = -338
				k = -686
				trnsetsize = 912
				epoch = 585
				minibatchcount = 497
				minibatchidx = -271
				cursize = -581
				idx0 = 745
				idx1 = -533
				decay = -77
				v = 678
			End If
			If session.rstate.stage = 0 Then
				GoTo lbl_0
			End If

			'
			' Routine body
			'

			'
			' Check correctness of inputs
			'
			alglib.ap.assert(s.npoints >= 0, "MLPContinueTrainingX: internal error - parameter S is not initialized or is spoiled(S.NPoints<0).")
			If s.rcpar Then
				ttype = 0
			Else
				ttype = 1
			End If
			If Not mlpbase.mlpissoftmax(session.network) Then
				ntype = 0
			Else
				ntype = 1
			End If
			alglib.ap.assert(ntype = ttype, "MLPContinueTrainingX: internal error - type of the resulting network is not similar to network type in trainer object.")
			mlpbase.mlpproperties(session.network, nin, nout, wcount)
			alglib.ap.assert(s.nin = nin, "MLPContinueTrainingX: internal error - number of inputs in trainer is not equal to number of inputs in the network.")
			alglib.ap.assert(s.nout = nout, "MLPContinueTrainingX: internal error - number of outputs in trainer is not equal to number of outputs in the network.")
			alglib.ap.assert(alglib.ap.len(subset) >= subsetsize, "MLPContinueTrainingX: internal error - parameter SubsetSize more than input subset size(Length(Subset)<SubsetSize).")
			For i = 0 To subsetsize - 1
				alglib.ap.assert(subset(i) >= 0 AndAlso subset(i) <= s.npoints - 1, "MLPContinueTrainingX: internal error - parameter Subset contains incorrect index(Subset[I]<0 or Subset[I]>S.NPoints-1).")
			Next

			'
			' Quick exit on empty training set
			'
			If s.npoints = 0 OrElse subsetsize = 0 Then
				result = False
				Return result
			End If

			'
			' Minibatch training
			'
			If session.algoused = 1 Then
				alglib.ap.assert(False, "MINIBATCH TRAINING IS NOT IMPLEMENTED YET")
			End If

			'
			' Last option: full batch training
			'
			decay = s.decay
			lbl_1:
			If Not minlbfgs.minlbfgsiteration(session.optimizer) Then
				GoTo lbl_2
			End If
			If Not session.optimizer.xupdated Then
				GoTo lbl_3
			End If
			For i_ = 0 To wcount - 1
				session.network.weights(i_) = session.optimizer.x(i_)
			Next
			session.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			lbl_3:
			For i_ = 0 To wcount - 1
				session.network.weights(i_) = session.optimizer.x(i_)
			Next
			If s.datatype = 0 Then
				mlpbase.mlpgradbatchsubset(session.network, s.densexy, s.npoints, subset, subsetsize, session.optimizer.f, _
					session.optimizer.g)
			End If
			If s.datatype = 1 Then
				mlpbase.mlpgradbatchsparsesubset(session.network, s.sparsexy, s.npoints, subset, subsetsize, session.optimizer.f, _
					session.optimizer.g)
			End If

			'
			' Increment number of operations performed on batch gradient
			'
			ngradbatch = ngradbatch + 1
			v = 0.0
			For i_ = 0 To wcount - 1
				v += session.network.weights(i_) * session.network.weights(i_)
			Next
			session.optimizer.f = session.optimizer.f + 0.5 * decay * v
			For i_ = 0 To wcount - 1
				session.optimizer.g(i_) = session.optimizer.g(i_) + decay * session.network.weights(i_)
			Next
			GoTo lbl_1
			lbl_2:
			minlbfgs.minlbfgsresultsbuf(session.optimizer, session.network.weights, session.optimizerrep)
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			session.rstate.ia(0) = nin
			session.rstate.ia(1) = nout
			session.rstate.ia(2) = wcount
			session.rstate.ia(3) = twcount
			session.rstate.ia(4) = ntype
			session.rstate.ia(5) = ttype
			session.rstate.ia(6) = i
			session.rstate.ia(7) = j
			session.rstate.ia(8) = k
			session.rstate.ia(9) = trnsetsize
			session.rstate.ia(10) = epoch
			session.rstate.ia(11) = minibatchcount
			session.rstate.ia(12) = minibatchidx
			session.rstate.ia(13) = cursize
			session.rstate.ia(14) = idx0
			session.rstate.ia(15) = idx1
			session.rstate.ra(0) = decay
			session.rstate.ra(1) = v
			Return result
		End Function


		'************************************************************************
'        Internal bagging subroutine.
'
'          -- ALGLIB --
'             Copyright 19.02.2009 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mlpebagginginternal(ensemble As mlpe.mlpensemble, xy As Double(,), npoints As Integer, decay As Double, restarts As Integer, wstep As Double, _
			maxits As Integer, lmalgorithm As Boolean, ByRef info As Integer, rep As mlpreport, ooberrors As mlpcvreport)
			Dim xys As Double(,) = New Double(-1, -1) {}
			Dim s As Boolean() = New Boolean(-1) {}
			Dim oobbuf As Double(,) = New Double(-1, -1) {}
			Dim oobcntbuf As Integer() = New Integer(-1) {}
			Dim x As Double() = New Double(-1) {}
			Dim y As Double() = New Double(-1) {}
			Dim dy As Double() = New Double(-1) {}
			Dim dsbuf As Double() = New Double(-1) {}
			Dim ccnt As Integer = 0
			Dim pcnt As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim tmprep As New mlpreport()
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim rs As New hqrnd.hqrndstate()
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0

			info = 0

			nin = mlpbase.mlpgetinputscount(ensemble.network)
			nout = mlpbase.mlpgetoutputscount(ensemble.network)
			wcount = mlpbase.mlpgetweightscount(ensemble.network)

			'
			' Test for inputs
			'
			If (Not lmalgorithm AndAlso CDbl(wstep) = CDbl(0)) AndAlso maxits = 0 Then
				info = -8
				Return
			End If
			If ((npoints <= 0 OrElse restarts < 1) OrElse CDbl(wstep) < CDbl(0)) OrElse maxits < 0 Then
				info = -1
				Return
			End If
			If mlpbase.mlpissoftmax(ensemble.network) Then
				For i = 0 To npoints - 1
					If CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) < 0 OrElse CInt(System.Math.Truncate(System.Math.Round(xy(i, nin)))) >= nout Then
						info = -2
						Return
					End If
				Next
			End If

			'
			' allocate temporaries
			'
			info = 2
			rep.ngrad = 0
			rep.nhess = 0
			rep.ncholesky = 0
			ooberrors.relclserror = 0
			ooberrors.avgce = 0
			ooberrors.rmserror = 0
			ooberrors.avgerror = 0
			ooberrors.avgrelerror = 0
			If mlpbase.mlpissoftmax(ensemble.network) Then
				ccnt = nin + 1
				pcnt = nin
			Else
				ccnt = nin + nout
				pcnt = nin + nout
			End If
			xys = New Double(npoints - 1, ccnt - 1) {}
			s = New Boolean(npoints - 1) {}
			oobbuf = New Double(npoints - 1, nout - 1) {}
			oobcntbuf = New Integer(npoints - 1) {}
			x = New Double(nin - 1) {}
			y = New Double(nout - 1) {}
			If mlpbase.mlpissoftmax(ensemble.network) Then
				dy = New Double(0) {}
			Else
				dy = New Double(nout - 1) {}
			End If
			For i = 0 To npoints - 1
				For j = 0 To nout - 1
					oobbuf(i, j) = 0
				Next
			Next
			For i = 0 To npoints - 1
				oobcntbuf(i) = 0
			Next

			'
			' main bagging cycle
			'
			hqrnd.hqrndrandomize(rs)
			For k = 0 To ensemble.ensemblesize - 1

				'
				' prepare dataset
				'
				For i = 0 To npoints - 1
					s(i) = False
				Next
				For i = 0 To npoints - 1
					j = hqrnd.hqrnduniformi(rs, npoints)
					s(j) = True
					For i_ = 0 To ccnt - 1
						xys(i, i_) = xy(j, i_)
					Next
				Next

				'
				' train
				'
				If lmalgorithm Then
					mlptrainlm(ensemble.network, xys, npoints, decay, restarts, info, _
						tmprep)
				Else
					mlptrainlbfgs(ensemble.network, xys, npoints, decay, restarts, wstep, _
						maxits, info, tmprep)
				End If
				If info < 0 Then
					Return
				End If

				'
				' save results
				'
				rep.ngrad = rep.ngrad + tmprep.ngrad
				rep.nhess = rep.nhess + tmprep.nhess
				rep.ncholesky = rep.ncholesky + tmprep.ncholesky
				i1_ = (0) - (k * wcount)
				For i_ = k * wcount To (k + 1) * wcount - 1
					ensemble.weights(i_) = ensemble.network.weights(i_ + i1_)
				Next
				i1_ = (0) - (k * pcnt)
				For i_ = k * pcnt To (k + 1) * pcnt - 1
					ensemble.columnmeans(i_) = ensemble.network.columnmeans(i_ + i1_)
				Next
				i1_ = (0) - (k * pcnt)
				For i_ = k * pcnt To (k + 1) * pcnt - 1
					ensemble.columnsigmas(i_) = ensemble.network.columnsigmas(i_ + i1_)
				Next

				'
				' OOB estimates
				'
				For i = 0 To npoints - 1
					If Not s(i) Then
						For i_ = 0 To nin - 1
							x(i_) = xy(i, i_)
						Next
						mlpbase.mlpprocess(ensemble.network, x, y)
						For i_ = 0 To nout - 1
							oobbuf(i, i_) = oobbuf(i, i_) + y(i_)
						Next
						oobcntbuf(i) = oobcntbuf(i) + 1
					End If
				Next
			Next

			'
			' OOB estimates
			'
			If mlpbase.mlpissoftmax(ensemble.network) Then
				bdss.dserrallocate(nout, dsbuf)
			Else
				bdss.dserrallocate(-nout, dsbuf)
			End If
			For i = 0 To npoints - 1
				If oobcntbuf(i) <> 0 Then
					v = CDbl(1) / CDbl(oobcntbuf(i))
					For i_ = 0 To nout - 1
						y(i_) = v * oobbuf(i, i_)
					Next
					If mlpbase.mlpissoftmax(ensemble.network) Then
						dy(0) = xy(i, nin)
					Else
						i1_ = (nin) - (0)
						For i_ = 0 To nout - 1
							dy(i_) = v * xy(i, i_ + i1_)
						Next
					End If
					bdss.dserraccumulate(dsbuf, y, dy)
				End If
			Next
			bdss.dserrfinish(dsbuf)
			ooberrors.relclserror = dsbuf(0)
			ooberrors.avgce = dsbuf(1)
			ooberrors.rmserror = dsbuf(2)
			ooberrors.avgerror = dsbuf(3)
			ooberrors.avgrelerror = dsbuf(4)
		End Sub


		'************************************************************************
'        This function initializes temporaries needed for training session.
'
'
'          -- ALGLIB --
'             Copyright 01.07.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub initmlptrnsession(networktrained As mlpbase.multilayerperceptron, randomizenetwork As Boolean, trainer As mlptrainer, session As smlptrnsession)
			Dim nin As Integer = 0
			Dim nout As Integer = 0
			Dim wcount As Integer = 0
			Dim pcount As Integer = 0
			Dim dummysubset As Integer() = New Integer(-1) {}


			'
			' Prepare network:
			' * copy input network to Session.Network
			' * re-initialize preprocessor and weights if RandomizeNetwork=True
			'
			mlpbase.mlpcopy(networktrained, session.network)
			If randomizenetwork Then
				alglib.ap.assert(trainer.datatype = 0 OrElse trainer.datatype = 1, "InitTemporaries: unexpected Trainer.DataType")
				If trainer.datatype = 0 Then
					mlpbase.mlpinitpreprocessorsubset(session.network, trainer.densexy, trainer.npoints, dummysubset, -1)
				End If
				If trainer.datatype = 1 Then
					mlpbase.mlpinitpreprocessorsparsesubset(session.network, trainer.sparsexy, trainer.npoints, dummysubset, -1)
				End If
				mlpbase.mlprandomize(session.network)
				session.randomizenetwork = True
			Else
				session.randomizenetwork = False
			End If

			'
			' Determine network geometry and initialize optimizer 
			'
			mlpbase.mlpproperties(session.network, nin, nout, wcount)
			minlbfgs.minlbfgscreate(wcount, System.Math.Min(wcount, trainer.lbfgsfactor), session.network.weights, session.optimizer)
			minlbfgs.minlbfgssetxrep(session.optimizer, True)

			'
			' Create buffers
			'
			session.wbuf0 = New Double(wcount - 1) {}
			session.wbuf1 = New Double(wcount - 1) {}

			'
			' Initialize session result
			'
			mlpbase.mlpexporttunableparameters(session.network, session.bestparameters, pcount)
			session.bestrmserror = Math.maxrealnumber
		End Sub


		'************************************************************************
'        This function initializes temporaries needed for training session.
'
'        ************************************************************************

		Private Shared Sub initmlptrnsessions(networktrained As mlpbase.multilayerperceptron, randomizenetwork As Boolean, trainer As mlptrainer, sessions As alglib.smp.shared_pool)
			Dim dummysubset As Integer() = New Integer(-1) {}
			Dim t As New smlptrnsession()
			Dim p As smlptrnsession = Nothing

			If alglib.smp.ae_shared_pool_is_initialized(sessions) Then

				'
				' Pool was already initialized.
				' Clear sessions stored in the pool.
				'
				alglib.smp.ae_shared_pool_first_recycled(sessions, p)
				While p IsNot Nothing
					alglib.ap.assert(mlpbase.mlpsamearchitecture(p.network, networktrained), "InitMLPTrnSessions: internal consistency error")
					p.bestrmserror = Math.maxrealnumber
					alglib.smp.ae_shared_pool_next_recycled(sessions, p)
				End While
			Else

				'
				' Prepare session and seed pool
				'
				initmlptrnsession(networktrained, randomizenetwork, trainer, t)
				alglib.smp.ae_shared_pool_set_seed(sessions, t)
			End If
		End Sub


		'************************************************************************
'        This function initializes temporaries needed for ensemble training.
'
'        ************************************************************************

		Private Shared Sub initmlpetrnsession(individualnetwork As mlpbase.multilayerperceptron, trainer As mlptrainer, session As mlpetrnsession)
			Dim dummysubset As Integer() = New Integer(-1) {}


			'
			' Prepare network:
			' * copy input network to Session.Network
			' * re-initialize preprocessor and weights if RandomizeNetwork=True
			'
			mlpbase.mlpcopy(individualnetwork, session.network)
			initmlptrnsessions(individualnetwork, True, trainer, session.mlpsessions)
			apserv.ivectorsetlengthatleast(session.trnsubset, trainer.npoints)
			apserv.ivectorsetlengthatleast(session.valsubset, trainer.npoints)
		End Sub


		'************************************************************************
'        This function initializes temporaries needed for training session.
'
'        ************************************************************************

		Private Shared Sub initmlpetrnsessions(individualnetwork As mlpbase.multilayerperceptron, trainer As mlptrainer, sessions As alglib.smp.shared_pool)
			Dim t As New mlpetrnsession()

			If Not alglib.smp.ae_shared_pool_is_initialized(sessions) Then
				initmlpetrnsession(individualnetwork, trainer, t)
				alglib.smp.ae_shared_pool_set_seed(sessions, t)
			End If
		End Sub


	End Class
	Public Class pca
		'************************************************************************
'        Principal components analysis
'
'        Subroutine  builds  orthogonal  basis  where  first  axis  corresponds  to
'        direction with maximum variance, second axis maximizes variance in subspace
'        orthogonal to first axis and so on.
'
'        It should be noted that, unlike LDA, PCA does not use class labels.
'
'        COMMERCIAL EDITION OF ALGLIB:
'
'          ! Commercial version of ALGLIB includes one  important  improvement   of
'          ! this function, which can be used from C++ and C#:
'          ! * Intel MKL support (lightweight Intel MKL is shipped with ALGLIB)
'          !
'          ! Intel MKL gives approximately constant  (with  respect  to  number  of
'          ! worker threads) acceleration factor which depends on CPU  being  used,
'          ! problem  size  and  "baseline"  ALGLIB  edition  which  is  used   for
'          ! comparison. Best results are achieved  for  high-dimensional  problems
'          ! (NVars is at least 256).
'          !
'          ! We recommend you to read 'Working with commercial version' section  of
'          ! ALGLIB Reference Manual in order to find out how to  use  performance-
'          ! related features provided by commercial edition of ALGLIB.
'
'        INPUT PARAMETERS:
'            X           -   dataset, array[0..NPoints-1,0..NVars-1].
'                            matrix contains ONLY INDEPENDENT VARIABLES.
'            NPoints     -   dataset size, NPoints>=0
'            NVars       -   number of independent variables, NVars>=1
'
'        OUTPUT PARAMETERS:
'            Info        -   return code:
'                            * -4, if SVD subroutine haven't converged
'                            * -1, if wrong parameters has been passed (NPoints<0,
'                                  NVars<1)
'                            *  1, if task is solved
'            S2          -   array[0..NVars-1]. variance values corresponding
'                            to basis vectors.
'            V           -   array[0..NVars-1,0..NVars-1]
'                            matrix, whose columns store basis vectors.
'
'          -- ALGLIB --
'             Copyright 25.08.2008 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub pcabuildbasis(x As Double(,), npoints As Integer, nvars As Integer, ByRef info As Integer, ByRef s2 As Double(), ByRef v As Double(,))
			Dim a As Double(,) = New Double(-1, -1) {}
			Dim u As Double(,) = New Double(-1, -1) {}
			Dim vt As Double(,) = New Double(-1, -1) {}
			Dim m As Double() = New Double(-1) {}
			Dim t As Double() = New Double(-1) {}
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim mean As Double = 0
			Dim variance As Double = 0
			Dim skewness As Double = 0
			Dim kurtosis As Double = 0
			Dim i_ As Integer = 0

			info = 0
			s2 = New Double(-1) {}
			v = New Double(-1, -1) {}


			'
			' Check input data
			'
			If npoints < 0 OrElse nvars < 1 Then
				info = -1
				Return
			End If
			info = 1

			'
			' Special case: NPoints=0
			'
			If npoints = 0 Then
				s2 = New Double(nvars - 1) {}
				v = New Double(nvars - 1, nvars - 1) {}
				For i = 0 To nvars - 1
					s2(i) = 0
				Next
				For i = 0 To nvars - 1
					For j = 0 To nvars - 1
						If i = j Then
							v(i, j) = 1
						Else
							v(i, j) = 0
						End If
					Next
				Next
				Return
			End If

			'
			' Calculate means
			'
			m = New Double(nvars - 1) {}
			t = New Double(npoints - 1) {}
			For j = 0 To nvars - 1
				For i_ = 0 To npoints - 1
					t(i_) = x(i_, j)
				Next
				basestat.samplemoments(t, npoints, mean, variance, skewness, kurtosis)
				m(j) = mean
			Next

			'
			' Center, apply SVD, prepare output
			'
			a = New Double(System.Math.Max(npoints, nvars) - 1, nvars - 1) {}
			For i = 0 To npoints - 1
				For i_ = 0 To nvars - 1
					a(i, i_) = x(i, i_)
				Next
				For i_ = 0 To nvars - 1
					a(i, i_) = a(i, i_) - m(i_)
				Next
			Next
			For i = npoints To nvars - 1
				For j = 0 To nvars - 1
					a(i, j) = 0
				Next
			Next
			If Not svd.rmatrixsvd(a, System.Math.Max(npoints, nvars), nvars, 0, 1, 2, _
				s2, u, vt) Then
				info = -4
				Return
			End If
			If npoints <> 1 Then
				For i = 0 To nvars - 1
					s2(i) = Math.sqr(s2(i)) / (npoints - 1)
				Next
			End If
			v = New Double(nvars - 1, nvars - 1) {}
			blas.copyandtranspose(vt, 0, nvars - 1, 0, nvars - 1, v, _
				0, nvars - 1, 0, nvars - 1)
		End Sub


	End Class
End Class

