'************************************************************************
'ALGLIB 3.9.0 (source code generated 2014-12-11)
'Copyright (c) Sergey Bochkanov (ALGLIB project).
'
'>>> SOURCE LICENSE >>>
'This program is free software; you can redistribute it and/or modify
'it under the terms of the GNU General Public License as published by
'the Free Software Foundation (www.fsf.org); either version 2 of the 
'License, or (at your option) any later version.
'
'This program is distributed in the hope that it will be useful,
'but WITHOUT ANY WARRANTY; without even the implied warranty of
'MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
'GNU General Public License for more details.
'
'A copy of the GNU General Public License is available at
'http://www.fsf.org/licensing/licenses
'>>> END OF LICENSE >>>
'************************************************************************

'#Pragma warning disable 162
'#Pragma warning disable 219

Public Partial Class alglib



End Class
Public Partial Class alglib



End Class
Public Partial Class alglib



End Class
Public Partial Class alglib



End Class
Public Partial Class alglib


	'************************************************************************
'    This object stores state of the nonlinear CG optimizer.
'
'    You should use ALGLIB functions to work with this object.
'    ************************************************************************

	Public Class mincgstate
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property needf() As Boolean
			Get
				Return _innerobj.needf
			End Get
			Set
				_innerobj.needf = value
			End Set
		End Property
		Public Property needfg() As Boolean
			Get
				Return _innerobj.needfg
			End Get
			Set
				_innerobj.needfg = value
			End Set
		End Property
		Public Property xupdated() As Boolean
			Get
				Return _innerobj.xupdated
			End Get
			Set
				_innerobj.xupdated = value
			End Set
		End Property
		Public Property f() As Double
			Get
				Return _innerobj.f
			End Get
			Set
				_innerobj.f = value
			End Set
		End Property
		Public ReadOnly Property g() As Double()
			Get
				Return _innerobj.g
			End Get
		End Property
		Public ReadOnly Property x() As Double()
			Get
				Return _innerobj.x
			End Get
		End Property

		Public Sub New()
			_innerobj = New mincg.mincgstate()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mincgstate(DirectCast(_innerobj.make_copy(), mincg.mincgstate))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mincg.mincgstate
		Public ReadOnly Property innerobj() As mincg.mincgstate
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mincg.mincgstate)
			_innerobj = obj
		End Sub
	End Class


	'************************************************************************
'    This structure stores optimization report:
'    * IterationsCount           total number of inner iterations
'    * NFEV                      number of gradient evaluations
'    * TerminationType           termination type (see below)
'
'    TERMINATION CODES
'
'    TerminationType field contains completion code, which can be:
'      -8    internal integrity control detected  infinite  or  NAN  values  in
'            function/gradient. Abnormal termination signalled.
'      -7    gradient verification failed.
'            See MinCGSetGradientCheck() for more information.
'       1    relative function improvement is no more than EpsF.
'       2    relative step is no more than EpsX.
'       4    gradient norm is no more than EpsG
'       5    MaxIts steps was taken
'       7    stopping conditions are too stringent,
'            further improvement is impossible,
'            X contains best point found so far.
'       8    terminated by user who called mincgrequesttermination(). X contains
'            point which was "current accepted" when  termination  request  was
'            submitted.
'
'    Other fields of this structure are not documented and should not be used!
'    ************************************************************************

	Public Class mincgreport
		Inherits alglibobject
		'
		' Public declarations
		'
		Public Property iterationscount() As Integer
			Get
				Return _innerobj.iterationscount
			End Get
			Set
				_innerobj.iterationscount = value
			End Set
		End Property
		Public Property nfev() As Integer
			Get
				Return _innerobj.nfev
			End Get
			Set
				_innerobj.nfev = value
			End Set
		End Property
		Public Property varidx() As Integer
			Get
				Return _innerobj.varidx
			End Get
			Set
				_innerobj.varidx = value
			End Set
		End Property
		Public Property terminationtype() As Integer
			Get
				Return _innerobj.terminationtype
			End Get
			Set
				_innerobj.terminationtype = value
			End Set
		End Property

		Public Sub New()
			_innerobj = New mincg.mincgreport()
		End Sub

		Public Overrides Function make_copy() As alglib.alglibobject
			Return New mincgreport(DirectCast(_innerobj.make_copy(), mincg.mincgreport))
		End Function

		'
		' Although some of declarations below are public, you should not use them
		' They are intended for internal use only
		'
		Private _innerobj As mincg.mincgreport
		Public ReadOnly Property innerobj() As mincg.mincgreport
			Get
				Return _innerobj
			End Get
		End Property
		Public Sub New(obj As mincg.mincgreport)
			_innerobj = obj
		End Sub
	End Class

	'************************************************************************
'            NONLINEAR CONJUGATE GRADIENT METHOD
'
'    DESCRIPTION:
'    The subroutine minimizes function F(x) of N arguments by using one of  the
'    nonlinear conjugate gradient methods.
'
'    These CG methods are globally convergent (even on non-convex functions) as
'    long as grad(f) is Lipschitz continuous in  a  some  neighborhood  of  the
'    L = { x : f(x)<=f(x0) }.
'
'
'    REQUIREMENTS:
'    Algorithm will request following information during its operation:
'    * function value F and its gradient G (simultaneously) at given point X
'
'
'    USAGE:
'    1. User initializes algorithm state with MinCGCreate() call
'    2. User tunes solver parameters with MinCGSetCond(), MinCGSetStpMax() and
'       other functions
'    3. User calls MinCGOptimize() function which takes algorithm  state   and
'       pointer (delegate, etc.) to callback function which calculates F/G.
'    4. User calls MinCGResults() to get solution
'    5. Optionally, user may call MinCGRestartFrom() to solve another  problem
'       with same N but another starting point and/or another function.
'       MinCGRestartFrom() allows to reuse already initialized structure.
'
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>0:
'                    * if given, only leading N elements of X are used
'                    * if not given, automatically determined from size of X
'        X       -   starting point, array[0..N-1].
'
'    OUTPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'
'      -- ALGLIB --
'         Copyright 25.03.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgcreate(n As Integer, x As Double(), ByRef state As mincgstate)
		state = New mincgstate()
		mincg.mincgcreate(n, x, state.innerobj)
		Return
	End Sub
	Public Shared Sub mincgcreate(x As Double(), ByRef state As mincgstate)
		Dim n As Integer

		state = New mincgstate()
		n = ap.len(x)
		mincg.mincgcreate(n, x, state.innerobj)

		Return
	End Sub

	'************************************************************************
'    The subroutine is finite difference variant of MinCGCreate(). It uses
'    finite differences in order to differentiate target function.
'
'    Description below contains information which is specific to this function
'    only. We recommend to read comments on MinCGCreate() in order to get more
'    information about creation of CG optimizer.
'
'    INPUT PARAMETERS:
'        N       -   problem dimension, N>0:
'                    * if given, only leading N elements of X are used
'                    * if not given, automatically determined from size of X
'        X       -   starting point, array[0..N-1].
'        DiffStep-   differentiation step, >0
'
'    OUTPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'
'    NOTES:
'    1. algorithm uses 4-point central formula for differentiation.
'    2. differentiation step along I-th axis is equal to DiffStep*S[I] where
'       S[] is scaling vector which can be set by MinCGSetScale() call.
'    3. we recommend you to use moderate values of  differentiation  step.  Too
'       large step will result in too large truncation  errors, while too small
'       step will result in too large numerical  errors.  1.0E-6  can  be  good
'       value to start with.
'    4. Numerical  differentiation  is   very   inefficient  -   one   gradient
'       calculation needs 4*N function evaluations. This function will work for
'       any N - either small (1...10), moderate (10...100) or  large  (100...).
'       However, performance penalty will be too severe for any N's except  for
'       small ones.
'       We should also say that code which relies on numerical  differentiation
'       is  less  robust  and  precise.  L-BFGS  needs  exact  gradient values.
'       Imprecise  gradient may slow down  convergence,  especially  on  highly
'       nonlinear problems.
'       Thus  we  recommend to use this function for fast prototyping on small-
'       dimensional problems only, and to implement analytical gradient as soon
'       as possible.
'
'      -- ALGLIB --
'         Copyright 16.05.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgcreatef(n As Integer, x As Double(), diffstep As Double, ByRef state As mincgstate)
		state = New mincgstate()
		mincg.mincgcreatef(n, x, diffstep, state.innerobj)
		Return
	End Sub
	Public Shared Sub mincgcreatef(x As Double(), diffstep As Double, ByRef state As mincgstate)
		Dim n As Integer

		state = New mincgstate()
		n = ap.len(x)
		mincg.mincgcreatef(n, x, diffstep, state.innerobj)

		Return
	End Sub

	'************************************************************************
'    This function sets stopping conditions for CG optimization algorithm.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'        EpsG    -   >=0
'                    The  subroutine  finishes  its  work   if   the  condition
'                    |v|<EpsG is satisfied, where:
'                    * |.| means Euclidian norm
'                    * v - scaled gradient vector, v[i]=g[i]*s[i]
'                    * g - gradient
'                    * s - scaling coefficients set by MinCGSetScale()
'        EpsF    -   >=0
'                    The  subroutine  finishes  its work if on k+1-th iteration
'                    the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
'                    is satisfied.
'        EpsX    -   >=0
'                    The subroutine finishes its work if  on  k+1-th  iteration
'                    the condition |v|<=EpsX is fulfilled, where:
'                    * |.| means Euclidian norm
'                    * v - scaled step vector, v[i]=dx[i]/s[i]
'                    * dx - ste pvector, dx=X(k+1)-X(k)
'                    * s - scaling coefficients set by MinCGSetScale()
'        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
'                    iterations is unlimited.
'
'    Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
'    automatic stopping criterion selection (small EpsX).
'
'      -- ALGLIB --
'         Copyright 02.04.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetcond(state As mincgstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

		mincg.mincgsetcond(state.innerobj, epsg, epsf, epsx, maxits)
		Return
	End Sub

	'************************************************************************
'    This function sets scaling coefficients for CG optimizer.
'
'    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'    size and gradient are scaled before comparison with tolerances).  Scale of
'    the I-th variable is a translation invariant measure of:
'    a) "how large" the variable is
'    b) how large the step should be to make significant changes in the function
'
'    Scaling is also used by finite difference variant of CG optimizer  -  step
'    along I-th axis is equal to DiffStep*S[I].
'
'    In   most   optimizers  (and  in  the  CG  too)  scaling is NOT a form  of
'    preconditioning. It just  affects  stopping  conditions.  You  should  set
'    preconditioner by separate call to one of the MinCGSetPrec...() functions.
'
'    There  is  special  preconditioning  mode, however,  which  uses   scaling
'    coefficients to form diagonal preconditioning matrix. You  can  turn  this
'    mode on, if you want.   But  you should understand that scaling is not the
'    same thing as preconditioning - these are two different, although  related
'    forms of tuning solver.
'
'    INPUT PARAMETERS:
'        State   -   structure stores algorithm state
'        S       -   array[N], non-zero scaling coefficients
'                    S[i] may be negative, sign doesn't matter.
'
'      -- ALGLIB --
'         Copyright 14.01.2011 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetscale(state As mincgstate, s As Double())

		mincg.mincgsetscale(state.innerobj, s)
		Return
	End Sub

	'************************************************************************
'    This function turns on/off reporting.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'        NeedXRep-   whether iteration reports are needed or not
'
'    If NeedXRep is True, algorithm will call rep() callback function if  it is
'    provided to MinCGOptimize().
'
'      -- ALGLIB --
'         Copyright 02.04.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetxrep(state As mincgstate, needxrep As Boolean)

		mincg.mincgsetxrep(state.innerobj, needxrep)
		Return
	End Sub

	'************************************************************************
'    This function sets CG algorithm.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'        CGType  -   algorithm type:
'                    * -1    automatic selection of the best algorithm
'                    * 0     DY (Dai and Yuan) algorithm
'                    * 1     Hybrid DY-HS algorithm
'
'      -- ALGLIB --
'         Copyright 02.04.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetcgtype(state As mincgstate, cgtype As Integer)

		mincg.mincgsetcgtype(state.innerobj, cgtype)
		Return
	End Sub

	'************************************************************************
'    This function sets maximum step length
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'        StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
'                    want to limit step length.
'
'    Use this subroutine when you optimize target function which contains exp()
'    or  other  fast  growing  functions,  and optimization algorithm makes too
'    large  steps  which  leads  to overflow. This function allows us to reject
'    steps  that  are  too  large  (and  therefore  expose  us  to the possible
'    overflow) without actually calculating function value at the x+stp*d.
'
'      -- ALGLIB --
'         Copyright 02.04.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetstpmax(state As mincgstate, stpmax As Double)

		mincg.mincgsetstpmax(state.innerobj, stpmax)
		Return
	End Sub

	'************************************************************************
'    This function allows to suggest initial step length to the CG algorithm.
'
'    Suggested  step  length  is used as starting point for the line search. It
'    can be useful when you have  badly  scaled  problem,  i.e.  when  ||grad||
'    (which is used as initial estimate for the first step) is many  orders  of
'    magnitude different from the desired step.
'
'    Line search  may  fail  on  such problems without good estimate of initial
'    step length. Imagine, for example, problem with ||grad||=10^50 and desired
'    step equal to 0.1 Line  search function will use 10^50  as  initial  step,
'    then  it  will  decrease step length by 2 (up to 20 attempts) and will get
'    10^44, which is still too large.
'
'    This function allows us to tell than line search should  be  started  from
'    some moderate step length, like 1.0, so algorithm will be able  to  detect
'    desired step length in a several searches.
'
'    Default behavior (when no step is suggested) is to use preconditioner,  if
'    it is available, to generate initial estimate of step length.
'
'    This function influences only first iteration of algorithm. It  should  be
'    called between MinCGCreate/MinCGRestartFrom() call and MinCGOptimize call.
'    Suggested step is ignored if you have preconditioner.
'
'    INPUT PARAMETERS:
'        State   -   structure used to store algorithm state.
'        Stp     -   initial estimate of the step length.
'                    Can be zero (no estimate).
'
'      -- ALGLIB --
'         Copyright 30.07.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsuggeststep(state As mincgstate, stp As Double)

		mincg.mincgsuggeststep(state.innerobj, stp)
		Return
	End Sub

	'************************************************************************
'    Modification of the preconditioner: preconditioning is turned off.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'
'    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'    iterations.
'
'      -- ALGLIB --
'         Copyright 13.10.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetprecdefault(state As mincgstate)

		mincg.mincgsetprecdefault(state.innerobj)
		Return
	End Sub

	'************************************************************************
'    Modification  of  the  preconditioner:  diagonal of approximate Hessian is
'    used.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'        D       -   diagonal of the approximate Hessian, array[0..N-1],
'                    (if larger, only leading N elements are used).
'
'    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'    iterations.
'
'    NOTE 2: D[i] should be positive. Exception will be thrown otherwise.
'
'    NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
'
'      -- ALGLIB --
'         Copyright 13.10.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetprecdiag(state As mincgstate, d As Double())

		mincg.mincgsetprecdiag(state.innerobj, d)
		Return
	End Sub

	'************************************************************************
'    Modification of the preconditioner: scale-based diagonal preconditioning.
'
'    This preconditioning mode can be useful when you  don't  have  approximate
'    diagonal of Hessian, but you know that your  variables  are  badly  scaled
'    (for  example,  one  variable is in [1,10], and another in [1000,100000]),
'    and most part of the ill-conditioning comes from different scales of vars.
'
'    In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
'    can greatly improve convergence.
'
'    IMPRTANT: you should set scale of your variables with MinCGSetScale() call
'    (before or after MinCGSetPrecScale() call). Without knowledge of the scale
'    of your variables scale-based preconditioner will be just unit matrix.
'
'    INPUT PARAMETERS:
'        State   -   structure which stores algorithm state
'
'    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'    iterations.
'
'      -- ALGLIB --
'         Copyright 13.10.2010 by Bochkanov Sergey
'    ************************************************************************

	Public Shared Sub mincgsetprecscale(state As mincgstate)

		mincg.mincgsetprecscale(state.innerobj)
		Return
	End Sub

	'************************************************************************
'    This function provides reverse communication interface
'    Reverse communication interface is not documented or recommended to use.
'    See below for functions which provide better documented API
'    ************************************************************************

	Public Shared Function mincgiteration(state As mincgstate) As Boolean

		Dim result As Boolean = mincg.mincgiteration(state.innerobj)
		Return result
	End Function
	'************************************************************************
'    This family of functions is used to launcn iterations of nonlinear optimizer
'
'    These functions accept following parameters:
'        func    -   callback which calculates function (or merit function)
'                    value func at given point x
'        grad    -   callback which calculates function (or merit function)
'                    value func and gradient grad at given point x
'        rep     -   optional callback which is called after each iteration
'                    can be null
'        obj     -   optional object which is passed to func/grad/hess/jac/rep
'                    can be null
'
'    NOTES:
'
'    1. This function has two different implementations: one which  uses  exact
'       (analytical) user-supplied  gradient, and one which uses function value
'       only  and  numerically  differentiates  function  in  order  to  obtain
'       gradient.
'
'       Depending  on  the  specific  function  used to create optimizer object
'       (either MinCGCreate()  for analytical gradient  or  MinCGCreateF()  for
'       numerical differentiation) you should  choose  appropriate  variant  of
'       MinCGOptimize() - one which accepts function AND gradient or one  which
'       accepts function ONLY.
'
'       Be careful to choose variant of MinCGOptimize()  which  corresponds  to
'       your optimization scheme! Table below lists different  combinations  of
'       callback (function/gradient) passed  to  MinCGOptimize()  and  specific
'       function used to create optimizer.
'
'
'                      |         USER PASSED TO MinCGOptimize()
'       CREATED WITH   |  function only   |  function and gradient
'       ------------------------------------------------------------
'       MinCGCreateF() |     work                FAIL
'       MinCGCreate()  |     FAIL                work
'
'       Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
'       function and MinCGOptimize() version. Attemps to use  such  combination
'       (for  example,  to create optimizer with  MinCGCreateF()  and  to  pass
'       gradient information to MinCGOptimize()) will lead to  exception  being
'       thrown. Either  you  did  not  pass  gradient when it WAS needed or you
'       passed gradient when it was NOT needed.
'
'      -- ALGLIB --
'         Copyright 20.04.2009 by Bochkanov Sergey
'
'    ************************************************************************

	Public Shared Sub mincgoptimize(state As mincgstate, func As ndimensional_func, rep As ndimensional_rep, obj As Object)
		If func Is Nothing Then
			Throw New alglibexception("ALGLIB: error in 'mincgoptimize()' (func is null)")
		End If
		While alglib.mincgiteration(state)
			If state.needf Then
				func(state.x, state.innerobj.f, obj)
				Continue While
			End If
			If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'mincgoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub mincgoptimize(state As mincgstate, grad As ndimensional_grad, rep As ndimensional_rep, obj As Object)
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'mincgoptimize()' (grad is null)")
        End If
        While alglib.mincgiteration(state)
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'mincgoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    Conjugate gradient results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution
    '        Rep     -   optimization report:
    '                    * Rep.TerminationType completetion code:
    '                        * -8    internal integrity control  detected  infinite
    '                                or NAN values in  function/gradient.  Abnormal
    '                                termination signalled.
    '                        * -7    gradient verification failed.
    '                                See MinCGSetGradientCheck() for more information.
    '                        *  1    relative function improvement is no more than
    '                                EpsF.
    '                        *  2    relative step is no more than EpsX.
    '                        *  4    gradient norm is no more than EpsG
    '                        *  5    MaxIts steps was taken
    '                        *  7    stopping conditions are too stringent,
    '                                further improvement is impossible,
    '                                we return best X found so far
    '                        *  8    terminated by user
    '                    * Rep.IterationsCount contains iterations count
    '                    * NFEV countains number of function calculations
    '
    '      -- ALGLIB --
    '         Copyright 20.04.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub mincgresults(state As mincgstate, ByRef x As Double(), ByRef rep As mincgreport)
        x = New Double(-1) {}
        rep = New mincgreport()
        mincg.mincgresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Conjugate gradient results
    '
    '    Buffered implementation of MinCGResults(), which uses pre-allocated buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 20.04.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub mincgresultsbuf(state As mincgstate, ByRef x As Double(), rep As mincgreport)

        mincg.mincgresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine  restarts  CG  algorithm from new point. All optimization
    '    parameters are left unchanged.
    '
    '    This  function  allows  to  solve multiple  optimization  problems  (which
    '    must have same number of dimensions) without object reallocation penalty.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure used to store algorithm state.
    '        X       -   new starting point.
    '
    '      -- ALGLIB --
    '         Copyright 30.07.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub mincgrestartfrom(state As mincgstate, x As Double())

        mincg.mincgrestartfrom(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This subroutine submits request for termination of running  optimizer.  It
    '    should be called from user-supplied callback when user decides that it  is
    '    time to "smoothly" terminate optimization process.  As  result,  optimizer
    '    stops at point which was "current accepted" when termination  request  was
    '    submitted and returns error code 8 (successful termination).
    '
    '    INPUT PARAMETERS:
    '        State   -   optimizer structure
    '
    '    NOTE: after  request  for  termination  optimizer  may   perform   several
    '          additional calls to user-supplied callbacks. It does  NOT  guarantee
    '          to stop immediately - it just guarantees that these additional calls
    '          will be discarded later.
    '
    '    NOTE: calling this function on optimizer which is NOT running will have no
    '          effect.
    '
    '    NOTE: multiple calls to this function are possible. First call is counted,
    '          subsequent calls are silently ignored.
    '
    '      -- ALGLIB --
    '         Copyright 08.10.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub mincgrequesttermination(state As mincgstate)

        mincg.mincgrequesttermination(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '
    '    This  subroutine  turns  on  verification  of  the  user-supplied analytic
    '    gradient:
    '    * user calls this subroutine before optimization begins
    '    * MinCGOptimize() is called
    '    * prior to  actual  optimization, for each component  of  parameters being
    '      optimized X[i] algorithm performs following steps:
    '      * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    '        where X[i] is i-th component of the initial point and S[i] is a  scale
    '        of i-th parameter
    '      * F(X) is evaluated at these trial points
    '      * we perform one more evaluation in the middle point of the interval
    '      * we  build  cubic  model using function values and derivatives at trial
    '        points and we compare its prediction with actual value in  the  middle
    '        point
    '      * in case difference between prediction and actual value is higher  than
    '        some predetermined threshold, algorithm stops with completion code -7;
    '        Rep.VarIdx is set to index of the parameter with incorrect derivative.
    '    * after verification is over, algorithm proceeds to the actual optimization.
    '
    '    NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
    '            is very costly and you should use  it  only  for  low  dimensional
    '            problems,  when  you  want  to  be  sure  that  you've   correctly
    '            calculated  analytic  derivatives.  You  should  not use it in the
    '            production code (unless you want to check derivatives provided  by
    '            some third party).
    '
    '    NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
    '            (so large that function behaviour is significantly non-cubic) will
    '            lead to false alarms. You may use  different  step  for  different
    '            parameters by means of setting scale with MinCGSetScale().
    '
    '    NOTE 3: this function may lead to false positives. In case it reports that
    '            I-th  derivative was calculated incorrectly, you may decrease test
    '            step  and  try  one  more  time  - maybe your function changes too
    '            sharply  and  your  step  is  too  large for such rapidly chanding
    '            function.
    '
    '    INPUT PARAMETERS:
    '        State       -   structure used to store algorithm state
    '        TestStep    -   verification step:
    '                        * TestStep=0 turns verification off
    '                        * TestStep>0 activates verification
    '
    '      -- ALGLIB --
    '         Copyright 31.05.2012 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub mincgsetgradientcheck(state As mincgstate, teststep As Double)

        mincg.mincgsetgradientcheck(state.innerobj, teststep)
        Return
    End Sub

End Class
Partial Public Class alglib


    '************************************************************************
    '    This object stores nonlinear optimizer state.
    '    You should use functions provided by MinBLEIC subpackage to work with this
    '    object
    '    ************************************************************************

    Public Class minbleicstate
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property needf() As Boolean
            Get
                Return _innerobj.needf
            End Get
            Set(value As Boolean)
                _innerobj.needf = value
            End Set
        End Property
        Public Property needfg() As Boolean
            Get
                Return _innerobj.needfg
            End Get
            Set(value As Boolean)
                _innerobj.needfg = value
            End Set
        End Property
        Public Property xupdated() As Boolean
            Get
                Return _innerobj.xupdated
            End Get
            Set(value As Boolean)
                _innerobj.xupdated = value
            End Set
        End Property
        Public Property f() As Double
            Get
                Return _innerobj.f
            End Get
            Set(value As Double)
                _innerobj.f = value
            End Set
        End Property
        Public ReadOnly Property g() As Double()
            Get
                Return _innerobj.g
            End Get
        End Property
        Public ReadOnly Property x() As Double()
            Get
                Return _innerobj.x
            End Get
        End Property

        Public Sub New()
            _innerobj = New minbleic.minbleicstate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minbleicstate(DirectCast(_innerobj.make_copy(), minbleic.minbleicstate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minbleic.minbleicstate
        Public ReadOnly Property innerobj() As minbleic.minbleicstate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minbleic.minbleicstate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '    This structure stores optimization report:
    '    * IterationsCount           number of iterations
    '    * NFEV                      number of gradient evaluations
    '    * TerminationType           termination type (see below)
    '
    '    TERMINATION CODES
    '
    '    TerminationType field contains completion code, which can be:
    '      -8    internal integrity control detected  infinite  or  NAN  values  in
    '            function/gradient. Abnormal termination signalled.
    '      -7    gradient verification failed.
    '            See MinBLEICSetGradientCheck() for more information.
    '      -3    inconsistent constraints. Feasible point is
    '            either nonexistent or too hard to find. Try to
    '            restart optimizer with better initial approximation
    '       1    relative function improvement is no more than EpsF.
    '       2    relative step is no more than EpsX.
    '       4    gradient norm is no more than EpsG
    '       5    MaxIts steps was taken
    '       7    stopping conditions are too stringent,
    '            further improvement is impossible,
    '            X contains best point found so far.
    '       8    terminated by user who called minbleicrequesttermination(). X contains
    '            point which was "current accepted" when  termination  request  was
    '            submitted.
    '
    '    ADDITIONAL FIELDS
    '
    '    There are additional fields which can be used for debugging:
    '    * DebugEqErr                error in the equality constraints (2-norm)
    '    * DebugFS                   f, calculated at projection of initial point
    '                                to the feasible set
    '    * DebugFF                   f, calculated at the final point
    '    * DebugDX                   |X_start-X_final|
    '    ************************************************************************

    Public Class minbleicreport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property iterationscount() As Integer
            Get
                Return _innerobj.iterationscount
            End Get
            Set(value As Integer)
                _innerobj.iterationscount = value
            End Set
        End Property
        Public Property nfev() As Integer
            Get
                Return _innerobj.nfev
            End Get
            Set(value As Integer)
                _innerobj.nfev = value
            End Set
        End Property
        Public Property varidx() As Integer
            Get
                Return _innerobj.varidx
            End Get
            Set(value As Integer)
                _innerobj.varidx = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property
        Public Property debugeqerr() As Double
            Get
                Return _innerobj.debugeqerr
            End Get
            Set(value As Double)
                _innerobj.debugeqerr = value
            End Set
        End Property
        Public Property debugfs() As Double
            Get
                Return _innerobj.debugfs
            End Get
            Set(value As Double)
                _innerobj.debugfs = value
            End Set
        End Property
        Public Property debugff() As Double
            Get
                Return _innerobj.debugff
            End Get
            Set(value As Double)
                _innerobj.debugff = value
            End Set
        End Property
        Public Property debugdx() As Double
            Get
                Return _innerobj.debugdx
            End Get
            Set(value As Double)
                _innerobj.debugdx = value
            End Set
        End Property
        Public Property debugfeasqpits() As Integer
            Get
                Return _innerobj.debugfeasqpits
            End Get
            Set(value As Integer)
                _innerobj.debugfeasqpits = value
            End Set
        End Property
        Public Property debugfeasgpaits() As Integer
            Get
                Return _innerobj.debugfeasgpaits
            End Get
            Set(value As Integer)
                _innerobj.debugfeasgpaits = value
            End Set
        End Property
        Public Property inneriterationscount() As Integer
            Get
                Return _innerobj.inneriterationscount
            End Get
            Set(value As Integer)
                _innerobj.inneriterationscount = value
            End Set
        End Property
        Public Property outeriterationscount() As Integer
            Get
                Return _innerobj.outeriterationscount
            End Get
            Set(value As Integer)
                _innerobj.outeriterationscount = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New minbleic.minbleicreport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minbleicreport(DirectCast(_innerobj.make_copy(), minbleic.minbleicreport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minbleic.minbleicreport
        Public ReadOnly Property innerobj() As minbleic.minbleicreport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minbleic.minbleicreport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '                         BOUND CONSTRAINED OPTIMIZATION
    '           WITH ADDITIONAL LINEAR EQUALITY AND INEQUALITY CONSTRAINTS
    '
    '    DESCRIPTION:
    '    The  subroutine  minimizes  function   F(x)  of N arguments subject to any
    '    combination of:
    '    * bound constraints
    '    * linear inequality constraints
    '    * linear equality constraints
    '
    '    REQUIREMENTS:
    '    * user must provide function value and gradient
    '    * starting point X0 must be feasible or
    '      not too far away from the feasible set
    '    * grad(f) must be Lipschitz continuous on a level set:
    '      L = { x : f(x)<=f(x0) }
    '    * function must be defined everywhere on the feasible set F
    '
    '    USAGE:
    '
    '    Constrained optimization if far more complex than the unconstrained one.
    '    Here we give very brief outline of the BLEIC optimizer. We strongly recommend
    '    you to read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
    '    on optimization, which is available at http://www.alglib.net/optimization/
    '
    '    1. User initializes algorithm state with MinBLEICCreate() call
    '
    '    2. USer adds boundary and/or linear constraints by calling
    '       MinBLEICSetBC() and MinBLEICSetLC() functions.
    '
    '    3. User sets stopping conditions with MinBLEICSetCond().
    '
    '    4. User calls MinBLEICOptimize() function which takes algorithm  state and
    '       pointer (delegate, etc.) to callback function which calculates F/G.
    '
    '    5. User calls MinBLEICResults() to get solution
    '
    '    6. Optionally user may call MinBLEICRestartFrom() to solve another problem
    '       with same N but another starting point.
    '       MinBLEICRestartFrom() allows to reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension, N>0:
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size ofX
    '        X       -   starting point, array[N]:
    '                    * it is better to set X to a feasible point
    '                    * but X can be infeasible, in which case algorithm will try
    '                      to find feasible point first, using X as initial
    '                      approximation.
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleiccreate(n As Integer, x As Double(), ByRef state As minbleicstate)
        state = New minbleicstate()
        minbleic.minbleiccreate(n, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minbleiccreate(x As Double(), ByRef state As minbleicstate)
        Dim n As Integer

        state = New minbleicstate()
        n = ap.len(x)
        minbleic.minbleiccreate(n, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    The subroutine is finite difference variant of MinBLEICCreate().  It  uses
    '    finite differences in order to differentiate target function.
    '
    '    Description below contains information which is specific to  this function
    '    only. We recommend to read comments on MinBLEICCreate() in  order  to  get
    '    more information about creation of BLEIC optimizer.
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension, N>0:
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size of X
    '        X       -   starting point, array[0..N-1].
    '        DiffStep-   differentiation step, >0
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    NOTES:
    '    1. algorithm uses 4-point central formula for differentiation.
    '    2. differentiation step along I-th axis is equal to DiffStep*S[I] where
    '       S[] is scaling vector which can be set by MinBLEICSetScale() call.
    '    3. we recommend you to use moderate values of  differentiation  step.  Too
    '       large step will result in too large truncation  errors, while too small
    '       step will result in too large numerical  errors.  1.0E-6  can  be  good
    '       value to start with.
    '    4. Numerical  differentiation  is   very   inefficient  -   one   gradient
    '       calculation needs 4*N function evaluations. This function will work for
    '       any N - either small (1...10), moderate (10...100) or  large  (100...).
    '       However, performance penalty will be too severe for any N's except  for
    '       small ones.
    '       We should also say that code which relies on numerical  differentiation
    '       is  less  robust and precise. CG needs exact gradient values. Imprecise
    '       gradient may slow  down  convergence, especially  on  highly  nonlinear
    '       problems.
    '       Thus  we  recommend to use this function for fast prototyping on small-
    '       dimensional problems only, and to implement analytical gradient as soon
    '       as possible.
    '
    '      -- ALGLIB --
    '         Copyright 16.05.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleiccreatef(n As Integer, x As Double(), diffstep As Double, ByRef state As minbleicstate)
        state = New minbleicstate()
        minbleic.minbleiccreatef(n, x, diffstep, state.innerobj)
        Return
    End Sub
    Public Shared Sub minbleiccreatef(x As Double(), diffstep As Double, ByRef state As minbleicstate)
        Dim n As Integer

        state = New minbleicstate()
        n = ap.len(x)
        minbleic.minbleiccreatef(n, x, diffstep, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This function sets boundary constraints for BLEIC optimizer.
    '
    '    Boundary constraints are inactive by default (after initial creation).
    '    They are preserved after algorithm restart with MinBLEICRestartFrom().
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        BndL    -   lower bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very small number or -INF.
    '        BndU    -   upper bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very large number or +INF.
    '
    '    NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
    '    variable will be "frozen" at X[i]=BndL[i]=BndU[i].
    '
    '    NOTE 2: this solver has following useful properties:
    '    * bound constraints are always satisfied exactly
    '    * function is evaluated only INSIDE area specified by  bound  constraints,
    '      even  when  numerical  differentiation is used (algorithm adjusts  nodes
    '      according to boundary constraints)
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetbc(state As minbleicstate, bndl As Double(), bndu As Double())

        minbleic.minbleicsetbc(state.innerobj, bndl, bndu)
        Return
    End Sub

    '************************************************************************
    '    This function sets linear constraints for BLEIC optimizer.
    '
    '    Linear constraints are inactive by default (after initial creation).
    '    They are preserved after algorithm restart with MinBLEICRestartFrom().
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinBLEICCreate call.
    '        C       -   linear constraints, array[K,N+1].
    '                    Each row of C represents one constraint, either equality
    '                    or inequality (see below):
    '                    * first N elements correspond to coefficients,
    '                    * last element corresponds to the right part.
    '                    All elements of C (including right part) must be finite.
    '        CT      -   type of constraints, array[K]:
    '                    * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
    '                    * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
    '                    * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
    '        K       -   number of equality/inequality constraints, K>=0:
    '                    * if given, only leading K elements of C/CT are used
    '                    * if not given, automatically determined from sizes of C/CT
    '
    '    NOTE 1: linear (non-bound) constraints are satisfied only approximately:
    '    * there always exists some minor violation (about Epsilon in magnitude)
    '      due to rounding errors
    '    * numerical differentiation, if used, may  lead  to  function  evaluations
    '      outside  of the feasible  area,   because   algorithm  does  NOT  change
    '      numerical differentiation formula according to linear constraints.
    '    If you want constraints to be  satisfied  exactly, try to reformulate your
    '    problem  in  such  manner  that  all constraints will become boundary ones
    '    (this kind of constraints is always satisfied exactly, both in  the  final
    '    solution and in all intermediate points).
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetlc(state As minbleicstate, c As Double(,), ct As Integer(), k As Integer)

        minbleic.minbleicsetlc(state.innerobj, c, ct, k)
        Return
    End Sub
    Public Shared Sub minbleicsetlc(state As minbleicstate, c As Double(,), ct As Integer())
        Dim k As Integer
        If (ap.rows(c) <> ap.len(ct)) Then
            Throw New alglibexception("Error while calling 'minbleicsetlc': looks like one of arguments has wrong size")
        End If

        k = ap.rows(c)
        minbleic.minbleicsetlc(state.innerobj, c, ct, k)

        Return
    End Sub

    '************************************************************************
    '    This function sets stopping conditions for the optimizer.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinBLEICSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes  its work if on k+1-th iteration
    '                    the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '                    is satisfied.
    '        EpsX    -   >=0
    '                    The subroutine finishes its work if  on  k+1-th  iteration
    '                    the condition |v|<=EpsX is fulfilled, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - step vector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinBLEICSetScale()
    '        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
    '                    iterations is unlimited.
    '
    '    Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
    '    to automatic stopping criterion selection.
    '
    '    NOTE: when SetCond() called with non-zero MaxIts, BLEIC solver may perform
    '          slightly more than MaxIts iterations. I.e., MaxIts  sets  non-strict
    '          limit on iterations count.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetcond(state As minbleicstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        minbleic.minbleicsetcond(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    This function sets scaling coefficients for BLEIC optimizer.
    '
    '    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
    '    size and gradient are scaled before comparison with tolerances).  Scale of
    '    the I-th variable is a translation invariant measure of:
    '    a) "how large" the variable is
    '    b) how large the step should be to make significant changes in the function
    '
    '    Scaling is also used by finite difference variant of the optimizer  - step
    '    along I-th axis is equal to DiffStep*S[I].
    '
    '    In  most  optimizers  (and  in  the  BLEIC  too)  scaling is NOT a form of
    '    preconditioning. It just  affects  stopping  conditions.  You  should  set
    '    preconditioner  by  separate  call  to  one  of  the  MinBLEICSetPrec...()
    '    functions.
    '
    '    There is a special  preconditioning  mode, however,  which  uses   scaling
    '    coefficients to form diagonal preconditioning matrix. You  can  turn  this
    '    mode on, if you want.   But  you should understand that scaling is not the
    '    same thing as preconditioning - these are two different, although  related
    '    forms of tuning solver.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        S       -   array[N], non-zero scaling coefficients
    '                    S[i] may be negative, sign doesn't matter.
    '
    '      -- ALGLIB --
    '         Copyright 14.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetscale(state As minbleicstate, s As Double())

        minbleic.minbleicsetscale(state.innerobj, s)
        Return
    End Sub

    '************************************************************************
    '    Modification of the preconditioner: preconditioning is turned off.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetprecdefault(state As minbleicstate)

        minbleic.minbleicsetprecdefault(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Modification  of  the  preconditioner:  diagonal of approximate Hessian is
    '    used.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        D       -   diagonal of the approximate Hessian, array[0..N-1],
    '                    (if larger, only leading N elements are used).
    '
    '    NOTE 1: D[i] should be positive. Exception will be thrown otherwise.
    '
    '    NOTE 2: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetprecdiag(state As minbleicstate, d As Double())

        minbleic.minbleicsetprecdiag(state.innerobj, d)
        Return
    End Sub

    '************************************************************************
    '    Modification of the preconditioner: scale-based diagonal preconditioning.
    '
    '    This preconditioning mode can be useful when you  don't  have  approximate
    '    diagonal of Hessian, but you know that your  variables  are  badly  scaled
    '    (for  example,  one  variable is in [1,10], and another in [1000,100000]),
    '    and most part of the ill-conditioning comes from different scales of vars.
    '
    '    In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
    '    can greatly improve convergence.
    '
    '    IMPRTANT: you should set scale of your variables  with  MinBLEICSetScale()
    '    call  (before  or after MinBLEICSetPrecScale() call). Without knowledge of
    '    the scale of your variables scale-based preconditioner will be  just  unit
    '    matrix.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetprecscale(state As minbleicstate)

        minbleic.minbleicsetprecscale(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This function turns on/off reporting.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        NeedXRep-   whether iteration reports are needed or not
    '
    '    If NeedXRep is True, algorithm will call rep() callback function if  it is
    '    provided to MinBLEICOptimize().
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetxrep(state As minbleicstate, needxrep As Boolean)

        minbleic.minbleicsetxrep(state.innerobj, needxrep)
        Return
    End Sub

    '************************************************************************
    '    This function sets maximum step length
    '
    '    IMPORTANT: this feature is hard to combine with preconditioning. You can't
    '    set upper limit on step length, when you solve optimization  problem  with
    '    linear (non-boundary) constraints AND preconditioner turned on.
    '
    '    When  non-boundary  constraints  are  present,  you  have to either a) use
    '    preconditioner, or b) use upper limit on step length.  YOU CAN'T USE BOTH!
    '    In this case algorithm will terminate with appropriate error code.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
    '                    want to limit step length.
    '
    '    Use this subroutine when you optimize target function which contains exp()
    '    or  other  fast  growing  functions,  and optimization algorithm makes too
    '    large  steps  which  lead   to overflow. This function allows us to reject
    '    steps  that  are  too  large  (and  therefore  expose  us  to the possible
    '    overflow) without actually calculating function value at the x+stp*d.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetstpmax(state As minbleicstate, stpmax As Double)

        minbleic.minbleicsetstpmax(state.innerobj, stpmax)
        Return
    End Sub

    '************************************************************************
    '    This function provides reverse communication interface
    '    Reverse communication interface is not documented or recommended to use.
    '    See below for functions which provide better documented API
    '    ************************************************************************

    Public Shared Function minbleiciteration(state As minbleicstate) As Boolean

        Dim result As Boolean = minbleic.minbleiciteration(state.innerobj)
        Return result
    End Function
    '************************************************************************
    '    This family of functions is used to launcn iterations of nonlinear optimizer
    '
    '    These functions accept following parameters:
    '        func    -   callback which calculates function (or merit function)
    '                    value func at given point x
    '        grad    -   callback which calculates function (or merit function)
    '                    value func and gradient grad at given point x
    '        rep     -   optional callback which is called after each iteration
    '                    can be null
    '        obj     -   optional object which is passed to func/grad/hess/jac/rep
    '                    can be null
    '
    '    NOTES:
    '
    '    1. This function has two different implementations: one which  uses  exact
    '       (analytical) user-supplied gradient,  and one which uses function value
    '       only  and  numerically  differentiates  function  in  order  to  obtain
    '       gradient.
    '
    '       Depending  on  the  specific  function  used to create optimizer object
    '       (either  MinBLEICCreate() for analytical gradient or  MinBLEICCreateF()
    '       for numerical differentiation) you should choose appropriate variant of
    '       MinBLEICOptimize() - one  which  accepts  function  AND gradient or one
    '       which accepts function ONLY.
    '
    '       Be careful to choose variant of MinBLEICOptimize() which corresponds to
    '       your optimization scheme! Table below lists different  combinations  of
    '       callback (function/gradient) passed to MinBLEICOptimize()  and specific
    '       function used to create optimizer.
    '
    '
    '                         |         USER PASSED TO MinBLEICOptimize()
    '       CREATED WITH      |  function only   |  function and gradient
    '       ------------------------------------------------------------
    '       MinBLEICCreateF() |     work                FAIL
    '       MinBLEICCreate()  |     FAIL                work
    '
    '       Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
    '       function  and  MinBLEICOptimize()  version.   Attemps   to   use   such
    '       combination (for  example,  to  create optimizer with MinBLEICCreateF()
    '       and  to  pass  gradient  information  to  MinCGOptimize()) will lead to
    '       exception being thrown. Either  you  did  not pass gradient when it WAS
    '       needed or you passed gradient when it was NOT needed.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '
    '    ************************************************************************

    Public Shared Sub minbleicoptimize(state As minbleicstate, func As ndimensional_func, rep As ndimensional_rep, obj As Object)
        If func Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minbleicoptimize()' (func is null)")
        End If
        While alglib.minbleiciteration(state)
            If state.needf Then
                func(state.x, state.innerobj.f, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minbleicoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minbleicoptimize(state As minbleicstate, grad As ndimensional_grad, rep As ndimensional_rep, obj As Object)
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minbleicoptimize()' (grad is null)")
        End If
        While alglib.minbleiciteration(state)
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minbleicoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    BLEIC results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution
    '        Rep     -   optimization report. You should check Rep.TerminationType
    '                    in  order  to  distinguish  successful  termination  from
    '                    unsuccessful one:
    '                    * -8    internal integrity control  detected  infinite or
    '                            NAN   values   in   function/gradient.   Abnormal
    '                            termination signalled.
    '                    * -7   gradient verification failed.
    '                           See MinBLEICSetGradientCheck() for more information.
    '                    * -3   inconsistent constraints. Feasible point is
    '                           either nonexistent or too hard to find. Try to
    '                           restart optimizer with better initial approximation
    '                    *  1   relative function improvement is no more than EpsF.
    '                    *  2   scaled step is no more than EpsX.
    '                    *  4   scaled gradient norm is no more than EpsG.
    '                    *  5   MaxIts steps was taken
    '                    *  8   terminated by user who called minbleicrequesttermination().
    '                           X contains point which was "current accepted"  when
    '                           termination request was submitted.
    '                    More information about fields of this  structure  can  be
    '                    found in the comments on MinBLEICReport datatype.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicresults(state As minbleicstate, ByRef x As Double(), ByRef rep As minbleicreport)
        x = New Double(-1) {}
        rep = New minbleicreport()
        minbleic.minbleicresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    BLEIC results
    '
    '    Buffered implementation of MinBLEICResults() which uses pre-allocated buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicresultsbuf(state As minbleicstate, ByRef x As Double(), rep As minbleicreport)

        minbleic.minbleicresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This subroutine restarts algorithm from new point.
    '    All optimization parameters (including constraints) are left unchanged.
    '
    '    This  function  allows  to  solve multiple  optimization  problems  (which
    '    must have  same number of dimensions) without object reallocation penalty.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinBLEICCreate call.
    '        X       -   new starting point.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicrestartfrom(state As minbleicstate, x As Double())

        minbleic.minbleicrestartfrom(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This subroutine submits request for termination of running  optimizer.  It
    '    should be called from user-supplied callback when user decides that it  is
    '    time to "smoothly" terminate optimization process.  As  result,  optimizer
    '    stops at point which was "current accepted" when termination  request  was
    '    submitted and returns error code 8 (successful termination).
    '
    '    INPUT PARAMETERS:
    '        State   -   optimizer structure
    '
    '    NOTE: after  request  for  termination  optimizer  may   perform   several
    '          additional calls to user-supplied callbacks. It does  NOT  guarantee
    '          to stop immediately - it just guarantees that these additional calls
    '          will be discarded later.
    '
    '    NOTE: calling this function on optimizer which is NOT running will have no
    '          effect.
    '
    '    NOTE: multiple calls to this function are possible. First call is counted,
    '          subsequent calls are silently ignored.
    '
    '      -- ALGLIB --
    '         Copyright 08.10.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicrequesttermination(state As minbleicstate)

        minbleic.minbleicrequesttermination(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine  turns  on  verification  of  the  user-supplied analytic
    '    gradient:
    '    * user calls this subroutine before optimization begins
    '    * MinBLEICOptimize() is called
    '    * prior to  actual  optimization, for each component  of  parameters being
    '      optimized X[i] algorithm performs following steps:
    '      * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    '        where X[i] is i-th component of the initial point and S[i] is a  scale
    '        of i-th parameter
    '      * if needed, steps are bounded with respect to constraints on X[]
    '      * F(X) is evaluated at these trial points
    '      * we perform one more evaluation in the middle point of the interval
    '      * we  build  cubic  model using function values and derivatives at trial
    '        points and we compare its prediction with actual value in  the  middle
    '        point
    '      * in case difference between prediction and actual value is higher  than
    '        some predetermined threshold, algorithm stops with completion code -7;
    '        Rep.VarIdx is set to index of the parameter with incorrect derivative.
    '    * after verification is over, algorithm proceeds to the actual optimization.
    '
    '    NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
    '            is very costly and you should use  it  only  for  low  dimensional
    '            problems,  when  you  want  to  be  sure  that  you've   correctly
    '            calculated  analytic  derivatives.  You  should  not use it in the
    '            production code (unless you want to check derivatives provided  by
    '            some third party).
    '
    '    NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
    '            (so large that function behaviour is significantly non-cubic) will
    '            lead to false alarms. You may use  different  step  for  different
    '            parameters by means of setting scale with MinBLEICSetScale().
    '
    '    NOTE 3: this function may lead to false positives. In case it reports that
    '            I-th  derivative was calculated incorrectly, you may decrease test
    '            step  and  try  one  more  time  - maybe your function changes too
    '            sharply  and  your  step  is  too  large for such rapidly chanding
    '            function.
    '
    '    INPUT PARAMETERS:
    '        State       -   structure used to store algorithm state
    '        TestStep    -   verification step:
    '                        * TestStep=0 turns verification off
    '                        * TestStep>0 activates verification
    '
    '      -- ALGLIB --
    '         Copyright 15.06.2012 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetgradientcheck(state As minbleicstate, teststep As Double)

        minbleic.minbleicsetgradientcheck(state.innerobj, teststep)
        Return
    End Sub

End Class
Partial Public Class alglib


    '************************************************************************
    '
    '    ************************************************************************

    Public Class minlbfgsstate
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property needf() As Boolean
            Get
                Return _innerobj.needf
            End Get
            Set(value As Boolean)
                _innerobj.needf = value
            End Set
        End Property
        Public Property needfg() As Boolean
            Get
                Return _innerobj.needfg
            End Get
            Set(value As Boolean)
                _innerobj.needfg = value
            End Set
        End Property
        Public Property xupdated() As Boolean
            Get
                Return _innerobj.xupdated
            End Get
            Set(value As Boolean)
                _innerobj.xupdated = value
            End Set
        End Property
        Public Property f() As Double
            Get
                Return _innerobj.f
            End Get
            Set(value As Double)
                _innerobj.f = value
            End Set
        End Property
        Public ReadOnly Property g() As Double()
            Get
                Return _innerobj.g
            End Get
        End Property
        Public ReadOnly Property x() As Double()
            Get
                Return _innerobj.x
            End Get
        End Property

        Public Sub New()
            _innerobj = New minlbfgs.minlbfgsstate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minlbfgsstate(DirectCast(_innerobj.make_copy(), minlbfgs.minlbfgsstate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minlbfgs.minlbfgsstate
        Public ReadOnly Property innerobj() As minlbfgs.minlbfgsstate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minlbfgs.minlbfgsstate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '    This structure stores optimization report:
    '    * IterationsCount           total number of inner iterations
    '    * NFEV                      number of gradient evaluations
    '    * TerminationType           termination type (see below)
    '
    '    TERMINATION CODES
    '
    '    TerminationType field contains completion code, which can be:
    '      -8    internal integrity control detected  infinite  or  NAN  values  in
    '            function/gradient. Abnormal termination signalled.
    '      -7    gradient verification failed.
    '            See MinLBFGSSetGradientCheck() for more information.
    '       1    relative function improvement is no more than EpsF.
    '       2    relative step is no more than EpsX.
    '       4    gradient norm is no more than EpsG
    '       5    MaxIts steps was taken
    '       7    stopping conditions are too stringent,
    '            further improvement is impossible,
    '            X contains best point found so far.
    '       8    terminated    by  user  who  called  minlbfgsrequesttermination().
    '            X contains point which was   "current accepted"  when  termination
    '            request was submitted.
    '
    '    Other fields of this structure are not documented and should not be used!
    '    ************************************************************************

    Public Class minlbfgsreport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property iterationscount() As Integer
            Get
                Return _innerobj.iterationscount
            End Get
            Set(value As Integer)
                _innerobj.iterationscount = value
            End Set
        End Property
        Public Property nfev() As Integer
            Get
                Return _innerobj.nfev
            End Get
            Set(value As Integer)
                _innerobj.nfev = value
            End Set
        End Property
        Public Property varidx() As Integer
            Get
                Return _innerobj.varidx
            End Get
            Set(value As Integer)
                _innerobj.varidx = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New minlbfgs.minlbfgsreport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minlbfgsreport(DirectCast(_innerobj.make_copy(), minlbfgs.minlbfgsreport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minlbfgs.minlbfgsreport
        Public ReadOnly Property innerobj() As minlbfgs.minlbfgsreport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minlbfgs.minlbfgsreport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '            LIMITED MEMORY BFGS METHOD FOR LARGE SCALE OPTIMIZATION
    '
    '    DESCRIPTION:
    '    The subroutine minimizes function F(x) of N arguments by  using  a  quasi-
    '    Newton method (LBFGS scheme) which is optimized to use  a  minimum  amount
    '    of memory.
    '    The subroutine generates the approximation of an inverse Hessian matrix by
    '    using information about the last M steps of the algorithm  (instead of N).
    '    It lessens a required amount of memory from a value  of  order  N^2  to  a
    '    value of order 2*N*M.
    '
    '
    '    REQUIREMENTS:
    '    Algorithm will request following information during its operation:
    '    * function value F and its gradient G (simultaneously) at given point X
    '
    '
    '    USAGE:
    '    1. User initializes algorithm state with MinLBFGSCreate() call
    '    2. User tunes solver parameters with MinLBFGSSetCond() MinLBFGSSetStpMax()
    '       and other functions
    '    3. User calls MinLBFGSOptimize() function which takes algorithm  state and
    '       pointer (delegate, etc.) to callback function which calculates F/G.
    '    4. User calls MinLBFGSResults() to get solution
    '    5. Optionally user may call MinLBFGSRestartFrom() to solve another problem
    '       with same N/M but another starting point and/or another function.
    '       MinLBFGSRestartFrom() allows to reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension. N>0
    '        M       -   number of corrections in the BFGS scheme of Hessian
    '                    approximation update. Recommended value:  3<=M<=7. The smaller
    '                    value causes worse convergence, the bigger will  not  cause  a
    '                    considerably better convergence, but will cause a fall in  the
    '                    performance. M<=N.
    '        X       -   initial solution approximation, array[0..N-1].
    '
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '
    '    NOTES:
    '    1. you may tune stopping conditions with MinLBFGSSetCond() function
    '    2. if target function contains exp() or other fast growing functions,  and
    '       optimization algorithm makes too large steps which leads  to  overflow,
    '       use MinLBFGSSetStpMax() function to bound algorithm's  steps.  However,
    '       L-BFGS rarely needs such a tuning.
    '
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgscreate(n As Integer, m As Integer, x As Double(), ByRef state As minlbfgsstate)
        state = New minlbfgsstate()
        minlbfgs.minlbfgscreate(n, m, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlbfgscreate(m As Integer, x As Double(), ByRef state As minlbfgsstate)
        Dim n As Integer

        state = New minlbfgsstate()
        n = ap.len(x)
        minlbfgs.minlbfgscreate(n, m, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    The subroutine is finite difference variant of MinLBFGSCreate().  It  uses
    '    finite differences in order to differentiate target function.
    '
    '    Description below contains information which is specific to  this function
    '    only. We recommend to read comments on MinLBFGSCreate() in  order  to  get
    '    more information about creation of LBFGS optimizer.
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension, N>0:
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size of X
    '        M       -   number of corrections in the BFGS scheme of Hessian
    '                    approximation update. Recommended value:  3<=M<=7. The smaller
    '                    value causes worse convergence, the bigger will  not  cause  a
    '                    considerably better convergence, but will cause a fall in  the
    '                    performance. M<=N.
    '        X       -   starting point, array[0..N-1].
    '        DiffStep-   differentiation step, >0
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    NOTES:
    '    1. algorithm uses 4-point central formula for differentiation.
    '    2. differentiation step along I-th axis is equal to DiffStep*S[I] where
    '       S[] is scaling vector which can be set by MinLBFGSSetScale() call.
    '    3. we recommend you to use moderate values of  differentiation  step.  Too
    '       large step will result in too large truncation  errors, while too small
    '       step will result in too large numerical  errors.  1.0E-6  can  be  good
    '       value to start with.
    '    4. Numerical  differentiation  is   very   inefficient  -   one   gradient
    '       calculation needs 4*N function evaluations. This function will work for
    '       any N - either small (1...10), moderate (10...100) or  large  (100...).
    '       However, performance penalty will be too severe for any N's except  for
    '       small ones.
    '       We should also say that code which relies on numerical  differentiation
    '       is   less  robust  and  precise.  LBFGS  needs  exact  gradient values.
    '       Imprecise gradient may slow  down  convergence,  especially  on  highly
    '       nonlinear problems.
    '       Thus  we  recommend to use this function for fast prototyping on small-
    '       dimensional problems only, and to implement analytical gradient as soon
    '       as possible.
    '
    '      -- ALGLIB --
    '         Copyright 16.05.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgscreatef(n As Integer, m As Integer, x As Double(), diffstep As Double, ByRef state As minlbfgsstate)
        state = New minlbfgsstate()
        minlbfgs.minlbfgscreatef(n, m, x, diffstep, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlbfgscreatef(m As Integer, x As Double(), diffstep As Double, ByRef state As minlbfgsstate)
        Dim n As Integer

        state = New minlbfgsstate()
        n = ap.len(x)
        minlbfgs.minlbfgscreatef(n, m, x, diffstep, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This function sets stopping conditions for L-BFGS optimization algorithm.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinLBFGSSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes  its work if on k+1-th iteration
    '                    the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '                    is satisfied.
    '        EpsX    -   >=0
    '                    The subroutine finishes its work if  on  k+1-th  iteration
    '                    the condition |v|<=EpsX is fulfilled, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - ste pvector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinLBFGSSetScale()
    '        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
    '                    iterations is unlimited.
    '
    '    Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
    '    automatic stopping criterion selection (small EpsX).
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetcond(state As minlbfgsstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        minlbfgs.minlbfgssetcond(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    This function turns on/off reporting.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        NeedXRep-   whether iteration reports are needed or not
    '
    '    If NeedXRep is True, algorithm will call rep() callback function if  it is
    '    provided to MinLBFGSOptimize().
    '
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetxrep(state As minlbfgsstate, needxrep As Boolean)

        minlbfgs.minlbfgssetxrep(state.innerobj, needxrep)
        Return
    End Sub

    '************************************************************************
    '    This function sets maximum step length
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        StpMax  -   maximum step length, >=0. Set StpMax to 0.0 (default),  if
    '                    you don't want to limit step length.
    '
    '    Use this subroutine when you optimize target function which contains exp()
    '    or  other  fast  growing  functions,  and optimization algorithm makes too
    '    large  steps  which  leads  to overflow. This function allows us to reject
    '    steps  that  are  too  large  (and  therefore  expose  us  to the possible
    '    overflow) without actually calculating function value at the x+stp*d.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetstpmax(state As minlbfgsstate, stpmax As Double)

        minlbfgs.minlbfgssetstpmax(state.innerobj, stpmax)
        Return
    End Sub

    '************************************************************************
    '    This function sets scaling coefficients for LBFGS optimizer.
    '
    '    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
    '    size and gradient are scaled before comparison with tolerances).  Scale of
    '    the I-th variable is a translation invariant measure of:
    '    a) "how large" the variable is
    '    b) how large the step should be to make significant changes in the function
    '
    '    Scaling is also used by finite difference variant of the optimizer  - step
    '    along I-th axis is equal to DiffStep*S[I].
    '
    '    In  most  optimizers  (and  in  the  LBFGS  too)  scaling is NOT a form of
    '    preconditioning. It just  affects  stopping  conditions.  You  should  set
    '    preconditioner  by  separate  call  to  one  of  the  MinLBFGSSetPrec...()
    '    functions.
    '
    '    There  is  special  preconditioning  mode, however,  which  uses   scaling
    '    coefficients to form diagonal preconditioning matrix. You  can  turn  this
    '    mode on, if you want.   But  you should understand that scaling is not the
    '    same thing as preconditioning - these are two different, although  related
    '    forms of tuning solver.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        S       -   array[N], non-zero scaling coefficients
    '                    S[i] may be negative, sign doesn't matter.
    '
    '      -- ALGLIB --
    '         Copyright 14.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetscale(state As minlbfgsstate, s As Double())

        minlbfgs.minlbfgssetscale(state.innerobj, s)
        Return
    End Sub

    '************************************************************************
    '    Modification  of  the  preconditioner:  default  preconditioner    (simple
    '    scaling, same for all elements of X) is used.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
    '    iterations.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetprecdefault(state As minlbfgsstate)

        minlbfgs.minlbfgssetprecdefault(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Modification of the preconditioner: Cholesky factorization of  approximate
    '    Hessian is used.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        P       -   triangular preconditioner, Cholesky factorization of
    '                    the approximate Hessian. array[0..N-1,0..N-1],
    '                    (if larger, only leading N elements are used).
    '        IsUpper -   whether upper or lower triangle of P is given
    '                    (other triangle is not referenced)
    '
    '    After call to this function preconditioner is changed to P  (P  is  copied
    '    into the internal buffer).
    '
    '    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
    '    iterations.
    '
    '    NOTE 2:  P  should  be nonsingular. Exception will be thrown otherwise.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetpreccholesky(state As minlbfgsstate, p As Double(,), isupper As Boolean)

        minlbfgs.minlbfgssetpreccholesky(state.innerobj, p, isupper)
        Return
    End Sub

    '************************************************************************
    '    Modification  of  the  preconditioner:  diagonal of approximate Hessian is
    '    used.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        D       -   diagonal of the approximate Hessian, array[0..N-1],
    '                    (if larger, only leading N elements are used).
    '
    '    NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
    '    iterations.
    '
    '    NOTE 2: D[i] should be positive. Exception will be thrown otherwise.
    '
    '    NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetprecdiag(state As minlbfgsstate, d As Double())

        minlbfgs.minlbfgssetprecdiag(state.innerobj, d)
        Return
    End Sub

    '************************************************************************
    '    Modification of the preconditioner: scale-based diagonal preconditioning.
    '
    '    This preconditioning mode can be useful when you  don't  have  approximate
    '    diagonal of Hessian, but you know that your  variables  are  badly  scaled
    '    (for  example,  one  variable is in [1,10], and another in [1000,100000]),
    '    and most part of the ill-conditioning comes from different scales of vars.
    '
    '    In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
    '    can greatly improve convergence.
    '
    '    IMPRTANT: you should set scale of your variables  with  MinLBFGSSetScale()
    '    call  (before  or after MinLBFGSSetPrecScale() call). Without knowledge of
    '    the scale of your variables scale-based preconditioner will be  just  unit
    '    matrix.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetprecscale(state As minlbfgsstate)

        minlbfgs.minlbfgssetprecscale(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This function provides reverse communication interface
    '    Reverse communication interface is not documented or recommended to use.
    '    See below for functions which provide better documented API
    '    ************************************************************************

    Public Shared Function minlbfgsiteration(state As minlbfgsstate) As Boolean

        Dim result As Boolean = minlbfgs.minlbfgsiteration(state.innerobj)
        Return result
    End Function
    '************************************************************************
    '    This family of functions is used to launcn iterations of nonlinear optimizer
    '
    '    These functions accept following parameters:
    '        func    -   callback which calculates function (or merit function)
    '                    value func at given point x
    '        grad    -   callback which calculates function (or merit function)
    '                    value func and gradient grad at given point x
    '        rep     -   optional callback which is called after each iteration
    '                    can be null
    '        obj     -   optional object which is passed to func/grad/hess/jac/rep
    '                    can be null
    '
    '    NOTES:
    '
    '    1. This function has two different implementations: one which  uses  exact
    '       (analytical) user-supplied gradient,  and one which uses function value
    '       only  and  numerically  differentiates  function  in  order  to  obtain
    '       gradient.
    '
    '       Depending  on  the  specific  function  used to create optimizer object
    '       (either MinLBFGSCreate() for analytical gradient  or  MinLBFGSCreateF()
    '       for numerical differentiation) you should choose appropriate variant of
    '       MinLBFGSOptimize() - one  which  accepts  function  AND gradient or one
    '       which accepts function ONLY.
    '
    '       Be careful to choose variant of MinLBFGSOptimize() which corresponds to
    '       your optimization scheme! Table below lists different  combinations  of
    '       callback (function/gradient) passed to MinLBFGSOptimize()  and specific
    '       function used to create optimizer.
    '
    '
    '                         |         USER PASSED TO MinLBFGSOptimize()
    '       CREATED WITH      |  function only   |  function and gradient
    '       ------------------------------------------------------------
    '       MinLBFGSCreateF() |     work                FAIL
    '       MinLBFGSCreate()  |     FAIL                work
    '
    '       Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
    '       function  and  MinLBFGSOptimize()  version.   Attemps   to   use   such
    '       combination (for example, to create optimizer with MinLBFGSCreateF() and
    '       to pass gradient information to MinCGOptimize()) will lead to exception
    '       being thrown. Either  you  did  not pass gradient when it WAS needed or
    '       you passed gradient when it was NOT needed.
    '
    '      -- ALGLIB --
    '         Copyright 20.03.2009 by Bochkanov Sergey
    '
    '    ************************************************************************

    Public Shared Sub minlbfgsoptimize(state As minlbfgsstate, func As ndimensional_func, rep As ndimensional_rep, obj As Object)
        If func Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlbfgsoptimize()' (func is null)")
        End If
        While alglib.minlbfgsiteration(state)
            If state.needf Then
                func(state.x, state.innerobj.f, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlbfgsoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minlbfgsoptimize(state As minlbfgsstate, grad As ndimensional_grad, rep As ndimensional_rep, obj As Object)
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlbfgsoptimize()' (grad is null)")
        End If
        While alglib.minlbfgsiteration(state)
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlbfgsoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    L-BFGS algorithm results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution
    '        Rep     -   optimization report:
    '                    * Rep.TerminationType completetion code:
    '                        * -8    internal integrity control  detected  infinite
    '                                or NAN values in  function/gradient.  Abnormal
    '                                termination signalled.
    '                        * -7    gradient verification failed.
    '                                See MinLBFGSSetGradientCheck() for more information.
    '                        * -2    rounding errors prevent further improvement.
    '                                X contains best point found.
    '                        * -1    incorrect parameters were specified
    '                        *  1    relative function improvement is no more than
    '                                EpsF.
    '                        *  2    relative step is no more than EpsX.
    '                        *  4    gradient norm is no more than EpsG
    '                        *  5    MaxIts steps was taken
    '                        *  7    stopping conditions are too stringent,
    '                                further improvement is impossible
    '                        *  8    terminated by user who called minlbfgsrequesttermination().
    '                                X contains point which was "current accepted" when
    '                                termination request was submitted.
    '                    * Rep.IterationsCount contains iterations count
    '                    * NFEV countains number of function calculations
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgsresults(state As minlbfgsstate, ByRef x As Double(), ByRef rep As minlbfgsreport)
        x = New Double(-1) {}
        rep = New minlbfgsreport()
        minlbfgs.minlbfgsresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    L-BFGS algorithm results
    '
    '    Buffered implementation of MinLBFGSResults which uses pre-allocated buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 20.08.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgsresultsbuf(state As minlbfgsstate, ByRef x As Double(), rep As minlbfgsreport)

        minlbfgs.minlbfgsresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine restarts LBFGS algorithm from new point. All optimization
    '    parameters are left unchanged.
    '
    '    This  function  allows  to  solve multiple  optimization  problems  (which
    '    must have same number of dimensions) without object reallocation penalty.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure used to store algorithm state
    '        X       -   new starting point.
    '
    '      -- ALGLIB --
    '         Copyright 30.07.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgsrestartfrom(state As minlbfgsstate, x As Double())

        minlbfgs.minlbfgsrestartfrom(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This subroutine submits request for termination of running  optimizer.  It
    '    should be called from user-supplied callback when user decides that it  is
    '    time to "smoothly" terminate optimization process.  As  result,  optimizer
    '    stops at point which was "current accepted" when termination  request  was
    '    submitted and returns error code 8 (successful termination).
    '
    '    INPUT PARAMETERS:
    '        State   -   optimizer structure
    '
    '    NOTE: after  request  for  termination  optimizer  may   perform   several
    '          additional calls to user-supplied callbacks. It does  NOT  guarantee
    '          to stop immediately - it just guarantees that these additional calls
    '          will be discarded later.
    '
    '    NOTE: calling this function on optimizer which is NOT running will have no
    '          effect.
    '
    '    NOTE: multiple calls to this function are possible. First call is counted,
    '          subsequent calls are silently ignored.
    '
    '      -- ALGLIB --
    '         Copyright 08.10.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgsrequesttermination(state As minlbfgsstate)

        minlbfgs.minlbfgsrequesttermination(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine  turns  on  verification  of  the  user-supplied analytic
    '    gradient:
    '    * user calls this subroutine before optimization begins
    '    * MinLBFGSOptimize() is called
    '    * prior to  actual  optimization, for each component  of  parameters being
    '      optimized X[i] algorithm performs following steps:
    '      * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    '        where X[i] is i-th component of the initial point and S[i] is a  scale
    '        of i-th parameter
    '      * if needed, steps are bounded with respect to constraints on X[]
    '      * F(X) is evaluated at these trial points
    '      * we perform one more evaluation in the middle point of the interval
    '      * we  build  cubic  model using function values and derivatives at trial
    '        points and we compare its prediction with actual value in  the  middle
    '        point
    '      * in case difference between prediction and actual value is higher  than
    '        some predetermined threshold, algorithm stops with completion code -7;
    '        Rep.VarIdx is set to index of the parameter with incorrect derivative.
    '    * after verification is over, algorithm proceeds to the actual optimization.
    '
    '    NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
    '            is very costly and you should use  it  only  for  low  dimensional
    '            problems,  when  you  want  to  be  sure  that  you've   correctly
    '            calculated  analytic  derivatives.  You  should  not use it in the
    '            production code (unless you want to check derivatives provided  by
    '            some third party).
    '
    '    NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
    '            (so large that function behaviour is significantly non-cubic) will
    '            lead to false alarms. You may use  different  step  for  different
    '            parameters by means of setting scale with MinLBFGSSetScale().
    '
    '    NOTE 3: this function may lead to false positives. In case it reports that
    '            I-th  derivative was calculated incorrectly, you may decrease test
    '            step  and  try  one  more  time  - maybe your function changes too
    '            sharply  and  your  step  is  too  large for such rapidly chanding
    '            function.
    '
    '    INPUT PARAMETERS:
    '        State       -   structure used to store algorithm state
    '        TestStep    -   verification step:
    '                        * TestStep=0 turns verification off
    '                        * TestStep>0 activates verification
    '
    '      -- ALGLIB --
    '         Copyright 24.05.2012 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetgradientcheck(state As minlbfgsstate, teststep As Double)

        minlbfgs.minlbfgssetgradientcheck(state.innerobj, teststep)
        Return
    End Sub

End Class
Partial Public Class alglib



End Class
Partial Public Class alglib



End Class
Partial Public Class alglib



End Class
Partial Public Class alglib


    '************************************************************************
    '    This object stores nonlinear optimizer state.
    '    You should use functions provided by MinQP subpackage to work with this
    '    object
    '    ************************************************************************

    Public Class minqpstate
        Inherits alglibobject
        '
        ' Public declarations
        '

        Public Sub New()
            _innerobj = New minqp.minqpstate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minqpstate(DirectCast(_innerobj.make_copy(), minqp.minqpstate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minqp.minqpstate
        Public ReadOnly Property innerobj() As minqp.minqpstate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minqp.minqpstate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '    This structure stores optimization report:
    '    * InnerIterationsCount      number of inner iterations
    '    * OuterIterationsCount      number of outer iterations
    '    * NCholesky                 number of Cholesky decomposition
    '    * NMV                       number of matrix-vector products
    '                                (only products calculated as part of iterative
    '                                process are counted)
    '    * TerminationType           completion code (see below)
    '
    '    Completion codes:
    '    * -5    inappropriate solver was used:
    '            * QuickQP solver for problem with general linear constraints
    '            * Cholesky solver for semidefinite or indefinite problems
    '            * Cholesky solver for problems with non-boundary constraints
    '    * -4    BLEIC-QP or QuickQP solver found unconstrained direction
    '            of negative curvature (function is unbounded from
    '            below  even  under  constraints),  no  meaningful
    '            minimum can be found.
    '    * -3    inconsistent constraints (or, maybe, feasible point is
    '            too hard to find). If you are sure that constraints are feasible,
    '            try to restart optimizer with better initial approximation.
    '    * -1    solver error
    '    *  1..4 successful completion
    '    *  5    MaxIts steps was taken
    '    *  7    stopping conditions are too stringent,
    '            further improvement is impossible,
    '            X contains best point found so far.
    '    ************************************************************************

    Public Class minqpreport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property inneriterationscount() As Integer
            Get
                Return _innerobj.inneriterationscount
            End Get
            Set(value As Integer)
                _innerobj.inneriterationscount = value
            End Set
        End Property
        Public Property outeriterationscount() As Integer
            Get
                Return _innerobj.outeriterationscount
            End Get
            Set(value As Integer)
                _innerobj.outeriterationscount = value
            End Set
        End Property
        Public Property nmv() As Integer
            Get
                Return _innerobj.nmv
            End Get
            Set(value As Integer)
                _innerobj.nmv = value
            End Set
        End Property
        Public Property ncholesky() As Integer
            Get
                Return _innerobj.ncholesky
            End Get
            Set(value As Integer)
                _innerobj.ncholesky = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New minqp.minqpreport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minqpreport(DirectCast(_innerobj.make_copy(), minqp.minqpreport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minqp.minqpreport
        Public ReadOnly Property innerobj() As minqp.minqpreport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minqp.minqpreport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '                        CONSTRAINED QUADRATIC PROGRAMMING
    '
    '    The subroutine creates QP optimizer. After initial creation,  it  contains
    '    default optimization problem with zero quadratic and linear terms  and  no
    '    constraints. You should set quadratic/linear terms with calls to functions
    '    provided by MinQP subpackage.
    '
    '    You should also choose appropriate QP solver and set it  and  its stopping
    '    criteria by means of MinQPSetAlgo??????() function. Then, you should start
    '    solution process by means of MinQPOptimize() call. Solution itself can  be
    '    obtained with MinQPResults() function.
    '
    '    INPUT PARAMETERS:
    '        N       -   problem size
    '
    '    OUTPUT PARAMETERS:
    '        State   -   optimizer with zero quadratic/linear terms
    '                    and no constraints
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpcreate(n As Integer, ByRef state As minqpstate)
        state = New minqpstate()
        minqp.minqpcreate(n, state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This function sets linear term for QP solver.
    '
    '    By default, linear term is zero.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        B       -   linear term, array[N].
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetlinearterm(state As minqpstate, b As Double())

        minqp.minqpsetlinearterm(state.innerobj, b)
        Return
    End Sub

    '************************************************************************
    '    This  function  sets  dense  quadratic  term  for  QP solver. By  default,
    '    quadratic term is zero.
    '
    '    SUPPORT BY ALGLIB QP ALGORITHMS:
    '
    '    Dense quadratic term can be handled by any of the QP algorithms  supported
    '    by ALGLIB QP Solver.
    '
    '    IMPORTANT:
    '
    '    This solver minimizes following  function:
    '        f(x) = 0.5*x'*A*x + b'*x.
    '    Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
    '        f(x) = x^2 + x
    '    you should rewrite your problem as follows:
    '        f(x) = 0.5*(2*x^2) + x
    '    and your matrix A will be equal to [[2.0]], not to [[1.0]]
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        A       -   matrix, array[N,N]
    '        IsUpper -   (optional) storage type:
    '                    * if True, symmetric matrix  A  is  given  by  its  upper
    '                      triangle, and the lower triangle isn used
    '                    * if False, symmetric matrix  A  is  given  by  its lower
    '                      triangle, and the upper triangle isn used
    '                    * if not given, both lower and upper  triangles  must  be
    '                      filled.
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetquadraticterm(state As minqpstate, a As Double(,), isupper As Boolean)

        minqp.minqpsetquadraticterm(state.innerobj, a, isupper)
        Return
    End Sub
    Public Shared Sub minqpsetquadraticterm(state As minqpstate, a As Double(,))
        Dim isupper As Boolean
        If Not alglib.ap.issymmetric(a) Then
            Throw New alglibexception("'a' parameter is not symmetric matrix")
        End If

        isupper = False
        minqp.minqpsetquadraticterm(state.innerobj, a, isupper)

        Return
    End Sub

    '************************************************************************
    '    This  function  sets  sparse  quadratic  term  for  QP solver. By default,
    '    quadratic term is zero.
    '
    '    IMPORTANT:
    '
    '    This solver minimizes following  function:
    '        f(x) = 0.5*x'*A*x + b'*x.
    '    Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
    '        f(x) = x^2 + x
    '    you should rewrite your problem as follows:
    '        f(x) = 0.5*(2*x^2) + x
    '    and your matrix A will be equal to [[2.0]], not to [[1.0]]
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        A       -   matrix, array[N,N]
    '        IsUpper -   (optional) storage type:
    '                    * if True, symmetric matrix  A  is  given  by  its  upper
    '                      triangle, and the lower triangle isn used
    '                    * if False, symmetric matrix  A  is  given  by  its lower
    '                      triangle, and the upper triangle isn used
    '                    * if not given, both lower and upper  triangles  must  be
    '                      filled.
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetquadratictermsparse(state As minqpstate, a As sparsematrix, isupper As Boolean)

        minqp.minqpsetquadratictermsparse(state.innerobj, a.innerobj, isupper)
        Return
    End Sub

    '************************************************************************
    '    This function sets starting point for QP solver. It is useful to have
    '    good initial approximation to the solution, because it will increase
    '    speed of convergence and identification of active constraints.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        X       -   starting point, array[N].
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetstartingpoint(state As minqpstate, x As Double())

        minqp.minqpsetstartingpoint(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This  function sets origin for QP solver. By default, following QP program
    '    is solved:
    '
    '        min(0.5*x'*A*x+b'*x)
    '
    '    This function allows to solve different problem:
    '
    '        min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        XOrigin -   origin, array[N].
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetorigin(state As minqpstate, xorigin As Double())

        minqp.minqpsetorigin(state.innerobj, xorigin)
        Return
    End Sub

    '************************************************************************
    '    This function sets scaling coefficients.
    '
    '    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
    '    size and gradient are scaled before comparison with tolerances).  Scale of
    '    the I-th variable is a translation invariant measure of:
    '    a) "how large" the variable is
    '    b) how large the step should be to make significant changes in the function
    '
    '    BLEIC-based QP solver uses scale for two purposes:
    '    * to evaluate stopping conditions
    '    * for preconditioning of the underlying BLEIC solver
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        S       -   array[N], non-zero scaling coefficients
    '                    S[i] may be negative, sign doesn't matter.
    '
    '      -- ALGLIB --
    '         Copyright 14.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetscale(state As minqpstate, s As Double())

        minqp.minqpsetscale(state.innerobj, s)
        Return
    End Sub

    '************************************************************************
    '    This function tells solver to use Cholesky-based algorithm. This algorithm
    '    was deprecated in ALGLIB 3.9.0 because its performance is inferior to that
    '    of BLEIC-QP or  QuickQP  on  high-dimensional  problems.  Furthermore,  it
    '    supports only dense convex QP problems.
    '
    '    This solver is no longer active by default.
    '
    '    We recommend you to switch to BLEIC-QP or QuickQP solver.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetalgocholesky(state As minqpstate)

        minqp.minqpsetalgocholesky(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This function tells solver to use BLEIC-based algorithm and sets  stopping
    '    criteria for the algorithm.
    '
    '    ALGORITHM FEATURES:
    '
    '    * supports dense and sparse QP problems
    '    * supports boundary and general linear equality/inequality constraints
    '    * can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
    '      long as they are bounded from below under constraints.
    '      Say, it is possible to solve "min{-x^2} subject to -1<=x<=+1".
    '      Of course, global  minimum  is found only  for  positive  definite   and
    '      semidefinite  problems.  As  for indefinite ones - only local minimum is
    '      found.
    '
    '    ALGORITHM OUTLINE:
    '
    '    * BLEIC-QP solver is just a driver function for MinBLEIC solver; it solves
    '      quadratic  programming   problem   as   general   linearly   constrained
    '      optimization problem, which is solved by means of BLEIC solver  (part of
    '      ALGLIB, active set method).
    '
    '    ALGORITHM LIMITATIONS:
    '
    '    * unlike QuickQP solver, this algorithm does not perform Newton steps  and
    '      does not use Level 3 BLAS. Being general-purpose active set  method,  it
    '      can activate constraints only one-by-one. Thus, its performance is lower
    '      than that of QuickQP.
    '    * its precision is also a bit  inferior  to  that  of   QuickQP.  BLEIC-QP
    '      performs only LBFGS steps (no Newton steps), which are good at detecting
    '      neighborhood of the solution, buy need many iterations to find  solution
    '      with more than 6 digits of precision.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinQPSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes its work if exploratory steepest
    '                    descent  step  on  k+1-th iteration  satisfies   following
    '                    condition:  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '        EpsX    -   >=0
    '                    The  subroutine  finishes its work if exploratory steepest
    '                    descent  step  on  k+1-th iteration  satisfies   following
    '                    condition:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - step vector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinQPSetScale()
    '        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
    '                    iterations is unlimited. NOTE: this  algorithm uses  LBFGS
    '                    iterations,  which  are  relatively  cheap,  but   improve
    '                    function value only a bit. So you will need many iterations
    '                    to converge - from 0.1*N to 10*N, depending  on  problem's
    '                    condition number.
    '
    '    IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
    '    BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!
    '
    '    Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
    '    to automatic stopping criterion selection (presently it is  small    step
    '    length, but it may change in the future versions of ALGLIB).
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetalgobleic(state As minqpstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        minqp.minqpsetalgobleic(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    This function tells solver to use QuickQP  algorithm:  special  extra-fast
    '    algorithm   for   problems  with  boundary-only constrants. It  may  solve
    '    non-convex  problems  as  long  as  they  are  bounded  from  below  under
    '    constraints.
    '
    '    ALGORITHM FEATURES:
    '    * many times (from 5x to 50x!) faster than BLEIC-based QP solver; utilizes
    '      accelerated methods for activation of constraints.
    '    * supports dense and sparse QP problems
    '    * supports ONLY boundary constraints; general linear constraints  are  NOT
    '      supported by this solver
    '    * can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
    '      long as they are bounded from below under constraints.
    '      Say, it is possible to solve "min{-x^2} subject to -1<=x<=+1".
    '      In convex/semidefinite case global minimum  is  returned,  in  nonconvex
    '      case - algorithm returns one of the local minimums.
    '
    '    ALGORITHM OUTLINE:
    '
    '    * algorithm  performs  two kinds of iterations: constrained CG  iterations
    '      and constrained Newton iterations
    '    * initially it performs small number of constrained CG  iterations,  which
    '      can efficiently activate/deactivate multiple constraints
    '    * after CG phase algorithm tries to calculate Cholesky  decomposition  and
    '      to perform several constrained Newton steps. If  Cholesky  decomposition
    '      failed (matrix is indefinite even under constraints),  we  perform  more
    '      CG iterations until we converge to such set of constraints  that  system
    '      matrix becomes  positive  definite.  Constrained  Newton  steps  greatly
    '      increase convergence speed and precision.
    '    * algorithm interleaves CG and Newton iterations which  allows  to  handle
    '      indefinite matrices (CG phase) and quickly converge after final  set  of
    '      constraints is found (Newton phase). Combination of CG and Newton phases
    '      is called "outer iteration".
    '    * it is possible to turn off Newton  phase  (beneficial  for  semidefinite
    '      problems - Cholesky decomposition will fail too often)
    '
    '    ALGORITHM LIMITATIONS:
    '
    '    * algorithm does not support general  linear  constraints;  only  boundary
    '      ones are supported
    '    * Cholesky decomposition for sparse problems  is  performed  with  Skyline
    '      Cholesky solver, which is intended for low-profile matrices. No profile-
    '      reducing reordering of variables is performed in this version of ALGLIB.
    '    * problems with near-zero negative eigenvalues (or exacty zero  ones)  may
    '      experience about 2-3x performance penalty. The reason is  that  Cholesky
    '      decomposition can not be performed until we identify directions of  zero
    '      and negative curvature and activate corresponding boundary constraints -
    '      but we need a lot of trial and errors because these directions  are hard
    '      to notice in the matrix spectrum.
    '      In this case you may turn off Newton phase of algorithm.
    '      Large negative eigenvalues  are  not  an  issue,  so  highly  non-convex
    '      problems can be solved very efficiently.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinQPSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes its work if exploratory steepest
    '                    descent  step  on  k+1-th iteration  satisfies   following
    '                    condition:  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '        EpsX    -   >=0
    '                    The  subroutine  finishes its work if exploratory steepest
    '                    descent  step  on  k+1-th iteration  satisfies   following
    '                    condition:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - step vector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinQPSetScale()
    '        MaxOuterIts-maximum number of OUTER iterations.  One  outer  iteration
    '                    includes some amount of CG iterations (from 5 to  ~N)  and
    '                    one or several (usually small amount) Newton steps.  Thus,
    '                    one outer iteration has high cost, but can greatly  reduce
    '                    funcation value.
    '        UseNewton-  use Newton phase or not:
    '                    * Newton phase improves performance of  positive  definite
    '                      dense problems (about 2 times improvement can be observed)
    '                    * can result in some performance penalty  on  semidefinite
    '                      or slightly negative definite  problems  -  each  Newton
    '                      phase will bring no improvement (Cholesky failure),  but
    '                      still will require computational time.
    '                    * if you doubt, you can turn off this  phase  -  optimizer
    '                      will retain its most of its high speed.
    '
    '    IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
    '    BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!
    '
    '    Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
    '    to automatic stopping criterion selection (presently it is  small    step
    '    length, but it may change in the future versions of ALGLIB).
    '
    '      -- ALGLIB --
    '         Copyright 22.05.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetalgoquickqp(state As minqpstate, epsg As Double, epsf As Double, epsx As Double, maxouterits As Integer, usenewton As Boolean)

        minqp.minqpsetalgoquickqp(state.innerobj, epsg, epsf, epsx, maxouterits, usenewton)
        Return
    End Sub

    '************************************************************************
    '    This function sets boundary constraints for QP solver
    '
    '    Boundary constraints are inactive by default (after initial creation).
    '    After  being  set,  they  are  preserved  until explicitly turned off with
    '    another SetBC() call.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        BndL    -   lower bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very small number or -INF (latter is recommended because
    '                    it will allow solver to use better algorithm).
    '        BndU    -   upper bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very large number or +INF (latter is recommended because
    '                    it will allow solver to use better algorithm).
    '
    '    NOTE: it is possible to specify BndL[i]=BndU[i]. In this case I-th
    '    variable will be "frozen" at X[i]=BndL[i]=BndU[i].
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetbc(state As minqpstate, bndl As Double(), bndu As Double())

        minqp.minqpsetbc(state.innerobj, bndl, bndu)
        Return
    End Sub

    '************************************************************************
    '    This function sets linear constraints for QP optimizer.
    '
    '    Linear constraints are inactive by default (after initial creation).
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinQPCreate call.
    '        C       -   linear constraints, array[K,N+1].
    '                    Each row of C represents one constraint, either equality
    '                    or inequality (see below):
    '                    * first N elements correspond to coefficients,
    '                    * last element corresponds to the right part.
    '                    All elements of C (including right part) must be finite.
    '        CT      -   type of constraints, array[K]:
    '                    * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
    '                    * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
    '                    * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
    '        K       -   number of equality/inequality constraints, K>=0:
    '                    * if given, only leading K elements of C/CT are used
    '                    * if not given, automatically determined from sizes of C/CT
    '
    '    NOTE 1: linear (non-bound) constraints are satisfied only approximately  -
    '            there always exists some minor violation (about 10^-10...10^-13)
    '            due to numerical errors.
    '
    '      -- ALGLIB --
    '         Copyright 19.06.2012 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpsetlc(state As minqpstate, c As Double(,), ct As Integer(), k As Integer)

        minqp.minqpsetlc(state.innerobj, c, ct, k)
        Return
    End Sub
    Public Shared Sub minqpsetlc(state As minqpstate, c As Double(,), ct As Integer())
        Dim k As Integer
        If (ap.rows(c) <> ap.len(ct)) Then
            Throw New alglibexception("Error while calling 'minqpsetlc': looks like one of arguments has wrong size")
        End If

        k = ap.rows(c)
        minqp.minqpsetlc(state.innerobj, c, ct, k)

        Return
    End Sub

    '************************************************************************
    '    This function solves quadratic programming problem.
    '
    '    Prior to calling this function you should choose solver by means of one of
    '    the following functions:
    '
    '    * MinQPSetAlgoQuickQP() - for QuickQP solver
    '    * MinQPSetAlgoBLEIC() - for BLEIC-QP solver
    '
    '    These functions also allow you to control stopping criteria of the solver.
    '    If you did not set solver,  MinQP  subpackage  will  automatically  select
    '    solver for your problem and will run it with default stopping criteria.
    '
    '    However, it is better to set explicitly solver and its stopping criteria.
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    You should use MinQPResults() function to access results after calls
    '    to this function.
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey.
    '         Special thanks to Elvira Illarionova  for  important  suggestions  on
    '         the linearly constrained QP algorithm.
    '    ************************************************************************

    Public Shared Sub minqpoptimize(state As minqpstate)

        minqp.minqpoptimize(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    QP solver results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution.
    '                    This array is allocated and initialized only when
    '                    Rep.TerminationType parameter is positive (success).
    '        Rep     -   optimization report. You should check Rep.TerminationType,
    '                    which contains completion code, and you may check  another
    '                    fields which contain another information  about  algorithm
    '                    functioning.
    '
    '                    Failure codes returned by algorithm are:
    '                    * -5    inappropriate solver was used:
    '                            * Cholesky solver for (semi)indefinite problems
    '                            * Cholesky solver for problems with sparse matrix
    '                            * QuickQP solver for problem with  general  linear
    '                              constraints
    '                    * -4    BLEIC-QP/QuickQP   solver    found   unconstrained
    '                            direction  of   negative  curvature  (function  is
    '                            unbounded from below even under constraints),   no
    '                            meaningful minimum can be found.
    '                    * -3    inconsistent constraints (or maybe  feasible point
    '                            is too  hard  to  find).  If  you  are  sure  that
    '                            constraints are feasible, try to restart optimizer
    '                            with better initial approximation.
    '
    '                    Completion codes specific for Cholesky algorithm:
    '                    *  4   successful completion
    '
    '                    Completion codes specific for BLEIC/QuickQP algorithms:
    '                    *  1   relative function improvement is no more than EpsF.
    '                    *  2   scaled step is no more than EpsX.
    '                    *  4   scaled gradient norm is no more than EpsG.
    '                    *  5   MaxIts steps was taken
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpresults(state As minqpstate, ByRef x As Double(), ByRef rep As minqpreport)
        x = New Double(-1) {}
        rep = New minqpreport()
        minqp.minqpresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    QP results
    '
    '    Buffered implementation of MinQPResults() which uses pre-allocated  buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 11.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minqpresultsbuf(state As minqpstate, ByRef x As Double(), rep As minqpreport)

        minqp.minqpresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

End Class
Partial Public Class alglib


    '************************************************************************
    '    Levenberg-Marquardt optimizer.
    '
    '    This structure should be created using one of the MinLMCreate???()
    '    functions. You should not access its fields directly; use ALGLIB functions
    '    to work with it.
    '    ************************************************************************

    Public Class minlmstate
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property needf() As Boolean
            Get
                Return _innerobj.needf
            End Get
            Set(value As Boolean)
                _innerobj.needf = value
            End Set
        End Property
        Public Property needfg() As Boolean
            Get
                Return _innerobj.needfg
            End Get
            Set(value As Boolean)
                _innerobj.needfg = value
            End Set
        End Property
        Public Property needfgh() As Boolean
            Get
                Return _innerobj.needfgh
            End Get
            Set(value As Boolean)
                _innerobj.needfgh = value
            End Set
        End Property
        Public Property needfi() As Boolean
            Get
                Return _innerobj.needfi
            End Get
            Set(value As Boolean)
                _innerobj.needfi = value
            End Set
        End Property
        Public Property needfij() As Boolean
            Get
                Return _innerobj.needfij
            End Get
            Set(value As Boolean)
                _innerobj.needfij = value
            End Set
        End Property
        Public Property xupdated() As Boolean
            Get
                Return _innerobj.xupdated
            End Get
            Set(value As Boolean)
                _innerobj.xupdated = value
            End Set
        End Property
        Public Property f() As Double
            Get
                Return _innerobj.f
            End Get
            Set(value As Double)
                _innerobj.f = value
            End Set
        End Property
        Public ReadOnly Property fi() As Double()
            Get
                Return _innerobj.fi
            End Get
        End Property
        Public ReadOnly Property g() As Double()
            Get
                Return _innerobj.g
            End Get
        End Property
        Public ReadOnly Property h() As Double(,)
            Get
                Return _innerobj.h
            End Get
        End Property
        Public ReadOnly Property j() As Double(,)
            Get
                Return _innerobj.j
            End Get
        End Property
        Public ReadOnly Property x() As Double()
            Get
                Return _innerobj.x
            End Get
        End Property

        Public Sub New()
            _innerobj = New minlm.minlmstate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minlmstate(DirectCast(_innerobj.make_copy(), minlm.minlmstate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minlm.minlmstate
        Public ReadOnly Property innerobj() As minlm.minlmstate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minlm.minlmstate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '    Optimization report, filled by MinLMResults() function
    '
    '    FIELDS:
    '    * TerminationType, completetion code:
    '        * -7    derivative correctness check failed;
    '                see Rep.WrongNum, Rep.WrongI, Rep.WrongJ for
    '                more information.
    '        * -3    constraints are inconsistent
    '        *  1    relative function improvement is no more than
    '                EpsF.
    '        *  2    relative step is no more than EpsX.
    '        *  4    gradient is no more than EpsG.
    '        *  5    MaxIts steps was taken
    '        *  7    stopping conditions are too stringent,
    '                further improvement is impossible
    '        *  8    terminated   by  user  who  called  MinLMRequestTermination().
    '                X contains point which was "current accepted" when termination
    '                request was submitted.
    '    * IterationsCount, contains iterations count
    '    * NFunc, number of function calculations
    '    * NJac, number of Jacobi matrix calculations
    '    * NGrad, number of gradient calculations
    '    * NHess, number of Hessian calculations
    '    * NCholesky, number of Cholesky decomposition calculations
    '    ************************************************************************

    Public Class minlmreport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property iterationscount() As Integer
            Get
                Return _innerobj.iterationscount
            End Get
            Set(value As Integer)
                _innerobj.iterationscount = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property
        Public Property funcidx() As Integer
            Get
                Return _innerobj.funcidx
            End Get
            Set(value As Integer)
                _innerobj.funcidx = value
            End Set
        End Property
        Public Property varidx() As Integer
            Get
                Return _innerobj.varidx
            End Get
            Set(value As Integer)
                _innerobj.varidx = value
            End Set
        End Property
        Public Property nfunc() As Integer
            Get
                Return _innerobj.nfunc
            End Get
            Set(value As Integer)
                _innerobj.nfunc = value
            End Set
        End Property
        Public Property njac() As Integer
            Get
                Return _innerobj.njac
            End Get
            Set(value As Integer)
                _innerobj.njac = value
            End Set
        End Property
        Public Property ngrad() As Integer
            Get
                Return _innerobj.ngrad
            End Get
            Set(value As Integer)
                _innerobj.ngrad = value
            End Set
        End Property
        Public Property nhess() As Integer
            Get
                Return _innerobj.nhess
            End Get
            Set(value As Integer)
                _innerobj.nhess = value
            End Set
        End Property
        Public Property ncholesky() As Integer
            Get
                Return _innerobj.ncholesky
            End Get
            Set(value As Integer)
                _innerobj.ncholesky = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New minlm.minlmreport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minlmreport(DirectCast(_innerobj.make_copy(), minlm.minlmreport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minlm.minlmreport
        Public ReadOnly Property innerobj() As minlm.minlmreport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minlm.minlmreport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '                    IMPROVED LEVENBERG-MARQUARDT METHOD FOR
    '                     NON-LINEAR LEAST SQUARES OPTIMIZATION
    '
    '    DESCRIPTION:
    '    This function is used to find minimum of function which is represented  as
    '    sum of squares:
    '        F(x) = f[0]^2(x[0],...,x[n-1]) + ... + f[m-1]^2(x[0],...,x[n-1])
    '    using value of function vector f[] and Jacobian of f[].
    '
    '
    '    REQUIREMENTS:
    '    This algorithm will request following information during its operation:
    '
    '    * function vector f[] at given point X
    '    * function vector f[] and Jacobian of f[] (simultaneously) at given point
    '
    '    There are several overloaded versions of  MinLMOptimize()  function  which
    '    correspond  to  different LM-like optimization algorithms provided by this
    '    unit. You should choose version which accepts fvec()  and jac() callbacks.
    '    First  one  is used to calculate f[] at given point, second one calculates
    '    f[] and Jacobian df[i]/dx[j].
    '
    '    You can try to initialize MinLMState structure with VJ  function and  then
    '    use incorrect version  of  MinLMOptimize()  (for  example,  version  which
    '    works  with  general  form function and does not provide Jacobian), but it
    '    will  lead  to  exception  being  thrown  after first attempt to calculate
    '    Jacobian.
    '
    '
    '    USAGE:
    '    1. User initializes algorithm state with MinLMCreateVJ() call
    '    2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
    '       other functions
    '    3. User calls MinLMOptimize() function which  takes algorithm  state   and
    '       callback functions.
    '    4. User calls MinLMResults() to get solution
    '    5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
    '       with same N/M but another starting point and/or another function.
    '       MinLMRestartFrom() allows to reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   dimension, N>1
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size of X
    '        M       -   number of functions f[i]
    '        X       -   initial solution, array[0..N-1]
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    NOTES:
    '    1. you may tune stopping conditions with MinLMSetCond() function
    '    2. if target function contains exp() or other fast growing functions,  and
    '       optimization algorithm makes too large steps which leads  to  overflow,
    '       use MinLMSetStpMax() function to bound algorithm's steps.
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatevj(n As Integer, m As Integer, x As Double(), ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatevj(n, m, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatevj(m As Integer, x As Double(), ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatevj(n, m, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '                    IMPROVED LEVENBERG-MARQUARDT METHOD FOR
    '                     NON-LINEAR LEAST SQUARES OPTIMIZATION
    '
    '    DESCRIPTION:
    '    This function is used to find minimum of function which is represented  as
    '    sum of squares:
    '        F(x) = f[0]^2(x[0],...,x[n-1]) + ... + f[m-1]^2(x[0],...,x[n-1])
    '    using value of function vector f[] only. Finite differences  are  used  to
    '    calculate Jacobian.
    '
    '
    '    REQUIREMENTS:
    '    This algorithm will request following information during its operation:
    '    * function vector f[] at given point X
    '
    '    There are several overloaded versions of  MinLMOptimize()  function  which
    '    correspond  to  different LM-like optimization algorithms provided by this
    '    unit. You should choose version which accepts fvec() callback.
    '
    '    You can try to initialize MinLMState structure with VJ  function and  then
    '    use incorrect version  of  MinLMOptimize()  (for  example,  version  which
    '    works with general form function and does not accept function vector), but
    '    it will  lead  to  exception being thrown after first attempt to calculate
    '    Jacobian.
    '
    '
    '    USAGE:
    '    1. User initializes algorithm state with MinLMCreateV() call
    '    2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
    '       other functions
    '    3. User calls MinLMOptimize() function which  takes algorithm  state   and
    '       callback functions.
    '    4. User calls MinLMResults() to get solution
    '    5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
    '       with same N/M but another starting point and/or another function.
    '       MinLMRestartFrom() allows to reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   dimension, N>1
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size of X
    '        M       -   number of functions f[i]
    '        X       -   initial solution, array[0..N-1]
    '        DiffStep-   differentiation step, >0
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    See also MinLMIteration, MinLMResults.
    '
    '    NOTES:
    '    1. you may tune stopping conditions with MinLMSetCond() function
    '    2. if target function contains exp() or other fast growing functions,  and
    '       optimization algorithm makes too large steps which leads  to  overflow,
    '       use MinLMSetStpMax() function to bound algorithm's steps.
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatev(n As Integer, m As Integer, x As Double(), diffstep As Double, ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatev(n, m, x, diffstep, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatev(m As Integer, x As Double(), diffstep As Double, ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatev(n, m, x, diffstep, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '        LEVENBERG-MARQUARDT-LIKE METHOD FOR NON-LINEAR OPTIMIZATION
    '
    '    DESCRIPTION:
    '    This  function  is  used  to  find  minimum  of general form (not "sum-of-
    '    -squares") function
    '        F = F(x[0], ..., x[n-1])
    '    using  its  gradient  and  Hessian.  Levenberg-Marquardt modification with
    '    L-BFGS pre-optimization and internal pre-conditioned  L-BFGS  optimization
    '    after each Levenberg-Marquardt step is used.
    '
    '
    '    REQUIREMENTS:
    '    This algorithm will request following information during its operation:
    '
    '    * function value F at given point X
    '    * F and gradient G (simultaneously) at given point X
    '    * F, G and Hessian H (simultaneously) at given point X
    '
    '    There are several overloaded versions of  MinLMOptimize()  function  which
    '    correspond  to  different LM-like optimization algorithms provided by this
    '    unit. You should choose version which accepts func(),  grad()  and  hess()
    '    function pointers. First pointer is used to calculate F  at  given  point,
    '    second  one  calculates  F(x)  and  grad F(x),  third one calculates F(x),
    '    grad F(x), hess F(x).
    '
    '    You can try to initialize MinLMState structure with FGH-function and  then
    '    use incorrect version of MinLMOptimize() (for example, version which  does
    '    not provide Hessian matrix), but it will lead to  exception  being  thrown
    '    after first attempt to calculate Hessian.
    '
    '
    '    USAGE:
    '    1. User initializes algorithm state with MinLMCreateFGH() call
    '    2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
    '       other functions
    '    3. User calls MinLMOptimize() function which  takes algorithm  state   and
    '       pointers (delegates, etc.) to callback functions.
    '    4. User calls MinLMResults() to get solution
    '    5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
    '       with same N but another starting point and/or another function.
    '       MinLMRestartFrom() allows to reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   dimension, N>1
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size of X
    '        X       -   initial solution, array[0..N-1]
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '
    '    NOTES:
    '    1. you may tune stopping conditions with MinLMSetCond() function
    '    2. if target function contains exp() or other fast growing functions,  and
    '       optimization algorithm makes too large steps which leads  to  overflow,
    '       use MinLMSetStpMax() function to bound algorithm's steps.
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatefgh(n As Integer, x As Double(), ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatefgh(n, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatefgh(x As Double(), ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatefgh(n, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This function sets stopping conditions for Levenberg-Marquardt optimization
    '    algorithm.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinLMSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes  its work if on k+1-th iteration
    '                    the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '                    is satisfied.
    '        EpsX    -   >=0
    '                    The subroutine finishes its work if  on  k+1-th  iteration
    '                    the condition |v|<=EpsX is fulfilled, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - ste pvector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinLMSetScale()
    '        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
    '                    iterations   is    unlimited.   Only   Levenberg-Marquardt
    '                    iterations  are  counted  (L-BFGS/CG  iterations  are  NOT
    '                    counted because their cost is very low compared to that of
    '                    LM).
    '
    '    Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
    '    automatic stopping criterion selection (small EpsX).
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetcond(state As minlmstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        minlm.minlmsetcond(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    This function turns on/off reporting.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        NeedXRep-   whether iteration reports are needed or not
    '
    '    If NeedXRep is True, algorithm will call rep() callback function if  it is
    '    provided to MinLMOptimize(). Both Levenberg-Marquardt and internal  L-BFGS
    '    iterations are reported.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetxrep(state As minlmstate, needxrep As Boolean)

        minlm.minlmsetxrep(state.innerobj, needxrep)
        Return
    End Sub

    '************************************************************************
    '    This function sets maximum step length
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
    '                    want to limit step length.
    '
    '    Use this subroutine when you optimize target function which contains exp()
    '    or  other  fast  growing  functions,  and optimization algorithm makes too
    '    large  steps  which  leads  to overflow. This function allows us to reject
    '    steps  that  are  too  large  (and  therefore  expose  us  to the possible
    '    overflow) without actually calculating function value at the x+stp*d.
    '
    '    NOTE: non-zero StpMax leads to moderate  performance  degradation  because
    '    intermediate  step  of  preconditioned L-BFGS optimization is incompatible
    '    with limits on step size.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetstpmax(state As minlmstate, stpmax As Double)

        minlm.minlmsetstpmax(state.innerobj, stpmax)
        Return
    End Sub

    '************************************************************************
    '    This function sets scaling coefficients for LM optimizer.
    '
    '    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
    '    size and gradient are scaled before comparison with tolerances).  Scale of
    '    the I-th variable is a translation invariant measure of:
    '    a) "how large" the variable is
    '    b) how large the step should be to make significant changes in the function
    '
    '    Generally, scale is NOT considered to be a form of preconditioner.  But LM
    '    optimizer is unique in that it uses scaling matrix both  in  the  stopping
    '    condition tests and as Marquardt damping factor.
    '
    '    Proper scaling is very important for the algorithm performance. It is less
    '    important for the quality of results, but still has some influence (it  is
    '    easier  to  converge  when  variables  are  properly  scaled, so premature
    '    stopping is possible when very badly scalled variables are  combined  with
    '    relaxed stopping conditions).
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        S       -   array[N], non-zero scaling coefficients
    '                    S[i] may be negative, sign doesn't matter.
    '
    '      -- ALGLIB --
    '         Copyright 14.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetscale(state As minlmstate, s As Double())

        minlm.minlmsetscale(state.innerobj, s)
        Return
    End Sub

    '************************************************************************
    '    This function sets boundary constraints for LM optimizer
    '
    '    Boundary constraints are inactive by default (after initial creation).
    '    They are preserved until explicitly turned off with another SetBC() call.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        BndL    -   lower bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very small number or -INF (latter is recommended because
    '                    it will allow solver to use better algorithm).
    '        BndU    -   upper bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very large number or +INF (latter is recommended because
    '                    it will allow solver to use better algorithm).
    '
    '    NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
    '    variable will be "frozen" at X[i]=BndL[i]=BndU[i].
    '
    '    NOTE 2: this solver has following useful properties:
    '    * bound constraints are always satisfied exactly
    '    * function is evaluated only INSIDE area specified by bound constraints
    '      or at its boundary
    '
    '      -- ALGLIB --
    '         Copyright 14.01.2011 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetbc(state As minlmstate, bndl As Double(), bndu As Double())

        minlm.minlmsetbc(state.innerobj, bndl, bndu)
        Return
    End Sub

    '************************************************************************
    '    This function is used to change acceleration settings
    '
    '    You can choose between three acceleration strategies:
    '    * AccType=0, no acceleration.
    '    * AccType=1, secant updates are used to update quadratic model after  each
    '      iteration. After fixed number of iterations (or after  model  breakdown)
    '      we  recalculate  quadratic  model  using  analytic  Jacobian  or  finite
    '      differences. Number of secant-based iterations depends  on  optimization
    '      settings: about 3 iterations - when we have analytic Jacobian, up to 2*N
    '      iterations - when we use finite differences to calculate Jacobian.
    '
    '    AccType=1 is recommended when Jacobian  calculation  cost  is  prohibitive
    '    high (several Mx1 function vector calculations  followed  by  several  NxN
    '    Cholesky factorizations are faster than calculation of one M*N  Jacobian).
    '    It should also be used when we have no Jacobian, because finite difference
    '    approximation takes too much time to compute.
    '
    '    Table below list  optimization  protocols  (XYZ  protocol  corresponds  to
    '    MinLMCreateXYZ) and acceleration types they support (and use by  default).
    '
    '    ACCELERATION TYPES SUPPORTED BY OPTIMIZATION PROTOCOLS:
    '
    '    protocol    0   1   comment
    '    V           +   +
    '    VJ          +   +
    '    FGH         +
    '
    '    DAFAULT VALUES:
    '
    '    protocol    0   1   comment
    '    V               x   without acceleration it is so slooooooooow
    '    VJ          x
    '    FGH         x
    '
    '    NOTE: this  function should be called before optimization. Attempt to call
    '    it during algorithm iterations may result in unexpected behavior.
    '
    '    NOTE: attempt to call this function with unsupported protocol/acceleration
    '    combination will result in exception being thrown.
    '
    '      -- ALGLIB --
    '         Copyright 14.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetacctype(state As minlmstate, acctype As Integer)

        minlm.minlmsetacctype(state.innerobj, acctype)
        Return
    End Sub

    '************************************************************************
    '    This function provides reverse communication interface
    '    Reverse communication interface is not documented or recommended to use.
    '    See below for functions which provide better documented API
    '    ************************************************************************

    Public Shared Function minlmiteration(state As minlmstate) As Boolean

        Dim result As Boolean = minlm.minlmiteration(state.innerobj)
        Return result
    End Function
    '************************************************************************
    '    This family of functions is used to launcn iterations of nonlinear optimizer
    '
    '    These functions accept following parameters:
    '        func    -   callback which calculates function (or merit function)
    '                    value func at given point x
    '        grad    -   callback which calculates function (or merit function)
    '                    value func and gradient grad at given point x
    '        hess    -   callback which calculates function (or merit function)
    '                    value func, gradient grad and Hessian hess at given point x
    '        fvec    -   callback which calculates function vector fi[]
    '                    at given point x
    '        jac     -   callback which calculates function vector fi[]
    '                    and Jacobian jac at given point x
    '        rep     -   optional callback which is called after each iteration
    '                    can be null
    '        obj     -   optional object which is passed to func/grad/hess/jac/rep
    '                    can be null
    '
    '    NOTES:
    '
    '    1. Depending on function used to create state  structure,  this  algorithm
    '       may accept Jacobian and/or Hessian and/or gradient.  According  to  the
    '       said above, there ase several versions of this function,  which  accept
    '       different sets of callbacks.
    '
    '       This flexibility opens way to subtle errors - you may create state with
    '       MinLMCreateFGH() (optimization using Hessian), but call function  which
    '       does not accept Hessian. So when algorithm will request Hessian,  there
    '       will be no callback to call. In this case exception will be thrown.
    '
    '       Be careful to avoid such errors because there is no way to find them at
    '       compile time - you can see them at runtime only.
    '
    '      -- ALGLIB --
    '         Copyright 10.03.2009 by Bochkanov Sergey
    '
    '    ************************************************************************

    Public Shared Sub minlmoptimize(state As minlmstate, fvec As ndimensional_fvec, rep As ndimensional_rep, obj As Object)
        If fvec Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (fvec is null)")
        End If
        While alglib.minlmiteration(state)
            If state.needfi Then
                fvec(state.x, state.innerobj.fi, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minlmoptimize(state As minlmstate, fvec As ndimensional_fvec, jac As ndimensional_jac, rep As ndimensional_rep, obj As Object)
        If fvec Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (fvec is null)")
        End If
        If jac Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (jac is null)")
        End If
        While alglib.minlmiteration(state)
            If state.needfi Then
                fvec(state.x, state.innerobj.fi, obj)
                Continue While
            End If
            If state.needfij Then
                jac(state.x, state.innerobj.fi, state.innerobj.j, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minlmoptimize(state As minlmstate, func As ndimensional_func, grad As ndimensional_grad, hess As ndimensional_hess, rep As ndimensional_rep, obj As Object)
        If func Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (func is null)")
        End If
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (grad is null)")
        End If
        If hess Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (hess is null)")
        End If
        While alglib.minlmiteration(state)
            If state.needf Then
                func(state.x, state.innerobj.f, obj)
                Continue While
            End If
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.needfgh Then
                hess(state.x, state.innerobj.f, state.innerobj.g, state.innerobj.h, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minlmoptimize(state As minlmstate, func As ndimensional_func, jac As ndimensional_jac, rep As ndimensional_rep, obj As Object)
        If func Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (func is null)")
        End If
        If jac Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (jac is null)")
        End If
        While alglib.minlmiteration(state)
            If state.needf Then
                func(state.x, state.innerobj.f, obj)
                Continue While
            End If
            If state.needfij Then
                jac(state.x, state.innerobj.fi, state.innerobj.j, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minlmoptimize(state As minlmstate, func As ndimensional_func, grad As ndimensional_grad, jac As ndimensional_jac, rep As ndimensional_rep, obj As Object)
        If func Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (func is null)")
        End If
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (grad is null)")
        End If
        If jac Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize()' (jac is null)")
        End If
        While alglib.minlmiteration(state)
            If state.needf Then
                func(state.x, state.innerobj.f, obj)
                Continue While
            End If
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.needfij Then
                jac(state.x, state.innerobj.fi, state.innerobj.j, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minlmoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    Levenberg-Marquardt algorithm results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution
    '        Rep     -   optimization  report;  includes  termination   codes   and
    '                    additional information. Termination codes are listed below,
    '                    see comments for this structure for more info.
    '                    Termination code is stored in rep.terminationtype field:
    '                    * -7    derivative correctness check failed;
    '                            see rep.wrongnum, rep.wrongi, rep.wrongj for
    '                            more information.
    '                    * -3    constraints are inconsistent
    '                    *  1    relative function improvement is no more than
    '                            EpsF.
    '                    *  2    relative step is no more than EpsX.
    '                    *  4    gradient is no more than EpsG.
    '                    *  5    MaxIts steps was taken
    '                    *  7    stopping conditions are too stringent,
    '                            further improvement is impossible
    '                    *  8    terminated by user who called minlmrequesttermination().
    '                            X contains point which was "current accepted" when
    '                            termination request was submitted.
    '
    '      -- ALGLIB --
    '         Copyright 10.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmresults(state As minlmstate, ByRef x As Double(), ByRef rep As minlmreport)
        x = New Double(-1) {}
        rep = New minlmreport()
        minlm.minlmresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Levenberg-Marquardt algorithm results
    '
    '    Buffered implementation of MinLMResults(), which uses pre-allocated buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 10.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmresultsbuf(state As minlmstate, ByRef x As Double(), rep As minlmreport)

        minlm.minlmresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine  restarts  LM  algorithm from new point. All optimization
    '    parameters are left unchanged.
    '
    '    This  function  allows  to  solve multiple  optimization  problems  (which
    '    must have same number of dimensions) without object reallocation penalty.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure used for reverse communication previously
    '                    allocated with MinLMCreateXXX call.
    '        X       -   new starting point.
    '
    '      -- ALGLIB --
    '         Copyright 30.07.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmrestartfrom(state As minlmstate, x As Double())

        minlm.minlmrestartfrom(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This subroutine submits request for termination of running  optimizer.  It
    '    should be called from user-supplied callback when user decides that it  is
    '    time to "smoothly" terminate optimization process.  As  result,  optimizer
    '    stops at point which was "current accepted" when termination  request  was
    '    submitted and returns error code 8 (successful termination).
    '
    '    INPUT PARAMETERS:
    '        State   -   optimizer structure
    '
    '    NOTE: after  request  for  termination  optimizer  may   perform   several
    '          additional calls to user-supplied callbacks. It does  NOT  guarantee
    '          to stop immediately - it just guarantees that these additional calls
    '          will be discarded later.
    '
    '    NOTE: calling this function on optimizer which is NOT running will have no
    '          effect.
    '
    '    NOTE: multiple calls to this function are possible. First call is counted,
    '          subsequent calls are silently ignored.
    '
    '      -- ALGLIB --
    '         Copyright 08.10.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmrequesttermination(state As minlmstate)

        minlm.minlmrequesttermination(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This is obsolete function.
    '
    '    Since ALGLIB 3.3 it is equivalent to MinLMCreateVJ().
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatevgj(n As Integer, m As Integer, x As Double(), ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatevgj(n, m, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatevgj(m As Integer, x As Double(), ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatevgj(n, m, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This is obsolete function.
    '
    '    Since ALGLIB 3.3 it is equivalent to MinLMCreateFJ().
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatefgj(n As Integer, m As Integer, x As Double(), ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatefgj(n, m, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatefgj(m As Integer, x As Double(), ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatefgj(n, m, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This function is considered obsolete since ALGLIB 3.1.0 and is present for
    '    backward  compatibility  only.  We  recommend  to use MinLMCreateVJ, which
    '    provides similar, but more consistent and feature-rich interface.
    '
    '      -- ALGLIB --
    '         Copyright 30.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmcreatefj(n As Integer, m As Integer, x As Double(), ByRef state As minlmstate)
        state = New minlmstate()
        minlm.minlmcreatefj(n, m, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minlmcreatefj(m As Integer, x As Double(), ByRef state As minlmstate)
        Dim n As Integer

        state = New minlmstate()
        n = ap.len(x)
        minlm.minlmcreatefj(n, m, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This  subroutine  turns  on  verification  of  the  user-supplied analytic
    '    gradient:
    '    * user calls this subroutine before optimization begins
    '    * MinLMOptimize() is called
    '    * prior to actual optimization, for  each  function Fi and each  component
    '      of parameters  being  optimized X[j] algorithm performs following steps:
    '      * two trial steps are made to X[j]-TestStep*S[j] and X[j]+TestStep*S[j],
    '        where X[j] is j-th parameter and S[j] is a scale of j-th parameter
    '      * if needed, steps are bounded with respect to constraints on X[]
    '      * Fi(X) is evaluated at these trial points
    '      * we perform one more evaluation in the middle point of the interval
    '      * we  build  cubic  model using function values and derivatives at trial
    '        points and we compare its prediction with actual value in  the  middle
    '        point
    '      * in case difference between prediction and actual value is higher  than
    '        some predetermined threshold, algorithm stops with completion code -7;
    '        Rep.VarIdx is set to index of the parameter with incorrect derivative,
    '        Rep.FuncIdx is set to index of the function.
    '    * after verification is over, algorithm proceeds to the actual optimization.
    '
    '    NOTE 1: verification  needs  N (parameters count) Jacobian evaluations. It
    '            is  very  costly  and  you  should use it only for low dimensional
    '            problems,  when  you  want  to  be  sure  that  you've   correctly
    '            calculated  analytic  derivatives.  You should not  use  it in the
    '            production code  (unless  you  want  to check derivatives provided
    '            by some third party).
    '
    '    NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
    '            (so large that function behaviour is significantly non-cubic) will
    '            lead to false alarms. You may use  different  step  for  different
    '            parameters by means of setting scale with MinLMSetScale().
    '
    '    NOTE 3: this function may lead to false positives. In case it reports that
    '            I-th  derivative was calculated incorrectly, you may decrease test
    '            step  and  try  one  more  time  - maybe your function changes too
    '            sharply  and  your  step  is  too  large for such rapidly chanding
    '            function.
    '
    '    INPUT PARAMETERS:
    '        State       -   structure used to store algorithm state
    '        TestStep    -   verification step:
    '                        * TestStep=0 turns verification off
    '                        * TestStep>0 activates verification
    '
    '      -- ALGLIB --
    '         Copyright 15.06.2012 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlmsetgradientcheck(state As minlmstate, teststep As Double)

        minlm.minlmsetgradientcheck(state.innerobj, teststep)
        Return
    End Sub

End Class
Partial Public Class alglib


    '************************************************************************
    '
    '    ************************************************************************

    Public Class minasastate
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property needfg() As Boolean
            Get
                Return _innerobj.needfg
            End Get
            Set(value As Boolean)
                _innerobj.needfg = value
            End Set
        End Property
        Public Property xupdated() As Boolean
            Get
                Return _innerobj.xupdated
            End Get
            Set(value As Boolean)
                _innerobj.xupdated = value
            End Set
        End Property
        Public Property f() As Double
            Get
                Return _innerobj.f
            End Get
            Set(value As Double)
                _innerobj.f = value
            End Set
        End Property
        Public ReadOnly Property g() As Double()
            Get
                Return _innerobj.g
            End Get
        End Property
        Public ReadOnly Property x() As Double()
            Get
                Return _innerobj.x
            End Get
        End Property

        Public Sub New()
            _innerobj = New mincomp.minasastate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minasastate(DirectCast(_innerobj.make_copy(), mincomp.minasastate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As mincomp.minasastate
        Public ReadOnly Property innerobj() As mincomp.minasastate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As mincomp.minasastate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '
    '    ************************************************************************

    Public Class minasareport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property iterationscount() As Integer
            Get
                Return _innerobj.iterationscount
            End Get
            Set(value As Integer)
                _innerobj.iterationscount = value
            End Set
        End Property
        Public Property nfev() As Integer
            Get
                Return _innerobj.nfev
            End Get
            Set(value As Integer)
                _innerobj.nfev = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property
        Public Property activeconstraints() As Integer
            Get
                Return _innerobj.activeconstraints
            End Get
            Set(value As Integer)
                _innerobj.activeconstraints = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New mincomp.minasareport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minasareport(DirectCast(_innerobj.make_copy(), mincomp.minasareport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As mincomp.minasareport
        Public ReadOnly Property innerobj() As mincomp.minasareport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As mincomp.minasareport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '    Obsolete function, use MinLBFGSSetPrecDefault() instead.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetdefaultpreconditioner(state As minlbfgsstate)

        mincomp.minlbfgssetdefaultpreconditioner(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Obsolete function, use MinLBFGSSetCholeskyPreconditioner() instead.
    '
    '      -- ALGLIB --
    '         Copyright 13.10.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minlbfgssetcholeskypreconditioner(state As minlbfgsstate, p As Double(,), isupper As Boolean)

        mincomp.minlbfgssetcholeskypreconditioner(state.innerobj, p, isupper)
        Return
    End Sub

    '************************************************************************
    '    This is obsolete function which was used by previous version of the  BLEIC
    '    optimizer. It does nothing in the current version of BLEIC.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetbarrierwidth(state As minbleicstate, mu As Double)

        mincomp.minbleicsetbarrierwidth(state.innerobj, mu)
        Return
    End Sub

    '************************************************************************
    '    This is obsolete function which was used by previous version of the  BLEIC
    '    optimizer. It does nothing in the current version of BLEIC.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minbleicsetbarrierdecay(state As minbleicstate, mudecay As Double)

        mincomp.minbleicsetbarrierdecay(state.innerobj, mudecay)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 25.03.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasacreate(n As Integer, x As Double(), bndl As Double(), bndu As Double(), ByRef state As minasastate)
        state = New minasastate()
        mincomp.minasacreate(n, x, bndl, bndu, state.innerobj)
        Return
    End Sub
    Public Shared Sub minasacreate(x As Double(), bndl As Double(), bndu As Double(), ByRef state As minasastate)
        Dim n As Integer
        If (ap.len(x) <> ap.len(bndl)) OrElse (ap.len(x) <> ap.len(bndu)) Then
            Throw New alglibexception("Error while calling 'minasacreate': looks like one of arguments has wrong size")
        End If
        state = New minasastate()
        n = ap.len(x)
        mincomp.minasacreate(n, x, bndl, bndu, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasasetcond(state As minasastate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        mincomp.minasasetcond(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasasetxrep(state As minasastate, needxrep As Boolean)

        mincomp.minasasetxrep(state.innerobj, needxrep)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasasetalgorithm(state As minasastate, algotype As Integer)

        mincomp.minasasetalgorithm(state.innerobj, algotype)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 02.04.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasasetstpmax(state As minasastate, stpmax As Double)

        mincomp.minasasetstpmax(state.innerobj, stpmax)
        Return
    End Sub

    '************************************************************************
    '    This function provides reverse communication interface
    '    Reverse communication interface is not documented or recommended to use.
    '    See below for functions which provide better documented API
    '    ************************************************************************

    Public Shared Function minasaiteration(state As minasastate) As Boolean

        Dim result As Boolean = mincomp.minasaiteration(state.innerobj)
        Return result
    End Function
    '************************************************************************
    '    This family of functions is used to launcn iterations of nonlinear optimizer
    '
    '    These functions accept following parameters:
    '        grad    -   callback which calculates function (or merit function)
    '                    value func and gradient grad at given point x
    '        rep     -   optional callback which is called after each iteration
    '                    can be null
    '        obj     -   optional object which is passed to func/grad/hess/jac/rep
    '                    can be null
    '
    '
    '      -- ALGLIB --
    '         Copyright 20.03.2009 by Bochkanov Sergey
    '
    '    ************************************************************************

    Public Shared Sub minasaoptimize(state As minasastate, grad As ndimensional_grad, rep As ndimensional_rep, obj As Object)
        If grad Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minasaoptimize()' (grad is null)")
        End If
        While alglib.minasaiteration(state)
            If state.needfg Then
                grad(state.x, state.innerobj.f, state.innerobj.g, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minasaoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 20.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasaresults(state As minasastate, ByRef x As Double(), ByRef rep As minasareport)
        x = New Double(-1) {}
        rep = New minasareport()
        mincomp.minasaresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 20.03.2009 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasaresultsbuf(state As minasastate, ByRef x As Double(), rep As minasareport)

        mincomp.minasaresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    Obsolete optimization algorithm.
    '    Was replaced by MinBLEIC subpackage.
    '
    '      -- ALGLIB --
    '         Copyright 30.07.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minasarestartfrom(state As minasastate, x As Double(), bndl As Double(), bndu As Double())

        mincomp.minasarestartfrom(state.innerobj, x, bndl, bndu)
        Return
    End Sub

End Class
Partial Public Class alglib


    '************************************************************************
    '    This object stores nonlinear optimizer state.
    '    You should use functions provided by MinNLC subpackage to work  with  this
    '    object
    '    ************************************************************************

    Public Class minnlcstate
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property needfi() As Boolean
            Get
                Return _innerobj.needfi
            End Get
            Set(value As Boolean)
                _innerobj.needfi = value
            End Set
        End Property
        Public Property needfij() As Boolean
            Get
                Return _innerobj.needfij
            End Get
            Set(value As Boolean)
                _innerobj.needfij = value
            End Set
        End Property
        Public Property xupdated() As Boolean
            Get
                Return _innerobj.xupdated
            End Get
            Set(value As Boolean)
                _innerobj.xupdated = value
            End Set
        End Property
        Public Property f() As Double
            Get
                Return _innerobj.f
            End Get
            Set(value As Double)
                _innerobj.f = value
            End Set
        End Property
        Public ReadOnly Property fi() As Double()
            Get
                Return _innerobj.fi
            End Get
        End Property
        Public ReadOnly Property j() As Double(,)
            Get
                Return _innerobj.j
            End Get
        End Property
        Public ReadOnly Property x() As Double()
            Get
                Return _innerobj.x
            End Get
        End Property

        Public Sub New()
            _innerobj = New minnlc.minnlcstate()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minnlcstate(DirectCast(_innerobj.make_copy(), minnlc.minnlcstate))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minnlc.minnlcstate
        Public ReadOnly Property innerobj() As minnlc.minnlcstate
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minnlc.minnlcstate)
            _innerobj = obj
        End Sub
    End Class


    '************************************************************************
    '    This structure stores optimization report:
    '    * IterationsCount           total number of inner iterations
    '    * NFEV                      number of gradient evaluations
    '    * TerminationType           termination type (see below)
    '
    '    TERMINATION CODES
    '
    '    TerminationType field contains completion code, which can be:
    '      -8    internal integrity control detected  infinite  or  NAN  values  in
    '            function/gradient. Abnormal termination signalled.
    '      -7    gradient verification failed.
    '            See MinNLCSetGradientCheck() for more information.
    '       1    relative function improvement is no more than EpsF.
    '       2    relative step is no more than EpsX.
    '       4    gradient norm is no more than EpsG
    '       5    MaxIts steps was taken
    '       7    stopping conditions are too stringent,
    '            further improvement is impossible,
    '            X contains best point found so far.
    '
    '    Other fields of this structure are not documented and should not be used!
    '    ************************************************************************

    Public Class minnlcreport
        Inherits alglibobject
        '
        ' Public declarations
        '
        Public Property iterationscount() As Integer
            Get
                Return _innerobj.iterationscount
            End Get
            Set(value As Integer)
                _innerobj.iterationscount = value
            End Set
        End Property
        Public Property nfev() As Integer
            Get
                Return _innerobj.nfev
            End Get
            Set(value As Integer)
                _innerobj.nfev = value
            End Set
        End Property
        Public Property varidx() As Integer
            Get
                Return _innerobj.varidx
            End Get
            Set(value As Integer)
                _innerobj.varidx = value
            End Set
        End Property
        Public Property funcidx() As Integer
            Get
                Return _innerobj.funcidx
            End Get
            Set(value As Integer)
                _innerobj.funcidx = value
            End Set
        End Property
        Public Property terminationtype() As Integer
            Get
                Return _innerobj.terminationtype
            End Get
            Set(value As Integer)
                _innerobj.terminationtype = value
            End Set
        End Property
        Public Property dbgphase0its() As Integer
            Get
                Return _innerobj.dbgphase0its
            End Get
            Set(value As Integer)
                _innerobj.dbgphase0its = value
            End Set
        End Property

        Public Sub New()
            _innerobj = New minnlc.minnlcreport()
        End Sub

        Public Overrides Function make_copy() As alglib.alglibobject
            Return New minnlcreport(DirectCast(_innerobj.make_copy(), minnlc.minnlcreport))
        End Function

        '
        ' Although some of declarations below are public, you should not use them
        ' They are intended for internal use only
        '
        Private _innerobj As minnlc.minnlcreport
        Public ReadOnly Property innerobj() As minnlc.minnlcreport
            Get
                Return _innerobj
            End Get
        End Property
        Public Sub New(obj As minnlc.minnlcreport)
            _innerobj = obj
        End Sub
    End Class

    '************************************************************************
    '                      NONLINEARLY  CONSTRAINED  OPTIMIZATION
    '                WITH PRECONDITIONED AUGMENTED LAGRANGIAN ALGORITHM
    '
    '    DESCRIPTION:
    '    The  subroutine  minimizes  function   F(x)  of N arguments subject to any
    '    combination of:
    '    * bound constraints
    '    * linear inequality constraints
    '    * linear equality constraints
    '    * nonlinear equality constraints Gi(x)=0
    '    * nonlinear inequality constraints Hi(x)<=0
    '
    '    REQUIREMENTS:
    '    * user must provide function value and gradient for F(), H(), G()
    '    * starting point X0 must be feasible or not too far away from the feasible
    '      set
    '    * F(), G(), H() are twice continuously differentiable on the feasible  set
    '      and its neighborhood
    '    * nonlinear constraints G() and H() must have non-zero gradient at  G(x)=0
    '      and at H(x)=0. Say, constraint like x^2>=1 is supported, but x^2>=0   is
    '      NOT supported.
    '
    '    USAGE:
    '
    '    Constrained optimization if far more complex than the  unconstrained  one.
    '    Nonlinearly constrained optimization is one of the most esoteric numerical
    '    procedures.
    '
    '    Here we give very brief outline  of  the  MinNLC  optimizer.  We  strongly
    '    recommend you to study examples in the ALGLIB Reference Manual and to read
    '    ALGLIB User Guide on optimization, which is available at
    '    http://www.alglib.net/optimization/
    '
    '    1. User initializes algorithm state with MinNLCCreate() call  and  chooses
    '       what NLC solver to use. There is some solver which is used by  default,
    '       with default settings, but you should NOT rely on  default  choice.  It
    '       may change in future releases of ALGLIB without notice, and no one  can
    '       guarantee that new solver will be  able  to  solve  your  problem  with
    '       default settings.
    '
    '       From the other side, if you choose solver explicitly, you can be pretty
    '       sure that it will work with new ALGLIB releases.
    '
    '       In the current release following solvers can be used:
    '       * AUL solver (activated with MinNLCSetAlgoAUL() function)
    '
    '    2. User adds boundary and/or linear and/or nonlinear constraints by  means
    '       of calling one of the following functions:
    '       a) MinNLCSetBC() for boundary constraints
    '       b) MinNLCSetLC() for linear constraints
    '       c) MinNLCSetNLC() for nonlinear constraints
    '       You may combine (a), (b) and (c) in one optimization problem.
    '
    '    3. User sets scale of the variables with MinNLCSetScale() function. It  is
    '       VERY important to set  scale  of  the  variables,  because  nonlinearly
    '       constrained problems are hard to solve when variables are badly scaled.
    '
    '    4. User sets  stopping  conditions  with  MinNLCSetCond(). If  NLC  solver
    '       uses  inner/outer  iteration  layout,  this  function   sets   stopping
    '       conditions for INNER iterations.
    '
    '    5. User chooses one of the  preconditioning  methods.  Preconditioning  is
    '       very  important  for  efficient  handling  of boundary/linear/nonlinear
    '       constraints. Without preconditioning algorithm would require  thousands
    '       of iterations even for simple problems.  Two  preconditioners  can   be
    '       used:
    '       * approximate LBFGS-based  preconditioner  which  should  be  used  for
    '         problems with almost orthogonal  constraints  (activated  by  calling
    '         MinNLCSetPrecInexact)
    '       * exact low-rank preconditiner (activated by MinNLCSetPrecExactLowRank)
    '         which should be used for problems with moderate number of constraints
    '         which do not have to be orthogonal.
    '
    '    6. Finally, user calls MinNLCOptimize()  function  which  takes  algorithm
    '       state and pointer (delegate, etc.) to callback function which calculates
    '       F/G/H.
    '
    '    7. User calls MinNLCResults() to get solution
    '
    '    8. Optionally user may call MinNLCRestartFrom() to solve  another  problem
    '       with same N but another starting point. MinNLCRestartFrom()  allows  to
    '       reuse already initialized structure.
    '
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension, N>0:
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size ofX
    '        X       -   starting point, array[N]:
    '                    * it is better to set X to a feasible point
    '                    * but X can be infeasible, in which case algorithm will try
    '                      to find feasible point first, using X as initial
    '                      approximation.
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlccreate(n As Integer, x As Double(), ByRef state As minnlcstate)
        state = New minnlcstate()
        minnlc.minnlccreate(n, x, state.innerobj)
        Return
    End Sub
    Public Shared Sub minnlccreate(x As Double(), ByRef state As minnlcstate)
        Dim n As Integer

        state = New minnlcstate()
        n = ap.len(x)
        minnlc.minnlccreate(n, x, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This subroutine is a finite  difference variant of MinNLCCreate(). It uses
    '    finite differences in order to differentiate target function.
    '
    '    Description below contains information which is specific to this  function
    '    only. We recommend to read comments on MinNLCCreate() in order to get more
    '    information about creation of NLC optimizer.
    '
    '    INPUT PARAMETERS:
    '        N       -   problem dimension, N>0:
    '                    * if given, only leading N elements of X are used
    '                    * if not given, automatically determined from size ofX
    '        X       -   starting point, array[N]:
    '                    * it is better to set X to a feasible point
    '                    * but X can be infeasible, in which case algorithm will try
    '                      to find feasible point first, using X as initial
    '                      approximation.
    '        DiffStep-   differentiation step, >0
    '
    '    OUTPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '
    '    NOTES:
    '    1. algorithm uses 4-point central formula for differentiation.
    '    2. differentiation step along I-th axis is equal to DiffStep*S[I] where
    '       S[] is scaling vector which can be set by MinNLCSetScale() call.
    '    3. we recommend you to use moderate values of  differentiation  step.  Too
    '       large step will result in too large TRUNCATION  errors, while too small
    '       step will result in too large NUMERICAL  errors.  1.0E-4  can  be  good
    '       value to start from.
    '    4. Numerical  differentiation  is   very   inefficient  -   one   gradient
    '       calculation needs 4*N function evaluations. This function will work for
    '       any N - either small (1...10), moderate (10...100) or  large  (100...).
    '       However, performance penalty will be too severe for any N's except  for
    '       small ones.
    '       We should also say that code which relies on numerical  differentiation
    '       is  less   robust   and  precise.  Imprecise  gradient  may  slow  down
    '       convergence, especially on highly nonlinear problems.
    '       Thus  we  recommend to use this function for fast prototyping on small-
    '       dimensional problems only, and to implement analytical gradient as soon
    '       as possible.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlccreatef(n As Integer, x As Double(), diffstep As Double, ByRef state As minnlcstate)
        state = New minnlcstate()
        minnlc.minnlccreatef(n, x, diffstep, state.innerobj)
        Return
    End Sub
    Public Shared Sub minnlccreatef(x As Double(), diffstep As Double, ByRef state As minnlcstate)
        Dim n As Integer

        state = New minnlcstate()
        n = ap.len(x)
        minnlc.minnlccreatef(n, x, diffstep, state.innerobj)

        Return
    End Sub

    '************************************************************************
    '    This function sets boundary constraints for NLC optimizer.
    '
    '    Boundary constraints are inactive by  default  (after  initial  creation).
    '    They are preserved after algorithm restart with  MinNLCRestartFrom().
    '
    '    You may combine boundary constraints with  general  linear ones - and with
    '    nonlinear ones! Boundary constraints are  handled  more  efficiently  than
    '    other types.  Thus,  if  your  problem  has  mixed  constraints,  you  may
    '    explicitly specify some of them as boundary and save some time/space.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        BndL    -   lower bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very small number or -INF.
    '        BndU    -   upper bounds, array[N].
    '                    If some (all) variables are unbounded, you may specify
    '                    very large number or +INF.
    '
    '    NOTE 1:  it is possible to specify  BndL[i]=BndU[i].  In  this  case  I-th
    '    variable will be "frozen" at X[i]=BndL[i]=BndU[i].
    '
    '    NOTE 2:  when you solve your problem  with  augmented  Lagrangian  solver,
    '             boundary constraints are  satisfied  only  approximately!  It  is
    '             possible   that  algorithm  will  evaluate  function  outside  of
    '             feasible area!
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetbc(state As minnlcstate, bndl As Double(), bndu As Double())

        minnlc.minnlcsetbc(state.innerobj, bndl, bndu)
        Return
    End Sub

    '************************************************************************
    '    This function sets linear constraints for MinNLC optimizer.
    '
    '    Linear constraints are inactive by default (after initial creation).  They
    '    are preserved after algorithm restart with MinNLCRestartFrom().
    '
    '    You may combine linear constraints with boundary ones - and with nonlinear
    '    ones! If your problem has mixed constraints, you  may  explicitly  specify
    '    some of them as linear. It  may  help  optimizer   to   handle  them  more
    '    efficiently.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinNLCCreate call.
    '        C       -   linear constraints, array[K,N+1].
    '                    Each row of C represents one constraint, either equality
    '                    or inequality (see below):
    '                    * first N elements correspond to coefficients,
    '                    * last element corresponds to the right part.
    '                    All elements of C (including right part) must be finite.
    '        CT      -   type of constraints, array[K]:
    '                    * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
    '                    * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
    '                    * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
    '        K       -   number of equality/inequality constraints, K>=0:
    '                    * if given, only leading K elements of C/CT are used
    '                    * if not given, automatically determined from sizes of C/CT
    '
    '    NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
    '            linear constraints are  satisfied  only   approximately!   It   is
    '            possible   that  algorithm  will  evaluate  function  outside   of
    '            feasible area!
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetlc(state As minnlcstate, c As Double(,), ct As Integer(), k As Integer)

        minnlc.minnlcsetlc(state.innerobj, c, ct, k)
        Return
    End Sub
    Public Shared Sub minnlcsetlc(state As minnlcstate, c As Double(,), ct As Integer())
        Dim k As Integer
        If (ap.rows(c) <> ap.len(ct)) Then
            Throw New alglibexception("Error while calling 'minnlcsetlc': looks like one of arguments has wrong size")
        End If

        k = ap.rows(c)
        minnlc.minnlcsetlc(state.innerobj, c, ct, k)

        Return
    End Sub

    '************************************************************************
    '    This function sets nonlinear constraints for MinNLC optimizer.
    '
    '    In fact, this function sets NUMBER of nonlinear  constraints.  Constraints
    '    itself (constraint functions) are passed to MinNLCOptimize() method.  This
    '    method requires user-defined vector function F[]  and  its  Jacobian  J[],
    '    where:
    '    * first component of F[] and first row  of  Jacobian  J[]  corresponds  to
    '      function being minimized
    '    * next NLEC components of F[] (and rows  of  J)  correspond  to  nonlinear
    '      equality constraints G_i(x)=0
    '    * next NLIC components of F[] (and rows  of  J)  correspond  to  nonlinear
    '      inequality constraints H_i(x)<=0
    '
    '    NOTE: you may combine nonlinear constraints with linear/boundary ones.  If
    '          your problem has mixed constraints, you  may explicitly specify some
    '          of them as linear ones. It may help optimizer to  handle  them  more
    '          efficiently.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinNLCCreate call.
    '        NLEC    -   number of Non-Linear Equality Constraints (NLEC), >=0
    '        NLIC    -   number of Non-Linear Inquality Constraints (NLIC), >=0
    '
    '    NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
    '            nonlinear constraints are satisfied only  approximately!   It   is
    '            possible   that  algorithm  will  evaluate  function  outside   of
    '            feasible area!
    '
    '    NOTE 2: algorithm scales variables  according  to   scale   specified   by
    '            MinNLCSetScale()  function,  so  it can handle problems with badly
    '            scaled variables (as long as we KNOW their scales).
    '
    '            However,  there  is  no  way  to  automatically  scale   nonlinear
    '            constraints Gi(x) and Hi(x). Inappropriate scaling  of  Gi/Hi  may
    '            ruin convergence. Solving problem with  constraint  "1000*G0(x)=0"
    '            is NOT same as solving it with constraint "0.001*G0(x)=0".
    '
    '            It  means  that  YOU  are  the  one who is responsible for correct
    '            scaling of nonlinear constraints Gi(x) and Hi(x). We recommend you
    '            to scale nonlinear constraints in such way that I-th component  of
    '            dG/dX (or dH/dx) has approximately unit  magnitude  (for  problems
    '            with unit scale)  or  has  magnitude approximately equal to 1/S[i]
    '            (where S is a scale set by MinNLCSetScale() function).
    '
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetnlc(state As minnlcstate, nlec As Integer, nlic As Integer)

        minnlc.minnlcsetnlc(state.innerobj, nlec, nlic)
        Return
    End Sub

    '************************************************************************
    '    This function sets stopping conditions for inner iterations of  optimizer.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        EpsG    -   >=0
    '                    The  subroutine  finishes  its  work   if   the  condition
    '                    |v|<EpsG is satisfied, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled gradient vector, v[i]=g[i]*s[i]
    '                    * g - gradient
    '                    * s - scaling coefficients set by MinNLCSetScale()
    '        EpsF    -   >=0
    '                    The  subroutine  finishes  its work if on k+1-th iteration
    '                    the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
    '                    is satisfied.
    '        EpsX    -   >=0
    '                    The subroutine finishes its work if  on  k+1-th  iteration
    '                    the condition |v|<=EpsX is fulfilled, where:
    '                    * |.| means Euclidian norm
    '                    * v - scaled step vector, v[i]=dx[i]/s[i]
    '                    * dx - step vector, dx=X(k+1)-X(k)
    '                    * s - scaling coefficients set by MinNLCSetScale()
    '        MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
    '                    iterations is unlimited.
    '
    '    Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
    '    to automatic stopping criterion selection.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetcond(state As minnlcstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)

        minnlc.minnlcsetcond(state.innerobj, epsg, epsf, epsx, maxits)
        Return
    End Sub

    '************************************************************************
    '    This function sets scaling coefficients for NLC optimizer.
    '
    '    ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
    '    size and gradient are scaled before comparison with tolerances).  Scale of
    '    the I-th variable is a translation invariant measure of:
    '    a) "how large" the variable is
    '    b) how large the step should be to make significant changes in the function
    '
    '    Scaling is also used by finite difference variant of the optimizer  - step
    '    along I-th axis is equal to DiffStep*S[I].
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        S       -   array[N], non-zero scaling coefficients
    '                    S[i] may be negative, sign doesn't matter.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetscale(state As minnlcstate, s As Double())

        minnlc.minnlcsetscale(state.innerobj, s)
        Return
    End Sub

    '************************************************************************
    '    This function sets preconditioner to "inexact LBFGS-based" mode.
    '
    '    Preconditioning is very important for convergence of  Augmented Lagrangian
    '    algorithm because presence of penalty term makes problem  ill-conditioned.
    '    Difference between  performance  of  preconditioned  and  unpreconditioned
    '    methods can be as large as 100x!
    '
    '    MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
    '    benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
    '    It also provides special unpreconditioned mode of operation which  can  be
    '    used for test purposes. Comments below discuss LBFGS-based preconditioner.
    '
    '    Inexact  LBFGS-based  preconditioner  uses L-BFGS  formula  combined  with
    '    orthogonality assumption to perform very fast updates. For a N-dimensional
    '    problem with K general linear or nonlinear constraints (boundary ones  are
    '    not counted) it has O(N*K) cost per iteration.  This   preconditioner  has
    '    best  quality  (less  iterations)  when   general   linear  and  nonlinear
    '    constraints are orthogonal to each other (orthogonality  with  respect  to
    '    boundary constraints is not required). Number of iterations increases when
    '    constraints  are  non-orthogonal, because algorithm assumes orthogonality,
    '    but still it is better than no preconditioner at all.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 26.09.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetprecinexact(state As minnlcstate)

        minnlc.minnlcsetprecinexact(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This function sets preconditioner to "exact low rank" mode.
    '
    '    Preconditioning is very important for convergence of  Augmented Lagrangian
    '    algorithm because presence of penalty term makes problem  ill-conditioned.
    '    Difference between  performance  of  preconditioned  and  unpreconditioned
    '    methods can be as large as 100x!
    '
    '    MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
    '    benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
    '    It also provides special unpreconditioned mode of operation which  can  be
    '    used for test purposes. Comments below discuss low rank preconditioner.
    '
    '    Exact low-rank preconditioner  uses  Woodbury  matrix  identity  to  build
    '    quadratic model of the penalized function. It has no  special  assumptions
    '    about orthogonality, so it is quite general. However, for a  N-dimensional
    '    problem with K general linear or nonlinear constraints (boundary ones  are
    '    not counted) it has O(N*K^2) cost per iteration (for  comparison:  inexact
    '    LBFGS-based preconditioner has O(N*K) cost).
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '        UpdateFreq- update frequency. Preconditioner is  rebuilt  after  every
    '                    UpdateFreq iterations. Recommended value: 10 or higher.
    '                    Zero value means that good default value will be used.
    '
    '      -- ALGLIB --
    '         Copyright 26.09.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetprecexactlowrank(state As minnlcstate, updatefreq As Integer)

        minnlc.minnlcsetprecexactlowrank(state.innerobj, updatefreq)
        Return
    End Sub

    '************************************************************************
    '    This function sets preconditioner to "turned off" mode.
    '
    '    Preconditioning is very important for convergence of  Augmented Lagrangian
    '    algorithm because presence of penalty term makes problem  ill-conditioned.
    '    Difference between  performance  of  preconditioned  and  unpreconditioned
    '    methods can be as large as 100x!
    '
    '    MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
    '    benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
    '    It also provides special unpreconditioned mode of operation which  can  be
    '    used for test purposes.
    '
    '    This function activates this test mode. Do not use it in  production  code
    '    to solve real-life problems.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure stores algorithm state
    '
    '      -- ALGLIB --
    '         Copyright 26.09.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetprecnone(state As minnlcstate)

        minnlc.minnlcsetprecnone(state.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This  function  tells MinNLC unit to use  Augmented  Lagrangian  algorithm
    '    for nonlinearly constrained  optimization.  This  algorithm  is  a  slight
    '    modification of one described in "A Modified Barrier-Augmented  Lagrangian
    '    Method for  Constrained  Minimization  (1999)"  by  D.GOLDFARB,  R.POLYAK,
    '    K. SCHEINBERG, I.YUZEFOVICH.
    '
    '    Augmented Lagrangian algorithm works by converting problem  of  minimizing
    '    F(x) subject to equality/inequality constraints   to unconstrained problem
    '    of the form
    '
    '        min[ f(x) +
    '            + Rho*PENALTY_EQ(x)   + SHIFT_EQ(x,Nu1) +
    '            + Rho*PENALTY_INEQ(x) + SHIFT_INEQ(x,Nu2) ]
    '
    '    where:
    '    * Rho is a fixed penalization coefficient
    '    * PENALTY_EQ(x) is a penalty term, which is used to APPROXIMATELY  enforce
    '      equality constraints
    '    * SHIFT_EQ(x) is a special "shift"  term  which  is  used  to  "fine-tune"
    '      equality constraints, greatly increasing precision
    '    * PENALTY_INEQ(x) is a penalty term which is used to approximately enforce
    '      inequality constraints
    '    * SHIFT_INEQ(x) is a special "shift"  term  which  is  used to "fine-tune"
    '      inequality constraints, greatly increasing precision
    '    * Nu1/Nu2 are vectors of Lagrange coefficients which are fine-tuned during
    '      outer iterations of algorithm
    '
    '    This  version  of  AUL  algorithm  uses   preconditioner,  which   greatly
    '    accelerates convergence. Because this  algorithm  is  similar  to  penalty
    '    methods,  it  may  perform  steps  into  infeasible  area.  All  kinds  of
    '    constraints (boundary, linear and nonlinear ones) may   be   violated   in
    '    intermediate points - and in the solution.  However,  properly  configured
    '    AUL method is significantly better at handling  constraints  than  barrier
    '    and/or penalty methods.
    '
    '    The very basic outline of algorithm is given below:
    '    1) first outer iteration is performed with "default"  values  of  Lagrange
    '       multipliers Nu1/Nu2. Solution quality is low (candidate  point  can  be
    '       too  far  away  from  true  solution; large violation of constraints is
    '       possible) and is comparable with that of penalty methods.
    '    2) subsequent outer iterations  refine  Lagrange  multipliers  and improve
    '       quality of the solution.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        Rho     -   penalty coefficient, Rho>0:
    '                    * large enough  that  algorithm  converges  with   desired
    '                      precision. Minimum value is 10*max(S'*diag(H)*S),  where
    '                      S is a scale matrix (set by MinNLCSetScale) and H  is  a
    '                      Hessian of the function being minimized. If you can  not
    '                      easily estimate Hessian norm,  see  our  recommendations
    '                      below.
    '                    * not TOO large to prevent ill-conditioning
    '                    * for unit-scale problems (variables and Hessian have unit
    '                      magnitude), Rho=100 or Rho=1000 can be used.
    '                    * it is important to note that Rho is internally multiplied
    '                      by scaling matrix, i.e. optimum value of Rho depends  on
    '                      scale of variables specified  by  MinNLCSetScale().
    '        ItsCnt  -   number of outer iterations:
    '                    * ItsCnt=0 means that small number of outer iterations  is
    '                      automatically chosen (10 iterations in current version).
    '                    * ItsCnt=1 means that AUL algorithm performs just as usual
    '                      barrier method.
    '                    * ItsCnt>1 means that  AUL  algorithm  performs  specified
    '                      number of outer iterations
    '
    '    HOW TO CHOOSE PARAMETERS
    '
    '    Nonlinear optimization is a tricky area and Augmented Lagrangian algorithm
    '    is sometimes hard to tune. Good values of  Rho  and  ItsCnt  are  problem-
    '    specific.  In  order  to  help  you   we   prepared   following   set   of
    '    recommendations:
    '
    '    * for  unit-scale  problems  (variables  and Hessian have unit magnitude),
    '      Rho=100 or Rho=1000 can be used.
    '
    '    * start from  some  small  value of Rho and solve problem  with  just  one
    '      outer iteration (ItcCnt=1). In this case algorithm behaves like  penalty
    '      method. Increase Rho in 2x or 10x steps until you  see  that  one  outer
    '      iteration returns point which is "rough approximation to solution".
    '
    '      It is very important to have Rho so  large  that  penalty  term  becomes
    '      constraining i.e. modified function becomes highly convex in constrained
    '      directions.
    '
    '      From the other side, too large Rho may prevent you  from  converging  to
    '      the solution. You can diagnose it by studying number of inner iterations
    '      performed by algorithm: too few (5-10 on  1000-dimensional  problem)  or
    '      too many (orders of magnitude more than  dimensionality)  usually  means
    '      that Rho is too large.
    '
    '    * with just one outer iteration you  usually  have  low-quality  solution.
    '      Some constraints can be violated with very  large  margin,  while  other
    '      ones (which are NOT violated in the true solution) can push final  point
    '      too far in the inner area of the feasible set.
    '
    '      For example, if you have constraint x0>=0 and true solution  x0=1,  then
    '      merely a presence of "x0>=0" will introduce a bias towards larger values
    '      of x0. Say, algorithm may stop at x0=1.5 instead of 1.0.
    '
    '    * after you found good Rho, you may increase number of  outer  iterations.
    '      ItsCnt=10 is a good value. Subsequent outer iteration will refine values
    '      of  Lagrange  multipliers.  Constraints  which  were  violated  will  be
    '      enforced, inactive constraints will be dropped (corresponding multipliers
    '      will be decreased). Ideally, you  should  see  10-1000x  improvement  in
    '      constraint handling (constraint violation is reduced).
    '
    '    * if  you  see  that  algorithm  converges  to  vicinity  of solution, but
    '      additional outer iterations do not refine solution,  it  may  mean  that
    '      algorithm is unstable - it wanders around true  solution,  but  can  not
    '      approach it. Sometimes algorithm may be stabilized by increasing Rho one
    '      more time, making it 5x or 10x larger.
    '
    '    SCALING OF CONSTRAINTS [IMPORTANT]
    '
    '    AUL optimizer scales   variables   according   to   scale   specified   by
    '    MinNLCSetScale() function, so it can handle  problems  with  badly  scaled
    '    variables (as long as we KNOW their scales).   However,  because  function
    '    being optimized is a mix  of  original  function and  constraint-dependent
    '    penalty  functions, it  is   important  to   rescale  both  variables  AND
    '    constraints.
    '
    '    Say,  if  you  minimize f(x)=x^2 subject to 1000000*x>=0,  then  you  have
    '    constraint whose scale is different from that of target  function (another
    '    example is 0.000001*x>=0). It is also possible to have constraints   whose
    '    scales  are   misaligned:   1000000*x0>=0, 0.000001*x1<=0.   Inappropriate
    '    scaling may ruin convergence because minimizing x^2 subject to x>=0 is NOT
    '    same as minimizing it subject to 1000000*x>=0.
    '
    '    Because we  know  coefficients  of  boundary/linear  constraints,  we  can
    '    automatically rescale and normalize them. However,  there  is  no  way  to
    '    automatically rescale nonlinear constraints Gi(x) and  Hi(x)  -  they  are
    '    black boxes.
    '
    '    It means that YOU are the one who is  responsible  for  correct scaling of
    '    nonlinear constraints  Gi(x)  and  Hi(x).  We  recommend  you  to  rescale
    '    nonlinear constraints in such way that I-th component of dG/dX (or  dH/dx)
    '    has magnitude approximately equal to 1/S[i] (where S  is  a  scale  set by
    '    MinNLCSetScale() function).
    '
    '    WHAT IF IT DOES NOT CONVERGE?
    '
    '    It is possible that AUL algorithm fails to converge to precise  values  of
    '    Lagrange multipliers. It stops somewhere around true solution, but candidate
    '    point is still too far from solution, and some constraints  are  violated.
    '    Such kind of failure is specific for Lagrangian algorithms -  technically,
    '    they stop at some point, but this point is not constrained solution.
    '
    '    There are exist several reasons why algorithm may fail to converge:
    '    a) too loose stopping criteria for inner iteration
    '    b) degenerate, redundant constraints
    '    c) target function has unconstrained extremum exactly at the  boundary  of
    '       some constraint
    '    d) numerical noise in the target function
    '
    '    In all these cases algorithm is unstable - each outer iteration results in
    '    large and almost random step which improves handling of some  constraints,
    '    but violates other ones (ideally  outer iterations should form a  sequence
    '    of progressively decreasing steps towards solution).
    '
    '    First reason possible is  that  too  loose  stopping  criteria  for  inner
    '    iteration were specified. Augmented Lagrangian algorithm solves a sequence
    '    of intermediate problems, and requries each of them to be solved with high
    '    precision. Insufficient precision results in incorrect update of  Lagrange
    '    multipliers.
    '
    '    Another reason is that you may have specified degenerate constraints: say,
    '    some constraint was repeated twice. In most cases AUL algorithm gracefully
    '    handles such situations, but sometimes it may spend too much time figuring
    '    out subtle degeneracies in constraint matrix.
    '
    '    Third reason is tricky and hard to diagnose. Consider situation  when  you
    '    minimize  f=x^2  subject to constraint x>=0.  Unconstrained   extremum  is
    '    located  exactly  at  the  boundary  of  constrained  area.  In  this case
    '    algorithm will tend to oscillate between negative  and  positive  x.  Each
    '    time it stops at x<0 it "reinforces" constraint x>=0, and each time it  is
    '    bounced to x>0 it "relaxes" constraint (and is  attracted  to  x<0).
    '
    '    Such situation  sometimes  happens  in  problems  with  hidden  symetries.
    '    Algorithm  is  got  caught  in  a  loop with  Lagrange  multipliers  being
    '    continuously increased/decreased. Luckily, such loop forms after at  least
    '    three iterations, so this problem can be solved by  DECREASING  number  of
    '    outer iterations down to 1-2 and increasing  penalty  coefficient  Rho  as
    '    much as possible.
    '
    '    Final reason is numerical noise. AUL algorithm is robust against  moderate
    '    noise (more robust than, say, active set methods),  but  large  noise  may
    '    destabilize algorithm.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetalgoaul(state As minnlcstate, rho As Double, itscnt As Integer)

        minnlc.minnlcsetalgoaul(state.innerobj, rho, itscnt)
        Return
    End Sub

    '************************************************************************
    '    This function turns on/off reporting.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure which stores algorithm state
    '        NeedXRep-   whether iteration reports are needed or not
    '
    '    If NeedXRep is True, algorithm will call rep() callback function if  it is
    '    provided to MinNLCOptimize().
    '
    '    NOTE: algorithm passes two parameters to rep() callback  -  current  point
    '          and penalized function value at current point. Important -  function
    '          value which is returned is NOT function being minimized. It  is  sum
    '          of the value of the function being minimized - and penalty term.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetxrep(state As minnlcstate, needxrep As Boolean)

        minnlc.minnlcsetxrep(state.innerobj, needxrep)
        Return
    End Sub

    '************************************************************************
    '    This function provides reverse communication interface
    '    Reverse communication interface is not documented or recommended to use.
    '    See below for functions which provide better documented API
    '    ************************************************************************

    Public Shared Function minnlciteration(state As minnlcstate) As Boolean

        Dim result As Boolean = minnlc.minnlciteration(state.innerobj)
        Return result
    End Function
    '************************************************************************
    '    This family of functions is used to launcn iterations of nonlinear optimizer
    '
    '    These functions accept following parameters:
    '        fvec    -   callback which calculates function vector fi[]
    '                    at given point x
    '        jac     -   callback which calculates function vector fi[]
    '                    and Jacobian jac at given point x
    '        rep     -   optional callback which is called after each iteration
    '                    can be null
    '        obj     -   optional object which is passed to func/grad/hess/jac/rep
    '                    can be null
    '
    '
    '    NOTES:
    '
    '    1. This function has two different implementations: one which  uses  exact
    '       (analytical) user-supplied Jacobian, and one which uses  only  function
    '       vector and numerically  differentiates  function  in  order  to  obtain
    '       gradient.
    '
    '       Depending  on  the  specific  function  used to create optimizer object
    '       you should choose appropriate variant of MinNLCOptimize() -  one  which
    '       accepts function AND Jacobian or one which accepts ONLY function.
    '
    '       Be careful to choose variant of MinNLCOptimize()  which  corresponds to
    '       your optimization scheme! Table below lists different  combinations  of
    '       callback (function/gradient) passed to MinNLCOptimize()   and  specific
    '       function used to create optimizer.
    '
    '
    '                         |         USER PASSED TO MinNLCOptimize()
    '       CREATED WITH      |  function only   |  function and gradient
    '       ------------------------------------------------------------
    '       MinNLCCreateF()   |     works               FAILS
    '       MinNLCCreate()    |     FAILS               works
    '
    '       Here "FAILS" denotes inappropriate combinations  of  optimizer creation
    '       function  and  MinNLCOptimize()  version.   Attemps   to    use    such
    '       combination will lead to exception. Either  you  did  not pass gradient
    '       when it WAS needed or you passed gradient when it was NOT needed.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '
    '    ************************************************************************

    Public Shared Sub minnlcoptimize(state As minnlcstate, fvec As ndimensional_fvec, rep As ndimensional_rep, obj As Object)
        If fvec Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minnlcoptimize()' (fvec is null)")
        End If
        While alglib.minnlciteration(state)
            If state.needfi Then
                fvec(state.x, state.innerobj.fi, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minnlcoptimize' (some derivatives were not provided?)")
        End While
    End Sub


    Public Shared Sub minnlcoptimize(state As minnlcstate, jac As ndimensional_jac, rep As ndimensional_rep, obj As Object)
        If jac Is Nothing Then
            Throw New alglibexception("ALGLIB: error in 'minnlcoptimize()' (jac is null)")
        End If
        While alglib.minnlciteration(state)
            If state.needfij Then
                jac(state.x, state.innerobj.fi, state.innerobj.j, obj)
                Continue While
            End If
            If state.innerobj.xupdated Then
                Call rep(state.innerobj.x, state.innerobj.f, obj)
                Continue While
            End If
            Throw New alglibexception("ALGLIB: error in 'minnlcoptimize' (some derivatives were not provided?)")
        End While
    End Sub



    '************************************************************************
    '    MinNLC results
    '
    '    INPUT PARAMETERS:
    '        State   -   algorithm state
    '
    '    OUTPUT PARAMETERS:
    '        X       -   array[0..N-1], solution
    '        Rep     -   optimization report. You should check Rep.TerminationType
    '                    in  order  to  distinguish  successful  termination  from
    '                    unsuccessful one:
    '                    * -8    internal integrity control  detected  infinite or
    '                            NAN   values   in   function/gradient.   Abnormal
    '                            termination signalled.
    '                    * -7   gradient verification failed.
    '                           See MinNLCSetGradientCheck() for more information.
    '                    *  1   relative function improvement is no more than EpsF.
    '                    *  2   scaled step is no more than EpsX.
    '                    *  4   scaled gradient norm is no more than EpsG.
    '                    *  5   MaxIts steps was taken
    '                    More information about fields of this  structure  can  be
    '                    found in the comments on MinNLCReport datatype.
    '
    '      -- ALGLIB --
    '         Copyright 06.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcresults(state As minnlcstate, ByRef x As Double(), ByRef rep As minnlcreport)
        x = New Double(-1) {}
        rep = New minnlcreport()
        minnlc.minnlcresults(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    NLC results
    '
    '    Buffered implementation of MinNLCResults() which uses pre-allocated buffer
    '    to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
    '    intended to be used in the inner cycles of performance critical algorithms
    '    where array reallocation penalty is too large to be ignored.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcresultsbuf(state As minnlcstate, ByRef x As Double(), rep As minnlcreport)

        minnlc.minnlcresultsbuf(state.innerobj, x, rep.innerobj)
        Return
    End Sub

    '************************************************************************
    '    This subroutine restarts algorithm from new point.
    '    All optimization parameters (including constraints) are left unchanged.
    '
    '    This  function  allows  to  solve multiple  optimization  problems  (which
    '    must have  same number of dimensions) without object reallocation penalty.
    '
    '    INPUT PARAMETERS:
    '        State   -   structure previously allocated with MinNLCCreate call.
    '        X       -   new starting point.
    '
    '      -- ALGLIB --
    '         Copyright 28.11.2010 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcrestartfrom(state As minnlcstate, x As Double())

        minnlc.minnlcrestartfrom(state.innerobj, x)
        Return
    End Sub

    '************************************************************************
    '    This  subroutine  turns  on  verification  of  the  user-supplied analytic
    '    gradient:
    '    * user calls this subroutine before optimization begins
    '    * MinNLCOptimize() is called
    '    * prior to  actual  optimization, for each component  of  parameters being
    '      optimized X[i] algorithm performs following steps:
    '      * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    '        where X[i] is i-th component of the initial point and S[i] is a  scale
    '        of i-th parameter
    '      * F(X) is evaluated at these trial points
    '      * we perform one more evaluation in the middle point of the interval
    '      * we  build  cubic  model using function values and derivatives at trial
    '        points and we compare its prediction with actual value in  the  middle
    '        point
    '      * in case difference between prediction and actual value is higher  than
    '        some predetermined threshold, algorithm stops with completion code -7;
    '        Rep.VarIdx is set to index of the parameter with incorrect derivative,
    '        and Rep.FuncIdx is set to index of the function.
    '    * after verification is over, algorithm proceeds to the actual optimization.
    '
    '    NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
    '            is very costly and you should use  it  only  for  low  dimensional
    '            problems,  when  you  want  to  be  sure  that  you've   correctly
    '            calculated  analytic  derivatives.  You  should  not use it in the
    '            production code (unless you want to check derivatives provided  by
    '            some third party).
    '
    '    NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
    '            (so large that function behaviour is significantly non-cubic) will
    '            lead to false alarms. You may use  different  step  for  different
    '            parameters by means of setting scale with MinNLCSetScale().
    '
    '    NOTE 3: this function may lead to false positives. In case it reports that
    '            I-th  derivative was calculated incorrectly, you may decrease test
    '            step  and  try  one  more  time  - maybe your function changes too
    '            sharply  and  your  step  is  too  large for such rapidly chanding
    '            function.
    '
    '    INPUT PARAMETERS:
    '        State       -   structure used to store algorithm state
    '        TestStep    -   verification step:
    '                        * TestStep=0 turns verification off
    '                        * TestStep>0 activates verification
    '
    '      -- ALGLIB --
    '         Copyright 15.06.2014 by Bochkanov Sergey
    '    ************************************************************************

    Public Shared Sub minnlcsetgradientcheck(state As minnlcstate, teststep As Double)

        minnlc.minnlcsetgradientcheck(state.innerobj, teststep)
        Return
    End Sub

End Class
Public Partial Class alglib
	Public Class optserv
		'************************************************************************
'        This structure is used to store temporary buffers for L-BFGS-based preconditioner.
'
'          -- ALGLIB --
'             Copyright 01.07.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Class precbuflbfgs
			Inherits apobject
			Public norms As Double()
			Public alpha As Double()
			Public rho As Double()
			Public yk As Double(,)
			Public idx As Integer()
			Public bufa As Double()
			Public bufb As Integer()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				norms = New Double(-1) {}
				alpha = New Double(-1) {}
				rho = New Double(-1) {}
				yk = New Double(-1, -1) {}
				idx = New Integer(-1) {}
				bufa = New Double(-1) {}
				bufb = New Integer(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New precbuflbfgs()
				_result.norms = DirectCast(norms.Clone(), Double())
				_result.alpha = DirectCast(alpha.Clone(), Double())
				_result.rho = DirectCast(rho.Clone(), Double())
				_result.yk = DirectCast(yk.Clone(), Double(,))
				_result.idx = DirectCast(idx.Clone(), Integer())
				_result.bufa = DirectCast(bufa.Clone(), Double())
				_result.bufb = DirectCast(bufb.Clone(), Integer())
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure is used to store temporary buffers for LowRank preconditioner.
'
'          -- ALGLIB --
'             Copyright 21.08.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Class precbuflowrank
			Inherits apobject
			Public n As Integer
			Public k As Integer
			Public d As Double()
			Public v As Double(,)
			Public bufc As Double()
			Public bufz As Double(,)
			Public bufw As Double(,)
			Public tmp As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				d = New Double(-1) {}
				v = New Double(-1, -1) {}
				bufc = New Double(-1) {}
				bufz = New Double(-1, -1) {}
				bufw = New Double(-1, -1) {}
				tmp = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New precbuflowrank()
				_result.n = n
				_result.k = k
				_result.d = DirectCast(d.Clone(), Double())
				_result.v = DirectCast(v.Clone(), Double(,))
				_result.bufc = DirectCast(bufc.Clone(), Double())
				_result.bufz = DirectCast(bufz.Clone(), Double(,))
				_result.bufw = DirectCast(bufw.Clone(), Double(,))
				_result.tmp = DirectCast(tmp.Clone(), Double())
				Return _result
			End Function
		End Class




		'************************************************************************
'        This subroutine is used to prepare threshold value which will be used for
'        trimming of the target function (see comments on TrimFunction() for more
'        information).
'
'        This function accepts only one parameter: function value at the starting
'        point. It returns threshold which will be used for trimming.
'
'          -- ALGLIB --
'             Copyright 10.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub trimprepare(f As Double, ByRef threshold As Double)
			threshold = 0

			threshold = 10 * (System.Math.Abs(f) + 1)
		End Sub


		'************************************************************************
'        This subroutine is used to "trim" target function, i.e. to do following
'        transformation:
'
'                           { {F,G}          if F<Threshold
'            {F_tr, G_tr} = {
'                           { {Threshold, 0} if F>=Threshold
'                           
'        Such transformation allows us to  solve  problems  with  singularities  by
'        redefining function in such way that it becomes bounded from above.
'
'          -- ALGLIB --
'             Copyright 10.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub trimfunction(ByRef f As Double, ByRef g As Double(), n As Integer, threshold As Double)
			Dim i As Integer = 0

			If CDbl(f) >= CDbl(threshold) Then
				f = threshold
				For i = 0 To n - 1
					g(i) = 0.0
				Next
			End If
		End Sub


		'************************************************************************
'        This function enforces boundary constraints in the X.
'
'        This function correctly (although a bit inefficient) handles BL[i] which
'        are -INF and BU[i] which are +INF.
'
'        We have NMain+NSlack  dimensional  X,  with first NMain components bounded
'        by BL/BU, and next NSlack ones bounded by non-negativity constraints.
'
'        INPUT PARAMETERS
'            X       -   array[NMain+NSlack], point
'            BL      -   array[NMain], lower bounds
'                        (may contain -INF, when bound is not present)
'            HaveBL  -   array[NMain], if HaveBL[i] is False,
'                        then i-th bound is not present
'            BU      -   array[NMain], upper bounds
'                        (may contain +INF, when bound is not present)
'            HaveBU  -   array[NMain], if HaveBU[i] is False,
'                        then i-th bound is not present
'
'        OUTPUT PARAMETERS
'            X       -   X with all constraints being enforced
'
'        It returns True when constraints are consistent,
'        False - when constraints are inconsistent.
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function enforceboundaryconstraints(ByRef x As Double(), bl As Double(), havebl As Boolean(), bu As Double(), havebu As Boolean(), nmain As Integer, _
			nslack As Integer) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0

			result = False
			For i = 0 To nmain - 1
				If (havebl(i) AndAlso havebu(i)) AndAlso CDbl(bl(i)) > CDbl(bu(i)) Then
					Return result
				End If
				If havebl(i) AndAlso CDbl(x(i)) < CDbl(bl(i)) Then
					x(i) = bl(i)
				End If
				If havebu(i) AndAlso CDbl(x(i)) > CDbl(bu(i)) Then
					x(i) = bu(i)
				End If
			Next
			For i = 0 To nslack - 1
				If CDbl(x(nmain + i)) < CDbl(0) Then
					x(nmain + i) = 0
				End If
			Next
			result = True
			Return result
		End Function


		'************************************************************************
'        This function projects gradient into feasible area of boundary constrained
'        optimization  problem.  X  can  be  infeasible  with  respect  to boundary
'        constraints.  We  have  NMain+NSlack  dimensional  X,   with  first  NMain 
'        components bounded by BL/BU, and next NSlack ones bounded by non-negativity
'        constraints.
'
'        INPUT PARAMETERS
'            X       -   array[NMain+NSlack], point
'            G       -   array[NMain+NSlack], gradient
'            BL      -   lower bounds (may contain -INF, when bound is not present)
'            HaveBL  -   if HaveBL[i] is False, then i-th bound is not present
'            BU      -   upper bounds (may contain +INF, when bound is not present)
'            HaveBU  -   if HaveBU[i] is False, then i-th bound is not present
'
'        OUTPUT PARAMETERS
'            G       -   projection of G. Components of G which satisfy one of the
'                        following
'                            (1) (X[I]<=BndL[I]) and (G[I]>0), OR
'                            (2) (X[I]>=BndU[I]) and (G[I]<0)
'                        are replaced by zeros.
'
'        NOTE 1: this function assumes that constraints are feasible. It throws
'        exception otherwise.
'
'        NOTE 2: in fact, projection of ANTI-gradient is calculated,  because  this
'        function trims components of -G which points outside of the feasible area.
'        However, working with -G is considered confusing, because all optimization
'        source work with G.
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub projectgradientintobc(x As Double(), ByRef g As Double(), bl As Double(), havebl As Boolean(), bu As Double(), havebu As Boolean(), _
			nmain As Integer, nslack As Integer)
			Dim i As Integer = 0

			For i = 0 To nmain - 1
				alglib.ap.assert((Not havebl(i) OrElse Not havebu(i)) OrElse CDbl(bl(i)) <= CDbl(bu(i)), "ProjectGradientIntoBC: internal error (infeasible constraints)")
				If (havebl(i) AndAlso CDbl(x(i)) <= CDbl(bl(i))) AndAlso CDbl(g(i)) > CDbl(0) Then
					g(i) = 0
				End If
				If (havebu(i) AndAlso CDbl(x(i)) >= CDbl(bu(i))) AndAlso CDbl(g(i)) < CDbl(0) Then
					g(i) = 0
				End If
			Next
			For i = 0 To nslack - 1
				If CDbl(x(nmain + i)) <= CDbl(0) AndAlso CDbl(g(nmain + i)) > CDbl(0) Then
					g(nmain + i) = 0
				End If
			Next
		End Sub


		'************************************************************************
'        Given
'            a) initial point X0[NMain+NSlack]
'               (feasible with respect to bound constraints)
'            b) step vector alpha*D[NMain+NSlack]
'            c) boundary constraints BndL[NMain], BndU[NMain]
'            d) implicit non-negativity constraints for slack variables
'        this  function  calculates  bound  on  the step length subject to boundary
'        constraints.
'
'        It returns:
'            *  MaxStepLen - such step length that X0+MaxStepLen*alpha*D is exactly
'               at the boundary given by constraints
'            *  VariableToFreeze - index of the constraint to be activated,
'               0 <= VariableToFreeze < NMain+NSlack
'            *  ValueToFreeze - value of the corresponding constraint.
'
'        Notes:
'            * it is possible that several constraints can be activated by the step
'              at once. In such cases only one constraint is returned. It is caller
'              responsibility to check other constraints. This function makes  sure
'              that we activate at least one constraint, and everything else is the
'              responsibility of the caller.
'            * steps smaller than MaxStepLen still can activate constraints due  to
'              numerical errors. Thus purpose of this  function  is  not  to  guard 
'              against accidental activation of the constraints - quite the reverse, 
'              its purpose is to activate at least constraint upon performing  step
'              which is too long.
'            * in case there is no constraints to activate, we return negative
'              VariableToFreeze and zero MaxStepLen and ValueToFreeze.
'            * this function assumes that constraints are consistent; it throws
'              exception otherwise.
'
'        INPUT PARAMETERS
'            X           -   array[NMain+NSlack], point. Must be feasible with respect 
'                            to bound constraints (exception will be thrown otherwise)
'            D           -   array[NMain+NSlack], step direction
'            alpha       -   scalar multiplier before D, alpha<>0
'            BndL        -   lower bounds, array[NMain]
'                            (may contain -INF, when bound is not present)
'            HaveBndL    -   array[NMain], if HaveBndL[i] is False,
'                            then i-th bound is not present
'            BndU        -   array[NMain], upper bounds
'                            (may contain +INF, when bound is not present)
'            HaveBndU    -   array[NMain], if HaveBndU[i] is False,
'                            then i-th bound is not present
'            NMain       -   number of main variables
'            NSlack      -   number of slack variables
'            
'        OUTPUT PARAMETERS
'            VariableToFreeze:
'                            * negative value     = step is unbounded, ValueToFreeze=0,
'                                                   MaxStepLen=0.
'                            * non-negative value = at least one constraint, given by
'                                                   this parameter, will  be  activated
'                                                   upon performing maximum step.
'            ValueToFreeze-  value of the variable which will be constrained
'            MaxStepLen  -   maximum length of the step. Can be zero when step vector
'                            looks outside of the feasible area.
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub calculatestepbound(x As Double(), d As Double(), alpha As Double, bndl As Double(), havebndl As Boolean(), bndu As Double(), _
			havebndu As Boolean(), nmain As Integer, nslack As Integer, ByRef variabletofreeze As Integer, ByRef valuetofreeze As Double, ByRef maxsteplen As Double)
			Dim i As Integer = 0
			Dim prevmax As Double = 0
			Dim initval As Double = 0

			variabletofreeze = 0
			valuetofreeze = 0
			maxsteplen = 0

			alglib.ap.assert(CDbl(alpha) <> CDbl(0), "CalculateStepBound: zero alpha")
			variabletofreeze = -1
			initval = Math.maxrealnumber
			maxsteplen = initval
			For i = 0 To nmain - 1
				If havebndl(i) AndAlso CDbl(alpha * d(i)) < CDbl(0) Then
					alglib.ap.assert(CDbl(x(i)) >= CDbl(bndl(i)), "CalculateStepBound: infeasible X")
					prevmax = maxsteplen
					maxsteplen = apserv.safeminposrv(x(i) - bndl(i), -(alpha * d(i)), maxsteplen)
					If CDbl(maxsteplen) < CDbl(prevmax) Then
						variabletofreeze = i
						valuetofreeze = bndl(i)
					End If
				End If
				If havebndu(i) AndAlso CDbl(alpha * d(i)) > CDbl(0) Then
					alglib.ap.assert(CDbl(x(i)) <= CDbl(bndu(i)), "CalculateStepBound: infeasible X")
					prevmax = maxsteplen
					maxsteplen = apserv.safeminposrv(bndu(i) - x(i), alpha * d(i), maxsteplen)
					If CDbl(maxsteplen) < CDbl(prevmax) Then
						variabletofreeze = i
						valuetofreeze = bndu(i)
					End If
				End If
			Next
			For i = 0 To nslack - 1
				If CDbl(alpha * d(nmain + i)) < CDbl(0) Then
					alglib.ap.assert(CDbl(x(nmain + i)) >= CDbl(0), "CalculateStepBound: infeasible X")
					prevmax = maxsteplen
					maxsteplen = apserv.safeminposrv(x(nmain + i), -(alpha * d(nmain + i)), maxsteplen)
					If CDbl(maxsteplen) < CDbl(prevmax) Then
						variabletofreeze = nmain + i
						valuetofreeze = 0
					End If
				End If
			Next
			If CDbl(maxsteplen) = CDbl(initval) Then
				valuetofreeze = 0
				maxsteplen = 0
			End If
		End Sub


		'************************************************************************
'        This function postprocesses bounded step by:
'        * analysing step length (whether it is equal to MaxStepLen) and activating 
'          constraint given by VariableToFreeze if needed
'        * checking for additional bound constraints to activate
'
'        This function uses final point of the step, quantities calculated  by  the
'        CalculateStepBound()  function.  As  result,  it  returns  point  which is 
'        exactly feasible with respect to boundary constraints.
'
'        NOTE 1: this function does NOT handle and check linear equality constraints
'        NOTE 2: when StepTaken=MaxStepLen we always activate at least one constraint
'
'        INPUT PARAMETERS
'            X           -   array[NMain+NSlack], final point to postprocess
'            XPrev       -   array[NMain+NSlack], initial point
'            BndL        -   lower bounds, array[NMain]
'                            (may contain -INF, when bound is not present)
'            HaveBndL    -   array[NMain], if HaveBndL[i] is False,
'                            then i-th bound is not present
'            BndU        -   array[NMain], upper bounds
'                            (may contain +INF, when bound is not present)
'            HaveBndU    -   array[NMain], if HaveBndU[i] is False,
'                            then i-th bound is not present
'            NMain       -   number of main variables
'            NSlack      -   number of slack variables
'            VariableToFreeze-result of CalculateStepBound()
'            ValueToFreeze-  result of CalculateStepBound()
'            StepTaken   -   actual step length (actual step is equal to the possibly 
'                            non-unit step direction vector times this parameter).
'                            StepTaken<=MaxStepLen.
'            MaxStepLen  -   result of CalculateStepBound()
'            
'        OUTPUT PARAMETERS
'            X           -   point bounded with respect to constraints.
'                            components corresponding to active constraints are exactly
'                            equal to the boundary values.
'                            
'        RESULT:
'            number of constraints activated in addition to previously active ones.
'            Constraints which were DEACTIVATED are ignored (do not influence
'            function value).
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function postprocessboundedstep(ByRef x As Double(), xprev As Double(), bndl As Double(), havebndl As Boolean(), bndu As Double(), havebndu As Boolean(), _
			nmain As Integer, nslack As Integer, variabletofreeze As Integer, valuetofreeze As Double, steptaken As Double, maxsteplen As Double) As Integer
			Dim result As Integer = 0
			Dim i As Integer = 0
			Dim wasactivated As New Boolean()

			alglib.ap.assert(variabletofreeze < 0 OrElse CDbl(steptaken) <= CDbl(maxsteplen))

			'
			' Activate constraints
			'
			If variabletofreeze >= 0 AndAlso CDbl(steptaken) = CDbl(maxsteplen) Then
				x(variabletofreeze) = valuetofreeze
			End If
			For i = 0 To nmain - 1
				If havebndl(i) AndAlso CDbl(x(i)) < CDbl(bndl(i)) Then
					x(i) = bndl(i)
				End If
				If havebndu(i) AndAlso CDbl(x(i)) > CDbl(bndu(i)) Then
					x(i) = bndu(i)
				End If
			Next
			For i = 0 To nslack - 1
				If CDbl(x(nmain + i)) <= CDbl(0) Then
					x(nmain + i) = 0
				End If
			Next

			'
			' Calculate number of constraints being activated
			'
			result = 0
			For i = 0 To nmain - 1
				wasactivated = CDbl(x(i)) <> CDbl(xprev(i)) AndAlso ((havebndl(i) AndAlso CDbl(x(i)) = CDbl(bndl(i))) OrElse (havebndu(i) AndAlso CDbl(x(i)) = CDbl(bndu(i))))
				wasactivated = wasactivated OrElse variabletofreeze = i
				If wasactivated Then
					result = result + 1
				End If
			Next
			For i = 0 To nslack - 1
				wasactivated = CDbl(x(nmain + i)) <> CDbl(xprev(nmain + i)) AndAlso CDbl(x(nmain + i)) = CDbl(0.0)
				wasactivated = wasactivated OrElse variabletofreeze = nmain + i
				If wasactivated Then
					result = result + 1
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        The  purpose  of  this  function is to prevent algorithm from "unsticking" 
'        from  the  active  bound  constraints  because  of  numerical noise in the
'        gradient or Hessian.
'
'        It is done by zeroing some components of the search direction D.  D[i]  is
'        zeroed when both (a) and (b) are true:
'        a) corresponding X[i] is exactly at the boundary
'        b) |D[i]*S[i]| <= DropTol*Sqrt(SUM(D[i]^2*S[I]^2))
'
'        D  can  be  step  direction , antigradient, gradient, or anything similar. 
'        Sign of D does not matter, nor matters step length.
'
'        NOTE 1: boundary constraints are expected to be consistent, as well as X
'                is expected to be feasible. Exception will be thrown otherwise.
'
'        INPUT PARAMETERS
'            D           -   array[NMain+NSlack], direction
'            X           -   array[NMain+NSlack], current point
'            BndL        -   lower bounds, array[NMain]
'                            (may contain -INF, when bound is not present)
'            HaveBndL    -   array[NMain], if HaveBndL[i] is False,
'                            then i-th bound is not present
'            BndU        -   array[NMain], upper bounds
'                            (may contain +INF, when bound is not present)
'            HaveBndU    -   array[NMain], if HaveBndU[i] is False,
'                            then i-th bound is not present
'            S           -   array[NMain+NSlack], scaling of the variables
'            NMain       -   number of main variables
'            NSlack      -   number of slack variables
'            DropTol     -   drop tolerance, >=0
'            
'        OUTPUT PARAMETERS
'            X           -   point bounded with respect to constraints.
'                            components corresponding to active constraints are exactly
'                            equal to the boundary values.
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub filterdirection(ByRef d As Double(), x As Double(), bndl As Double(), havebndl As Boolean(), bndu As Double(), havebndu As Boolean(), _
			s As Double(), nmain As Integer, nslack As Integer, droptol As Double)
			Dim i As Integer = 0
			Dim scalednorm As Double = 0
			Dim isactive As New Boolean()

			scalednorm = 0.0
			For i = 0 To nmain + nslack - 1
				scalednorm = scalednorm + Math.sqr(d(i) * s(i))
			Next
			scalednorm = System.Math.sqrt(scalednorm)
			For i = 0 To nmain - 1
				alglib.ap.assert(Not havebndl(i) OrElse CDbl(x(i)) >= CDbl(bndl(i)), "FilterDirection: infeasible point")
				alglib.ap.assert(Not havebndu(i) OrElse CDbl(x(i)) <= CDbl(bndu(i)), "FilterDirection: infeasible point")
				isactive = (havebndl(i) AndAlso CDbl(x(i)) = CDbl(bndl(i))) OrElse (havebndu(i) AndAlso CDbl(x(i)) = CDbl(bndu(i)))
				If isactive AndAlso CDbl(System.Math.Abs(d(i) * s(i))) <= CDbl(droptol * scalednorm) Then
					d(i) = 0.0
				End If
			Next
			For i = 0 To nslack - 1
				alglib.ap.assert(CDbl(x(nmain + i)) >= CDbl(0), "FilterDirection: infeasible point")
				If CDbl(x(nmain + i)) = CDbl(0) AndAlso CDbl(System.Math.Abs(d(nmain + i) * s(nmain + i))) <= CDbl(droptol * scalednorm) Then
					d(nmain + i) = 0.0
				End If
			Next
		End Sub


		'************************************************************************
'        This function returns number of bound constraints whose state was  changed
'        (either activated or deactivated) when making step from XPrev to X.
'
'        Constraints are considered:
'        * active - when we are exactly at the boundary
'        * inactive - when we are not at the boundary
'
'        You should note that antigradient direction is NOT taken into account when
'        we make decions on the constraint status.
'
'        INPUT PARAMETERS
'            X           -   array[NMain+NSlack], final point.
'                            Must be feasible with respect to bound constraints.
'            XPrev       -   array[NMain+NSlack], initial point.
'                            Must be feasible with respect to bound constraints.
'            BndL        -   lower bounds, array[NMain]
'                            (may contain -INF, when bound is not present)
'            HaveBndL    -   array[NMain], if HaveBndL[i] is False,
'                            then i-th bound is not present
'            BndU        -   array[NMain], upper bounds
'                            (may contain +INF, when bound is not present)
'            HaveBndU    -   array[NMain], if HaveBndU[i] is False,
'                            then i-th bound is not present
'            NMain       -   number of main variables
'            NSlack      -   number of slack variables
'            
'        RESULT:
'            number of constraints whose state was changed.
'
'          -- ALGLIB --
'             Copyright 10.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function numberofchangedconstraints(x As Double(), xprev As Double(), bndl As Double(), havebndl As Boolean(), bndu As Double(), havebndu As Boolean(), _
			nmain As Integer, nslack As Integer) As Integer
			Dim result As Integer = 0
			Dim i As Integer = 0
			Dim statuschanged As New Boolean()

			result = 0
			For i = 0 To nmain - 1
				If CDbl(x(i)) <> CDbl(xprev(i)) Then
					statuschanged = False
					If havebndl(i) AndAlso (CDbl(x(i)) = CDbl(bndl(i)) OrElse CDbl(xprev(i)) = CDbl(bndl(i))) Then
						statuschanged = True
					End If
					If havebndu(i) AndAlso (CDbl(x(i)) = CDbl(bndu(i)) OrElse CDbl(xprev(i)) = CDbl(bndu(i))) Then
						statuschanged = True
					End If
					If statuschanged Then
						result = result + 1
					End If
				End If
			Next
			For i = 0 To nslack - 1
				If CDbl(x(nmain + i)) <> CDbl(xprev(nmain + i)) AndAlso (CDbl(x(nmain + i)) = CDbl(0) OrElse CDbl(xprev(nmain + i)) = CDbl(0)) Then
					result = result + 1
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        This function finds feasible point of  (NMain+NSlack)-dimensional  problem
'        subject to NMain explicit boundary constraints (some  constraints  can  be
'        omitted), NSlack implicit non-negativity constraints,  K  linear  equality
'        constraints.
'
'        INPUT PARAMETERS
'            X           -   array[NMain+NSlack], initial point.
'            BndL        -   lower bounds, array[NMain]
'                            (may contain -INF, when bound is not present)
'            HaveBndL    -   array[NMain], if HaveBndL[i] is False,
'                            then i-th bound is not present
'            BndU        -   array[NMain], upper bounds
'                            (may contain +INF, when bound is not present)
'            HaveBndU    -   array[NMain], if HaveBndU[i] is False,
'                            then i-th bound is not present
'            NMain       -   number of main variables
'            NSlack      -   number of slack variables
'            CE          -   array[K,NMain+NSlack+1], equality  constraints CE*x=b.
'                            Rows contain constraints, first  NMain+NSlack  columns
'                            contain coefficients before X[], last  column  contain
'                            right part.
'            K           -   number of linear constraints
'            EpsI        -   infeasibility (error in the right part) allowed in the
'                            solution
'
'        OUTPUT PARAMETERS:
'            X           -   feasible point or best infeasible point found before
'                            algorithm termination
'            QPIts       -   number of QP iterations (for debug purposes)
'            GPAIts      -   number of GPA iterations (for debug purposes)
'            
'        RESULT:
'            True in case X is feasible, False - if it is infeasible.
'
'          -- ALGLIB --
'             Copyright 20.01.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function findfeasiblepoint(ByRef x As Double(), bndl As Double(), havebndl As Boolean(), bndu As Double(), havebndu As Boolean(), nmain As Integer, _
			nslack As Integer, ce As Double(,), k As Integer, epsi As Double, ByRef qpits As Integer, ByRef gpaits As Integer) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim permx As Double() = New Double(-1) {}
			Dim xn As Double() = New Double(-1) {}
			Dim xa As Double() = New Double(-1) {}
			Dim newtonstep As Double() = New Double(-1) {}
			Dim g As Double() = New Double(-1) {}
			Dim pg As Double() = New Double(-1) {}
			Dim a As Double(,) = New Double(-1, -1) {}
			Dim armijostep As Double = 0
			Dim armijobeststep As Double = 0
			Dim armijobestfeas As Double = 0
			Dim v As Double = 0
			Dim mx As Double = 0
			Dim feaserr As Double = 0
			Dim feasold As Double = 0
			Dim feasnew As Double = 0
			Dim pgnorm As Double = 0
			Dim vn As Double = 0
			Dim vd As Double = 0
			Dim stp As Double = 0
			Dim vartofreeze As Integer = 0
			Dim valtofreeze As Double = 0
			Dim maxsteplen As Double = 0
			Dim werechangesinconstraints As New Boolean()
			Dim stage1isover As New Boolean()
			Dim converged As New Boolean()
			Dim activeconstraints As Double() = New Double(-1) {}
			Dim tmpk As Double() = New Double(-1) {}
			Dim colnorms As Double() = New Double(-1) {}
			Dim nactive As Integer = 0
			Dim nfree As Integer = 0
			Dim nsvd As Integer = 0
			Dim p1 As Integer() = New Integer(-1) {}
			Dim p2 As Integer() = New Integer(-1) {}
			Dim buf As New apserv.apbuffers()
			Dim w As Double() = New Double(-1) {}
			Dim s As Double() = New Double(-1) {}
			Dim u As Double(,) = New Double(-1, -1) {}
			Dim vt As Double(,) = New Double(-1, -1) {}
			Dim itscount As Integer = 0
			Dim itswithintolerance As Integer = 0
			Dim maxitswithintolerance As Integer = 0
			Dim gparuns As Integer = 0
			Dim maxarmijoruns As Integer = 0
			Dim i_ As Integer = 0

			ce = DirectCast(ce.Clone(), Double(,))
			qpits = 0
			gpaits = 0

			maxitswithintolerance = 3
			maxarmijoruns = 5
			qpits = 0
			gpaits = 0

			'
			' Initial enforcement of the feasibility with respect to boundary constraints
			' NOTE: after this block we assume that boundary constraints are consistent.
			'
			If Not enforceboundaryconstraints(x, bndl, havebndl, bndu, havebndu, nmain, _
				nslack) Then
				result = False
				Return result
			End If
			If k = 0 Then

				'
				' No linear constraints, we can exit right now
				'
				result = True
				Return result
			End If

			'
			' Scale rows of CE in such way that max(CE[i,0..nmain+nslack-1])=1 for any i=0..k-1
			'
			For i = 0 To k - 1
				v = 0.0
				For j = 0 To nmain + nslack - 1
					v = System.Math.Max(v, System.Math.Abs(ce(i, j)))
				Next
				If CDbl(v) <> CDbl(0) Then
					v = 1 / v
					For i_ = 0 To nmain + nslack
						ce(i, i_) = v * ce(i, i_)
					Next
				End If
			Next

			'
			' Allocate temporaries
			'
			xn = New Double(nmain + (nslack - 1)) {}
			xa = New Double(nmain + (nslack - 1)) {}
			permx = New Double(nmain + (nslack - 1)) {}
			g = New Double(nmain + (nslack - 1)) {}
			pg = New Double(nmain + (nslack - 1)) {}
			tmpk = New Double(k - 1) {}
			a = New Double(k - 1, nmain + (nslack - 1)) {}
			activeconstraints = New Double(nmain + (nslack - 1)) {}
			newtonstep = New Double(nmain + (nslack - 1)) {}
			s = New Double(nmain + (nslack - 1)) {}
			colnorms = New Double(nmain + (nslack - 1)) {}
			For i = 0 To nmain + nslack - 1
				s(i) = 1.0
				colnorms(i) = 0.0
				For j = 0 To k - 1
					colnorms(i) = colnorms(i) + Math.sqr(ce(j, i))
				Next
			Next

			'
			' K>0, we have linear equality constraints combined with bound constraints.
			'
			' Try to find feasible point as minimizer of the quadratic function
			'     F(x) = 0.5*||CE*x-b||^2 = 0.5*x'*(CE'*CE)*x - (b'*CE)*x + 0.5*b'*b
			' subject to boundary constraints given by BL, BU and non-negativity of
			' the slack variables. BTW, we drop constant term because it does not
			' actually influences on the solution.
			'
			' Below we will assume that K>0.
			'
			itswithintolerance = 0
			itscount = 0
			While True

				'
				' Stage 0: check for exact convergence
				'
				converged = True
				feaserr = 0
				For i = 0 To k - 1

					'
					' Calculate:
					' * V - error in the right part
					' * MX - maximum term in the left part
					'
					' Terminate if error in the right part is not greater than 100*Eps*MX.
					'
					' IMPORTANT: we must perform check for non-strict inequality, i.e. to use <= instead of <.
					'            it will allow us to easily handle situations with zero rows of CE.
					'
					mx = 0
					v = -ce(i, nmain + nslack)
					For j = 0 To nmain + nslack - 1
						mx = System.Math.Max(mx, System.Math.Abs(ce(i, j) * x(j)))
						v = v + ce(i, j) * x(j)
					Next
					feaserr = feaserr + Math.sqr(v)
					converged = converged AndAlso CDbl(System.Math.Abs(v)) <= CDbl(100 * Math.machineepsilon * mx)
				Next
				feaserr = System.Math.sqrt(feaserr)
				If converged Then
					result = CDbl(feaserr) <= CDbl(epsi)
					Return result
				End If

				'
				' Stage 1: equality constrained quadratic programming
				'
				' * treat active bound constraints as equality ones (constraint is considered 
				'   active when we are at the boundary, independently of the antigradient direction)
				' * calculate unrestricted Newton step to point XM (which may be infeasible)
				'   calculate MaxStepLen = largest step in direction of XM which retains feasibility.
				' * perform bounded step from X to XN:
				'   a) XN=XM                  (if XM is feasible)
				'   b) XN=X-MaxStepLen*(XM-X) (otherwise)
				' * X := XN
				' * if XM (Newton step subject to currently active constraints) was feasible, goto Stage 2
				' * repeat Stage 1
				'
				' NOTE 1: in order to solve constrained qudratic subproblem we will have to reorder
				'         variables in such way that ones corresponding to inactive constraints will
				'         be first, and active ones will be last in the list. CE and X are now
				'                                                       [ xi ]
				'         separated into two parts: CE = [CEi CEa], x = [    ], where CEi/Xi correspond
				'                                                       [ xa ]
				'         to INACTIVE constraints, and CEa/Xa correspond to the ACTIVE ones.
				'
				'         Now, instead of F=0.5*x'*(CE'*CE)*x - (b'*CE)*x + 0.5*b'*b, we have
				'         F(xi) = 0.5*(CEi*xi,CEi*xi) + (CEa*xa-b,CEi*xi) + (0.5*CEa*xa-b,CEa*xa).
				'         Here xa is considered constant, i.e. we optimize with respect to xi, leaving xa fixed.
				'
				'         We can solve it by performing SVD of CEi and calculating pseudoinverse of the
				'         Hessian matrix. Of course, we do NOT calculate pseudoinverse explicitly - we
				'         just use singular vectors to perform implicit multiplication by it.
				'
				'
				While True

					'
					' Calculate G - gradient subject to equality constraints,
					' multiply it by inverse of the Hessian diagonal to obtain initial
					' step vector.
					'
					' Bound step subject to constraints which can be activated,
					' run Armijo search with increasing step size.
					' Search is terminated when feasibility error stops to decrease.
					'
					' NOTE: it is important to test for "stops to decrease" instead
					' of "starts to increase" in order to correctly handle cases with
					' zero CE.
					'
					armijobeststep = 0.0
					armijobestfeas = 0.0
					For i = 0 To nmain + nslack - 1
						g(i) = 0
					Next
					For i = 0 To k - 1
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += ce(i, i_) * x(i_)
						Next
						v = v - ce(i, nmain + nslack)
						armijobestfeas = armijobestfeas + Math.sqr(v)
						For i_ = 0 To nmain + nslack - 1
							g(i_) = g(i_) + v * ce(i, i_)
						Next
					Next
					armijobestfeas = System.Math.sqrt(armijobestfeas)
					For i = 0 To nmain - 1
						If havebndl(i) AndAlso CDbl(x(i)) = CDbl(bndl(i)) Then
							g(i) = 0.0
						End If
						If havebndu(i) AndAlso CDbl(x(i)) = CDbl(bndu(i)) Then
							g(i) = 0.0
						End If
					Next
					For i = 0 To nslack - 1
						If CDbl(x(nmain + i)) = CDbl(0.0) Then
							g(nmain + i) = 0.0
						End If
					Next
					v = 0.0
					For i = 0 To nmain + nslack - 1
						If CDbl(Math.sqr(colnorms(i))) <> CDbl(0) Then
							newtonstep(i) = -(g(i) / Math.sqr(colnorms(i)))
						Else
							newtonstep(i) = 0.0
						End If
						v = v + Math.sqr(newtonstep(i))
					Next
					If CDbl(v) = CDbl(0) Then

						'
						' Constrained gradient is zero, QP iterations are over
						'
						Exit While
					End If
					calculatestepbound(x, newtonstep, 1.0, bndl, havebndl, bndu, _
						havebndu, nmain, nslack, vartofreeze, valtofreeze, maxsteplen)
					If vartofreeze >= 0 AndAlso CDbl(maxsteplen) = CDbl(0) Then

						'
						' Can not perform step, QP iterations are over
						'
						Exit While
					End If
					If vartofreeze >= 0 Then
						armijostep = System.Math.Min(1.0, maxsteplen)
					Else
						armijostep = 1
					End If
					While True
						For i_ = 0 To nmain + nslack - 1
							xa(i_) = x(i_)
						Next
						For i_ = 0 To nmain + nslack - 1
							xa(i_) = xa(i_) + armijostep * newtonstep(i_)
						Next
						enforceboundaryconstraints(xa, bndl, havebndl, bndu, havebndu, nmain, _
							nslack)
						feaserr = 0.0
						For i = 0 To k - 1
							v = 0.0
							For i_ = 0 To nmain + nslack - 1
								v += ce(i, i_) * xa(i_)
							Next
							v = v - ce(i, nmain + nslack)
							feaserr = feaserr + Math.sqr(v)
						Next
						feaserr = System.Math.sqrt(feaserr)
						If CDbl(feaserr) >= CDbl(armijobestfeas) Then
							Exit While
						End If
						armijobestfeas = feaserr
						armijobeststep = armijostep
						armijostep = 2.0 * armijostep
					End While
					For i_ = 0 To nmain + nslack - 1
						x(i_) = x(i_) + armijobeststep * newtonstep(i_)
					Next
					enforceboundaryconstraints(x, bndl, havebndl, bndu, havebndu, nmain, _
						nslack)

					'
					' Determine number of active and free constraints
					'
					nactive = 0
					For i = 0 To nmain - 1
						activeconstraints(i) = 0
						If havebndl(i) AndAlso CDbl(x(i)) = CDbl(bndl(i)) Then
							activeconstraints(i) = 1
						End If
						If havebndu(i) AndAlso CDbl(x(i)) = CDbl(bndu(i)) Then
							activeconstraints(i) = 1
						End If
						If CDbl(activeconstraints(i)) > CDbl(0) Then
							nactive = nactive + 1
						End If
					Next
					For i = 0 To nslack - 1
						activeconstraints(nmain + i) = 0
						If CDbl(x(nmain + i)) = CDbl(0.0) Then
							activeconstraints(nmain + i) = 1
						End If
						If CDbl(activeconstraints(nmain + i)) > CDbl(0) Then
							nactive = nactive + 1
						End If
					Next
					nfree = nmain + nslack - nactive
					If nfree = 0 Then
						Exit While
					End If
					qpits = qpits + 1

					'
					' Reorder variables
					'
					tsort.tagsortbuf(activeconstraints, nmain + nslack, p1, p2, buf)
					For i = 0 To k - 1
						For j = 0 To nmain + nslack - 1
							a(i, j) = ce(i, j)
						Next
					Next
					For j = 0 To nmain + nslack - 1
						permx(j) = x(j)
					Next
					For j = 0 To nmain + nslack - 1
						If p2(j) <> j Then
							idx0 = p2(j)
							idx1 = j
							For i = 0 To k - 1
								v = a(i, idx0)
								a(i, idx0) = a(i, idx1)
								a(i, idx1) = v
							Next
							v = permx(idx0)
							permx(idx0) = permx(idx1)
							permx(idx1) = v
						End If
					Next

					'
					' Calculate (unprojected) gradient:
					' G(xi) = CEi'*(CEi*xi + CEa*xa - b)
					'
					For i = 0 To nfree - 1
						g(i) = 0
					Next
					For i = 0 To k - 1
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += a(i, i_) * permx(i_)
						Next
						tmpk(i) = v - ce(i, nmain + nslack)
					Next
					For i = 0 To k - 1
						v = tmpk(i)
						For i_ = 0 To nfree - 1
							g(i_) = g(i_) + v * a(i, i_)
						Next
					Next

					'
					' Calculate Newton step using SVD of CEi:
					'     F(xi)  = 0.5*xi'*H*xi + g'*xi    (Taylor decomposition)
					'     XN     = -H^(-1)*g               (new point, solution of the QP subproblem)
					'     H      = CEi'*CEi                
					'     CEi    = U*W*V'                  (SVD of CEi)
					'     H      = V*W^2*V'                 
					'     H^(-1) = V*W^(-2)*V'
					'     step     = -V*W^(-2)*V'*g          (it is better to perform multiplication from right to left)
					'
					' NOTE 1: we do NOT need left singular vectors to perform Newton step.
					'
					nsvd = System.Math.Min(k, nfree)
					If Not svd.rmatrixsvd(a, k, nfree, 0, 1, 2, _
						w, u, vt) Then
						result = False
						Return result
					End If
					For i = 0 To nsvd - 1
						v = 0.0
						For i_ = 0 To nfree - 1
							v += vt(i, i_) * g(i_)
						Next
						tmpk(i) = v
					Next
					For i = 0 To nsvd - 1

						'
						' It is important to have strict ">" in order to correctly 
						' handle zero singular values.
						'
						If CDbl(Math.sqr(w(i))) > CDbl(Math.sqr(w(0)) * (nmain + nslack) * Math.machineepsilon) Then
							tmpk(i) = tmpk(i) / Math.sqr(w(i))
						Else
							tmpk(i) = 0
						End If
					Next
					For i = 0 To nmain + nslack - 1
						newtonstep(i) = 0
					Next
					For i = 0 To nsvd - 1
						v = tmpk(i)
						For i_ = 0 To nfree - 1
							newtonstep(i_) = newtonstep(i_) - v * vt(i, i_)
						Next
					Next
					For j = nmain + nslack - 1 To 0 Step -1
						If p2(j) <> j Then
							idx0 = p2(j)
							idx1 = j
							v = newtonstep(idx0)
							newtonstep(idx0) = newtonstep(idx1)
							newtonstep(idx1) = v
						End If
					Next

					'
					' NewtonStep contains Newton step subject to active bound constraints.
					'
					' Such step leads us to the minimizer of the equality constrained F,
					' but such minimizer may be infeasible because some constraints which
					' are inactive at the initial point can be violated at the solution.
					'
					' Thus, we perform optimization in two stages:
					' a) perform bounded Newton step, i.e. step in the Newton direction
					'    until activation of the first constraint
					' b) in case (MaxStepLen>0)and(MaxStepLen<1), perform additional iteration
					'    of the Armijo line search in the rest of the Newton direction.
					'
					calculatestepbound(x, newtonstep, 1.0, bndl, havebndl, bndu, _
						havebndu, nmain, nslack, vartofreeze, valtofreeze, maxsteplen)
					If vartofreeze >= 0 AndAlso CDbl(maxsteplen) = CDbl(0) Then

						'
						' Activation of the constraints prevent us from performing step,
						' QP iterations are over
						'
						Exit While
					End If
					If vartofreeze >= 0 Then
						v = System.Math.Min(1.0, maxsteplen)
					Else
						v = 1.0
					End If
					For i_ = 0 To nmain + nslack - 1
						xn(i_) = v * newtonstep(i_)
					Next
					For i_ = 0 To nmain + nslack - 1
						xn(i_) = xn(i_) + x(i_)
					Next
					postprocessboundedstep(xn, x, bndl, havebndl, bndu, havebndu, _
						nmain, nslack, vartofreeze, valtofreeze, v, maxsteplen)
					If CDbl(maxsteplen) > CDbl(0) AndAlso CDbl(maxsteplen) < CDbl(1) Then

						'
						' Newton step was restricted by activation of the constraints,
						' perform Armijo iteration.
						'
						' Initial estimate for best step is zero step. We try different
						' step sizes, from the 1-MaxStepLen (residual of the full Newton
						' step) to progressively smaller and smaller steps.
						'
						armijobeststep = 0.0
						armijobestfeas = 0.0
						For i = 0 To k - 1
							v = 0.0
							For i_ = 0 To nmain + nslack - 1
								v += ce(i, i_) * xn(i_)
							Next
							v = v - ce(i, nmain + nslack)
							armijobestfeas = armijobestfeas + Math.sqr(v)
						Next
						armijobestfeas = System.Math.sqrt(armijobestfeas)
						armijostep = 1 - maxsteplen
						For j = 0 To maxarmijoruns - 1
							For i_ = 0 To nmain + nslack - 1
								xa(i_) = xn(i_)
							Next
							For i_ = 0 To nmain + nslack - 1
								xa(i_) = xa(i_) + armijostep * newtonstep(i_)
							Next
							enforceboundaryconstraints(xa, bndl, havebndl, bndu, havebndu, nmain, _
								nslack)
							feaserr = 0.0
							For i = 0 To k - 1
								v = 0.0
								For i_ = 0 To nmain + nslack - 1
									v += ce(i, i_) * xa(i_)
								Next
								v = v - ce(i, nmain + nslack)
								feaserr = feaserr + Math.sqr(v)
							Next
							feaserr = System.Math.sqrt(feaserr)
							If CDbl(feaserr) < CDbl(armijobestfeas) Then
								armijobestfeas = feaserr
								armijobeststep = armijostep
							End If
							armijostep = 0.5 * armijostep
						Next
						For i_ = 0 To nmain + nslack - 1
							xa(i_) = xn(i_)
						Next
						For i_ = 0 To nmain + nslack - 1
							xa(i_) = xa(i_) + armijobeststep * newtonstep(i_)
						Next
						enforceboundaryconstraints(xa, bndl, havebndl, bndu, havebndu, nmain, _
							nslack)
					Else

						'
						' Armijo iteration is not performed
						'
						For i_ = 0 To nmain + nslack - 1
							xa(i_) = xn(i_)
						Next
					End If
					stage1isover = CDbl(maxsteplen) >= CDbl(1) OrElse CDbl(maxsteplen) = CDbl(0)

					'
					' Calculate feasibility errors for old and new X.
					' These quantinies are used for debugging purposes only.
					' However, we can leave them in release code because performance impact is insignificant.
					'
					' Update X. Exit if needed.
					'
					feasold = 0
					feasnew = 0
					For i = 0 To k - 1
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += ce(i, i_) * x(i_)
						Next
						feasold = feasold + Math.sqr(v - ce(i, nmain + nslack))
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += ce(i, i_) * xa(i_)
						Next
						feasnew = feasnew + Math.sqr(v - ce(i, nmain + nslack))
					Next
					feasold = System.Math.sqrt(feasold)
					feasnew = System.Math.sqrt(feasnew)
					If CDbl(feasnew) >= CDbl(feasold) Then
						Exit While
					End If
					For i_ = 0 To nmain + nslack - 1
						x(i_) = xa(i_)
					Next
					If stage1isover Then
						Exit While
					End If
				End While

				'
				' Stage 2: gradient projection algorithm (GPA)
				'
				' * calculate feasibility error (with respect to linear equality constraints)
				' * calculate gradient G of F, project it into feasible area (G => PG)
				' * exit if norm(PG) is exactly zero or feasibility error is smaller than EpsC
				' * let XM be exact minimum of F along -PG (XM may be infeasible).
				'   calculate MaxStepLen = largest step in direction of -PG which retains feasibility.
				' * perform bounded step from X to XN:
				'   a) XN=XM              (if XM is feasible)
				'   b) XN=X-MaxStepLen*PG (otherwise)
				' * X := XN
				' * stop after specified number of iterations or when no new constraints was activated
				'
				' NOTES:
				' * grad(F) = (CE'*CE)*x - (b'*CE)^T
				' * CE[i] denotes I-th row of CE
				' * XM = X+stp*(-PG) where stp=(grad(F(X)),PG)/(CE*PG,CE*PG).
				'   Here PG is a projected gradient, but in fact it can be arbitrary non-zero 
				'   direction vector - formula for minimum of F along PG still will be correct.
				'
				werechangesinconstraints = False
				For gparuns = 1 To k

					'
					' calculate feasibility error and G
					'
					feaserr = 0
					For i = 0 To nmain + nslack - 1
						g(i) = 0
					Next
					For i = 0 To k - 1

						'
						' G += CE[i]^T * (CE[i]*x-b[i])
						'
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += ce(i, i_) * x(i_)
						Next
						v = v - ce(i, nmain + nslack)
						feaserr = feaserr + Math.sqr(v)
						For i_ = 0 To nmain + nslack - 1
							g(i_) = g(i_) + v * ce(i, i_)
						Next
					Next

					'
					' project G, filter it (strip numerical noise)
					'
					For i_ = 0 To nmain + nslack - 1
						pg(i_) = g(i_)
					Next
					projectgradientintobc(x, pg, bndl, havebndl, bndu, havebndu, _
						nmain, nslack)
					filterdirection(pg, x, bndl, havebndl, bndu, havebndu, _
						s, nmain, nslack, 1E-09)
					For i = 0 To nmain + nslack - 1
						If CDbl(Math.sqr(colnorms(i))) <> CDbl(0) Then
							pg(i) = pg(i) / Math.sqr(colnorms(i))
						Else
							pg(i) = 0.0
						End If
					Next

					'
					' Check GNorm and feasibility.
					' Exit when GNorm is exactly zero.
					'
					pgnorm = 0.0
					For i_ = 0 To nmain + nslack - 1
						pgnorm += pg(i_) * pg(i_)
					Next
					feaserr = System.Math.sqrt(feaserr)
					pgnorm = System.Math.sqrt(pgnorm)
					If CDbl(pgnorm) = CDbl(0) Then
						result = CDbl(feaserr) <= CDbl(epsi)
						Return result
					End If

					'
					' calculate planned step length
					'
					vn = 0.0
					For i_ = 0 To nmain + nslack - 1
						vn += g(i_) * pg(i_)
					Next
					vd = 0
					For i = 0 To k - 1
						v = 0.0
						For i_ = 0 To nmain + nslack - 1
							v += ce(i, i_) * pg(i_)
						Next
						vd = vd + Math.sqr(v)
					Next
					stp = vn / vd

					'
					' Calculate step bound.
					' Perform bounded step and post-process it
					'
					calculatestepbound(x, pg, -1.0, bndl, havebndl, bndu, _
						havebndu, nmain, nslack, vartofreeze, valtofreeze, maxsteplen)
					If vartofreeze >= 0 AndAlso CDbl(maxsteplen) = CDbl(0) Then
						result = False
						Return result
					End If
					If vartofreeze >= 0 Then
						v = System.Math.Min(stp, maxsteplen)
					Else
						v = stp
					End If
					For i_ = 0 To nmain + nslack - 1
						xn(i_) = x(i_)
					Next
					For i_ = 0 To nmain + nslack - 1
						xn(i_) = xn(i_) - v * pg(i_)
					Next
					postprocessboundedstep(xn, x, bndl, havebndl, bndu, havebndu, _
						nmain, nslack, vartofreeze, valtofreeze, v, maxsteplen)

					'
					' update X
					' check stopping criteria
					'
					werechangesinconstraints = werechangesinconstraints OrElse numberofchangedconstraints(xn, x, bndl, havebndl, bndu, havebndu, _
						nmain, nslack) > 0
					For i_ = 0 To nmain + nslack - 1
						x(i_) = xn(i_)
					Next
					gpaits = gpaits + 1
					If Not werechangesinconstraints Then
						Exit For
					End If
				Next

				'
				' Stage 3: decide to stop algorithm or not to stop
				'
				' 1. we can stop when last GPA run did NOT changed constraints status.
				'    It means that we've found final set of the active constraints even
				'    before GPA made its run. And it means that Newton step moved us to
				'    the minimum subject to the present constraints.
				'    Depending on feasibility error, True or False is returned.
				'
				feaserr = 0
				For i = 0 To k - 1
					v = 0.0
					For i_ = 0 To nmain + nslack - 1
						v += ce(i, i_) * x(i_)
					Next
					v = v - ce(i, nmain + nslack)
					feaserr = feaserr + Math.sqr(v)
				Next
				feaserr = System.Math.sqrt(feaserr)
				If CDbl(feaserr) <= CDbl(epsi) Then
					itswithintolerance = itswithintolerance + 1
				Else
					itswithintolerance = 0
				End If
				If Not werechangesinconstraints OrElse itswithintolerance >= maxitswithintolerance Then
					result = CDbl(feaserr) <= CDbl(epsi)
					Return result
				End If
				itscount = itscount + 1
			End While
			Return result
		End Function


		'************************************************************************
'            This function checks that input derivatives are right. First it scales
'        parameters DF0 and DF1 from segment [A;B] to [0;1]. Then it builds Hermite
'        spline and derivative of it in 0.5. Search scale as Max(DF0,DF1, |F0-F1|).
'        Right derivative has to satisfy condition:
'            |H-F|/S<=0,01, |H'-F'|/S<=0,01.
'            
'        INPUT PARAMETERS:
'            F0  -   function's value in X-TestStep point;
'            DF0 -   derivative's value in X-TestStep point;
'            F1  -   function's value in X+TestStep point;
'            DF1 -   derivative's value in X+TestStep point;
'            F   -   testing function's value;
'            DF  -   testing derivative's value;
'           Width-   width of verification segment.
'
'        RESULT:
'            If input derivatives is right then function returns true, else 
'            function returns false.
'            
'          -- ALGLIB --
'             Copyright 29.05.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function derivativecheck(f0 As Double, df0 As Double, f1 As Double, df1 As Double, f As Double, df As Double, _
			width As Double) As Boolean
			Dim result As New Boolean()
			Dim s As Double = 0
			Dim h As Double = 0
			Dim dh As Double = 0

			df = width * df
			df0 = width * df0
			df1 = width * df1
			s = System.Math.Max(System.Math.Max(System.Math.Abs(df0), System.Math.Abs(df1)), System.Math.Abs(f1 - f0))
			h = 0.5 * f0 + 0.125 * df0 + 0.5 * f1 - 0.125 * df1
			dh = -(1.5 * f0) - 0.25 * df0 + 1.5 * f1 - 0.25 * df1
			If CDbl(s) <> CDbl(0) Then
				If CDbl(System.Math.Abs(h - f) / s) > CDbl(0.001) OrElse CDbl(System.Math.Abs(dh - df) / s) > CDbl(0.001) Then
					result = False
					Return result
				End If
			Else
				If CDbl(h - f) <> CDbl(0.0) OrElse CDbl(dh - df) <> CDbl(0.0) Then
					result = False
					Return result
				End If
			End If
			result = True
			Return result
		End Function


		'************************************************************************
'        Having quadratic target function
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        and its parabolic model along direction D
'
'            F(x0+alpha*D) = D2*alpha^2 + D1*alpha
'            
'        this function estimates numerical errors in the coefficients of the model.
'            
'        It is important that this  function  does  NOT calculate D1/D2  -  it only
'        estimates numerical errors introduced during evaluation and compares their
'        magnitudes against magnitudes of numerical errors. As result, one of three
'        outcomes is returned for each coefficient:
'            * "true" coefficient is almost surely positive
'            * "true" coefficient is almost surely negative
'            * numerical errors in coefficient are so large that it can not be
'              reliably distinguished from zero
'
'        INPUT PARAMETERS:
'            AbsASum -   SUM(|A[i,j]|)
'            AbsASum2-   SUM(A[i,j]^2)
'            MB      -   max(|B|)
'            MX      -   max(|X|)
'            MD      -   max(|D|)
'            D1      -   linear coefficient
'            D2      -   quadratic coefficient
'
'        OUTPUT PARAMETERS:
'            D1Est   -   estimate of D1 sign,  accounting  for  possible  numerical
'                        errors:
'                        * >0    means "almost surely positive" (D1>0 and large)
'                        * <0    means "almost surely negative" (D1<0 and large)
'                        * =0    means "pessimistic estimate  of  numerical  errors
'                                in D1 is larger than magnitude of D1 itself; it is
'                                impossible to reliably distinguish D1 from zero".
'            D2Est   -   estimate of D2 sign,  accounting  for  possible  numerical
'                        errors:
'                        * >0    means "almost surely positive" (D2>0 and large)
'                        * <0    means "almost surely negative" (D2<0 and large)
'                        * =0    means "pessimistic estimate  of  numerical  errors
'                                in D2 is larger than magnitude of D2 itself; it is
'                                impossible to reliably distinguish D2 from zero".
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub estimateparabolicmodel(absasum As Double, absasum2 As Double, mx As Double, mb As Double, md As Double, d1 As Double, _
			d2 As Double, ByRef d1est As Integer, ByRef d2est As Integer)
			Dim d1esterror As Double = 0
			Dim d2esterror As Double = 0
			Dim eps As Double = 0
			Dim e1 As Double = 0
			Dim e2 As Double = 0

			d1est = 0
			d2est = 0


			'
			' Error estimates:
			'
			' * error in D1=d'*(A*x+b) is estimated as
			'   ED1 = eps*MAX_ABS(D)*(MAX_ABS(X)*ENORM(A)+MAX_ABS(B))
			' * error in D2=0.5*d'*A*d is estimated as
			'   ED2 = eps*MAX_ABS(D)^2*ENORM(A)
			'
			' Here ENORM(A) is some pseudo-norm which reflects the way numerical
			' error accumulates during addition. Two ways of accumulation are
			' possible - worst case (errors always increase) and mean-case (errors
			' may cancel each other). We calculate geometrical average of both:
			' * ENORM_WORST(A) = SUM(|A[i,j]|)         error in N-term sum grows as O(N)
			' * ENORM_MEAN(A)  = SQRT(SUM(A[i,j]^2))   error in N-term sum grows as O(sqrt(N))
			' * ENORM(A)       = SQRT(ENORM_WORST(A),ENORM_MEAN(A))
			'
			eps = 4 * Math.machineepsilon
			e1 = eps * md * (mx * absasum + mb)
			e2 = eps * md * (mx * System.Math.sqrt(absasum2) + mb)
			d1esterror = System.Math.sqrt(e1 * e2)
			If CDbl(System.Math.Abs(d1)) <= CDbl(d1esterror) Then
				d1est = 0
			Else
				d1est = System.Math.Sign(d1)
			End If
			e1 = eps * md * md * absasum
			e2 = eps * md * md * System.Math.sqrt(absasum2)
			d2esterror = System.Math.sqrt(e1 * e2)
			If CDbl(System.Math.Abs(d2)) <= CDbl(d2esterror) Then
				d2est = 0
			Else
				d2est = System.Math.Sign(d2)
			End If
		End Sub


		'************************************************************************
'        This function calculates inexact rank-K preconditioner for Hessian  matrix
'        H=D+W'*C*W, where:
'        * H is a Hessian matrix, which is approximated by D/W/C
'        * D is a diagonal matrix with positive entries
'        * W is a rank-K correction
'        * C is a diagonal factor of rank-K correction
'
'        This preconditioner is inexact but fast - it requires O(N*K)  time  to  be
'        applied. Its main purpose - to be  used  in  barrier/penalty/AUL  methods,
'        where ill-conditioning is created by combination of two factors:
'        * simple bounds on variables => ill-conditioned D
'        * general barrier/penalty => correction W  with large coefficient C (makes
'          problem ill-conditioned) but W itself is well conditioned.
'
'        Preconditioner P is calculated by artificially constructing a set of  BFGS
'        updates which tries to reproduce behavior of H:
'        * Sk = Wk (k-th row of W)
'        * Yk = (D+Wk'*Ck*Wk)*Sk
'        * Yk/Sk are reordered by ascending of C[k]*norm(Wk)^2
'
'        Here we assume that rows of Wk are orthogonal or nearly orthogonal,  which
'        allows us to have O(N*K+K^2) update instead of O(N*K^2) one. Reordering of
'        updates is essential for having good performance on non-orthogonal problems
'        (updates which do not add much of curvature are added first,  and  updates
'        which add very large eigenvalues are added last and override effect of the
'        first updates).
'
'        On input this function takes direction S and components of H.
'        On output it returns inv(H)*S
'
'          -- ALGLIB --
'             Copyright 30.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub inexactlbfgspreconditioner(s As Double(), n As Integer, d As Double(), c As Double(), w As Double(,), k As Integer, _
			buf As precbuflbfgs)
			Dim idx As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim vx As Double = 0
			Dim vy As Double = 0
			Dim i_ As Integer = 0

			apserv.rvectorsetlengthatleast(buf.norms, k)
			apserv.rvectorsetlengthatleast(buf.alpha, k)
			apserv.rvectorsetlengthatleast(buf.rho, k)
			apserv.rmatrixsetlengthatleast(buf.yk, k, n)
			apserv.ivectorsetlengthatleast(buf.idx, k)

			'
			' Check inputs
			'
			For i = 0 To n - 1
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "InexactLBFGSPreconditioner: D[]<=0")
			Next
			For i = 0 To k - 1
				alglib.ap.assert(CDbl(c(i)) >= CDbl(0), "InexactLBFGSPreconditioner: C[]<0")
			Next

			'
			' Reorder linear terms according to increase of second derivative.
			' Fill Norms[] array.
			'
			For idx = 0 To k - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += w(idx, i_) * w(idx, i_)
				Next
				buf.norms(idx) = v * c(idx)
				buf.idx(idx) = idx
			Next
			tsort.tagsortfasti(buf.norms, buf.idx, buf.bufa, buf.bufb, k)

			'
			' Apply updates
			'
			For idx = 0 To k - 1

				'
				' Select update to perform (ordered by ascending of second derivative)
				'
				i = buf.idx(idx)

				'
				' Calculate YK and Rho
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += w(i, i_) * w(i, i_)
				Next
				v = v * c(i)
				For j = 0 To n - 1
					buf.yk(i, j) = (d(j) + v) * w(i, j)
				Next
				v = 0.0
				v0 = 0.0
				v1 = 0.0
				For j = 0 To n - 1
					vx = w(i, j)
					vy = buf.yk(i, j)
					v = v + vx * vy
					v0 = v0 + vx * vx
					v1 = v1 + vy * vy
				Next
				If (CDbl(v) > CDbl(0) AndAlso CDbl(v0 * v1) > CDbl(0)) AndAlso CDbl(v / System.Math.sqrt(v0 * v1)) > CDbl(n * 10 * Math.machineepsilon) Then
					buf.rho(i) = 1 / v
				Else
					buf.rho(i) = 0.0
				End If
			Next
			For idx = k - 1 To 0 Step -1

				'
				' Select update to perform (ordered by ascending of second derivative)
				'
				i = buf.idx(idx)

				'
				' Calculate Alpha[] according to L-BFGS algorithm
				' and update S[]
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += w(i, i_) * s(i_)
				Next
				v = buf.rho(i) * v
				buf.alpha(i) = v
				For i_ = 0 To n - 1
					s(i_) = s(i_) - v * buf.yk(i, i_)
				Next
			Next
			For j = 0 To n - 1
				s(j) = s(j) / d(j)
			Next
			For idx = 0 To k - 1

				'
				' Select update to perform (ordered by ascending of second derivative)
				'
				i = buf.idx(idx)

				'
				' Calculate Beta according to L-BFGS algorithm
				' and update S[]
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += buf.yk(i, i_) * s(i_)
				Next
				v = buf.alpha(i) - buf.rho(i) * v
				For i_ = 0 To n - 1
					s(i_) = s(i_) + v * w(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        This function prepares exact low-rank preconditioner  for  Hessian  matrix
'        H=D+W'*C*W, where:
'        * H is a Hessian matrix, which is approximated by D/W/C
'        * D is a diagonal matrix with positive entries
'        * W is a rank-K correction
'        * C is a diagonal factor of rank-K correction, positive semidefinite
'
'        This preconditioner is exact but relatively slow -  it  requires  O(N*K^2)
'        time to be prepared and O(N*K) time to be applied. It is  calculated  with
'        the help of Woodbury matrix identity.
'
'        It should be used as follows:
'        * PrepareLowRankPreconditioner() call PREPARES data structure
'        * subsequent calls to ApplyLowRankPreconditioner() APPLY preconditioner to
'          user-specified search direction.
'
'          -- ALGLIB --
'             Copyright 30.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub preparelowrankpreconditioner(d As Double(), c As Double(), w As Double(,), n As Integer, k As Integer, buf As precbuflowrank)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim b As New Boolean()


			'
			' Check inputs
			'
			alglib.ap.assert(n > 0, "PrepareLowRankPreconditioner: N<=0")
			alglib.ap.assert(k >= 0, "PrepareLowRankPreconditioner: N<=0")
			For i = 0 To n - 1
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "PrepareLowRankPreconditioner: D[]<=0")
			Next
			For i = 0 To k - 1
				alglib.ap.assert(CDbl(c(i)) >= CDbl(0), "PrepareLowRankPreconditioner: C[]<0")
			Next

			'
			' Prepare buffer structure; skip zero entries of update.
			'
			apserv.rvectorsetlengthatleast(buf.d, n)
			apserv.rmatrixsetlengthatleast(buf.v, k, n)
			apserv.rvectorsetlengthatleast(buf.bufc, k)
			apserv.rmatrixsetlengthatleast(buf.bufw, k + 1, n)
			buf.n = n
			buf.k = 0
			For i = 0 To k - 1

				'
				' Estimate magnitude of update row; skip zero rows (either W or C are zero)
				'
				v = 0.0
				For j = 0 To n - 1
					v = v + w(i, j) * w(i, j)
				Next
				v = v * c(i)
				If CDbl(v) = CDbl(0) Then
					Continue For
				End If
				alglib.ap.assert(CDbl(v) > CDbl(0), "PrepareLowRankPreconditioner: internal error")

				'
				' Copy non-zero update to buffer
				'
				buf.bufc(buf.k) = c(i)
				For j = 0 To n - 1
					buf.v(buf.k, j) = w(i, j)
					buf.bufw(buf.k, j) = w(i, j)
				Next
				apserv.inc(buf.k)
			Next

			'
			' Reset K (for convenience)
			'
			k = buf.k

			'
			' Prepare diagonal factor; quick exit for K=0
			'
			For i = 0 To n - 1
				buf.d(i) = 1 / d(i)
			Next
			If k = 0 Then
				Return
			End If

			'
			' Use Woodbury matrix identity
			'
			apserv.rmatrixsetlengthatleast(buf.bufz, k, k)
			For i = 0 To k - 1
				For j = 0 To k - 1
					buf.bufz(i, j) = 0.0
				Next
			Next
			For i = 0 To k - 1
				buf.bufz(i, i) = 1 / buf.bufc(i)
			Next
			For j = 0 To n - 1
				buf.bufw(k, j) = 1 / System.Math.sqrt(d(j))
			Next
			For i = 0 To k - 1
				For j = 0 To n - 1
					buf.bufw(i, j) = buf.bufw(i, j) * buf.bufw(k, j)
				Next
			Next
			ablas.rmatrixgemm(k, k, n, 1.0, buf.bufw, 0, _
				0, 0, buf.bufw, 0, 0, 1, _
				1.0, buf.bufz, 0, 0)
			b = trfac.spdmatrixcholeskyrec(buf.bufz, 0, k, True, buf.tmp)
			alglib.ap.assert(b, "PrepareLowRankPreconditioner: internal error (Cholesky failure)")
			ablas.rmatrixlefttrsm(k, n, buf.bufz, 0, 0, True, _
				False, 1, buf.v, 0, 0)
			For i = 0 To k - 1
				For j = 0 To n - 1
					buf.v(i, j) = buf.v(i, j) * buf.d(j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function apply exact low-rank preconditioner prepared by
'        PrepareLowRankPreconditioner function (see its comments for more information).
'
'          -- ALGLIB --
'             Copyright 30.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub applylowrankpreconditioner(s As Double(), buf As precbuflowrank)
			Dim n As Integer = 0
			Dim k As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0

			n = buf.n
			k = buf.k
			apserv.rvectorsetlengthatleast(buf.tmp, n)
			For j = 0 To n - 1
				buf.tmp(j) = buf.d(j) * s(j)
			Next
			For i = 0 To k - 1
				v = 0.0
				For j = 0 To n - 1
					v = v + buf.v(i, j) * s(j)
				Next
				For j = 0 To n - 1
					buf.tmp(j) = buf.tmp(j) - v * buf.v(i, j)
				Next
			Next
			For i = 0 To n - 1
				s(i) = buf.tmp(i)
			Next
		End Sub


	End Class
	Public Class cqmodels
		'************************************************************************
'        This structure describes convex quadratic model of the form:
'            f(x) = 0.5*(Alpha*x'*A*x + Tau*x'*D*x) + 0.5*Theta*(Q*x-r)'*(Q*x-r) + b'*x
'        where:
'            * Alpha>=0, Tau>=0, Theta>=0, Alpha+Tau>0.
'            * A is NxN matrix, Q is NxK matrix (N>=1, K>=0), b is Nx1 vector,
'              D is NxN diagonal matrix.
'            * "main" quadratic term Alpha*A+Lambda*D is symmetric
'              positive definite
'        Structure may contain optional equality constraints of the form x[i]=x0[i],
'        in this case functions provided by this unit calculate Newton step subject
'        to these equality constraints.
'        ************************************************************************

		Public Class convexquadraticmodel
			Inherits apobject
			Public n As Integer
			Public k As Integer
			Public alpha As Double
			Public tau As Double
			Public theta As Double
			Public a As Double(,)
			Public q As Double(,)
			Public b As Double()
			Public r As Double()
			Public xc As Double()
			Public d As Double()
			Public activeset As Boolean()
			Public tq2dense As Double(,)
			Public tk2 As Double(,)
			Public tq2diag As Double()
			Public tq1 As Double()
			Public tk1 As Double()
			Public tq0 As Double
			Public tk0 As Double
			Public txc As Double()
			Public tb As Double()
			Public nfree As Integer
			Public ecakind As Integer
			Public ecadense As Double(,)
			Public eq As Double(,)
			Public eccm As Double(,)
			Public ecadiag As Double()
			Public eb As Double()
			Public ec As Double
			Public tmp0 As Double()
			Public tmp1 As Double()
			Public tmpg As Double()
			Public tmp2 As Double(,)
			Public ismaintermchanged As Boolean
			Public issecondarytermchanged As Boolean
			Public islineartermchanged As Boolean
			Public isactivesetchanged As Boolean
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				a = New Double(-1, -1) {}
				q = New Double(-1, -1) {}
				b = New Double(-1) {}
				r = New Double(-1) {}
				xc = New Double(-1) {}
				d = New Double(-1) {}
				activeset = New Boolean(-1) {}
				tq2dense = New Double(-1, -1) {}
				tk2 = New Double(-1, -1) {}
				tq2diag = New Double(-1) {}
				tq1 = New Double(-1) {}
				tk1 = New Double(-1) {}
				txc = New Double(-1) {}
				tb = New Double(-1) {}
				ecadense = New Double(-1, -1) {}
				eq = New Double(-1, -1) {}
				eccm = New Double(-1, -1) {}
				ecadiag = New Double(-1) {}
				eb = New Double(-1) {}
				tmp0 = New Double(-1) {}
				tmp1 = New Double(-1) {}
				tmpg = New Double(-1) {}
				tmp2 = New Double(-1, -1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New convexquadraticmodel()
				_result.n = n
				_result.k = k
				_result.alpha = alpha
				_result.tau = tau
				_result.theta = theta
				_result.a = DirectCast(a.Clone(), Double(,))
				_result.q = DirectCast(q.Clone(), Double(,))
				_result.b = DirectCast(b.Clone(), Double())
				_result.r = DirectCast(r.Clone(), Double())
				_result.xc = DirectCast(xc.Clone(), Double())
				_result.d = DirectCast(d.Clone(), Double())
				_result.activeset = DirectCast(activeset.Clone(), Boolean())
				_result.tq2dense = DirectCast(tq2dense.Clone(), Double(,))
				_result.tk2 = DirectCast(tk2.Clone(), Double(,))
				_result.tq2diag = DirectCast(tq2diag.Clone(), Double())
				_result.tq1 = DirectCast(tq1.Clone(), Double())
				_result.tk1 = DirectCast(tk1.Clone(), Double())
				_result.tq0 = tq0
				_result.tk0 = tk0
				_result.txc = DirectCast(txc.Clone(), Double())
				_result.tb = DirectCast(tb.Clone(), Double())
				_result.nfree = nfree
				_result.ecakind = ecakind
				_result.ecadense = DirectCast(ecadense.Clone(), Double(,))
				_result.eq = DirectCast(eq.Clone(), Double(,))
				_result.eccm = DirectCast(eccm.Clone(), Double(,))
				_result.ecadiag = DirectCast(ecadiag.Clone(), Double())
				_result.eb = DirectCast(eb.Clone(), Double())
				_result.ec = ec
				_result.tmp0 = DirectCast(tmp0.Clone(), Double())
				_result.tmp1 = DirectCast(tmp1.Clone(), Double())
				_result.tmpg = DirectCast(tmpg.Clone(), Double())
				_result.tmp2 = DirectCast(tmp2.Clone(), Double(,))
				_result.ismaintermchanged = ismaintermchanged
				_result.issecondarytermchanged = issecondarytermchanged
				_result.islineartermchanged = islineartermchanged
				_result.isactivesetchanged = isactivesetchanged
				Return _result
			End Function
		End Class




		Public Const newtonrefinementits As Integer = 3


		'************************************************************************
'        This subroutine is used to initialize CQM. By default, empty NxN model  is
'        generated, with Alpha=Lambda=Theta=0.0 and zero b.
'
'        Previously allocated buffer variables are reused as much as possible.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqminit(n As Integer, s As convexquadraticmodel)
			Dim i As Integer = 0

			s.n = n
			s.k = 0
			s.nfree = n
			s.ecakind = -1
			s.alpha = 0.0
			s.tau = 0.0
			s.theta = 0.0
			s.ismaintermchanged = True
			s.issecondarytermchanged = True
			s.islineartermchanged = True
			s.isactivesetchanged = True
			apserv.bvectorsetlengthatleast(s.activeset, n)
			apserv.rvectorsetlengthatleast(s.xc, n)
			apserv.rvectorsetlengthatleast(s.eb, n)
			apserv.rvectorsetlengthatleast(s.tq1, n)
			apserv.rvectorsetlengthatleast(s.txc, n)
			apserv.rvectorsetlengthatleast(s.tb, n)
			apserv.rvectorsetlengthatleast(s.b, s.n)
			apserv.rvectorsetlengthatleast(s.tk1, s.n)
			For i = 0 To n - 1
				s.activeset(i) = False
				s.xc(i) = 0.0
				s.b(i) = 0.0
			Next
		End Sub


		'************************************************************************
'        This subroutine changes main quadratic term of the model.
'
'        INPUT PARAMETERS:
'            S       -   model
'            A       -   NxN matrix, only upper or lower triangle is referenced
'            IsUpper -   True, when matrix is stored in upper triangle
'            Alpha   -   multiplier; when Alpha=0, A is not referenced at all
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmseta(s As convexquadraticmodel, a As Double(,), isupper As Boolean, alpha As Double)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0

			alglib.ap.assert(Math.isfinite(alpha) AndAlso CDbl(alpha) >= CDbl(0), "CQMSetA: Alpha<0 or is not finite number")
			alglib.ap.assert(CDbl(alpha) = CDbl(0) OrElse apserv.isfinitertrmatrix(a, s.n, isupper), "CQMSetA: A is not finite NxN matrix")
			s.alpha = alpha
			If CDbl(alpha) > CDbl(0) Then
				apserv.rmatrixsetlengthatleast(s.a, s.n, s.n)
				apserv.rmatrixsetlengthatleast(s.ecadense, s.n, s.n)
				apserv.rmatrixsetlengthatleast(s.tq2dense, s.n, s.n)
				For i = 0 To s.n - 1
					For j = i To s.n - 1
						If isupper Then
							v = a(i, j)
						Else
							v = a(j, i)
						End If
						s.a(i, j) = v
						s.a(j, i) = v
					Next
				Next
			End If
			s.ismaintermchanged = True
		End Sub


		'************************************************************************
'        This subroutine changes main quadratic term of the model.
'
'        INPUT PARAMETERS:
'            S       -   model
'            A       -   possibly preallocated buffer
'            
'        OUTPUT PARAMETERS:
'            A       -   NxN matrix, full matrix is returned.
'                        Zero matrix is returned if model is empty.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmgeta(s As convexquadraticmodel, ByRef a As Double(,))
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim n As Integer = 0

			n = s.n
			apserv.rmatrixsetlengthatleast(a, n, n)
			If CDbl(s.alpha) > CDbl(0) Then
				v = s.alpha
				For i = 0 To n - 1
					For j = 0 To n - 1
						a(i, j) = v * s.a(i, j)
					Next
				Next
			Else
				For i = 0 To n - 1
					For j = 0 To n - 1
						a(i, j) = 0.0
					Next
				Next
			End If
		End Sub


		'************************************************************************
'        This subroutine rewrites diagonal of the main quadratic term of the  model
'        (dense  A)  by  vector  Z/Alpha (current value of the Alpha coefficient is
'        used).
'
'        IMPORTANT: in  case  model  has  no  dense  quadratic  term, this function
'                   allocates N*N dense matrix of zeros, and fills its diagonal  by
'                   non-zero values.
'
'        INPUT PARAMETERS:
'            S       -   model
'            Z       -   new diagonal, array[N]
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmrewritedensediagonal(s As convexquadraticmodel, z As Double())
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			n = s.n
			If CDbl(s.alpha) = CDbl(0) Then
				apserv.rmatrixsetlengthatleast(s.a, s.n, s.n)
				apserv.rmatrixsetlengthatleast(s.ecadense, s.n, s.n)
				apserv.rmatrixsetlengthatleast(s.tq2dense, s.n, s.n)
				For i = 0 To n - 1
					For j = 0 To n - 1
						s.a(i, j) = 0.0
					Next
				Next
				s.alpha = 1.0
			End If
			For i = 0 To s.n - 1
				s.a(i, i) = z(i) / s.alpha
			Next
			s.ismaintermchanged = True
		End Sub


		'************************************************************************
'        This subroutine changes diagonal quadratic term of the model.
'
'        INPUT PARAMETERS:
'            S       -   model
'            D       -   array[N], semidefinite diagonal matrix
'            Tau     -   multiplier; when Tau=0, D is not referenced at all
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmsetd(s As convexquadraticmodel, d As Double(), tau As Double)
			Dim i As Integer = 0

			alglib.ap.assert(Math.isfinite(tau) AndAlso CDbl(tau) >= CDbl(0), "CQMSetD: Tau<0 or is not finite number")
			alglib.ap.assert(CDbl(tau) = CDbl(0) OrElse apserv.isfinitevector(d, s.n), "CQMSetD: D is not finite Nx1 vector")
			s.tau = tau
			If CDbl(tau) > CDbl(0) Then
				apserv.rvectorsetlengthatleast(s.d, s.n)
				apserv.rvectorsetlengthatleast(s.ecadiag, s.n)
				apserv.rvectorsetlengthatleast(s.tq2diag, s.n)
				For i = 0 To s.n - 1
					alglib.ap.assert(CDbl(d(i)) >= CDbl(0), "CQMSetD: D[i]<0")
					s.d(i) = d(i)
				Next
			End If
			s.ismaintermchanged = True
		End Sub


		'************************************************************************
'        This subroutine drops main quadratic term A from the model. It is same  as
'        call  to  CQMSetA()  with  zero  A,   but gives better performance because
'        algorithm  knows  that  matrix  is  zero  and  can  optimize    subsequent
'        calculations.
'
'        INPUT PARAMETERS:
'            S       -   model
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmdropa(s As convexquadraticmodel)
			s.alpha = 0.0
			s.ismaintermchanged = True
		End Sub


		'************************************************************************
'        This subroutine changes linear term of the model
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmsetb(s As convexquadraticmodel, b As Double())
			Dim i As Integer = 0

			alglib.ap.assert(apserv.isfinitevector(b, s.n), "CQMSetB: B is not finite vector")
			apserv.rvectorsetlengthatleast(s.b, s.n)
			For i = 0 To s.n - 1
				s.b(i) = b(i)
			Next
			s.islineartermchanged = True
		End Sub


		'************************************************************************
'        This subroutine changes linear term of the model
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmsetq(s As convexquadraticmodel, q As Double(,), r As Double(), k As Integer, theta As Double)
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(k >= 0, "CQMSetQ: K<0")
			alglib.ap.assert((k = 0 OrElse CDbl(theta) = CDbl(0)) OrElse apserv.apservisfinitematrix(q, k, s.n), "CQMSetQ: Q is not finite matrix")
			alglib.ap.assert((k = 0 OrElse CDbl(theta) = CDbl(0)) OrElse apserv.isfinitevector(r, k), "CQMSetQ: R is not finite vector")
			alglib.ap.assert(Math.isfinite(theta) AndAlso CDbl(theta) >= CDbl(0), "CQMSetQ: Theta<0 or is not finite number")

			'
			' degenerate case: K=0 or Theta=0
			'
			If k = 0 OrElse CDbl(theta) = CDbl(0) Then
				s.k = 0
				s.theta = 0
				s.issecondarytermchanged = True
				Return
			End If

			'
			' General case: both Theta>0 and K>0
			'
			s.k = k
			s.theta = theta
			apserv.rmatrixsetlengthatleast(s.q, s.k, s.n)
			apserv.rvectorsetlengthatleast(s.r, s.k)
			apserv.rmatrixsetlengthatleast(s.eq, s.k, s.n)
			apserv.rmatrixsetlengthatleast(s.eccm, s.k, s.k)
			apserv.rmatrixsetlengthatleast(s.tk2, s.k, s.n)
			For i = 0 To s.k - 1
				For j = 0 To s.n - 1
					s.q(i, j) = q(i, j)
				Next
				s.r(i) = r(i)
			Next
			s.issecondarytermchanged = True
		End Sub


		'************************************************************************
'        This subroutine changes active set
'
'        INPUT PARAMETERS
'            S       -   model
'            X       -   array[N], constraint values
'            ActiveSet-  array[N], active set. If ActiveSet[I]=True, then I-th
'                        variables is constrained to X[I].
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmsetactiveset(s As convexquadraticmodel, x As Double(), activeset As Boolean())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(x) >= s.n, "CQMSetActiveSet: Length(X)<N")
			alglib.ap.assert(alglib.ap.len(activeset) >= s.n, "CQMSetActiveSet: Length(ActiveSet)<N")
			For i = 0 To s.n - 1
				s.isactivesetchanged = s.isactivesetchanged OrElse (s.activeset(i) AndAlso Not activeset(i))
				s.isactivesetchanged = s.isactivesetchanged OrElse (activeset(i) AndAlso Not s.activeset(i))
				s.activeset(i) = activeset(i)
				If activeset(i) Then
					alglib.ap.assert(Math.isfinite(x(i)), "CQMSetActiveSet: X[] contains infinite constraints")
					s.isactivesetchanged = s.isactivesetchanged OrElse CDbl(s.xc(i)) <> CDbl(x(i))
					s.xc(i) = x(i)
				End If
			Next
		End Sub


		'************************************************************************
'        This subroutine evaluates model at X. Active constraints are ignored.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function cqmeval(s As convexquadraticmodel, x As Double()) As Double
			Dim result As Double = 0
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMEval: X is not finite vector")
			result = 0.0

			'
			' main quadratic term
			'
			If CDbl(s.alpha) > CDbl(0) Then
				For i = 0 To n - 1
					For j = 0 To n - 1
						result = result + s.alpha * 0.5 * x(i) * s.a(i, j) * x(j)
					Next
				Next
			End If
			If CDbl(s.tau) > CDbl(0) Then
				For i = 0 To n - 1
					result = result + 0.5 * Math.sqr(x(i)) * s.tau * s.d(i)
				Next
			End If

			'
			' secondary quadratic term
			'
			If CDbl(s.theta) > CDbl(0) Then
				For i = 0 To s.k - 1
					v = 0.0
					For i_ = 0 To n - 1
						v += s.q(i, i_) * x(i_)
					Next
					result = result + 0.5 * s.theta * Math.sqr(v - s.r(i))
				Next
			End If

			'
			' linear term
			'
			For i = 0 To s.n - 1
				result = result + x(i) * s.b(i)
			Next
			Return result
		End Function


		'************************************************************************
'        This subroutine evaluates model at X. Active constraints are ignored.
'        It returns:
'            R   -   model value
'            Noise-  estimate of the numerical noise in data
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmevalx(s As convexquadraticmodel, x As Double(), ByRef r As Double, ByRef noise As Double)
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim v2 As Double = 0
			Dim mxq As Double = 0
			Dim eps As Double = 0

			r = 0
			noise = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMEval: X is not finite vector")
			r = 0.0
			noise = 0.0
			eps = 2 * Math.machineepsilon
			mxq = 0.0

			'
			' Main quadratic term.
			'
			' Noise from the main quadratic term is equal to the
			' maximum summand in the term.
			'
			If CDbl(s.alpha) > CDbl(0) Then
				For i = 0 To n - 1
					For j = 0 To n - 1
						v = s.alpha * 0.5 * x(i) * s.a(i, j) * x(j)
						r = r + v
						noise = System.Math.Max(noise, eps * System.Math.Abs(v))
					Next
				Next
			End If
			If CDbl(s.tau) > CDbl(0) Then
				For i = 0 To n - 1
					v = 0.5 * Math.sqr(x(i)) * s.tau * s.d(i)
					r = r + v
					noise = System.Math.Max(noise, eps * System.Math.Abs(v))
				Next
			End If

			'
			' secondary quadratic term
			'
			' Noise from the secondary quadratic term is estimated as follows:
			' * noise in qi*x-r[i] is estimated as
			'   Eps*MXQ = Eps*max(|r[i]|, |q[i,j]*x[j]|)
			' * noise in (qi*x-r[i])^2 is estimated as
			'   NOISE = (|qi*x-r[i]|+Eps*MXQ)^2-(|qi*x-r[i]|)^2
			'         = Eps*MXQ*(2*|qi*x-r[i]|+Eps*MXQ)
			'
			If CDbl(s.theta) > CDbl(0) Then
				For i = 0 To s.k - 1
					v = 0.0
					mxq = System.Math.Abs(s.r(i))
					For j = 0 To n - 1
						v2 = s.q(i, j) * x(j)
						v = v + v2
						mxq = System.Math.Max(mxq, System.Math.Abs(v2))
					Next
					r = r + 0.5 * s.theta * Math.sqr(v - s.r(i))
					noise = System.Math.Max(noise, eps * mxq * (2 * System.Math.Abs(v - s.r(i)) + eps * mxq))
				Next
			End If

			'
			' linear term
			'
			For i = 0 To s.n - 1
				r = r + x(i) * s.b(i)
				noise = System.Math.Max(noise, eps * System.Math.Abs(x(i) * s.b(i)))
			Next

			'
			' Final update of the noise
			'
			noise = n * noise
		End Sub


		'************************************************************************
'        This  subroutine  evaluates  gradient of the model; active constraints are
'        ignored.
'
'        INPUT PARAMETERS:
'            S       -   convex model
'            X       -   point, array[N]
'            G       -   possibly preallocated buffer; resized, if too small
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmgradunconstrained(s As convexquadraticmodel, x As Double(), ByRef g As Double())
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMEvalGradUnconstrained: X is not finite vector")
			apserv.rvectorsetlengthatleast(g, n)
			For i = 0 To n - 1
				g(i) = 0
			Next

			'
			' main quadratic term
			'
			If CDbl(s.alpha) > CDbl(0) Then
				For i = 0 To n - 1
					v = 0.0
					For j = 0 To n - 1
						v = v + s.alpha * s.a(i, j) * x(j)
					Next
					g(i) = g(i) + v
				Next
			End If
			If CDbl(s.tau) > CDbl(0) Then
				For i = 0 To n - 1
					g(i) = g(i) + x(i) * s.tau * s.d(i)
				Next
			End If

			'
			' secondary quadratic term
			'
			If CDbl(s.theta) > CDbl(0) Then
				For i = 0 To s.k - 1
					v = 0.0
					For i_ = 0 To n - 1
						v += s.q(i, i_) * x(i_)
					Next
					v = s.theta * (v - s.r(i))
					For i_ = 0 To n - 1
						g(i_) = g(i_) + v * s.q(i, i_)
					Next
				Next
			End If

			'
			' linear term
			'
			For i = 0 To n - 1
				g(i) = g(i) + s.b(i)
			Next
		End Sub


		'************************************************************************
'        This subroutine evaluates x'*(0.5*alpha*A+tau*D)*x
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function cqmxtadx2(s As convexquadraticmodel, x As Double()) As Double
			Dim result As Double = 0
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMEval: X is not finite vector")
			result = 0.0

			'
			' main quadratic term
			'
			If CDbl(s.alpha) > CDbl(0) Then
				For i = 0 To n - 1
					For j = 0 To n - 1
						result = result + s.alpha * 0.5 * x(i) * s.a(i, j) * x(j)
					Next
				Next
			End If
			If CDbl(s.tau) > CDbl(0) Then
				For i = 0 To n - 1
					result = result + 0.5 * Math.sqr(x(i)) * s.tau * s.d(i)
				Next
			End If
			Return result
		End Function


		'************************************************************************
'        This subroutine evaluates (0.5*alpha*A+tau*D)*x
'
'        Y is automatically resized if needed
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmadx(s As convexquadraticmodel, x As Double(), ByRef y As Double())
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMEval: X is not finite vector")
			apserv.rvectorsetlengthatleast(y, n)

			'
			' main quadratic term
			'
			For i = 0 To n - 1
				y(i) = 0
			Next
			If CDbl(s.alpha) > CDbl(0) Then
				For i = 0 To n - 1
					v = 0.0
					For i_ = 0 To n - 1
						v += s.a(i, i_) * x(i_)
					Next
					y(i) = y(i) + s.alpha * v
				Next
			End If
			If CDbl(s.tau) > CDbl(0) Then
				For i = 0 To n - 1
					y(i) = y(i) + x(i) * s.tau * s.d(i)
				Next
			End If
		End Sub


		'************************************************************************
'        This subroutine finds optimum of the model. It returns  False  on  failure
'        (indefinite/semidefinite matrix).  Optimum  is  found  subject  to  active
'        constraints.
'
'        INPUT PARAMETERS
'            S       -   model
'            X       -   possibly preallocated buffer; automatically resized, if
'                        too small enough.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function cqmconstrainedoptimum(s As convexquadraticmodel, ByRef x As Double()) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim k As Integer = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim cidx0 As Integer = 0
			Dim itidx As Integer = 0
			Dim i_ As Integer = 0


			'
			' Rebuild internal structures
			'
			If Not cqmrebuild(s) Then
				result = False
				Return result
			End If
			n = s.n
			k = s.k
			nfree = s.nfree
			result = True

			'
			' Calculate initial point for the iterative refinement:
			' * free components are set to zero
			' * constrained components are set to their constrained values
			'
			apserv.rvectorsetlengthatleast(x, n)
			For i = 0 To n - 1
				If s.activeset(i) Then
					x(i) = s.xc(i)
				Else
					x(i) = 0
				End If
			Next

			'
			' Iterative refinement.
			'
			' In an ideal world without numerical errors it would be enough
			' to make just one Newton step from initial point:
			'   x_new = -H^(-1)*grad(x=0)
			' However, roundoff errors can significantly deteriorate quality
			' of the solution. So we have to recalculate gradient and to
			' perform Newton steps several times.
			'
			' Below we perform fixed number of Newton iterations.
			'
			For itidx = 0 To newtonrefinementits - 1

				'
				' Calculate gradient at the current point.
				' Move free components of the gradient in the beginning.
				'
				cqmgradunconstrained(s, x, s.tmpg)
				cidx0 = 0
				For i = 0 To n - 1
					If Not s.activeset(i) Then
						s.tmpg(cidx0) = s.tmpg(i)
						cidx0 = cidx0 + 1
					End If
				Next

				'
				' Free components of the extrema are calculated in the first NFree elements of TXC.
				'
				' First, we have to calculate original Newton step, without rank-K perturbations
				'
				For i_ = 0 To nfree - 1
					s.txc(i_) = -s.tmpg(i_)
				Next
				cqmsolveea(s, s.txc, s.tmp0)

				'
				' Then, we account for rank-K correction.
				' Woodbury matrix identity is used.
				'
				If s.k > 0 AndAlso CDbl(s.theta) > CDbl(0) Then
					apserv.rvectorsetlengthatleast(s.tmp0, System.Math.Max(nfree, k))
					apserv.rvectorsetlengthatleast(s.tmp1, System.Math.Max(nfree, k))
					For i_ = 0 To nfree - 1
						s.tmp1(i_) = -s.tmpg(i_)
					Next
					cqmsolveea(s, s.tmp1, s.tmp0)
					For i = 0 To k - 1
						v = 0.0
						For i_ = 0 To nfree - 1
							v += s.eq(i, i_) * s.tmp1(i_)
						Next
						s.tmp0(i) = v
					Next
					fbls.fblscholeskysolve(s.eccm, 1.0, k, True, s.tmp0, s.tmp1)
					For i = 0 To nfree - 1
						s.tmp1(i) = 0.0
					Next
					For i = 0 To k - 1
						v = s.tmp0(i)
						For i_ = 0 To nfree - 1
							s.tmp1(i_) = s.tmp1(i_) + v * s.eq(i, i_)
						Next
					Next
					cqmsolveea(s, s.tmp1, s.tmp0)
					For i_ = 0 To nfree - 1
						s.txc(i_) = s.txc(i_) - s.tmp1(i_)
					Next
				End If

				'
				' Unpack components from TXC into X. We pass through all
				' free components of X and add our step.
				'
				cidx0 = 0
				For i = 0 To n - 1
					If Not s.activeset(i) Then
						x(i) = x(i) + s.txc(cidx0)
						cidx0 = cidx0 + 1
					End If
				Next
			Next
			Return result
		End Function


		'************************************************************************
'        This function scales vector  by  multiplying it by inverse of the diagonal
'        of the Hessian matrix. It should be used to  accelerate  steepest  descent
'        phase of the QP solver.
'
'        Although  it  is  called  "scale-grad",  it  can be called for any vector,
'        whether it is gradient, anti-gradient, or just some vector.
'
'        This function does NOT takes into account current set of  constraints,  it
'        just performs matrix-vector multiplication  without  taking  into  account
'        constraints.
'
'        INPUT PARAMETERS:
'            S       -   model
'            X       -   vector to scale
'
'        OUTPUT PARAMETERS:
'            X       -   scaled vector
'            
'        NOTE:
'            when called for non-SPD matrices, it silently skips components of X
'            which correspond to zero or negative diagonal elements.
'            
'        NOTE:
'            this function uses diagonals of A and D; it ignores Q - rank-K term of
'            the quadratic model.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub cqmscalevector(s As convexquadraticmodel, ByRef x As Double())
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim v As Double = 0

			n = s.n
			For i = 0 To n - 1
				v = 0.0
				If CDbl(s.alpha) > CDbl(0) Then
					v = v + s.a(i, i)
				End If
				If CDbl(s.tau) > CDbl(0) Then
					v = v + s.d(i)
				End If
				If CDbl(v) > CDbl(0) Then
					x(i) = x(i) / v
				End If
			Next
		End Sub


		'************************************************************************
'        This subroutine calls CQMRebuild() and evaluates model at X subject to
'        active constraints.
'
'        It  is  intended  for  debug  purposes only, because it evaluates model by
'        means of temporaries, which were calculated  by  CQMRebuild().  The   only
'        purpose of this function  is  to  check  correctness  of  CQMRebuild()  by
'        comparing results of this function with ones obtained by CQMEval(),  which
'        is  used  as  reference  point. The  idea is that significant deviation in
'        results  of  these  two  functions  is  evidence  of  some  error  in  the
'        CQMRebuild().
'
'        NOTE: suffix T denotes that temporaries marked by T-prefix are used. There
'              is one more variant of this function, which uses  "effective"  model
'              built by CQMRebuild().
'
'        NOTE2: in case CQMRebuild() fails (due to model non-convexity), this
'              function returns NAN.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function cqmdebugconstrainedevalt(s As convexquadraticmodel, x As Double()) As Double
			Dim result As Double = 0
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMDebugConstrainedEvalT: X is not finite vector")
			If Not cqmrebuild(s) Then
				result = [Double].NaN
				Return result
			End If
			result = 0.0
			nfree = s.nfree

			'
			' Reorder variables
			'
			j = 0
			For i = 0 To n - 1
				If Not s.activeset(i) Then
					alglib.ap.assert(j < nfree, "CQMDebugConstrainedEvalT: internal error")
					s.txc(j) = x(i)
					j = j + 1
				End If
			Next

			'
			' TQ2, TQ1, TQ0
			'
			'
			If CDbl(s.alpha) > CDbl(0) Then

				'
				' Dense TQ2
				'
				For i = 0 To nfree - 1
					For j = 0 To nfree - 1
						result = result + 0.5 * s.txc(i) * s.tq2dense(i, j) * s.txc(j)
					Next
				Next
			Else

				'
				' Diagonal TQ2
				'
				For i = 0 To nfree - 1
					result = result + 0.5 * s.tq2diag(i) * Math.sqr(s.txc(i))
				Next
			End If
			For i = 0 To nfree - 1
				result = result + s.tq1(i) * s.txc(i)
			Next
			result = result + s.tq0

			'
			' TK2, TK1, TK0
			'
			If s.k > 0 AndAlso CDbl(s.theta) > CDbl(0) Then
				For i = 0 To s.k - 1
					v = 0
					For j = 0 To nfree - 1
						v = v + s.tk2(i, j) * s.txc(j)
					Next
					result = result + 0.5 * Math.sqr(v)
				Next
				For i = 0 To nfree - 1
					result = result + s.tk1(i) * s.txc(i)
				Next
				result = result + s.tk0
			End If

			'
			' TB (Bf and Bc parts)
			'
			For i = 0 To n - 1
				result = result + s.tb(i) * s.txc(i)
			Next
			Return result
		End Function


		'************************************************************************
'        This subroutine calls CQMRebuild() and evaluates model at X subject to
'        active constraints.
'
'        It  is  intended  for  debug  purposes only, because it evaluates model by
'        means of "effective" matrices built by CQMRebuild(). The only  purpose  of
'        this function is to check correctness of CQMRebuild() by comparing results
'        of this function with  ones  obtained  by  CQMEval(),  which  is  used  as
'        reference  point.  The  idea  is  that significant deviation in results of
'        these two functions is evidence of some error in the CQMRebuild().
'
'        NOTE: suffix E denotes that effective matrices. There is one more  variant
'              of this function, which uses temporary matrices built by
'              CQMRebuild().
'
'        NOTE2: in case CQMRebuild() fails (due to model non-convexity), this
'              function returns NAN.
'
'          -- ALGLIB --
'             Copyright 12.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function cqmdebugconstrainedevale(s As convexquadraticmodel, x As Double()) As Double
			Dim result As Double = 0
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0

			n = s.n
			alglib.ap.assert(apserv.isfinitevector(x, n), "CQMDebugConstrainedEvalE: X is not finite vector")
			If Not cqmrebuild(s) Then
				result = [Double].NaN
				Return result
			End If
			result = 0.0
			nfree = s.nfree

			'
			' Reorder variables
			'
			j = 0
			For i = 0 To n - 1
				If Not s.activeset(i) Then
					alglib.ap.assert(j < nfree, "CQMDebugConstrainedEvalE: internal error")
					s.txc(j) = x(i)
					j = j + 1
				End If
			Next

			'
			' ECA
			'
			alglib.ap.assert((s.ecakind = 0 OrElse s.ecakind = 1) OrElse (s.ecakind = -1 AndAlso nfree = 0), "CQMDebugConstrainedEvalE: unexpected ECAKind")
			If s.ecakind = 0 Then

				'
				' Dense ECA
				'
				For i = 0 To nfree - 1
					v = 0.0
					For j = i To nfree - 1
						v = v + s.ecadense(i, j) * s.txc(j)
					Next
					result = result + 0.5 * Math.sqr(v)
				Next
			End If
			If s.ecakind = 1 Then

				'
				' Diagonal ECA
				'
				For i = 0 To nfree - 1
					result = result + 0.5 * Math.sqr(s.ecadiag(i) * s.txc(i))
				Next
			End If

			'
			' EQ
			'
			For i = 0 To s.k - 1
				v = 0.0
				For j = 0 To nfree - 1
					v = v + s.eq(i, j) * s.txc(j)
				Next
				result = result + 0.5 * Math.sqr(v)
			Next

			'
			' EB
			'
			For i = 0 To nfree - 1
				result = result + s.eb(i) * s.txc(i)
			Next

			'
			' EC
			'
			result = result + s.ec
			Return result
		End Function


		'************************************************************************
'        Internal function, rebuilds "effective" model subject to constraints.
'        Returns False on failure (non-SPD main quadratic term)
'
'          -- ALGLIB --
'             Copyright 10.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function cqmrebuild(s As convexquadraticmodel) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim k As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ridx0 As Integer = 0
			Dim ridx1 As Integer = 0
			Dim cidx0 As Integer = 0
			Dim cidx1 As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			If CDbl(s.alpha) = CDbl(0) AndAlso CDbl(s.tau) = CDbl(0) Then

				'
				' Non-SPD model, quick exit
				'
				result = False
				Return result
			End If
			result = True
			n = s.n
			k = s.k

			'
			' Determine number of free variables.
			' Fill TXC - array whose last N-NFree elements store constraints.
			'
			If s.isactivesetchanged Then
				s.nfree = 0
				For i = 0 To n - 1
					If Not s.activeset(i) Then
						s.nfree = s.nfree + 1
					End If
				Next
				j = s.nfree
				For i = 0 To n - 1
					If s.activeset(i) Then
						s.txc(j) = s.xc(i)
						j = j + 1
					End If
				Next
			End If
			nfree = s.nfree

			'
			' Re-evaluate TQ2/TQ1/TQ0, if needed
			'
			If s.isactivesetchanged OrElse s.ismaintermchanged Then

				'
				' Handle cases Alpha>0 and Alpha=0 separately:
				' * in the first case we have dense matrix
				' * in the second one we have diagonal matrix, which can be
				'   handled more efficiently
				'
				If CDbl(s.alpha) > CDbl(0) Then

					'
					' Alpha>0, dense QP
					'
					' Split variables into two groups - free (F) and constrained (C). Reorder
					' variables in such way that free vars come first, constrained are last:
					' x = [xf, xc].
					' 
					' Main quadratic term x'*(alpha*A+tau*D)*x now splits into quadratic part,
					' linear part and constant part:
					'                   ( alpha*Aff+tau*Df  alpha*Afc        ) ( xf )              
					'   0.5*( xf' xc' )*(                                    )*(    ) =
					'                   ( alpha*Acf         alpha*Acc+tau*Dc ) ( xc )
					'
					'   = 0.5*xf'*(alpha*Aff+tau*Df)*xf + (alpha*Afc*xc)'*xf + 0.5*xc'(alpha*Acc+tau*Dc)*xc
					'                    
					' We store these parts into temporary variables:
					' * alpha*Aff+tau*Df, alpha*Afc, alpha*Acc+tau*Dc are stored into upper
					'   triangle of TQ2
					' * alpha*Afc*xc is stored into TQ1
					' * 0.5*xc'(alpha*Acc+tau*Dc)*xc is stored into TQ0
					'
					' Below comes first part of the work - generation of TQ2:
					' * we pass through rows of A and copy I-th row into upper block (Aff/Afc) or
					'   lower one (Acf/Acc) of TQ2, depending on presence of X[i] in the active set.
					'   RIdx0 variable contains current position for insertion into upper block,
					'   RIdx1 contains current position for insertion into lower one.
					' * within each row, we copy J-th element into left half (Aff/Acf) or right
					'   one (Afc/Acc), depending on presence of X[j] in the active set. CIdx0
					'   contains current position for insertion into left block, CIdx1 contains
					'   position for insertion into right one.
					' * during copying, we multiply elements by alpha and add diagonal matrix D.
					'
					ridx0 = 0
					ridx1 = s.nfree
					For i = 0 To n - 1
						cidx0 = 0
						cidx1 = s.nfree
						For j = 0 To n - 1
							If Not s.activeset(i) AndAlso Not s.activeset(j) Then

								'
								' Element belongs to Aff
								'
								v = s.alpha * s.a(i, j)
								If i = j AndAlso CDbl(s.tau) > CDbl(0) Then
									v = v + s.tau * s.d(i)
								End If
								s.tq2dense(ridx0, cidx0) = v
							End If
							If Not s.activeset(i) AndAlso s.activeset(j) Then

								'
								' Element belongs to Afc
								'
								s.tq2dense(ridx0, cidx1) = s.alpha * s.a(i, j)
							End If
							If s.activeset(i) AndAlso Not s.activeset(j) Then

								'
								' Element belongs to Acf
								'
								s.tq2dense(ridx1, cidx0) = s.alpha * s.a(i, j)
							End If
							If s.activeset(i) AndAlso s.activeset(j) Then

								'
								' Element belongs to Acc
								'
								v = s.alpha * s.a(i, j)
								If i = j AndAlso CDbl(s.tau) > CDbl(0) Then
									v = v + s.tau * s.d(i)
								End If
								s.tq2dense(ridx1, cidx1) = v
							End If
							If s.activeset(j) Then
								cidx1 = cidx1 + 1
							Else
								cidx0 = cidx0 + 1
							End If
						Next
						If s.activeset(i) Then
							ridx1 = ridx1 + 1
						Else
							ridx0 = ridx0 + 1
						End If
					Next

					'
					' Now we have TQ2, and we can evaluate TQ1.
					' In the special case when we have Alpha=0, NFree=0 or NFree=N,
					' TQ1 is filled by zeros.
					'
					For i = 0 To n - 1
						s.tq1(i) = 0.0
					Next
					If s.nfree > 0 AndAlso s.nfree < n Then
						ablas.rmatrixmv(s.nfree, n - s.nfree, s.tq2dense, 0, s.nfree, 0, _
							s.txc, s.nfree, s.tq1, 0)
					End If

					'
					' And finally, we evaluate TQ0.
					'
					v = 0.0
					For i = s.nfree To n - 1
						For j = s.nfree To n - 1
							v = v + 0.5 * s.txc(i) * s.tq2dense(i, j) * s.txc(j)
						Next
					Next
					s.tq0 = v
				Else

					'
					' Alpha=0, diagonal QP
					'
					' Split variables into two groups - free (F) and constrained (C). Reorder
					' variables in such way that free vars come first, constrained are last:
					' x = [xf, xc].
					' 
					' Main quadratic term x'*(tau*D)*x now splits into quadratic and constant
					' parts:
					'                   ( tau*Df        ) ( xf )              
					'   0.5*( xf' xc' )*(               )*(    ) =
					'                   (        tau*Dc ) ( xc )
					'
					'   = 0.5*xf'*(tau*Df)*xf + 0.5*xc'(tau*Dc)*xc
					'                    
					' We store these parts into temporary variables:
					' * tau*Df is stored in TQ2Diag
					' * 0.5*xc'(tau*Dc)*xc is stored into TQ0
					'
					s.tq0 = 0.0
					ridx0 = 0
					For i = 0 To n - 1
						If Not s.activeset(i) Then
							s.tq2diag(ridx0) = s.tau * s.d(i)
							ridx0 = ridx0 + 1
						Else
							s.tq0 = s.tq0 + 0.5 * s.tau * s.d(i) * Math.sqr(s.xc(i))
						End If
					Next
					For i = 0 To n - 1
						s.tq1(i) = 0.0
					Next
				End If
			End If

			'
			' Re-evaluate TK2/TK1/TK0, if needed
			'
			If s.isactivesetchanged OrElse s.issecondarytermchanged Then

				'
				' Split variables into two groups - free (F) and constrained (C). Reorder
				' variables in such way that free vars come first, constrained are last:
				' x = [xf, xc].
				' 
				' Secondary term theta*(Q*x-r)'*(Q*x-r) now splits into quadratic part,
				' linear part and constant part:
				'             (          ( xf )     )'  (          ( xf )     )
				'   0.5*theta*( (Qf Qc)'*(    ) - r ) * ( (Qf Qc)'*(    ) - r ) =
				'             (          ( xc )     )   (          ( xc )     )
				'
				'   = 0.5*theta*xf'*(Qf'*Qf)*xf + theta*((Qc*xc-r)'*Qf)*xf + 
				'     + theta*(-r'*(Qc*xc-r)-0.5*r'*r+0.5*xc'*Qc'*Qc*xc)
				'                    
				' We store these parts into temporary variables:
				' * sqrt(theta)*Qf is stored into TK2
				' * theta*((Qc*xc-r)'*Qf) is stored into TK1
				' * theta*(-r'*(Qc*xc-r)-0.5*r'*r+0.5*xc'*Qc'*Qc*xc) is stored into TK0
				'
				' We use several other temporaries to store intermediate results:
				' * Tmp0 - to store Qc*xc-r
				' * Tmp1 - to store Qc*xc
				'
				' Generation of TK2/TK1/TK0 is performed as follows:
				' * we fill TK2/TK1/TK0 (to handle K=0 or Theta=0)
				' * other steps are performed only for K>0 and Theta>0
				' * we pass through columns of Q and copy I-th column into left block (Qf) or
				'   right one (Qc) of TK2, depending on presence of X[i] in the active set.
				'   CIdx0 variable contains current position for insertion into upper block,
				'   CIdx1 contains current position for insertion into lower one.
				' * we calculate Qc*xc-r and store it into Tmp0
				' * we calculate TK0 and TK1
				' * we multiply leading part of TK2 which stores Qf by sqrt(theta)
				'   it is important to perform this step AFTER calculation of TK0 and TK1,
				'   because we need original (non-modified) Qf to calculate TK0 and TK1.
				'
				For j = 0 To n - 1
					For i = 0 To k - 1
						s.tk2(i, j) = 0.0
					Next
					s.tk1(j) = 0.0
				Next
				s.tk0 = 0.0
				If s.k > 0 AndAlso CDbl(s.theta) > CDbl(0) Then

					'
					' Split Q into Qf and Qc
					' Calculate Qc*xc-r, store in Tmp0
					'
					apserv.rvectorsetlengthatleast(s.tmp0, k)
					apserv.rvectorsetlengthatleast(s.tmp1, k)
					cidx0 = 0
					cidx1 = nfree
					For i = 0 To k - 1
						s.tmp1(i) = 0.0
					Next
					For j = 0 To n - 1
						If s.activeset(j) Then
							For i = 0 To k - 1
								s.tk2(i, cidx1) = s.q(i, j)
								s.tmp1(i) = s.tmp1(i) + s.q(i, j) * s.txc(cidx1)
							Next
							cidx1 = cidx1 + 1
						Else
							For i = 0 To k - 1
								s.tk2(i, cidx0) = s.q(i, j)
							Next
							cidx0 = cidx0 + 1
						End If
					Next
					For i = 0 To k - 1
						s.tmp0(i) = s.tmp1(i) - s.r(i)
					Next

					'
					' Calculate TK0
					'
					v = 0.0
					For i = 0 To k - 1
						v = v + s.theta * (0.5 * Math.sqr(s.tmp1(i)) - s.r(i) * s.tmp0(i) - 0.5 * Math.sqr(s.r(i)))
					Next
					s.tk0 = v

					'
					' Calculate TK1
					'
					If nfree > 0 Then
						For i = 0 To k - 1
							v = s.theta * s.tmp0(i)
							For i_ = 0 To nfree - 1
								s.tk1(i_) = s.tk1(i_) + v * s.tk2(i, i_)
							Next
						Next
					End If

					'
					' Calculate TK2
					'
					If nfree > 0 Then
						v = System.Math.sqrt(s.theta)
						For i = 0 To k - 1
							For i_ = 0 To nfree - 1
								s.tk2(i, i_) = v * s.tk2(i, i_)
							Next
						Next
					End If
				End If
			End If

			'
			' Re-evaluate TB
			'
			If s.isactivesetchanged OrElse s.islineartermchanged Then
				ridx0 = 0
				ridx1 = nfree
				For i = 0 To n - 1
					If s.activeset(i) Then
						s.tb(ridx1) = s.b(i)
						ridx1 = ridx1 + 1
					Else
						s.tb(ridx0) = s.b(i)
						ridx0 = ridx0 + 1
					End If
				Next
			End If

			'
			' Compose ECA: either dense ECA or diagonal ECA
			'
			If (s.isactivesetchanged OrElse s.ismaintermchanged) AndAlso nfree > 0 Then
				If CDbl(s.alpha) > CDbl(0) Then

					'
					' Dense ECA
					'
					s.ecakind = 0
					For i = 0 To nfree - 1
						For j = i To nfree - 1
							s.ecadense(i, j) = s.tq2dense(i, j)
						Next
					Next
					If Not trfac.spdmatrixcholeskyrec(s.ecadense, 0, nfree, True, s.tmp0) Then
						result = False
						Return result
					End If
				Else

					'
					' Diagonal ECA
					'
					s.ecakind = 1
					For i = 0 To nfree - 1
						If CDbl(s.tq2diag(i)) < CDbl(0) Then
							result = False
							Return result
						End If
						s.ecadiag(i) = System.Math.sqrt(s.tq2diag(i))
					Next
				End If
			End If

			'
			' Compose EQ
			'
			If s.isactivesetchanged OrElse s.issecondarytermchanged Then
				For i = 0 To k - 1
					For j = 0 To nfree - 1
						s.eq(i, j) = s.tk2(i, j)
					Next
				Next
			End If

			'
			' Calculate ECCM
			'
			If ((((s.isactivesetchanged OrElse s.ismaintermchanged) OrElse s.issecondarytermchanged) AndAlso s.k > 0) AndAlso CDbl(s.theta) > CDbl(0)) AndAlso nfree > 0 Then

				'
				' Calculate ECCM - Cholesky factor of the "effective" capacitance
				' matrix CM = I + EQ*inv(EffectiveA)*EQ'.
				'
				' We calculate CM as follows:
				'   CM = I + EQ*inv(EffectiveA)*EQ'
				'      = I + EQ*ECA^(-1)*ECA^(-T)*EQ'
				'      = I + (EQ*ECA^(-1))*(EQ*ECA^(-1))'
				'
				' Then we perform Cholesky decomposition of CM.
				'
				apserv.rmatrixsetlengthatleast(s.tmp2, k, n)
				ablas.rmatrixcopy(k, nfree, s.eq, 0, 0, s.tmp2, _
					0, 0)
				alglib.ap.assert(s.ecakind = 0 OrElse s.ecakind = 1, "CQMRebuild: unexpected ECAKind")
				If s.ecakind = 0 Then
					ablas.rmatrixrighttrsm(k, nfree, s.ecadense, 0, 0, True, _
						False, 0, s.tmp2, 0, 0)
				End If
				If s.ecakind = 1 Then
					For i = 0 To k - 1
						For j = 0 To nfree - 1
							s.tmp2(i, j) = s.tmp2(i, j) / s.ecadiag(j)
						Next
					Next
				End If
				For i = 0 To k - 1
					For j = 0 To k - 1
						s.eccm(i, j) = 0.0
					Next
					s.eccm(i, i) = 1.0
				Next
				ablas.rmatrixsyrk(k, nfree, 1.0, s.tmp2, 0, 0, _
					0, 1.0, s.eccm, 0, 0, True)
				If Not trfac.spdmatrixcholeskyrec(s.eccm, 0, k, True, s.tmp0) Then
					result = False
					Return result
				End If
			End If

			'
			' Compose EB and EC
			'
			' NOTE: because these quantities are cheap to compute, we do not
			' use caching here.
			'
			For i = 0 To nfree - 1
				s.eb(i) = s.tq1(i) + s.tk1(i) + s.tb(i)
			Next
			s.ec = s.tq0 + s.tk0
			For i = nfree To n - 1
				s.ec = s.ec + s.tb(i) * s.txc(i)
			Next

			'
			' Change cache status - everything is cached 
			'
			s.ismaintermchanged = False
			s.issecondarytermchanged = False
			s.islineartermchanged = False
			s.isactivesetchanged = False
			Return result
		End Function


		'************************************************************************
'        Internal function, solves system Effective_A*x = b.
'        It should be called after successful completion of CQMRebuild().
'
'        INPUT PARAMETERS:
'            S       -   quadratic model, after call to CQMRebuild()
'            X       -   right part B, array[S.NFree]
'            Tmp     -   temporary array, automatically reallocated if needed
'
'        OUTPUT PARAMETERS:
'            X       -   solution, array[S.NFree]
'            
'        NOTE: when called with zero S.NFree, returns silently
'        NOTE: this function assumes that EA is non-degenerate
'
'          -- ALGLIB --
'             Copyright 10.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub cqmsolveea(s As convexquadraticmodel, ByRef x As Double(), ByRef tmp As Double())
			Dim i As Integer = 0

			alglib.ap.assert((s.ecakind = 0 OrElse s.ecakind = 1) OrElse (s.ecakind = -1 AndAlso s.nfree = 0), "CQMSolveEA: unexpected ECAKind")
			If s.ecakind = 0 Then

				'
				' Dense ECA, use FBLSCholeskySolve() dense solver.
				'
				fbls.fblscholeskysolve(s.ecadense, 1.0, s.nfree, True, x, tmp)
			End If
			If s.ecakind = 1 Then

				'
				' Diagonal ECA
				'
				For i = 0 To s.nfree - 1
					x(i) = x(i) / Math.sqr(s.ecadiag(i))
				Next
			End If
		End Sub


	End Class
	Public Class snnls
		'************************************************************************
'        This structure is a SNNLS (Specialized Non-Negative Least Squares) solver.
'
'        It solves problems of the form |A*x-b|^2 => min subject to  non-negativity
'        constraints on SOME components of x, with structured A (first  NS  columns
'        are just unit matrix, next ND columns store dense part).
'
'        This solver is suited for solution of many sequential NNLS  subproblems  -
'        it keeps track of previously allocated memory and reuses  it  as  much  as
'        possible.
'        ************************************************************************

		Public Class snnlssolver
			Inherits apobject
			Public ns As Integer
			Public nd As Integer
			Public nr As Integer
			Public densea As Double(,)
			Public b As Double()
			Public nnc As Boolean()
			Public refinementits As Integer
			Public debugflops As Double
			Public debugmaxnewton As Integer
			Public xn As Double()
			Public tmpz As Double(,)
			Public tmpca As Double(,)
			Public g As Double()
			Public d As Double()
			Public dx As Double()
			Public diagaa As Double()
			Public cb As Double()
			Public cx As Double()
			Public cborg As Double()
			Public columnmap As Integer()
			Public rowmap As Integer()
			Public tmpcholesky As Double()
			Public r As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				densea = New Double(-1, -1) {}
				b = New Double(-1) {}
				nnc = New Boolean(-1) {}
				xn = New Double(-1) {}
				tmpz = New Double(-1, -1) {}
				tmpca = New Double(-1, -1) {}
				g = New Double(-1) {}
				d = New Double(-1) {}
				dx = New Double(-1) {}
				diagaa = New Double(-1) {}
				cb = New Double(-1) {}
				cx = New Double(-1) {}
				cborg = New Double(-1) {}
				columnmap = New Integer(-1) {}
				rowmap = New Integer(-1) {}
				tmpcholesky = New Double(-1) {}
				r = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New snnlssolver()
				_result.ns = ns
				_result.nd = nd
				_result.nr = nr
				_result.densea = DirectCast(densea.Clone(), Double(,))
				_result.b = DirectCast(b.Clone(), Double())
				_result.nnc = DirectCast(nnc.Clone(), Boolean())
				_result.refinementits = refinementits
				_result.debugflops = debugflops
				_result.debugmaxnewton = debugmaxnewton
				_result.xn = DirectCast(xn.Clone(), Double())
				_result.tmpz = DirectCast(tmpz.Clone(), Double(,))
				_result.tmpca = DirectCast(tmpca.Clone(), Double(,))
				_result.g = DirectCast(g.Clone(), Double())
				_result.d = DirectCast(d.Clone(), Double())
				_result.dx = DirectCast(dx.Clone(), Double())
				_result.diagaa = DirectCast(diagaa.Clone(), Double())
				_result.cb = DirectCast(cb.Clone(), Double())
				_result.cx = DirectCast(cx.Clone(), Double())
				_result.cborg = DirectCast(cborg.Clone(), Double())
				_result.columnmap = DirectCast(columnmap.Clone(), Integer())
				_result.rowmap = DirectCast(rowmap.Clone(), Integer())
				_result.tmpcholesky = DirectCast(tmpcholesky.Clone(), Double())
				_result.r = DirectCast(r.Clone(), Double())
				Return _result
			End Function
		End Class




		Public Const iterativerefinementits As Integer = 3


		'************************************************************************
'        This subroutine is used to initialize SNNLS solver.
'
'        By default, empty NNLS problem is produced, but we allocated enough  space
'        to store problems with NSMax+NDMax columns and  NRMax  rows.  It  is  good
'        place to provide algorithm with initial estimate of the space requirements,
'        although you may underestimate problem size or even pass zero estimates  -
'        in this case buffer variables will be resized automatically  when  you set
'        NNLS problem.
'
'        Previously allocated buffer variables are reused as much as possible. This
'        function does not clear structure completely, it tries to preserve as much
'        dynamically allocated memory as possible.
'
'          -- ALGLIB --
'             Copyright 10.10.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub snnlsinit(nsmax As Integer, ndmax As Integer, nrmax As Integer, s As snnlssolver)
			s.ns = 0
			s.nd = 0
			s.nr = 0
			apserv.rmatrixsetlengthatleast(s.densea, nrmax, ndmax)
			apserv.rmatrixsetlengthatleast(s.tmpca, nrmax, ndmax)
			apserv.rmatrixsetlengthatleast(s.tmpz, ndmax, ndmax)
			apserv.rvectorsetlengthatleast(s.b, nrmax)
			apserv.bvectorsetlengthatleast(s.nnc, nsmax + ndmax)
			s.debugflops = 0.0
			s.debugmaxnewton = 0
			s.refinementits = iterativerefinementits
		End Sub


		'************************************************************************
'        This subroutine is used to set NNLS problem:
'
'                ( [ 1     |      ]   [   ]   [   ] )^2
'                ( [   1   |      ]   [   ]   [   ] )
'            min ( [     1 |  Ad  ] * [ x ] - [ b ] )    s.t. x>=0
'                ( [       |      ]   [   ]   [   ] )
'                ( [       |      ]   [   ]   [   ] )
'
'        where:
'        * identity matrix has NS*NS size (NS<=NR, NS can be zero)
'        * dense matrix Ad has NR*ND size
'        * b is NR*1 vector
'        * x is (NS+ND)*1 vector
'        * all elements of x are non-negative (this constraint can be removed later
'          by calling SNNLSDropNNC() function)
'
'        Previously allocated buffer variables are reused as much as possible.
'        After you set problem, you can solve it with SNNLSSolve().
'
'        INPUT PARAMETERS:
'            S   -   SNNLS solver, must be initialized with SNNLSInit() call
'            A   -   array[NR,ND], dense part of the system
'            B   -   array[NR], right part
'            NS  -   size of the sparse part of the system, 0<=NS<=NR
'            ND  -   size of the dense part of the system, ND>=0
'            NR  -   rows count, NR>0
'
'        NOTE:
'            1. You can have NS+ND=0, solver will correctly accept such combination
'               and return empty array as problem solution.
'            
'          -- ALGLIB --
'             Copyright 10.10.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub snnlssetproblem(s As snnlssolver, a As Double(,), b As Double(), ns As Integer, nd As Integer, nr As Integer)
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(nd >= 0, "SNNLSSetProblem: ND<0")
			alglib.ap.assert(ns >= 0, "SNNLSSetProblem: NS<0")
			alglib.ap.assert(nr > 0, "SNNLSSetProblem: NR<=0")
			alglib.ap.assert(ns <= nr, "SNNLSSetProblem: NS>NR")
			alglib.ap.assert(alglib.ap.rows(a) >= nr OrElse nd = 0, "SNNLSSetProblem: rows(A)<NR")
			alglib.ap.assert(alglib.ap.cols(a) >= nd, "SNNLSSetProblem: cols(A)<ND")
			alglib.ap.assert(alglib.ap.len(b) >= nr, "SNNLSSetProblem: length(B)<NR")
			alglib.ap.assert(apserv.apservisfinitematrix(a, nr, nd), "SNNLSSetProblem: A contains INF/NAN")
			alglib.ap.assert(apserv.isfinitevector(b, nr), "SNNLSSetProblem: B contains INF/NAN")

			'
			' Copy problem
			'
			s.ns = ns
			s.nd = nd
			s.nr = nr
			If nd > 0 Then
				apserv.rmatrixsetlengthatleast(s.densea, nr, nd)
				For i = 0 To nr - 1
					For i_ = 0 To nd - 1
						s.densea(i, i_) = a(i, i_)
					Next
				Next
			End If
			apserv.rvectorsetlengthatleast(s.b, nr)
			For i_ = 0 To nr - 1
				s.b(i_) = b(i_)
			Next
			apserv.bvectorsetlengthatleast(s.nnc, ns + nd)
			For i = 0 To ns + nd - 1
				s.nnc(i) = True
			Next
		End Sub


		'************************************************************************
'        This subroutine drops non-negativity constraint from the  problem  set  by
'        SNNLSSetProblem() call. This function must be called AFTER problem is set,
'        because each SetProblem() call resets constraints to their  default  state
'        (all constraints are present).
'
'        INPUT PARAMETERS:
'            S   -   SNNLS solver, must be initialized with SNNLSInit() call,
'                    problem must be set with SNNLSSetProblem() call.
'            Idx -   constraint index, 0<=IDX<NS+ND
'            
'          -- ALGLIB --
'             Copyright 10.10.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub snnlsdropnnc(s As snnlssolver, idx As Integer)
			alglib.ap.assert(idx >= 0, "SNNLSDropNNC: Idx<0")
			alglib.ap.assert(idx < s.ns + s.nd, "SNNLSDropNNC: Idx>=NS+ND")
			s.nnc(idx) = False
		End Sub


		'************************************************************************
'        This subroutine is used to solve NNLS problem.
'
'        INPUT PARAMETERS:
'            S   -   SNNLS solver, must be initialized with SNNLSInit() call and
'                    problem must be set up with SNNLSSetProblem() call.
'            X   -   possibly preallocated buffer, automatically resized if needed
'
'        OUTPUT PARAMETERS:
'            X   -   array[NS+ND], solution
'            
'        NOTE:
'            1. You can have NS+ND=0, solver will correctly accept such combination
'               and return empty array as problem solution.
'            
'            2. Internal field S.DebugFLOPS contains rough estimate of  FLOPs  used
'               to solve problem. It can be used for debugging purposes. This field
'               is real-valued.
'            
'          -- ALGLIB --
'             Copyright 10.10.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub snnlssolve(s As snnlssolver, ByRef x As Double())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ns As Integer = 0
			Dim nd As Integer = 0
			Dim nr As Integer = 0
			Dim nsc As Integer = 0
			Dim ndc As Integer = 0
			Dim newtoncnt As Integer = 0
			Dim terminationneeded As New Boolean()
			Dim eps As Double = 0
			Dim fcur As Double = 0
			Dim fprev As Double = 0
			Dim fcand As Double = 0
			Dim noiselevel As Double = 0
			Dim noisetolerance As Double = 0
			Dim stplen As Double = 0
			Dim d2 As Double = 0
			Dim d1 As Double = 0
			Dim d0 As Double = 0
			Dim wasactivation As New Boolean()
			Dim rfsits As Integer = 0
			Dim lambdav As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0
			Dim i1_ As Integer = 0


			'
			' Prepare
			'
			ns = s.ns
			nd = s.nd
			nr = s.nr
			s.debugflops = 0.0

			'
			' Handle special cases:
			' * NS+ND=0
			' * ND=0
			'
			If ns + nd = 0 Then
				Return
			End If
			If nd = 0 Then
				apserv.rvectorsetlengthatleast(x, ns)
				For i = 0 To ns - 1
					x(i) = s.b(i)
					If s.nnc(i) Then
						x(i) = System.Math.Max(x(i), 0.0)
					End If
				Next
				Return
			End If

			'
			' Main cycle of BLEIC-SNNLS algorithm.
			' Below we assume that ND>0.
			'
			apserv.rvectorsetlengthatleast(x, ns + nd)
			apserv.rvectorsetlengthatleast(s.xn, ns + nd)
			apserv.rvectorsetlengthatleast(s.g, ns + nd)
			apserv.rvectorsetlengthatleast(s.d, ns + nd)
			apserv.rvectorsetlengthatleast(s.r, nr)
			apserv.rvectorsetlengthatleast(s.diagaa, nd)
			apserv.rvectorsetlengthatleast(s.dx, ns + nd)
			For i = 0 To ns + nd - 1
				x(i) = 0.0
			Next
			eps = 2 * Math.machineepsilon
			noisetolerance = 10.0
			lambdav = 1000000.0 * Math.machineepsilon
			newtoncnt = 0
			While True

				'
				' Phase 1: perform steepest descent step.
				'
				' TerminationNeeded control variable is set on exit from this loop:
				' * TerminationNeeded=False in case we have to proceed to Phase 2 (Newton step)
				' * TerminationNeeded=True in case we found solution (step along projected gradient is small enough)
				'
				' Temporaries used:
				' * R      (I|A)*x-b
				'
				' NOTE 1. It is assumed that initial point X is feasible. This feasibility
				'         is retained during all iterations.
				'
				terminationneeded = False
				While True

					'
					' Calculate gradient G and constrained descent direction D
					'
					For i = 0 To nr - 1
						i1_ = (ns) - (0)
						v = 0.0
						For i_ = 0 To nd - 1
							v += s.densea(i, i_) * x(i_ + i1_)
						Next
						If i < ns Then
							v = v + x(i)
						End If
						s.r(i) = v - s.b(i)
					Next
					For i = 0 To ns - 1
						s.g(i) = s.r(i)
					Next
					For i = ns To ns + nd - 1
						s.g(i) = 0.0
					Next
					For i = 0 To nr - 1
						v = s.r(i)
						i1_ = (0) - (ns)
						For i_ = ns To ns + nd - 1
							s.g(i_) = s.g(i_) + v * s.densea(i, i_ + i1_)
						Next
					Next
					For i = 0 To ns + nd - 1
						If (s.nnc(i) AndAlso CDbl(x(i)) <= CDbl(0)) AndAlso CDbl(s.g(i)) > CDbl(0) Then
							s.d(i) = 0.0
						Else
							s.d(i) = -s.g(i)
						End If
					Next
					s.debugflops = s.debugflops + 2 * 2 * nr * nd

					'
					' Build quadratic model of F along descent direction:
					'     F(x+alpha*d) = D2*alpha^2 + D1*alpha + D0
					'
					' Estimate numerical noise in the X (noise level is used
					' to classify step as singificant or insignificant). Noise
					' comes from two sources:
					' * noise when calculating rows of (I|A)*x
					' * noise when calculating norm of residual
					'
					' In case function curvature is negative or product of descent
					' direction and gradient is non-negative, iterations are terminated.
					'
					' NOTE: D0 is not actually used, but we prefer to maintain it.
					'
					fprev = 0.0
					For i_ = 0 To nr - 1
						fprev += s.r(i_) * s.r(i_)
					Next
					fprev = fprev / 2
					noiselevel = 0.0
					For i = 0 To nr - 1

						'
						' Estimate noise introduced by I-th row of (I|A)*x
						'
						v = 0.0
						If i < ns Then
							v = eps * x(i)
						End If
						For j = 0 To nd - 1
							v = System.Math.Max(v, eps * System.Math.Abs(s.densea(i, j) * x(ns + j)))
						Next
						v = 2 * System.Math.Abs(s.r(i) * v) + v * v

						'
						' Add to summary noise in the model
						'
						noiselevel = noiselevel + v
					Next
					noiselevel = System.Math.Max(noiselevel, eps * fprev)
					d2 = 0.0
					For i = 0 To nr - 1
						i1_ = (ns) - (0)
						v = 0.0
						For i_ = 0 To nd - 1
							v += s.densea(i, i_) * s.d(i_ + i1_)
						Next
						If i < ns Then
							v = v + s.d(i)
						End If
						d2 = d2 + 0.5 * Math.sqr(v)
					Next
					v = 0.0
					For i_ = 0 To ns + nd - 1
						v += s.d(i_) * s.g(i_)
					Next
					d1 = v
					d0 = fprev
					If CDbl(d2) <= CDbl(0) OrElse CDbl(d1) >= CDbl(0) Then
						terminationneeded = True
						Exit While
					End If
					s.debugflops = s.debugflops + 2 * nr * nd
					apserv.touchreal(d0)

					'
					' Perform full (unconstrained) step with length StpLen in direction D.
					'
					' We can terminate iterations in case one of two criteria is met:
					' 1. function change is dominated by noise (or function actually increased
					'    instead of decreasing)
					' 2. relative change in X is small enough
					'
					' First condition is not enough to guarantee algorithm termination because
					' sometimes our noise estimate is too optimistic (say, in situations when
					' function value at solition is zero).
					'
					stplen = -(d1 / (2 * d2))
					For i_ = 0 To ns + nd - 1
						s.xn(i_) = x(i_)
					Next
					For i_ = 0 To ns + nd - 1
						s.xn(i_) = s.xn(i_) + stplen * s.d(i_)
					Next
					fcand = 0.0
					For i = 0 To nr - 1
						i1_ = (ns) - (0)
						v = 0.0
						For i_ = 0 To nd - 1
							v += s.densea(i, i_) * s.xn(i_ + i1_)
						Next
						If i < ns Then
							v = v + s.xn(i)
						End If
						fcand = fcand + 0.5 * Math.sqr(v - s.b(i))
					Next
					s.debugflops = s.debugflops + 2 * nr * nd
					If CDbl(fcand) >= CDbl(fprev - noiselevel * noisetolerance) Then
						terminationneeded = True
						Exit While
					End If
					v = 0
					For i = 0 To ns + nd - 1
						v0 = System.Math.Abs(x(i))
						v1 = System.Math.Abs(s.xn(i))
						If CDbl(v0) <> CDbl(0) OrElse CDbl(v1) <> CDbl(0) Then
							v = System.Math.Max(v, System.Math.Abs(x(i) - s.xn(i)) / System.Math.Max(v0, v1))
						End If
					Next
					If CDbl(v) <= CDbl(eps * noisetolerance) Then
						terminationneeded = True
						Exit While
					End If

					'
					' Perform step one more time, now with non-negativity constraints.
					'
					' NOTE: complicated code below which deals with VarIdx temporary makes
					'       sure that in case unconstrained step leads us outside of feasible
					'       area, we activate at least one constraint.
					'
					wasactivation = boundedstepandactivation(x, s.xn, s.nnc, ns + nd)
					fcur = 0.0
					For i = 0 To nr - 1
						i1_ = (ns) - (0)
						v = 0.0
						For i_ = 0 To nd - 1
							v += s.densea(i, i_) * x(i_ + i1_)
						Next
						If i < ns Then
							v = v + x(i)
						End If
						fcur = fcur + 0.5 * Math.sqr(v - s.b(i))
					Next
					s.debugflops = s.debugflops + 2 * nr * nd

					'
					' Depending on results, decide what to do:
					' 1. In case step was performed without activation of constraints,
					'    we proceed to Newton method
					' 2. In case there was activated at least one constraint, we repeat
					'    steepest descent step.
					'
					If Not wasactivation Then

						'
						' Step without activation, proceed to Newton
						'
						Exit While
					End If
				End While
				If terminationneeded Then
					Exit While
				End If

				'
				' Phase 2: Newton method.
				'
				apserv.rvectorsetlengthatleast(s.cx, ns + nd)
				apserv.ivectorsetlengthatleast(s.columnmap, ns + nd)
				apserv.ivectorsetlengthatleast(s.rowmap, nr)
				apserv.rmatrixsetlengthatleast(s.tmpca, nr, nd)
				apserv.rmatrixsetlengthatleast(s.tmpz, nd, nd)
				apserv.rvectorsetlengthatleast(s.cborg, nr)
				apserv.rvectorsetlengthatleast(s.cb, nr)
				terminationneeded = False
				While True

					'
					' Prepare equality constrained subproblem with NSC<=NS "sparse"
					' variables and NDC<=ND "dense" variables.
					'
					' First, we reorder variables (columns) and move all unconstrained
					' variables "to the left", ColumnMap stores this permutation.
					'
					' Then, we reorder first NS rows of A and first NS elements of B in
					' such way that we still have identity matrix in first NSC columns
					' of problem. This permutation is stored in RowMap.
					'
					nsc = 0
					ndc = 0
					For i = 0 To ns - 1
						If Not (s.nnc(i) AndAlso CDbl(x(i)) = CDbl(0)) Then
							s.columnmap(nsc) = i
							nsc = nsc + 1
						End If
					Next
					For i = ns To ns + nd - 1
						If Not (s.nnc(i) AndAlso CDbl(x(i)) = CDbl(0)) Then
							s.columnmap(nsc + ndc) = i
							ndc = ndc + 1
						End If
					Next
					For i = 0 To nsc - 1
						s.rowmap(i) = s.columnmap(i)
					Next
					j = nsc
					For i = 0 To ns - 1
						If s.nnc(i) AndAlso CDbl(x(i)) = CDbl(0) Then
							s.rowmap(j) = i
							j = j + 1
						End If
					Next
					For i = ns To nr - 1
						s.rowmap(i) = i
					Next

					'
					' Now, permutations are ready, and we can copy/reorder
					' A, B and X to CA, CB and CX.
					'
					For i = 0 To nsc + ndc - 1
						s.cx(i) = x(s.columnmap(i))
					Next
					For i = 0 To nr - 1
						For j = 0 To ndc - 1
							s.tmpca(i, j) = s.densea(s.rowmap(i), s.columnmap(nsc + j) - ns)
						Next
						s.cb(i) = s.b(s.rowmap(i))
					Next

					'
					' Solve equality constrained subproblem.
					'
					If ndc > 0 Then

						'
						' NDC>0.
						'
						' Solve subproblem using Newton-type algorithm. We have a
						' NR*(NSC+NDC) linear least squares subproblem
						'
						'         | ( I  AU )   ( XU )   ( BU ) |^2
						'     min | (       ) * (    ) - (    ) |
						'         | ( 0  AL )   ( XL )   ( BL ) |
						'
						' where:
						' * I is a NSC*NSC identity matrix
						' * AU is NSC*NDC dense matrix (first NSC rows of CA)
						' * AL is (NR-NSC)*NDC dense matrix (next NR-NSC rows of CA)
						' * BU and BL are correspondingly sized parts of CB
						'
						' After conversion to normal equations and small regularization,
						' we get:
						'
						'     ( I   AU ) (  XU )   ( BU            )
						'     (        )*(     ) = (               )
						'     ( AU' Y  ) (  XL )   ( AU'*BU+AL'*BL )
						'
						' where Y = AU'*AU + AL'*AL + lambda*diag(AU'*AU+AL'*AL).
						'
						' With Schur Complement Method this system can be solved in
						' O(NR*NDC^2+NDC^3) operations. In order to solve it we multiply
						' first row by AU' and subtract it from the second one. As result,
						' we get system
						'
						'     Z*XL = AL'*BL, where Z=AL'*AL+lambda*diag(AU'*AU+AL'*AL)
						'
						' We can easily solve it for XL, and we can get XU as XU = BU-AU*XL.
						'
						' We will start solution from calculating Cholesky decomposition of Z.
						'
						For i = 0 To nr - 1
							s.cborg(i) = s.cb(i)
						Next
						For i = 0 To ndc - 1
							s.diagaa(i) = 0
						Next
						For i = 0 To nr - 1
							For j = 0 To ndc - 1
								s.diagaa(j) = s.diagaa(j) + Math.sqr(s.tmpca(i, j))
							Next
						Next
						For j = 0 To ndc - 1
							If CDbl(s.diagaa(j)) = CDbl(0) Then
								s.diagaa(j) = 1
							End If
						Next
						While True

							'
							' NOTE: we try to factorize Z. In case of failure we increase
							'       regularization parameter and try again.
							'
							s.debugflops = s.debugflops + 2 * (nr - nsc) * Math.sqr(ndc) + System.Math.Pow(ndc, 3) / 3
							For i = 0 To ndc - 1
								For j = 0 To ndc - 1
									s.tmpz(i, j) = 0.0
								Next
							Next
							ablas.rmatrixsyrk(ndc, nr - nsc, 1.0, s.tmpca, nsc, 0, _
								2, 0.0, s.tmpz, 0, 0, True)
							For i = 0 To ndc - 1
								s.tmpz(i, i) = s.tmpz(i, i) + lambdav * s.diagaa(i)
							Next
							If trfac.spdmatrixcholeskyrec(s.tmpz, 0, ndc, True, s.tmpcholesky) Then
								Exit While
							End If
							lambdav = lambdav * 10
						End While

						'
						' We have Cholesky decomposition of Z, now we can solve system:
						' * we start from initial point CX
						' * we perform several iterations of refinement:
						'   * BU_new := BU_orig - XU_cur - AU*XL_cur
						'   * BL_new := BL_orig - AL*XL_cur
						'   * solve for BU_new/BL_new, obtain solution dx
						'   * XU_cur := XU_cur + dx_u
						'   * XL_cur := XL_cur + dx_l
						' * BU_new/BL_new are stored in CB, original right part is
						'   stored in CBOrg, correction to X is stored in DX, current
						'   X is stored in CX
						'
						For rfsits = 1 To s.refinementits
							For i = 0 To nr - 1
								i1_ = (nsc) - (0)
								v = 0.0
								For i_ = 0 To ndc - 1
									v += s.tmpca(i, i_) * s.cx(i_ + i1_)
								Next
								s.cb(i) = s.cborg(i) - v
								If i < nsc Then
									s.cb(i) = s.cb(i) - s.cx(i)
								End If
							Next
							s.debugflops = s.debugflops + 2 * nr * ndc
							For i = 0 To ndc - 1
								s.dx(i) = 0.0
							Next
							For i = nsc To nr - 1
								v = s.cb(i)
								For i_ = 0 To ndc - 1
									s.dx(i_) = s.dx(i_) + v * s.tmpca(i, i_)
								Next
							Next
							fbls.fblscholeskysolve(s.tmpz, 1.0, ndc, True, s.dx, s.tmpcholesky)
							s.debugflops = s.debugflops + 2 * ndc * ndc
							i1_ = (0) - (nsc)
							For i_ = nsc To nsc + ndc - 1
								s.cx(i_) = s.cx(i_) + s.dx(i_ + i1_)
							Next
							For i = 0 To nsc - 1
								v = 0.0
								For i_ = 0 To ndc - 1
									v += s.tmpca(i, i_) * s.dx(i_)
								Next
								s.cx(i) = s.cx(i) + s.cb(i) - v
							Next
							s.debugflops = s.debugflops + 2 * nsc * ndc
						Next
					Else

						'
						' NDC=0.
						'
						' We have a NR*NSC linear least squares subproblem
						'
						'     min |XU-BU|^2
						'
						' solution is easy to find - it is XU=BU!
						'
						For i = 0 To nsc - 1
							s.cx(i) = s.cb(i)
						Next
					End If
					For i = 0 To ns + nd - 1
						s.xn(i) = x(i)
					Next
					For i = 0 To nsc + ndc - 1
						s.xn(s.columnmap(i)) = s.cx(i)
					Next
					newtoncnt = newtoncnt + 1

					'
					' Step to candidate point.
					' If no constraints was added, accept candidate point XN and move to next phase.
					' Terminate, if number of Newton iterations exceeded DebugMaxNewton counter.
					'
					terminationneeded = s.debugmaxnewton > 0 AndAlso newtoncnt >= s.debugmaxnewton
					If Not boundedstepandactivation(x, s.xn, s.nnc, ns + nd) Then
						Exit While
					End If
					If terminationneeded Then
						Exit While
					End If
				End While
				If terminationneeded Then
					Exit While
				End If
			End While
		End Sub


		'************************************************************************
'        Having feasible current point XC and possibly infeasible candidate   point
'        XN,  this  function  performs  longest  step  from  XC to XN which retains
'        feasibility. In case XN is found to be infeasible, at least one constraint
'        is activated.
'
'        For example, if we have:
'          XC=0.5
'          XN=-1.2
'          x>=0
'        then this function will move us to X=0 and activate constraint "x>=0".
'
'        INPUT PARAMETERS:
'            XC      -   current point, must be feasible with respect to
'                        all constraints
'            XN      -   candidate point, can be infeasible with respect to some
'                        constraints
'            NNC     -   NNC[i] is True when I-th variable is non-negatively
'                        constrained
'            N       -   variable count
'
'        OUTPUT PARAMETERS:
'            XC      -   new position
'
'        RESULT:
'            True in case at least one constraint was activated by step
'
'          -- ALGLIB --
'             Copyright 19.10.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function boundedstepandactivation(xc As Double(), xn As Double(), nnc As Boolean(), n As Integer) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim varidx As Integer = 0
			Dim vmax As Double = 0
			Dim v As Double = 0
			Dim stplen As Double = 0


			'
			' Check constraints.
			'
			' NOTE: it is important to test for XN[i]<XC[i] (strict inequality,
			'       allows to handle correctly situations with XC[i]=0 without
			'       activating already active constraints), but to check for
			'       XN[i]<=0 (non-strict inequality, correct handling of some
			'       special cases when unconstrained step ends at the boundary).
			'
			result = False
			varidx = -1
			vmax = Math.maxrealnumber
			For i = 0 To n - 1
				If (nnc(i) AndAlso CDbl(xn(i)) < CDbl(xc(i))) AndAlso CDbl(xn(i)) <= CDbl(0.0) Then
					v = vmax
					vmax = apserv.safeminposrv(xc(i), xc(i) - xn(i), vmax)
					If CDbl(vmax) < CDbl(v) Then
						varidx = i
					End If
				End If
			Next
			stplen = System.Math.Min(vmax, 1.0)

			'
			' Perform step with activation.
			'
			' NOTE: it is important to use (1-StpLen)*XC + StpLen*XN because
			'       it allows us to step exactly to XN when StpLen=1, even in
			'       the presence of numerical errors.
			'
			For i = 0 To n - 1
				xc(i) = (1 - stplen) * xc(i) + stplen * xn(i)
			Next
			If varidx >= 0 Then
				xc(varidx) = 0.0
				result = True
			End If
			For i = 0 To n - 1
				If nnc(i) AndAlso CDbl(xc(i)) < CDbl(0.0) Then
					xc(i) = 0.0
					result = True
				End If
			Next
			Return result
		End Function


	End Class
	Public Class sactivesets
		'************************************************************************
'        This structure describes set of linear constraints (boundary  and  general
'        ones) which can be active and inactive. It also has functionality to  work
'        with current point and current  gradient  (determine  active  constraints,
'        move current point, project gradient into  constrained  subspace,  perform
'        constrained preconditioning and so on.
'
'        This  structure  is  intended  to  be  used  by constrained optimizers for
'        management of all constraint-related functionality.
'
'        External code may access following internal fields of the structure:
'            XC          -   stores current point, array[N].
'                            can be accessed only in optimization mode
'            ActiveSet   -   active set, array[N+NEC+NIC]:
'                            * ActiveSet[I]>0    I-th constraint is in the active set
'                            * ActiveSet[I]=0    I-th constraint is at the boundary, but inactive
'                            * ActiveSet[I]<0    I-th constraint is far from the boundary (and inactive)
'                            * elements from 0 to N-1 correspond to boundary constraints
'                            * elements from N to N+NEC+NIC-1 correspond to linear constraints
'                            * elements from N to N+NEC-1 are always +1
'            PBasis,
'            IBasis,
'            SBasis      -   after call to SASRebuildBasis() these  matrices  store
'                            active constraints, reorthogonalized with  respect  to
'                            some inner product:
'                            a) for PBasis - one  given  by  preconditioner  matrix
'                               (inverse Hessian)
'                            b) for SBasis - one given by square of the scale matrix
'                            c) for IBasis - traditional dot product
'                            
'                            array[BasisSize,N+1], where BasisSize is a  number  of
'                            positive elements in  ActiveSet[N:N+NEC+NIC-1].  First
'                            N columns store linear term, last column stores  right
'                            part. All  three  matrices  are linearly equivalent to
'                            each other, span(PBasis)=span(IBasis)=span(SBasis).
'                            
'                            IMPORTANT: you have to call  SASRebuildBasis()  before
'                                       accessing these arrays  in  order  to  make
'                                       sure that they are up to date.
'            BasisSize   -   basis size (PBasis/SBasis/IBasis)
'        ************************************************************************

		Public Class sactiveset
			Inherits apobject
			Public n As Integer
			Public algostate As Integer
			Public xc As Double()
			Public hasxc As Boolean
			Public s As Double()
			Public h As Double()
			Public activeset As Integer()
			Public basisisready As Boolean
			Public sbasis As Double(,)
			Public pbasis As Double(,)
			Public ibasis As Double(,)
			Public basissize As Integer
			Public constraintschanged As Boolean
			Public hasbndl As Boolean()
			Public hasbndu As Boolean()
			Public bndl As Double()
			Public bndu As Double()
			Public cleic As Double(,)
			Public nec As Integer
			Public nic As Integer
			Public mtx As Double()
			Public mtas As Integer()
			Public cdtmp As Double()
			Public corrtmp As Double()
			Public unitdiagonal As Double()
			Public solver As snnls.snnlssolver
			Public scntmp As Double()
			Public tmp0 As Double()
			Public tmpfeas As Double()
			Public tmpm0 As Double(,)
			Public rctmps As Double()
			Public rctmpg As Double()
			Public rctmprightpart As Double()
			Public rctmpdense0 As Double(,)
			Public rctmpdense1 As Double(,)
			Public rctmpisequality As Boolean()
			Public rctmpconstraintidx As Integer()
			Public rctmplambdas As Double()
			Public tmpbasis As Double(,)
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				xc = New Double(-1) {}
				s = New Double(-1) {}
				h = New Double(-1) {}
				activeset = New Integer(-1) {}
				sbasis = New Double(-1, -1) {}
				pbasis = New Double(-1, -1) {}
				ibasis = New Double(-1, -1) {}
				hasbndl = New Boolean(-1) {}
				hasbndu = New Boolean(-1) {}
				bndl = New Double(-1) {}
				bndu = New Double(-1) {}
				cleic = New Double(-1, -1) {}
				mtx = New Double(-1) {}
				mtas = New Integer(-1) {}
				cdtmp = New Double(-1) {}
				corrtmp = New Double(-1) {}
				unitdiagonal = New Double(-1) {}
				solver = New snnls.snnlssolver()
				scntmp = New Double(-1) {}
				tmp0 = New Double(-1) {}
				tmpfeas = New Double(-1) {}
				tmpm0 = New Double(-1, -1) {}
				rctmps = New Double(-1) {}
				rctmpg = New Double(-1) {}
				rctmprightpart = New Double(-1) {}
				rctmpdense0 = New Double(-1, -1) {}
				rctmpdense1 = New Double(-1, -1) {}
				rctmpisequality = New Boolean(-1) {}
				rctmpconstraintidx = New Integer(-1) {}
				rctmplambdas = New Double(-1) {}
				tmpbasis = New Double(-1, -1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New sactiveset()
				_result.n = n
				_result.algostate = algostate
				_result.xc = DirectCast(xc.Clone(), Double())
				_result.hasxc = hasxc
				_result.s = DirectCast(s.Clone(), Double())
				_result.h = DirectCast(h.Clone(), Double())
				_result.activeset = DirectCast(activeset.Clone(), Integer())
				_result.basisisready = basisisready
				_result.sbasis = DirectCast(sbasis.Clone(), Double(,))
				_result.pbasis = DirectCast(pbasis.Clone(), Double(,))
				_result.ibasis = DirectCast(ibasis.Clone(), Double(,))
				_result.basissize = basissize
				_result.constraintschanged = constraintschanged
				_result.hasbndl = DirectCast(hasbndl.Clone(), Boolean())
				_result.hasbndu = DirectCast(hasbndu.Clone(), Boolean())
				_result.bndl = DirectCast(bndl.Clone(), Double())
				_result.bndu = DirectCast(bndu.Clone(), Double())
				_result.cleic = DirectCast(cleic.Clone(), Double(,))
				_result.nec = nec
				_result.nic = nic
				_result.mtx = DirectCast(mtx.Clone(), Double())
				_result.mtas = DirectCast(mtas.Clone(), Integer())
				_result.cdtmp = DirectCast(cdtmp.Clone(), Double())
				_result.corrtmp = DirectCast(corrtmp.Clone(), Double())
				_result.unitdiagonal = DirectCast(unitdiagonal.Clone(), Double())
				_result.solver = DirectCast(solver.make_copy(), snnls.snnlssolver)
				_result.scntmp = DirectCast(scntmp.Clone(), Double())
				_result.tmp0 = DirectCast(tmp0.Clone(), Double())
				_result.tmpfeas = DirectCast(tmpfeas.Clone(), Double())
				_result.tmpm0 = DirectCast(tmpm0.Clone(), Double(,))
				_result.rctmps = DirectCast(rctmps.Clone(), Double())
				_result.rctmpg = DirectCast(rctmpg.Clone(), Double())
				_result.rctmprightpart = DirectCast(rctmprightpart.Clone(), Double())
				_result.rctmpdense0 = DirectCast(rctmpdense0.Clone(), Double(,))
				_result.rctmpdense1 = DirectCast(rctmpdense1.Clone(), Double(,))
				_result.rctmpisequality = DirectCast(rctmpisequality.Clone(), Boolean())
				_result.rctmpconstraintidx = DirectCast(rctmpconstraintidx.Clone(), Integer())
				_result.rctmplambdas = DirectCast(rctmplambdas.Clone(), Double())
				_result.tmpbasis = DirectCast(tmpbasis.Clone(), Double(,))
				Return _result
			End Function
		End Class




		'************************************************************************
'        This   subroutine   is   used  to initialize active set. By default, empty
'        N-variable model with no constraints is  generated.  Previously  allocated
'        buffer variables are reused as much as possible.
'
'        Two use cases for this object are described below.
'
'        CASE 1 - STEEPEST DESCENT:
'
'            SASInit()
'            repeat:
'                SASReactivateConstraints()
'                SASDescentDirection()
'                SASExploreDirection()
'                SASMoveTo()
'            until convergence
'
'        CASE 1 - PRECONDITIONED STEEPEST DESCENT:
'
'            SASInit()
'            repeat:
'                SASReactivateConstraintsPrec()
'                SASDescentDirectionPrec()
'                SASExploreDirection()
'                SASMoveTo()
'            until convergence
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasinit(n As Integer, s As sactiveset)
			Dim i As Integer = 0

			s.n = n
			s.algostate = 0

			'
			' Constraints
			'
			s.constraintschanged = True
			s.nec = 0
			s.nic = 0
			apserv.rvectorsetlengthatleast(s.bndl, n)
			apserv.bvectorsetlengthatleast(s.hasbndl, n)
			apserv.rvectorsetlengthatleast(s.bndu, n)
			apserv.bvectorsetlengthatleast(s.hasbndu, n)
			For i = 0 To n - 1
				s.bndl(i) = [Double].NegativeInfinity
				s.bndu(i) = [Double].PositiveInfinity
				s.hasbndl(i) = False
				s.hasbndu(i) = False
			Next

			'
			' current point, scale
			'
			s.hasxc = False
			apserv.rvectorsetlengthatleast(s.xc, n)
			apserv.rvectorsetlengthatleast(s.s, n)
			apserv.rvectorsetlengthatleast(s.h, n)
			For i = 0 To n - 1
				s.xc(i) = 0.0
				s.s(i) = 1.0
				s.h(i) = 1.0
			Next

			'
			' Other
			'
			apserv.rvectorsetlengthatleast(s.unitdiagonal, n)
			For i = 0 To n - 1
				s.unitdiagonal(i) = 1.0
			Next
		End Sub


		'************************************************************************
'        This function sets scaling coefficients for SAS object.
'
'        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'        size and gradient are scaled before comparison with tolerances).  Scale of
'        the I-th variable is a translation invariant measure of:
'        a) "how large" the variable is
'        b) how large the step should be to make significant changes in the function
'
'        During orthogonalization phase, scale is used to calculate drop tolerances
'        (whether vector is significantly non-zero or not).
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            S       -   array[N], non-zero scaling coefficients
'                        S[i] may be negative, sign doesn't matter.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sassetscale(state As sactiveset, s As Double())
			Dim i As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASSetScale: you may change scale only in modification mode")
			alglib.ap.assert(alglib.ap.len(s) >= state.n, "SASSetScale: Length(S)<N")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(s(i)), "SASSetScale: S contains infinite or NAN elements")
				alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "SASSetScale: S contains zero elements")
			Next
			For i = 0 To state.n - 1
				state.s(i) = System.Math.Abs(s(i))
			Next
		End Sub


		'************************************************************************
'        Modification  of  the  preconditioner:  diagonal of approximate Hessian is
'        used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            D       -   diagonal of the approximate Hessian, array[0..N-1],
'                        (if larger, only leading N elements are used).
'
'        NOTE 1: D[i] should be positive. Exception will be thrown otherwise.
'
'        NOTE 2: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sassetprecdiag(state As sactiveset, d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASSetPrecDiag: you may change preconditioner only in modification mode")
			alglib.ap.assert(alglib.ap.len(d) >= state.n, "SASSetPrecDiag: D is too short")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(d(i)), "SASSetPrecDiag: D contains infinite or NAN elements")
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "SASSetPrecDiag: D contains non-positive elements")
			Next
			For i = 0 To state.n - 1
				state.h(i) = d(i)
			Next
		End Sub


		'************************************************************************
'        This function sets/changes boundary constraints.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            BndL    -   lower bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very small number or -INF.
'            BndU    -   upper bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very large number or +INF.
'
'        NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
'        variable will be "frozen" at X[i]=BndL[i]=BndU[i].
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sassetbc(state As sactiveset, bndl As Double(), bndu As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASSetBC: you may change constraints only in modification mode")
			n = state.n
			alglib.ap.assert(alglib.ap.len(bndl) >= n, "SASSetBC: Length(BndL)<N")
			alglib.ap.assert(alglib.ap.len(bndu) >= n, "SASSetBC: Length(BndU)<N")
			For i = 0 To n - 1
				alglib.ap.assert(Math.isfinite(bndl(i)) OrElse [Double].IsNegativeInfinity(bndl(i)), "SASSetBC: BndL contains NAN or +INF")
				alglib.ap.assert(Math.isfinite(bndu(i)) OrElse [Double].IsPositiveInfinity(bndu(i)), "SASSetBC: BndL contains NAN or -INF")
				state.bndl(i) = bndl(i)
				state.hasbndl(i) = Math.isfinite(bndl(i))
				state.bndu(i) = bndu(i)
				state.hasbndu(i) = Math.isfinite(bndu(i))
			Next
			state.constraintschanged = True
		End Sub


		'************************************************************************
'        This function sets linear constraints for SAS object.
'
'        Linear constraints are inactive by default (after initial creation).
'
'        INPUT PARAMETERS:
'            State   -   SAS structure
'            C       -   linear constraints, array[K,N+1].
'                        Each row of C represents one constraint, either equality
'                        or inequality (see below):
'                        * first N elements correspond to coefficients,
'                        * last element corresponds to the right part.
'                        All elements of C (including right part) must be finite.
'            CT      -   type of constraints, array[K]:
'                        * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
'                        * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
'                        * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
'            K       -   number of equality/inequality constraints, K>=0
'
'        NOTE 1: linear (non-bound) constraints are satisfied only approximately:
'        * there always exists some minor violation (about Epsilon in magnitude)
'          due to rounding errors
'        * numerical differentiation, if used, may  lead  to  function  evaluations
'          outside  of the feasible  area,   because   algorithm  does  NOT  change
'          numerical differentiation formula according to linear constraints.
'        If you want constraints to be  satisfied  exactly, try to reformulate your
'        problem  in  such  manner  that  all constraints will become boundary ones
'        (this kind of constraints is always satisfied exactly, both in  the  final
'        solution and in all intermediate points).
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sassetlc(state As sactiveset, c As Double(,), ct As Integer(), k As Integer)
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASSetLC: you may change constraints only in modification mode")
			n = state.n

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(k >= 0, "SASSetLC: K<0")
			alglib.ap.assert(alglib.ap.cols(c) >= n + 1 OrElse k = 0, "SASSetLC: Cols(C)<N+1")
			alglib.ap.assert(alglib.ap.rows(c) >= k, "SASSetLC: Rows(C)<K")
			alglib.ap.assert(alglib.ap.len(ct) >= k, "SASSetLC: Length(CT)<K")
			alglib.ap.assert(apserv.apservisfinitematrix(c, k, n + 1), "SASSetLC: C contains infinite or NaN values!")

			'
			' Handle zero K
			'
			If k = 0 Then
				state.nec = 0
				state.nic = 0
				state.constraintschanged = True
				Return
			End If

			'
			' Equality constraints are stored first, in the upper
			' NEC rows of State.CLEIC matrix. Inequality constraints
			' are stored in the next NIC rows.
			'
			' NOTE: we convert inequality constraints to the form
			' A*x<=b before copying them.
			'
			apserv.rmatrixsetlengthatleast(state.cleic, k, n + 1)
			state.nec = 0
			state.nic = 0
			For i = 0 To k - 1
				If ct(i) = 0 Then
					For i_ = 0 To n
						state.cleic(state.nec, i_) = c(i, i_)
					Next
					state.nec = state.nec + 1
				End If
			Next
			For i = 0 To k - 1
				If ct(i) <> 0 Then
					If ct(i) > 0 Then
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = -c(i, i_)
						Next
					Else
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = c(i, i_)
						Next
					End If
					state.nic = state.nic + 1
				End If
			Next

			'
			' Mark state as changed
			'
			state.constraintschanged = True
		End Sub


		'************************************************************************
'        Another variation of SASSetLC(), which accepts  linear  constraints  using
'        another representation.
'
'        Linear constraints are inactive by default (after initial creation).
'
'        INPUT PARAMETERS:
'            State   -   SAS structure
'            CLEIC   -   linear constraints, array[NEC+NIC,N+1].
'                        Each row of C represents one constraint:
'                        * first N elements correspond to coefficients,
'                        * last element corresponds to the right part.
'                        First NEC rows store equality constraints, next NIC -  are
'                        inequality ones.
'                        All elements of C (including right part) must be finite.
'            NEC     -   number of equality constraints, NEC>=0
'            NIC     -   number of inequality constraints, NIC>=0
'
'        NOTE 1: linear (non-bound) constraints are satisfied only approximately:
'        * there always exists some minor violation (about Epsilon in magnitude)
'          due to rounding errors
'        * numerical differentiation, if used, may  lead  to  function  evaluations
'          outside  of the feasible  area,   because   algorithm  does  NOT  change
'          numerical differentiation formula according to linear constraints.
'        If you want constraints to be  satisfied  exactly, try to reformulate your
'        problem  in  such  manner  that  all constraints will become boundary ones
'        (this kind of constraints is always satisfied exactly, both in  the  final
'        solution and in all intermediate points).
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sassetlcx(state As sactiveset, cleic As Double(,), nec As Integer, nic As Integer)
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASSetLCX: you may change constraints only in modification mode")
			n = state.n

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(nec >= 0, "SASSetLCX: NEC<0")
			alglib.ap.assert(nic >= 0, "SASSetLCX: NIC<0")
			alglib.ap.assert(alglib.ap.cols(cleic) >= n + 1 OrElse nec + nic = 0, "SASSetLCX: Cols(CLEIC)<N+1")
			alglib.ap.assert(alglib.ap.rows(cleic) >= nec + nic, "SASSetLCX: Rows(CLEIC)<NEC+NIC")
			alglib.ap.assert(apserv.apservisfinitematrix(cleic, nec + nic, n + 1), "SASSetLCX: CLEIC contains infinite or NaN values!")

			'
			' Store constraints
			'
			apserv.rmatrixsetlengthatleast(state.cleic, nec + nic, n + 1)
			state.nec = nec
			state.nic = nic
			For i = 0 To nec + nic - 1
				For j = 0 To n
					state.cleic(i, j) = cleic(i, j)
				Next
			Next

			'
			' Mark state as changed
			'
			state.constraintschanged = True
		End Sub


		'************************************************************************
'        This subroutine turns on optimization mode:
'        1. feasibility in X is enforced  (in case X=S.XC and constraints  have not
'           changed, algorithm just uses X without any modifications at all)
'        2. constraints are marked as "candidate" or "inactive"
'
'        INPUT PARAMETERS:
'            S   -   active set object
'            X   -   initial point (candidate), array[N]. It is expected that X
'                    contains only finite values (we do not check it).
'            
'        OUTPUT PARAMETERS:
'            S   -   state is changed
'            X   -   initial point can be changed to enforce feasibility
'            
'        RESULT:
'            True in case feasible point was found (mode was changed to "optimization")
'            False in case no feasible point was found (mode was not changed)
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function sasstartoptimization(state As sactiveset, x As Double()) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(state.algostate = 0, "SASStartOptimization: already in optimization mode")
			result = False
			n = state.n
			nec = state.nec
			nic = state.nic

			'
			' Enforce feasibility and calculate set of "candidate"/"active" constraints.
			' Always active equality constraints are marked as "active", all other constraints
			' are marked as "candidate".
			'
			apserv.ivectorsetlengthatleast(state.activeset, n + nec + nic)
			For i = 0 To n - 1
				If state.hasbndl(i) AndAlso state.hasbndu(i) Then
					If CDbl(state.bndl(i)) > CDbl(state.bndu(i)) Then
						Return result
					End If
				End If
			Next
			For i_ = 0 To n - 1
				state.xc(i_) = x(i_)
			Next
			If state.nec + state.nic > 0 Then

				'
				' General linear constraints are present; general code is used.
				'
				apserv.rvectorsetlengthatleast(state.tmp0, n)
				apserv.rvectorsetlengthatleast(state.tmpfeas, n + state.nic)
				apserv.rmatrixsetlengthatleast(state.tmpm0, state.nec + state.nic, n + state.nic + 1)
				For i = 0 To state.nec + state.nic - 1
					For i_ = 0 To n - 1
						state.tmpm0(i, i_) = state.cleic(i, i_)
					Next
					For j = n To n + state.nic - 1
						state.tmpm0(i, j) = 0
					Next
					If i >= state.nec Then
						state.tmpm0(i, n + i - state.nec) = 1.0
					End If
					state.tmpm0(i, n + state.nic) = state.cleic(i, n)
				Next
				For i_ = 0 To n - 1
					state.tmpfeas(i_) = state.xc(i_)
				Next
				For i = 0 To state.nic - 1
					v = 0.0
					For i_ = 0 To n - 1
						v += state.cleic(i + state.nec, i_) * state.xc(i_)
					Next
					state.tmpfeas(i + n) = System.Math.Max(state.cleic(i + state.nec, n) - v, 0.0)
				Next
				If Not optserv.findfeasiblepoint(state.tmpfeas, state.bndl, state.hasbndl, state.bndu, state.hasbndu, n, _
					state.nic, state.tmpm0, state.nec + state.nic, 1E-06, i, j) Then
					Return result
				End If
				For i_ = 0 To n - 1
					state.xc(i_) = state.tmpfeas(i_)
				Next
				For i = 0 To n - 1
					If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
						state.activeset(i) = 1
						Continue For
					End If
					If (state.hasbndl(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndl(i))) OrElse (state.hasbndu(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndu(i))) Then
						state.activeset(i) = 0
						Continue For
					End If
					state.activeset(i) = -1
				Next
				For i = 0 To state.nec - 1
					state.activeset(n + i) = 1
				Next
				For i = 0 To state.nic - 1
					If CDbl(state.tmpfeas(n + i)) = CDbl(0) Then
						state.activeset(n + state.nec + i) = 0
					Else
						state.activeset(n + state.nec + i) = -1
					End If
				Next
			Else

				'
				' Only bound constraints are present, quick code can be used
				'
				For i = 0 To n - 1
					state.activeset(i) = -1
					If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
						state.activeset(i) = 1
						state.xc(i) = state.bndl(i)
						Continue For
					End If
					If state.hasbndl(i) AndAlso CDbl(state.xc(i)) <= CDbl(state.bndl(i)) Then
						state.xc(i) = state.bndl(i)
						state.activeset(i) = 0
						Continue For
					End If
					If state.hasbndu(i) AndAlso CDbl(state.xc(i)) >= CDbl(state.bndu(i)) Then
						state.xc(i) = state.bndu(i)
						state.activeset(i) = 0
						Continue For
					End If
				Next
			End If

			'
			' Change state, allocate temporaries
			'
			result = True
			state.algostate = 1
			state.basisisready = False
			state.hasxc = True
			apserv.rmatrixsetlengthatleast(state.pbasis, System.Math.Min(nec + nic, n), n + 1)
			apserv.rmatrixsetlengthatleast(state.ibasis, System.Math.Min(nec + nic, n), n + 1)
			apserv.rmatrixsetlengthatleast(state.sbasis, System.Math.Min(nec + nic, n), n + 1)
			Return result
		End Function


		'************************************************************************
'        This function explores search direction and calculates bound for  step  as
'        well as information for activation of constraints.
'
'        INPUT PARAMETERS:
'            State       -   SAS structure which stores current point and all other
'                            active set related information
'            D           -   descent direction to explore
'
'        OUTPUT PARAMETERS:
'            StpMax      -   upper  limit  on  step  length imposed by yet inactive
'                            constraints. Can be  zero  in  case  some  constraints
'                            can be activated by zero step.  Equal  to  some  large
'                            value in case step is unlimited.
'            CIdx        -   -1 for unlimited step, in [0,N+NEC+NIC) in case of
'                            limited step.
'            VVal        -   value which is assigned to X[CIdx] during activation.
'                            For CIdx<0 or CIdx>=N some dummy value is assigned to
'                            this parameter.
'        ************************************************************************

		Public Shared Sub sasexploredirection(state As sactiveset, d As Double(), ByRef stpmax As Double, ByRef cidx As Integer, ByRef vval As Double)
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim i As Integer = 0
			Dim prevmax As Double = 0
			Dim vc As Double = 0
			Dim vd As Double = 0
			Dim i_ As Integer = 0

			stpmax = 0
			cidx = 0
			vval = 0

			alglib.ap.assert(state.algostate = 1, "SASExploreDirection: is not in optimization mode")
			n = state.n
			nec = state.nec
			nic = state.nic
			cidx = -1
			vval = 0
			stpmax = 1E+50
			For i = 0 To n - 1
				If state.activeset(i) <= 0 Then
					alglib.ap.assert(Not state.hasbndl(i) OrElse CDbl(state.xc(i)) >= CDbl(state.bndl(i)), "SASExploreDirection: internal error - infeasible X")
					alglib.ap.assert(Not state.hasbndu(i) OrElse CDbl(state.xc(i)) <= CDbl(state.bndu(i)), "SASExploreDirection: internal error - infeasible X")
					If state.hasbndl(i) AndAlso CDbl(d(i)) < CDbl(0) Then
						prevmax = stpmax
						stpmax = apserv.safeminposrv(state.xc(i) - state.bndl(i), -d(i), stpmax)
						If CDbl(stpmax) < CDbl(prevmax) Then
							cidx = i
							vval = state.bndl(i)
						End If
					End If
					If state.hasbndu(i) AndAlso CDbl(d(i)) > CDbl(0) Then
						prevmax = stpmax
						stpmax = apserv.safeminposrv(state.bndu(i) - state.xc(i), d(i), stpmax)
						If CDbl(stpmax) < CDbl(prevmax) Then
							cidx = i
							vval = state.bndu(i)
						End If
					End If
				End If
			Next
			For i = nec To nec + nic - 1
				If state.activeset(n + i) <= 0 Then
					vc = 0.0
					For i_ = 0 To n - 1
						vc += state.cleic(i, i_) * state.xc(i_)
					Next
					vc = vc - state.cleic(i, n)
					vd = 0.0
					For i_ = 0 To n - 1
						vd += state.cleic(i, i_) * d(i_)
					Next
					If CDbl(vd) <= CDbl(0) Then
						Continue For
					End If
					If CDbl(vc) < CDbl(0) Then

						'
						' XC is strictly feasible with respect to I-th constraint,
						' we can perform non-zero step because there is non-zero distance
						' between XC and bound.
						'
						prevmax = stpmax
						stpmax = apserv.safeminposrv(-vc, vd, stpmax)
						If CDbl(stpmax) < CDbl(prevmax) Then
							cidx = n + i
						End If
					Else

						'
						' XC is at the boundary (or slightly beyond it), and step vector
						' points beyond the boundary.
						'
						' The only thing we can do is to perform zero step and activate
						' I-th constraint.
						'
						stpmax = 0
						cidx = n + i
					End If
				End If
			Next
		End Sub


		'************************************************************************
'        This subroutine moves current point to XN, which can be:
'        a) point in the direction previously explored  with  SASExploreDirection()
'           function (in this case NeedAct/CIdx/CVal are used)
'        b) point in arbitrary direction, not necessarily previously  checked  with
'           SASExploreDirection() function.
'
'        Step may activate one constraint. It is assumed than XN  is  approximately
'        feasible (small error as  large  as several  ulps  is  possible).   Strict
'        feasibility  with  respect  to  bound  constraints  is  enforced    during
'        activation, feasibility with respect to general linear constraints is  not
'        enforced.
'
'        This function activates boundary constraints, such that both is True:
'        1) XC[I] is not at the boundary
'        2) XN[I] is at the boundary or beyond it
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            XN      -   new point.
'            NeedAct -   True in case one constraint needs activation
'            CIdx    -   index of constraint, in [0,N+NEC+NIC).
'                        Ignored if NeedAct is false.
'                        This value is calculated by SASExploreDirection().
'            CVal    -   for CIdx in [0,N) this field stores value which is
'                        assigned to XC[CIdx] during activation. CVal is ignored in
'                        other cases.
'                        This value is calculated by SASExploreDirection().
'            
'        OUTPUT PARAMETERS:
'            S       -   current point and list of active constraints are changed.
'
'        RESULT:
'            >0, in case at least one inactive non-candidate constraint was activated
'            =0, in case only "candidate" constraints were activated
'            <0, in case no constraints were activated by the step
'
'        NOTE: in general case State.XC<>XN because activation of  constraints  may
'              slightly change current point (to enforce feasibility).
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function sasmoveto(state As sactiveset, xn As Double(), needact As Boolean, cidx As Integer, cval As Double) As Integer
			Dim result As Integer = 0
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim i As Integer = 0
			Dim wasactivation As New Boolean()

			alglib.ap.assert(state.algostate = 1, "SASMoveTo: is not in optimization mode")
			n = state.n
			nec = state.nec
			nic = state.nic

			'
			' Save previous state, update current point
			'
			apserv.rvectorsetlengthatleast(state.mtx, n)
			apserv.ivectorsetlengthatleast(state.mtas, n + nec + nic)
			For i = 0 To n - 1
				state.mtx(i) = state.xc(i)
				state.xc(i) = xn(i)
			Next
			For i = 0 To n + nec + nic - 1
				state.mtas(i) = state.activeset(i)
			Next

			'
			' Activate constraints
			'
			wasactivation = False
			If needact Then

				'
				' Activation
				'
				alglib.ap.assert(cidx >= 0 AndAlso cidx < n + nec + nic, "SASMoveTo: incorrect CIdx")
				If cidx < n Then

					'
					' CIdx in [0,N-1] means that bound constraint was activated.
					' We activate it explicitly to avoid situation when roundoff-error
					' prevents us from moving EXACTLY to x=CVal.
					'
					state.xc(cidx) = cval
				End If
				state.activeset(cidx) = 1
				wasactivation = True
			End If
			For i = 0 To n - 1

				'
				' Post-check (some constraints may be activated because of numerical errors)
				'
				If (state.hasbndl(i) AndAlso CDbl(state.xc(i)) <= CDbl(state.bndl(i))) AndAlso CDbl(state.xc(i)) <> CDbl(state.mtx(i)) Then
					state.xc(i) = state.bndl(i)
					state.activeset(i) = 1
					wasactivation = True
				End If
				If (state.hasbndu(i) AndAlso CDbl(state.xc(i)) >= CDbl(state.bndu(i))) AndAlso CDbl(state.xc(i)) <> CDbl(state.mtx(i)) Then
					state.xc(i) = state.bndu(i)
					state.activeset(i) = 1
					wasactivation = True
				End If
			Next

			'
			' Determine return status:
			' * -1 in case no constraints were activated
			' *  0 in case only "candidate" constraints were activated
			' * +1 in case at least one "non-candidate" constraint was activated
			'
			If wasactivation Then

				'
				' Step activated one/several constraints, but sometimes it is spurious
				' activation - RecalculateConstraints() tells us that constraint is
				' inactive (negative Largrange multiplier), but step activates it
				' because of numerical noise.
				'
				' This block of code checks whether step activated truly new constraints
				' (ones which were not in the active set at the solution):
				'
				' * for non-boundary constraint it is enough to check that previous value
				'   of ActiveSet[i] is negative (=far from boundary), and new one is
				'   positive (=we are at the boundary, constraint is activated).
				'
				' * for boundary constraints previous criterion won't work. Each variable
				'   has two constraints, and simply checking their status is not enough -
				'   we have to correctly identify cases when we leave one boundary
				'   (PrevActiveSet[i]=0) and move to another boundary (ActiveSet[i]>0).
				'   Such cases can be identified if we compare previous X with new X.
				'
				' In case only "candidate" constraints were activated, result variable
				' is set to 0. In case at least one new constraint was activated, result
				' is set to 1.
				'
				result = 0
				For i = 0 To n - 1
					If state.activeset(i) > 0 AndAlso CDbl(state.xc(i)) <> CDbl(state.mtx(i)) Then
						result = 1
					End If
				Next
				For i = n To n + state.nec + state.nic - 1
					If state.mtas(i) < 0 AndAlso state.activeset(i) > 0 Then
						result = 1
					End If
				Next
			Else

				'
				' No activation, return -1
				'
				result = -1
			End If

			'
			' Invalidate basis
			'
			state.basisisready = False
			Return result
		End Function


		'************************************************************************
'        This subroutine performs immediate activation of one constraint:
'        * "immediate" means that we do not have to move to activate it
'        * in case boundary constraint is activated, we enforce current point to be
'          exactly at the boundary
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            CIdx    -   index of constraint, in [0,N+NEC+NIC).
'                        This value is calculated by SASExploreDirection().
'            CVal    -   for CIdx in [0,N) this field stores value which is
'                        assigned to XC[CIdx] during activation. CVal is ignored in
'                        other cases.
'                        This value is calculated by SASExploreDirection().
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasimmediateactivation(state As sactiveset, cidx As Integer, cval As Double)
			alglib.ap.assert(state.algostate = 1, "SASMoveTo: is not in optimization mode")
			If cidx < state.n Then
				state.xc(cidx) = cval
			End If
			state.activeset(cidx) = 1
			state.basisisready = False
		End Sub


		'************************************************************************
'        This subroutine calculates descent direction subject to current active set.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            G       -   array[N], gradient
'            D       -   possibly prealocated buffer;
'                        automatically resized if needed.
'            
'        OUTPUT PARAMETERS:
'            D       -   descent direction projected onto current active set.
'                        Components of D which correspond to active boundary
'                        constraints are forced to be exactly zero.
'                        In case D is non-zero, it is normalized to have unit norm.
'                        
'        NOTE: in  case active set has N  active  constraints  (or  more),  descent
'              direction is forced to be exactly zero.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasconstraineddescent(state As sactiveset, g As Double(), ByRef d As Double())
			alglib.ap.assert(state.algostate = 1, "SASConstrainedDescent: is not in optimization mode")
			sasrebuildbasis(state)
			constraineddescent(state, g, state.unitdiagonal, state.ibasis, True, d)
		End Sub


		'************************************************************************
'        This  subroutine  calculates  preconditioned  descent direction subject to
'        current active set.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            G       -   array[N], gradient
'            D       -   possibly prealocated buffer;
'                        automatically resized if needed.
'            
'        OUTPUT PARAMETERS:
'            D       -   descent direction projected onto current active set.
'                        Components of D which correspond to active boundary
'                        constraints are forced to be exactly zero.
'                        In case D is non-zero, it is normalized to have unit norm.
'                        
'        NOTE: in  case active set has N  active  constraints  (or  more),  descent
'              direction is forced to be exactly zero.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasconstraineddescentprec(state As sactiveset, g As Double(), ByRef d As Double())
			alglib.ap.assert(state.algostate = 1, "SASConstrainedDescentPrec: is not in optimization mode")
			sasrebuildbasis(state)
			constraineddescent(state, g, state.h, state.pbasis, True, d)
		End Sub


		'************************************************************************
'        This subroutine calculates projection   of  direction  vector  to  current
'        active set.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            D       -   array[N], direction
'            
'        OUTPUT PARAMETERS:
'            D       -   direction projected onto current active set.
'                        Components of D which correspond to active boundary
'                        constraints are forced to be exactly zero.
'                        
'        NOTE: in  case active set has N  active  constraints  (or  more),  descent
'              direction is forced to be exactly zero.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasconstraineddirection(state As sactiveset, ByRef d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(state.algostate = 1, "SASConstrainedAntigradientPrec: is not in optimization mode")
			sasrebuildbasis(state)
			constraineddescent(state, d, state.unitdiagonal, state.ibasis, False, state.cdtmp)
			For i = 0 To state.n - 1
				d(i) = -state.cdtmp(i)
			Next
		End Sub


		'************************************************************************
'        This subroutine calculates product of direction vector and  preconditioner
'        multiplied subject to current active set.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            D       -   array[N], direction
'            
'        OUTPUT PARAMETERS:
'            D       -   preconditioned direction projected onto current active set.
'                        Components of D which correspond to active boundary
'                        constraints are forced to be exactly zero.
'                        
'        NOTE: in  case active set has N  active  constraints  (or  more),  descent
'              direction is forced to be exactly zero.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasconstraineddirectionprec(state As sactiveset, ByRef d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(state.algostate = 1, "SASConstrainedAntigradientPrec: is not in optimization mode")
			sasrebuildbasis(state)
			constraineddescent(state, d, state.h, state.pbasis, False, state.cdtmp)
			For i = 0 To state.n - 1
				d(i) = -state.cdtmp(i)
			Next
		End Sub


		'************************************************************************
'        This  subroutine  performs  correction of some (possibly infeasible) point
'        with respect to a) current active set, b) all boundary  constraints,  both
'        active and inactive:
'
'        0) we calculate L1 penalty term for violation of active linear constraints
'           (one which is returned by SASActiveLCPenalty1() function).
'        1) first, it performs projection (orthogonal with respect to scale  matrix
'           S) of X into current active set: X -> X1.
'        2) next, we perform projection with respect to  ALL  boundary  constraints
'           which are violated at X1: X1 -> X2.
'        3) X is replaced by X2.
'
'        The idea is that this function can preserve and enforce feasibility during
'        optimization, and additional penalty parameter can be used to prevent algo
'        from leaving feasible set because of rounding errors.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            X       -   array[N], candidate point
'            
'        OUTPUT PARAMETERS:
'            X       -   "improved" candidate point:
'                        a) feasible with respect to all boundary constraints
'                        b) feasibility with respect to active set is retained at
'                           good level.
'            Penalty -   penalty term, which can be added to function value if user
'                        wants to penalize violation of constraints (recommended).
'                        
'        NOTE: this function is not intended to find exact  projection  (i.e.  best
'              approximation) of X into feasible set. It just improves situation  a
'              bit.
'              Regular  use  of   this function will help you to retain feasibility
'              - if you already have something to start  with  and  constrain  your
'              steps is such way that the only source of infeasibility are roundoff
'              errors.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sascorrection(state As sactiveset, x As Double(), ByRef penalty As Double)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			penalty = 0

			alglib.ap.assert(state.algostate = 1, "SASCorrection: is not in optimization mode")
			sasrebuildbasis(state)
			n = state.n
			apserv.rvectorsetlengthatleast(state.corrtmp, n)

			'
			' Calculate penalty term.
			'
			penalty = sasactivelcpenalty1(state, x)

			'
			' Perform projection 1.
			'
			' This projecton is given by:
			'
			'     x_proj = x - S*S*As'*(As*x-b)
			'
			' where x is original x before projection, S is a scale matrix,
			' As is a matrix of equality constraints (active set) which were
			' orthogonalized with respect to inner product given by S (i.e. we
			' have As*S*S'*As'=I), b is a right part of the orthogonalized
			' constraints.
			'
			' NOTE: you can verify that x_proj is strictly feasible w.r.t.
			'       active set by multiplying it by As - you will get
			'       As*x_proj = As*x - As*x + b = b.
			'
			'       This formula for projection can be obtained by solving
			'       following minimization problem.
			'
			'           min ||inv(S)*(x_proj-x)||^2 s.t. As*x_proj=b
			'       
			'
			For i_ = 0 To n - 1
				state.corrtmp(i_) = x(i_)
			Next
			For i = 0 To state.basissize - 1
				v = -state.sbasis(i, n)
				For j = 0 To n - 1
					v = v + state.sbasis(i, j) * state.corrtmp(j)
				Next
				For j = 0 To n - 1
					state.corrtmp(j) = state.corrtmp(j) - v * state.sbasis(i, j) * Math.sqr(state.s(j))
				Next
			Next
			For i = 0 To n - 1
				If state.activeset(i) > 0 Then
					state.corrtmp(i) = state.xc(i)
				End If
			Next

			'
			' Perform projection 2
			'
			For i = 0 To n - 1
				x(i) = state.corrtmp(i)
				If state.hasbndl(i) AndAlso CDbl(x(i)) < CDbl(state.bndl(i)) Then
					x(i) = state.bndl(i)
				End If
				If state.hasbndu(i) AndAlso CDbl(x(i)) > CDbl(state.bndu(i)) Then
					x(i) = state.bndu(i)
				End If
			Next
		End Sub


		'************************************************************************
'        This  subroutine returns L1 penalty for violation of active general linear
'        constraints (violation of boundary or inactive linear constraints  is  not
'        added to penalty).
'
'        Penalty term is equal to:
'            
'            Penalty = SUM( Abs((C_i*x-R_i)/Alpha_i) )
'            
'        Here:
'        * summation is performed for I=0...NEC+NIC-1, ActiveSet[N+I]>0
'          (only for rows of CLEIC which are in active set)
'        * C_i is I-th row of CLEIC
'        * R_i is corresponding right part
'        * S is a scale matrix
'        * Alpha_i = ||S*C_i|| - is a scaling coefficient which "normalizes"
'          I-th summation term according to its scale.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            X       -   array[N], candidate point
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function sasactivelcpenalty1(state As sactiveset, x As Double()) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim v As Double = 0
			Dim alpha As Double = 0
			Dim p As Double = 0

			alglib.ap.assert(state.algostate = 1, "SASActiveLCPenalty1: is not in optimization mode")
			sasrebuildbasis(state)
			n = state.n
			nec = state.nec
			nic = state.nic

			'
			' Calculate penalty term.
			'
			result = 0
			For i = 0 To nec + nic - 1
				If state.activeset(n + i) > 0 Then
					alpha = 0
					p = -state.cleic(i, n)
					For j = 0 To n - 1
						v = state.cleic(i, j)
						p = p + v * x(j)
						alpha = alpha + Math.sqr(v * state.s(j))
					Next
					alpha = System.Math.sqrt(alpha)
					If CDbl(alpha) <> CDbl(0) Then
						result = result + System.Math.Abs(p / alpha)
					End If
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        This subroutine calculates scaled norm of  vector  after  projection  onto
'        subspace of active constraints. Most often this function is used  to  test
'        stopping conditions.
'
'        INPUT PARAMETERS:
'            S       -   active set object
'            D       -   vector whose norm is calculated
'            
'        RESULT:
'            Vector norm (after projection and scaling)
'            
'        NOTE: projection is performed first, scaling is performed after projection
'                        
'        NOTE: if we have N active constraints, zero value (exact zero) is returned
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function sasscaledconstrainednorm(state As sactiveset, d As Double()) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim n As Integer = 0
			Dim v As Double = 0
			Dim nactive As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(state.algostate = 1, "SASMoveTo: is not in optimization mode")
			n = state.n
			apserv.rvectorsetlengthatleast(state.scntmp, n)

			'
			' Prepare basis (if needed)
			'
			sasrebuildbasis(state)

			'
			' Calculate descent direction
			'
			nactive = 0
			For i = 0 To n - 1
				If state.activeset(i) > 0 Then
					state.scntmp(i) = 0
					nactive = nactive + 1
				Else
					state.scntmp(i) = d(i)
				End If
			Next
			If nactive + state.basissize >= n Then

				'
				' Quick exit if number of active constraints is N or larger
				'
				result = 0.0
				Return result
			End If
			For i = 0 To state.basissize - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += state.ibasis(i, i_) * state.scntmp(i_)
				Next
				For i_ = 0 To n - 1
					state.scntmp(i_) = state.scntmp(i_) - v * state.ibasis(i, i_)
				Next
			Next
			v = 0.0
			For i = 0 To n - 1
				v = v + Math.sqr(state.s(i) * state.scntmp(i))
			Next
			result = System.Math.sqrt(v)
			Return result
		End Function


		'************************************************************************
'        This subroutine turns off optimization mode.
'
'        INPUT PARAMETERS:
'            S   -   active set object
'            
'        OUTPUT PARAMETERS:
'            S   -   state is changed
'
'        NOTE: this function can be called many times for optimizer which was
'              already stopped.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasstopoptimization(state As sactiveset)
			state.algostate = 0
		End Sub


		'************************************************************************
'        This function recalculates constraints - activates  and  deactivates  them
'        according to gradient value at current point. Algorithm  assumes  that  we
'        want to make steepest descent step from  current  point;  constraints  are
'        activated and deactivated in such way that we won't violate any constraint
'        by steepest descent step.
'
'        After call to this function active set is ready to  try  steepest  descent
'        step (SASDescentDirection-SASExploreDirection-SASMoveTo).
'
'        Only already "active" and "candidate" elements of ActiveSet are  examined;
'        constraints which are not active are not examined.
'
'        INPUT PARAMETERS:
'            State       -   active set object
'            GC          -   array[N], gradient at XC
'            
'        OUTPUT PARAMETERS:
'            State       -   active set object, with new set of constraint
'
'          -- ALGLIB --
'             Copyright 26.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasreactivateconstraints(state As sactiveset, gc As Double())
			alglib.ap.assert(state.algostate = 1, "SASReactivateConstraints: must be in optimization mode")
			reactivateconstraints(state, gc, state.unitdiagonal)
		End Sub


		'************************************************************************
'        This function recalculates constraints - activates  and  deactivates  them
'        according to gradient value at current point.
'
'        Algorithm  assumes  that  we  want  to make Quasi-Newton step from current
'        point with diagonal Quasi-Newton matrix H. Constraints are  activated  and
'        deactivated in such way that we won't violate any constraint by step.
'
'        After call to  this  function  active set is ready to  try  preconditioned
'        steepest descent step (SASDescentDirection-SASExploreDirection-SASMoveTo).
'
'        Only already "active" and "candidate" elements of ActiveSet are  examined;
'        constraints which are not active are not examined.
'
'        INPUT PARAMETERS:
'            State       -   active set object
'            GC          -   array[N], gradient at XC
'            
'        OUTPUT PARAMETERS:
'            State       -   active set object, with new set of constraint
'
'          -- ALGLIB --
'             Copyright 26.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasreactivateconstraintsprec(state As sactiveset, gc As Double())
			alglib.ap.assert(state.algostate = 1, "SASReactivateConstraintsPrec: must be in optimization mode")
			reactivateconstraints(state, gc, state.h)
		End Sub


		'************************************************************************
'        This function builds three orthonormal basises for current active set:
'        * P-orthogonal one, which is orthogonalized with inner product
'          (x,y) = x'*P*y, where P=inv(H) is current preconditioner
'        * S-orthogonal one, which is orthogonalized with inner product
'          (x,y) = x'*S'*S*y, where S is diagonal scaling matrix
'        * I-orthogonal one, which is orthogonalized with standard dot product
'
'        NOTE: all sets of orthogonal vectors are guaranteed  to  have  same  size.
'              P-orthogonal basis is built first, I/S-orthogonal basises are forced
'              to have same number of vectors as P-orthogonal one (padded  by  zero
'              vectors if needed).
'              
'        NOTE: this function tracks changes in active set; first call  will  result
'              in reorthogonalization
'
'        INPUT PARAMETERS:
'            State   -   active set object
'            H       -   diagonal preconditioner, H[i]>0
'
'        OUTPUT PARAMETERS:
'            State   -   active set object with new basis
'            
'          -- ALGLIB --
'             Copyright 20.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub sasrebuildbasis(state As sactiveset)
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim t As Integer = 0
			Dim nactivelin As Integer = 0
			Dim nactivebnd As Integer = 0
			Dim v As Double = 0
			Dim vmax As Double = 0
			Dim kmax As Integer = 0
			Dim i_ As Integer = 0

			If state.basisisready Then
				Return
			End If
			n = state.n
			nec = state.nec
			nic = state.nic
			apserv.rmatrixsetlengthatleast(state.tmpbasis, nec + nic, n + 1)
			state.basissize = 0
			state.basisisready = True

			'
			' Determine number of active boundary and non-boundary
			' constraints, move them to TmpBasis. Quick exit if no
			' non-boundary constraints were detected.
			'
			nactivelin = 0
			nactivebnd = 0
			For i = 0 To nec + nic - 1
				If state.activeset(n + i) > 0 Then
					nactivelin = nactivelin + 1
				End If
			Next
			For j = 0 To n - 1
				If state.activeset(j) > 0 Then
					nactivebnd = nactivebnd + 1
				End If
			Next
			If nactivelin = 0 Then
				Return
			End If

			'
			' Orthogonalize linear constraints (inner product is given by preconditioner)
			' with respect to each other and boundary ones:
			' * normalize all constraints
			' * orthogonalize with respect to boundary ones
			' * repeat:
			'   * if basisSize+nactivebnd=n - TERMINATE
			'   * choose largest row from TmpBasis
			'   * if row norm is too small  - TERMINATE
			'   * add row to basis, normalize
			'   * remove from TmpBasis, orthogonalize other constraints with respect to this one
			'
			nactivelin = 0
			For i = 0 To nec + nic - 1
				If state.activeset(n + i) > 0 Then
					For i_ = 0 To n
						state.tmpbasis(nactivelin, i_) = state.cleic(i, i_)
					Next
					nactivelin = nactivelin + 1
				End If
			Next
			For i = 0 To nactivelin - 1
				v = 0.0
				For j = 0 To n - 1
					v = v + Math.sqr(state.tmpbasis(i, j)) / state.h(j)
				Next
				If CDbl(v) > CDbl(0) Then
					v = 1 / System.Math.sqrt(v)
					For j = 0 To n
						state.tmpbasis(i, j) = state.tmpbasis(i, j) * v
					Next
				End If
			Next
			For j = 0 To n - 1
				If state.activeset(j) > 0 Then
					For i = 0 To nactivelin - 1
						state.tmpbasis(i, n) = state.tmpbasis(i, n) - state.tmpbasis(i, j) * state.xc(j)
						state.tmpbasis(i, j) = 0.0
					Next
				End If
			Next
			While state.basissize + nactivebnd < n

				'
				' Find largest vector, add to basis
				'
				vmax = -1
				kmax = -1
				For i = 0 To nactivelin - 1
					v = 0.0
					For j = 0 To n - 1
						v = v + Math.sqr(state.tmpbasis(i, j)) / state.h(j)
					Next
					v = System.Math.sqrt(v)
					If CDbl(v) > CDbl(vmax) Then
						vmax = v
						kmax = i
					End If
				Next
				If CDbl(vmax) < CDbl(10000.0 * Math.machineepsilon) Then
					Exit While
				End If
				v = 1 / vmax
				For i_ = 0 To n
					state.pbasis(state.basissize, i_) = v * state.tmpbasis(kmax, i_)
				Next
				state.basissize = state.basissize + 1

				'
				' Reorthogonalize other vectors with respect to chosen one.
				' Remove it from the array.
				'
				For i = 0 To nactivelin - 1
					If i <> kmax Then
						v = 0
						For j = 0 To n - 1
							v = v + state.pbasis(state.basissize - 1, j) * state.tmpbasis(i, j) / state.h(j)
						Next
						For i_ = 0 To n
							state.tmpbasis(i, i_) = state.tmpbasis(i, i_) - v * state.pbasis(state.basissize - 1, i_)
						Next
					End If
				Next
				For j = 0 To n
					state.tmpbasis(kmax, j) = 0
				Next
			End While

			'
			' Orthogonalize linear constraints using traditional dot product
			' with respect to each other and boundary ones.
			'
			' NOTE: we force basis size to be equal to one which was computed
			'       at the previous step, with preconditioner-based inner product.
			'
			nactivelin = 0
			For i = 0 To nec + nic - 1
				If state.activeset(n + i) > 0 Then
					For i_ = 0 To n
						state.tmpbasis(nactivelin, i_) = state.cleic(i, i_)
					Next
					nactivelin = nactivelin + 1
				End If
			Next
			For i = 0 To nactivelin - 1
				v = 0.0
				For j = 0 To n - 1
					v = v + Math.sqr(state.tmpbasis(i, j))
				Next
				If CDbl(v) > CDbl(0) Then
					v = 1 / System.Math.sqrt(v)
					For j = 0 To n
						state.tmpbasis(i, j) = state.tmpbasis(i, j) * v
					Next
				End If
			Next
			For j = 0 To n - 1
				If state.activeset(j) > 0 Then
					For i = 0 To nactivelin - 1
						state.tmpbasis(i, n) = state.tmpbasis(i, n) - state.tmpbasis(i, j) * state.xc(j)
						state.tmpbasis(i, j) = 0.0
					Next
				End If
			Next
			For t = 0 To state.basissize - 1

				'
				' Find largest vector, add to basis.
				'
				vmax = -1
				kmax = -1
				For i = 0 To nactivelin - 1
					v = 0.0
					For j = 0 To n - 1
						v = v + Math.sqr(state.tmpbasis(i, j))
					Next
					v = System.Math.sqrt(v)
					If CDbl(v) > CDbl(vmax) Then
						vmax = v
						kmax = i
					End If
				Next
				If CDbl(vmax) = CDbl(0) Then
					For j = 0 To n
						state.ibasis(t, j) = 0.0
					Next
					Continue For
				End If
				v = 1 / vmax
				For i_ = 0 To n
					state.ibasis(t, i_) = v * state.tmpbasis(kmax, i_)
				Next

				'
				' Reorthogonalize other vectors with respect to chosen one.
				' Remove it from the array.
				'
				For i = 0 To nactivelin - 1
					If i <> kmax Then
						v = 0
						For j = 0 To n - 1
							v = v + state.ibasis(t, j) * state.tmpbasis(i, j)
						Next
						For i_ = 0 To n
							state.tmpbasis(i, i_) = state.tmpbasis(i, i_) - v * state.ibasis(t, i_)
						Next
					End If
				Next
				For j = 0 To n
					state.tmpbasis(kmax, j) = 0
				Next
			Next

			'
			' Orthogonalize linear constraints using inner product given by
			' scale matrix.
			'
			' NOTE: we force basis size to be equal to one which was computed
			'       with preconditioner-based inner product.
			'
			nactivelin = 0
			For i = 0 To nec + nic - 1
				If state.activeset(n + i) > 0 Then
					For i_ = 0 To n
						state.tmpbasis(nactivelin, i_) = state.cleic(i, i_)
					Next
					nactivelin = nactivelin + 1
				End If
			Next
			For i = 0 To nactivelin - 1
				v = 0.0
				For j = 0 To n - 1
					v = v + Math.sqr(state.tmpbasis(i, j) * state.s(j))
				Next
				If CDbl(v) > CDbl(0) Then
					v = 1 / System.Math.sqrt(v)
					For j = 0 To n
						state.tmpbasis(i, j) = state.tmpbasis(i, j) * v
					Next
				End If
			Next
			For j = 0 To n - 1
				If state.activeset(j) > 0 Then
					For i = 0 To nactivelin - 1
						state.tmpbasis(i, n) = state.tmpbasis(i, n) - state.tmpbasis(i, j) * state.xc(j)
						state.tmpbasis(i, j) = 0.0
					Next
				End If
			Next
			For t = 0 To state.basissize - 1

				'
				' Find largest vector, add to basis.
				'
				vmax = -1
				kmax = -1
				For i = 0 To nactivelin - 1
					v = 0.0
					For j = 0 To n - 1
						v = v + Math.sqr(state.tmpbasis(i, j) * state.s(j))
					Next
					v = System.Math.sqrt(v)
					If CDbl(v) > CDbl(vmax) Then
						vmax = v
						kmax = i
					End If
				Next
				If CDbl(vmax) = CDbl(0) Then
					For j = 0 To n
						state.sbasis(t, j) = 0.0
					Next
					Continue For
				End If
				v = 1 / vmax
				For i_ = 0 To n
					state.sbasis(t, i_) = v * state.tmpbasis(kmax, i_)
				Next

				'
				' Reorthogonalize other vectors with respect to chosen one.
				' Remove it from the array.
				'
				For i = 0 To nactivelin - 1
					If i <> kmax Then
						v = 0
						For j = 0 To n - 1
							v = v + state.sbasis(t, j) * state.tmpbasis(i, j) * Math.sqr(state.s(j))
						Next
						For i_ = 0 To n
							state.tmpbasis(i, i_) = state.tmpbasis(i, i_) - v * state.sbasis(t, i_)
						Next
					End If
				Next
				For j = 0 To n
					state.tmpbasis(kmax, j) = 0
				Next
			Next
		End Sub


		'************************************************************************
'        This  subroutine  calculates  preconditioned  descent direction subject to
'        current active set.
'
'        INPUT PARAMETERS:
'            State   -   active set object
'            G       -   array[N], gradient
'            H       -   array[N], Hessian matrix
'            HA      -   active constraints orthogonalized in such way
'                        that HA*inv(H)*HA'= I.
'            Normalize-  whether we need normalized descent or not
'            D       -   possibly preallocated buffer; automatically resized.
'            
'        OUTPUT PARAMETERS:
'            D       -   descent direction projected onto current active set.
'                        Components of D which correspond to active boundary
'                        constraints are forced to be exactly zero.
'                        In case D is non-zero and Normalize is True, it is
'                        normalized to have unit norm.
'                        
'        NOTE: if we have N active constraints, D is explicitly set to zero.
'
'          -- ALGLIB --
'             Copyright 21.12.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub constraineddescent(state As sactiveset, g As Double(), h As Double(), ha As Double(,), normalize As Boolean, ByRef d As Double())
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0
			Dim v As Double = 0
			Dim nactive As Integer = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(state.algostate = 1, "SAS: internal error in ConstrainedDescent() - not in optimization mode")
			alglib.ap.assert(state.basisisready, "SAS: internal error in ConstrainedDescent() - no basis")
			n = state.n
			apserv.rvectorsetlengthatleast(d, n)

			'
			' Calculate preconditioned constrained descent direction:
			'
			'     d := -inv(H)*( g - HA'*(HA*inv(H)*g) )
			'
			' Formula above always gives direction which is orthogonal to rows of HA.
			' You can verify it by multiplication of both sides by HA[i] (I-th row),
			' taking into account that HA*inv(H)*HA'= I (by definition of HA - it is
			' orthogonal basis with inner product given by inv(H)).
			'
			nactive = 0
			For i = 0 To n - 1
				If state.activeset(i) > 0 Then
					d(i) = 0
					nactive = nactive + 1
				Else
					d(i) = g(i)
				End If
			Next
			For i = 0 To state.basissize - 1
				v = 0.0
				For j = 0 To n - 1
					v = v + ha(i, j) * d(j) / h(j)
				Next
				For i_ = 0 To n - 1
					d(i_) = d(i_) - v * ha(i, i_)
				Next
				nactive = nactive + 1
			Next
			v = 0.0
			For i = 0 To n - 1
				If state.activeset(i) > 0 Then
					d(i) = 0
				Else
					d(i) = -(d(i) / h(i))
					v = v + Math.sqr(d(i))
				End If
			Next
			v = System.Math.sqrt(v)
			If nactive >= n Then
				v = 0
				For i = 0 To n - 1
					d(i) = 0
				Next
			End If
			If normalize AndAlso CDbl(v) > CDbl(0) Then
				For i = 0 To n - 1
					d(i) = d(i) / v
				Next
			End If
		End Sub


		'************************************************************************
'        This function recalculates constraints - activates  and  deactivates  them
'        according to gradient value at current point.
'
'        Algorithm  assumes  that  we  want  to make Quasi-Newton step from current
'        point with diagonal Quasi-Newton matrix H. Constraints are  activated  and
'        deactivated in such way that we won't violate any constraint by step.
'
'        Only already "active" and "candidate" elements of ActiveSet are  examined;
'        constraints which are not active are not examined.
'
'        INPUT PARAMETERS:
'            State       -   active set object
'            GC          -   array[N], gradient at XC
'            H           -   array[N], Hessian matrix
'            
'        OUTPUT PARAMETERS:
'            State       -   active set object, with new set of constraint
'
'          -- ALGLIB --
'             Copyright 26.09.2012 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub reactivateconstraints(state As sactiveset, gc As Double(), h As Double())
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim idx0 As Integer = 0
			Dim idx1 As Integer = 0
			Dim v As Double = 0
			Dim nactivebnd As Integer = 0
			Dim nactivelin As Integer = 0
			Dim nactiveconstraints As Integer = 0
			Dim rowscale As Double = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(state.algostate = 1, "SASReactivateConstraintsPrec: must be in optimization mode")

			'
			' Prepare
			'
			n = state.n
			nec = state.nec
			nic = state.nic
			state.basisisready = False

			'
			' Handle important special case - no linear constraints,
			' only boundary constraints are present
			'
			If nec + nic = 0 Then
				For i = 0 To n - 1
					If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
						state.activeset(i) = 1
						Continue For
					End If
					If (state.hasbndl(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndl(i))) AndAlso CDbl(gc(i)) >= CDbl(0) Then
						state.activeset(i) = 1
						Continue For
					End If
					If (state.hasbndu(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndu(i))) AndAlso CDbl(gc(i)) <= CDbl(0) Then
						state.activeset(i) = 1
						Continue For
					End If
					state.activeset(i) = -1
				Next
				Return
			End If

			'
			' General case.
			' Allocate temporaries.
			'
			apserv.rvectorsetlengthatleast(state.rctmpg, n)
			apserv.rvectorsetlengthatleast(state.rctmprightpart, n)
			apserv.rvectorsetlengthatleast(state.rctmps, n)
			apserv.rmatrixsetlengthatleast(state.rctmpdense0, n, nec + nic)
			apserv.rmatrixsetlengthatleast(state.rctmpdense1, n, nec + nic)
			apserv.bvectorsetlengthatleast(state.rctmpisequality, n + nec + nic)
			apserv.ivectorsetlengthatleast(state.rctmpconstraintidx, n + nec + nic)

			'
			' Calculate descent direction
			'
			For i_ = 0 To n - 1
				state.rctmpg(i_) = -gc(i_)
			Next

			'
			' Determine candidates to the active set.
			'
			' After this block constraints become either "inactive" (ActiveSet[i]<0)
			' or "candidates" (ActiveSet[i]=0). Previously active constraints always
			' become "candidates".
			'
			For i = 0 To n + nec + nic - 1
				If state.activeset(i) > 0 Then
					state.activeset(i) = 0
				Else
					state.activeset(i) = -1
				End If
			Next
			nactiveconstraints = 0
			nactivebnd = 0
			nactivelin = 0
			For i = 0 To n - 1

				'
				' Activate boundary constraints:
				' * copy constraint index to RCTmpConstraintIdx
				' * set corresponding element of ActiveSet[] to "candidate"
				' * fill RCTmpS by either +1 (lower bound) or -1 (upper bound)
				' * set RCTmpIsEquality to False (BndL<BndU) or True (BndL=BndU)
				' * increase counters
				'
				If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then

					'
					' Equality constraint is activated
					'
					state.rctmpconstraintidx(nactiveconstraints) = i
					state.activeset(i) = 0
					state.rctmps(i) = 1.0
					state.rctmpisequality(nactiveconstraints) = True
					nactiveconstraints = nactiveconstraints + 1
					nactivebnd = nactivebnd + 1
					Continue For
				End If
				If state.hasbndl(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndl(i)) Then

					'
					' Lower bound is activated
					'
					state.rctmpconstraintidx(nactiveconstraints) = i
					state.activeset(i) = 0
					state.rctmps(i) = -1.0
					state.rctmpisequality(nactiveconstraints) = False
					nactiveconstraints = nactiveconstraints + 1
					nactivebnd = nactivebnd + 1
					Continue For
				End If
				If state.hasbndu(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndu(i)) Then

					'
					' Upper bound is activated
					'
					state.rctmpconstraintidx(nactiveconstraints) = i
					state.activeset(i) = 0
					state.rctmps(i) = 1.0
					state.rctmpisequality(nactiveconstraints) = False
					nactiveconstraints = nactiveconstraints + 1
					nactivebnd = nactivebnd + 1
					Continue For
				End If
			Next
			For i = 0 To nec + nic - 1
				If i >= nec AndAlso state.activeset(n + i) < 0 Then

					'
					' Inequality constraints are skipped if both (a) constraint was
					' not active, and (b) we are too far away from the boundary.
					'
					rowscale = 0.0
					v = -state.cleic(i, n)
					For j = 0 To n - 1
						v = v + state.cleic(i, j) * state.xc(j)
						rowscale = System.Math.Max(rowscale, System.Math.Abs(state.cleic(i, j) * state.s(j)))
					Next
					If CDbl(v) <= CDbl(-(100000.0 * Math.machineepsilon * rowscale)) Then

						'
						' NOTE: it is important to check for non-strict inequality
						'       because we have to correctly handle zero constraint
						'       0*x<=0
						'
						Continue For
					End If
				End If
				For i_ = 0 To n - 1
					state.rctmpdense0(i_, nactivelin) = state.cleic(i, i_)
				Next
				state.rctmpconstraintidx(nactiveconstraints) = n + i
				state.activeset(n + i) = 0
				state.rctmpisequality(nactiveconstraints) = i < nec
				nactiveconstraints = nactiveconstraints + 1
				nactivelin = nactivelin + 1
			Next

			'
			' Skip if no "candidate" constraints was found
			'
			If nactiveconstraints = 0 Then
				For i = 0 To n - 1
					If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
						state.activeset(i) = 1
						Continue For
					End If
					If (state.hasbndl(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndl(i))) AndAlso CDbl(gc(i)) >= CDbl(0) Then
						state.activeset(i) = 1
						Continue For
					End If
					If (state.hasbndu(i) AndAlso CDbl(state.xc(i)) = CDbl(state.bndu(i))) AndAlso CDbl(gc(i)) <= CDbl(0) Then
						state.activeset(i) = 1
						Continue For
					End If
				Next
				Return
			End If

			'
			' General case.
			'
			' APPROACH TO CONSTRAINTS ACTIVATION/DEACTIVATION
			'
			' We have NActiveConstraints "candidates": NActiveBnd boundary candidates,
			' NActiveLin linear candidates. Indexes of boundary constraints are stored
			' in RCTmpConstraintIdx[0:NActiveBnd-1], indexes of linear ones are stored
			' in RCTmpConstraintIdx[NActiveBnd:NActiveBnd+NActiveLin-1]. Some of the
			' constraints are equality ones, some are inequality - as specified by 
			' RCTmpIsEquality[i].
			'
			' Now we have to determine active subset of "candidates" set. In order to
			' do so we solve following constrained minimization problem:
			'         (                         )^2
			'     min ( SUM(lambda[i]*A[i]) + G )
			'         (                         )
			' Here:
			' * G is a gradient (column vector)
			' * A[i] is a column vector, linear (left) part of I-th constraint.
			'   I=0..NActiveConstraints-1, first NActiveBnd elements of A are just
			'   subset of identity matrix (boundary constraints), next NActiveLin
			'   elements are subset of rows of the matrix of general linear constraints.
			' * lambda[i] is a Lagrange multiplier corresponding to I-th constraint
			'
			' NOTE: for preconditioned setting A is replaced by A*H^(-0.5), G is
			'       replaced by G*H^(-0.5). We apply this scaling at the last stage,
			'       before passing data to NNLS solver.
			'
			' Minimization is performed subject to non-negativity constraints on
			' lambda[i] corresponding to inequality constraints. Inequality constraints
			' which correspond to non-zero lambda are activated, equality constraints
			' are always considered active.
			'
			' Informally speaking, we "decompose" descent direction -G and represent
			' it as sum of constraint vectors and "residual" part (which is equal to
			' the actual descent direction subject to constraints).
			'
			' SOLUTION OF THE NNLS PROBLEM
			'
			' We solve this optimization problem with Non-Negative Least Squares solver,
			' which can efficiently solve least squares problems of the form
			'
			'         ( [ I | AU ]     )^2
			'     min ( [   |    ]*x-b )   s.t. non-negativity constraints on some x[i]
			'         ( [ 0 | AL ]     )
			'
			' In order to use this solver we have to rearrange rows of A[] and G in
			' such way that first NActiveBnd columns of A store identity matrix (before
			' sorting non-zero elements are randomly distributed in the first NActiveBnd
			' columns of A, during sorting we move them to first NActiveBnd rows).
			'
			' Then we create instance of NNLS solver (we reuse instance left from the
			' previous run of the optimization problem) and solve NNLS problem.
			'
			idx0 = 0
			idx1 = nactivebnd
			For i = 0 To n - 1
				If state.activeset(i) >= 0 Then
					v = 1 / System.Math.sqrt(h(i))
					For j = 0 To nactivelin - 1
						state.rctmpdense1(idx0, j) = state.rctmpdense0(i, j) / state.rctmps(i) * v
					Next
					state.rctmprightpart(idx0) = state.rctmpg(i) / state.rctmps(i) * v
					idx0 = idx0 + 1
				Else
					v = 1 / System.Math.sqrt(h(i))
					For j = 0 To nactivelin - 1
						state.rctmpdense1(idx1, j) = state.rctmpdense0(i, j) * v
					Next
					state.rctmprightpart(idx1) = state.rctmpg(i) * v
					idx1 = idx1 + 1
				End If
			Next
			snnls.snnlsinit(n, nec + nic, n, state.solver)
			snnls.snnlssetproblem(state.solver, state.rctmpdense1, state.rctmprightpart, nactivebnd, nactiveconstraints - nactivebnd, n)
			For i = 0 To nactiveconstraints - 1
				If state.rctmpisequality(i) Then
					snnls.snnlsdropnnc(state.solver, i)
				End If
			Next
			snnls.snnlssolve(state.solver, state.rctmplambdas)

			'
			' After solution of the problem we activate equality constraints (always active)
			' and inequality constraints with non-zero Lagrange multipliers. Then we reorthogonalize
			' active constraints.
			'
			For i = 0 To n + nec + nic - 1
				state.activeset(i) = -1
			Next
			For i = 0 To nactiveconstraints - 1
				If state.rctmpisequality(i) OrElse CDbl(state.rctmplambdas(i)) > CDbl(0) Then
					state.activeset(state.rctmpconstraintidx(i)) = 1
				Else
					state.activeset(state.rctmpconstraintidx(i)) = 0
				End If
			Next
			sasrebuildbasis(state)
		End Sub


	End Class
	Public Class mincg
		'************************************************************************
'        This object stores state of the nonlinear CG optimizer.
'
'        You should use ALGLIB functions to work with this object.
'        ************************************************************************

		Public Class mincgstate
			Inherits apobject
			Public n As Integer
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public stpmax As Double
			Public suggestedstep As Double
			Public xrep As Boolean
			Public drep As Boolean
			Public cgtype As Integer
			Public prectype As Integer
			Public diagh As Double()
			Public diaghl2 As Double()
			Public vcorr As Double(,)
			Public vcnt As Integer
			Public s As Double()
			Public diffstep As Double
			Public nfev As Integer
			Public mcstage As Integer
			Public k As Integer
			Public xk As Double()
			Public dk As Double()
			Public xn As Double()
			Public dn As Double()
			Public d As Double()
			Public fold As Double
			Public stp As Double
			Public curstpmax As Double
			Public yk As Double()
			Public lastgoodstep As Double
			Public lastscaledstep As Double
			Public mcinfo As Integer
			Public innerresetneeded As Boolean
			Public terminationneeded As Boolean
			Public trimthreshold As Double
			Public rstimer As Integer
			Public x As Double()
			Public f As Double
			Public g As Double()
			Public needf As Boolean
			Public needfg As Boolean
			Public xupdated As Boolean
			Public algpowerup As Boolean
			Public lsstart As Boolean
			Public lsend As Boolean
			Public userterminationneeded As Boolean
			Public teststep As Double
			Public rstate As rcommstate
			Public repiterationscount As Integer
			Public repnfev As Integer
			Public repvaridx As Integer
			Public repterminationtype As Integer
			Public debugrestartscount As Integer
			Public lstate As linmin.linminstate
			Public fbase As Double
			Public fm2 As Double
			Public fm1 As Double
			Public fp1 As Double
			Public fp2 As Double
			Public betahs As Double
			Public betady As Double
			Public work0 As Double()
			Public work1 As Double()
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				diagh = New Double(-1) {}
				diaghl2 = New Double(-1) {}
				vcorr = New Double(-1, -1) {}
				s = New Double(-1) {}
				xk = New Double(-1) {}
				dk = New Double(-1) {}
				xn = New Double(-1) {}
				dn = New Double(-1) {}
				d = New Double(-1) {}
				yk = New Double(-1) {}
				x = New Double(-1) {}
				g = New Double(-1) {}
				rstate = New rcommstate()
				lstate = New linmin.linminstate()
				work0 = New Double(-1) {}
				work1 = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mincgstate()
				_result.n = n
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				_result.stpmax = stpmax
				_result.suggestedstep = suggestedstep
				_result.xrep = xrep
				_result.drep = drep
				_result.cgtype = cgtype
				_result.prectype = prectype
				_result.diagh = DirectCast(diagh.Clone(), Double())
				_result.diaghl2 = DirectCast(diaghl2.Clone(), Double())
				_result.vcorr = DirectCast(vcorr.Clone(), Double(,))
				_result.vcnt = vcnt
				_result.s = DirectCast(s.Clone(), Double())
				_result.diffstep = diffstep
				_result.nfev = nfev
				_result.mcstage = mcstage
				_result.k = k
				_result.xk = DirectCast(xk.Clone(), Double())
				_result.dk = DirectCast(dk.Clone(), Double())
				_result.xn = DirectCast(xn.Clone(), Double())
				_result.dn = DirectCast(dn.Clone(), Double())
				_result.d = DirectCast(d.Clone(), Double())
				_result.fold = fold
				_result.stp = stp
				_result.curstpmax = curstpmax
				_result.yk = DirectCast(yk.Clone(), Double())
				_result.lastgoodstep = lastgoodstep
				_result.lastscaledstep = lastscaledstep
				_result.mcinfo = mcinfo
				_result.innerresetneeded = innerresetneeded
				_result.terminationneeded = terminationneeded
				_result.trimthreshold = trimthreshold
				_result.rstimer = rstimer
				_result.x = DirectCast(x.Clone(), Double())
				_result.f = f
				_result.g = DirectCast(g.Clone(), Double())
				_result.needf = needf
				_result.needfg = needfg
				_result.xupdated = xupdated
				_result.algpowerup = algpowerup
				_result.lsstart = lsstart
				_result.lsend = lsend
				_result.userterminationneeded = userterminationneeded
				_result.teststep = teststep
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.repiterationscount = repiterationscount
				_result.repnfev = repnfev
				_result.repvaridx = repvaridx
				_result.repterminationtype = repterminationtype
				_result.debugrestartscount = debugrestartscount
				_result.lstate = DirectCast(lstate.make_copy(), linmin.linminstate)
				_result.fbase = fbase
				_result.fm2 = fm2
				_result.fm1 = fm1
				_result.fp1 = fp1
				_result.fp2 = fp2
				_result.betahs = betahs
				_result.betady = betady
				_result.work0 = DirectCast(work0.Clone(), Double())
				_result.work1 = DirectCast(work1.Clone(), Double())
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure stores optimization report:
'        * IterationsCount           total number of inner iterations
'        * NFEV                      number of gradient evaluations
'        * TerminationType           termination type (see below)
'
'        TERMINATION CODES
'
'        TerminationType field contains completion code, which can be:
'          -8    internal integrity control detected  infinite  or  NAN  values  in
'                function/gradient. Abnormal termination signalled.
'          -7    gradient verification failed.
'                See MinCGSetGradientCheck() for more information.
'           1    relative function improvement is no more than EpsF.
'           2    relative step is no more than EpsX.
'           4    gradient norm is no more than EpsG
'           5    MaxIts steps was taken
'           7    stopping conditions are too stringent,
'                further improvement is impossible,
'                X contains best point found so far.
'           8    terminated by user who called mincgrequesttermination(). X contains
'                point which was "current accepted" when  termination  request  was
'                submitted.
'                
'        Other fields of this structure are not documented and should not be used!
'        ************************************************************************

		Public Class mincgreport
			Inherits apobject
			Public iterationscount As Integer
			Public nfev As Integer
			Public varidx As Integer
			Public terminationtype As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New mincgreport()
				_result.iterationscount = iterationscount
				_result.nfev = nfev
				_result.varidx = varidx
				_result.terminationtype = terminationtype
				Return _result
			End Function
		End Class




		Public Const rscountdownlen As Integer = 10
		Public Const gtol As Double = 0.3


		'************************************************************************
'                NONLINEAR CONJUGATE GRADIENT METHOD
'
'        DESCRIPTION:
'        The subroutine minimizes function F(x) of N arguments by using one of  the
'        nonlinear conjugate gradient methods.
'
'        These CG methods are globally convergent (even on non-convex functions) as
'        long as grad(f) is Lipschitz continuous in  a  some  neighborhood  of  the
'        L = { x : f(x)<=f(x0) }.
'
'
'        REQUIREMENTS:
'        Algorithm will request following information during its operation:
'        * function value F and its gradient G (simultaneously) at given point X
'
'
'        USAGE:
'        1. User initializes algorithm state with MinCGCreate() call
'        2. User tunes solver parameters with MinCGSetCond(), MinCGSetStpMax() and
'           other functions
'        3. User calls MinCGOptimize() function which takes algorithm  state   and
'           pointer (delegate, etc.) to callback function which calculates F/G.
'        4. User calls MinCGResults() to get solution
'        5. Optionally, user may call MinCGRestartFrom() to solve another  problem
'           with same N but another starting point and/or another function.
'           MinCGRestartFrom() allows to reuse already initialized structure.
'
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size of X
'            X       -   starting point, array[0..N-1].
'
'        OUTPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'          -- ALGLIB --
'             Copyright 25.03.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgcreate(n As Integer, x As Double(), state As mincgstate)
			alglib.ap.assert(n >= 1, "MinCGCreate: N too small!")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinCGCreate: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinCGCreate: X contains infinite or NaN values!")
			mincginitinternal(n, 0.0, state)
			mincgrestartfrom(state, x)
		End Sub


		'************************************************************************
'        The subroutine is finite difference variant of MinCGCreate(). It uses
'        finite differences in order to differentiate target function.
'
'        Description below contains information which is specific to this function
'        only. We recommend to read comments on MinCGCreate() in order to get more
'        information about creation of CG optimizer.
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size of X
'            X       -   starting point, array[0..N-1].
'            DiffStep-   differentiation step, >0
'
'        OUTPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTES:
'        1. algorithm uses 4-point central formula for differentiation.
'        2. differentiation step along I-th axis is equal to DiffStep*S[I] where
'           S[] is scaling vector which can be set by MinCGSetScale() call.
'        3. we recommend you to use moderate values of  differentiation  step.  Too
'           large step will result in too large truncation  errors, while too small
'           step will result in too large numerical  errors.  1.0E-6  can  be  good
'           value to start with.
'        4. Numerical  differentiation  is   very   inefficient  -   one   gradient
'           calculation needs 4*N function evaluations. This function will work for
'           any N - either small (1...10), moderate (10...100) or  large  (100...).
'           However, performance penalty will be too severe for any N's except  for
'           small ones.
'           We should also say that code which relies on numerical  differentiation
'           is  less  robust  and  precise.  L-BFGS  needs  exact  gradient values.
'           Imprecise  gradient may slow down  convergence,  especially  on  highly
'           nonlinear problems.
'           Thus  we  recommend to use this function for fast prototyping on small-
'           dimensional problems only, and to implement analytical gradient as soon
'           as possible.
'
'          -- ALGLIB --
'             Copyright 16.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgcreatef(n As Integer, x As Double(), diffstep As Double, state As mincgstate)
			alglib.ap.assert(n >= 1, "MinCGCreateF: N too small!")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinCGCreateF: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinCGCreateF: X contains infinite or NaN values!")
			alglib.ap.assert(Math.isfinite(diffstep), "MinCGCreateF: DiffStep is infinite or NaN!")
			alglib.ap.assert(CDbl(diffstep) > CDbl(0), "MinCGCreateF: DiffStep is non-positive!")
			mincginitinternal(n, diffstep, state)
			mincgrestartfrom(state, x)
		End Sub


		'************************************************************************
'        This function sets stopping conditions for CG optimization algorithm.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            EpsG    -   >=0
'                        The  subroutine  finishes  its  work   if   the  condition
'                        |v|<EpsG is satisfied, where:
'                        * |.| means Euclidian norm
'                        * v - scaled gradient vector, v[i]=g[i]*s[i]
'                        * g - gradient
'                        * s - scaling coefficients set by MinCGSetScale()
'            EpsF    -   >=0
'                        The  subroutine  finishes  its work if on k+1-th iteration
'                        the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
'                        is satisfied.
'            EpsX    -   >=0
'                        The subroutine finishes its work if  on  k+1-th  iteration
'                        the condition |v|<=EpsX is fulfilled, where:
'                        * |.| means Euclidian norm
'                        * v - scaled step vector, v[i]=dx[i]/s[i]
'                        * dx - ste pvector, dx=X(k+1)-X(k)
'                        * s - scaling coefficients set by MinCGSetScale()
'            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
'                        iterations is unlimited.
'
'        Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
'        automatic stopping criterion selection (small EpsX).
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetcond(state As mincgstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(epsg), "MinCGSetCond: EpsG is not finite number!")
			alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinCGSetCond: negative EpsG!")
			alglib.ap.assert(Math.isfinite(epsf), "MinCGSetCond: EpsF is not finite number!")
			alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinCGSetCond: negative EpsF!")
			alglib.ap.assert(Math.isfinite(epsx), "MinCGSetCond: EpsX is not finite number!")
			alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinCGSetCond: negative EpsX!")
			alglib.ap.assert(maxits >= 0, "MinCGSetCond: negative MaxIts!")
			If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
				epsx = 1E-06
			End If
			state.epsg = epsg
			state.epsf = epsf
			state.epsx = epsx
			state.maxits = maxits
		End Sub


		'************************************************************************
'        This function sets scaling coefficients for CG optimizer.
'
'        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'        size and gradient are scaled before comparison with tolerances).  Scale of
'        the I-th variable is a translation invariant measure of:
'        a) "how large" the variable is
'        b) how large the step should be to make significant changes in the function
'
'        Scaling is also used by finite difference variant of CG optimizer  -  step
'        along I-th axis is equal to DiffStep*S[I].
'
'        In   most   optimizers  (and  in  the  CG  too)  scaling is NOT a form  of
'        preconditioning. It just  affects  stopping  conditions.  You  should  set
'        preconditioner by separate call to one of the MinCGSetPrec...() functions.
'
'        There  is  special  preconditioning  mode, however,  which  uses   scaling
'        coefficients to form diagonal preconditioning matrix. You  can  turn  this
'        mode on, if you want.   But  you should understand that scaling is not the
'        same thing as preconditioning - these are two different, although  related
'        forms of tuning solver.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            S       -   array[N], non-zero scaling coefficients
'                        S[i] may be negative, sign doesn't matter.
'
'          -- ALGLIB --
'             Copyright 14.01.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetscale(state As mincgstate, s As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(s) >= state.n, "MinCGSetScale: Length(S)<N")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(s(i)), "MinCGSetScale: S contains infinite or NAN elements")
				alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinCGSetScale: S contains zero elements")
				state.s(i) = System.Math.Abs(s(i))
			Next
		End Sub


		'************************************************************************
'        This function turns on/off reporting.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedXRep-   whether iteration reports are needed or not
'
'        If NeedXRep is True, algorithm will call rep() callback function if  it is
'        provided to MinCGOptimize().
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetxrep(state As mincgstate, needxrep As Boolean)
			state.xrep = needxrep
		End Sub


		'************************************************************************
'        This function turns on/off line search reports.
'        These reports are described in more details in developer-only  comments on
'        MinCGState object.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedDRep-   whether line search reports are needed or not
'
'        This function is intended for private use only. Turning it on artificially
'        may cause program failure.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetdrep(state As mincgstate, needdrep As Boolean)
			state.drep = needdrep
		End Sub


		'************************************************************************
'        This function sets CG algorithm.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            CGType  -   algorithm type:
'                        * -1    automatic selection of the best algorithm
'                        * 0     DY (Dai and Yuan) algorithm
'                        * 1     Hybrid DY-HS algorithm
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetcgtype(state As mincgstate, cgtype As Integer)
			alglib.ap.assert(cgtype >= -1 AndAlso cgtype <= 1, "MinCGSetCGType: incorrect CGType!")
			If cgtype = -1 Then
				cgtype = 1
			End If
			state.cgtype = cgtype
		End Sub


		'************************************************************************
'        This function sets maximum step length
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
'                        want to limit step length.
'
'        Use this subroutine when you optimize target function which contains exp()
'        or  other  fast  growing  functions,  and optimization algorithm makes too
'        large  steps  which  leads  to overflow. This function allows us to reject
'        steps  that  are  too  large  (and  therefore  expose  us  to the possible
'        overflow) without actually calculating function value at the x+stp*d.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetstpmax(state As mincgstate, stpmax As Double)
			alglib.ap.assert(Math.isfinite(stpmax), "MinCGSetStpMax: StpMax is not finite!")
			alglib.ap.assert(CDbl(stpmax) >= CDbl(0), "MinCGSetStpMax: StpMax<0!")
			state.stpmax = stpmax
		End Sub


		'************************************************************************
'        This function allows to suggest initial step length to the CG algorithm.
'
'        Suggested  step  length  is used as starting point for the line search. It
'        can be useful when you have  badly  scaled  problem,  i.e.  when  ||grad||
'        (which is used as initial estimate for the first step) is many  orders  of
'        magnitude different from the desired step.
'
'        Line search  may  fail  on  such problems without good estimate of initial
'        step length. Imagine, for example, problem with ||grad||=10^50 and desired
'        step equal to 0.1 Line  search function will use 10^50  as  initial  step,
'        then  it  will  decrease step length by 2 (up to 20 attempts) and will get
'        10^44, which is still too large.
'
'        This function allows us to tell than line search should  be  started  from
'        some moderate step length, like 1.0, so algorithm will be able  to  detect
'        desired step length in a several searches.
'
'        Default behavior (when no step is suggested) is to use preconditioner,  if
'        it is available, to generate initial estimate of step length.
'
'        This function influences only first iteration of algorithm. It  should  be
'        called between MinCGCreate/MinCGRestartFrom() call and MinCGOptimize call.
'        Suggested step is ignored if you have preconditioner.
'
'        INPUT PARAMETERS:
'            State   -   structure used to store algorithm state.
'            Stp     -   initial estimate of the step length.
'                        Can be zero (no estimate).
'
'          -- ALGLIB --
'             Copyright 30.07.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsuggeststep(state As mincgstate, stp As Double)
			alglib.ap.assert(Math.isfinite(stp), "MinCGSuggestStep: Stp is infinite or NAN")
			alglib.ap.assert(CDbl(stp) >= CDbl(0), "MinCGSuggestStep: Stp<0")
			state.suggestedstep = stp
		End Sub


		'************************************************************************
'        This developer-only function allows to retrieve  unscaled  (!)  length  of
'        last good step (i.e. step which resulted in sufficient decrease of  target
'        function).
'
'        It can be used in for solution  of  sequential  optimization  subproblems,
'        where MinCGSuggestStep()  is  called  with  length  of  previous  step  as
'        parameter.
'
'        INPUT PARAMETERS:
'            State   -   structure used to store algorithm state.
'            
'        RESULT:
'            length of last good step being accepted
'            
'        NOTE:
'            result of this function is undefined if you called it before
'
'          -- ALGLIB --
'             Copyright 30.07.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mincglastgoodstep(state As mincgstate) As Double
			Dim result As Double = 0

			result = state.lastgoodstep
			Return result
		End Function


		'************************************************************************
'        Modification of the preconditioner: preconditioning is turned off.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetprecdefault(state As mincgstate)
			state.prectype = 0
			state.innerresetneeded = True
		End Sub


		'************************************************************************
'        Modification  of  the  preconditioner:  diagonal of approximate Hessian is
'        used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            D       -   diagonal of the approximate Hessian, array[0..N-1],
'                        (if larger, only leading N elements are used).
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'        NOTE 2: D[i] should be positive. Exception will be thrown otherwise.
'
'        NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetprecdiag(state As mincgstate, d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(d) >= state.n, "MinCGSetPrecDiag: D is too short")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(d(i)), "MinCGSetPrecDiag: D contains infinite or NAN elements")
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "MinCGSetPrecDiag: D contains non-positive elements")
			Next
			mincgsetprecdiagfast(state, d)
		End Sub


		'************************************************************************
'        Modification of the preconditioner: scale-based diagonal preconditioning.
'
'        This preconditioning mode can be useful when you  don't  have  approximate
'        diagonal of Hessian, but you know that your  variables  are  badly  scaled
'        (for  example,  one  variable is in [1,10], and another in [1000,100000]),
'        and most part of the ill-conditioning comes from different scales of vars.
'
'        In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
'        can greatly improve convergence.
'
'        IMPRTANT: you should set scale of your variables with MinCGSetScale() call
'        (before or after MinCGSetPrecScale() call). Without knowledge of the scale
'        of your variables scale-based preconditioner will be just unit matrix.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetprecscale(state As mincgstate)
			state.prectype = 3
			state.innerresetneeded = True
		End Sub


		'************************************************************************
'        NOTES:
'
'        1. This function has two different implementations: one which  uses  exact
'           (analytical) user-supplied  gradient, and one which uses function value
'           only  and  numerically  differentiates  function  in  order  to  obtain
'           gradient.
'           
'           Depending  on  the  specific  function  used to create optimizer object
'           (either MinCGCreate()  for analytical gradient  or  MinCGCreateF()  for
'           numerical differentiation) you should  choose  appropriate  variant  of
'           MinCGOptimize() - one which accepts function AND gradient or one  which
'           accepts function ONLY.
'
'           Be careful to choose variant of MinCGOptimize()  which  corresponds  to
'           your optimization scheme! Table below lists different  combinations  of
'           callback (function/gradient) passed  to  MinCGOptimize()  and  specific
'           function used to create optimizer.
'           
'
'                          |         USER PASSED TO MinCGOptimize()
'           CREATED WITH   |  function only   |  function and gradient
'           ------------------------------------------------------------
'           MinCGCreateF() |     work                FAIL
'           MinCGCreate()  |     FAIL                work
'
'           Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
'           function and MinCGOptimize() version. Attemps to use  such  combination
'           (for  example,  to create optimizer with  MinCGCreateF()  and  to  pass
'           gradient information to MinCGOptimize()) will lead to  exception  being
'           thrown. Either  you  did  not  pass  gradient when it WAS needed or you
'           passed gradient when it was NOT needed.
'
'          -- ALGLIB --
'             Copyright 20.04.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function mincgiteration(state As mincgstate) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim betak As Double = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstate.stage >= 0 Then
				n = state.rstate.ia(0)
				i = state.rstate.ia(1)
				betak = state.rstate.ra(0)
				v = state.rstate.ra(1)
				vv = state.rstate.ra(2)
			Else
				n = -983
				i = -989
				betak = -834
				v = 900
				vv = -287
			End If
			If state.rstate.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstate.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstate.stage = 2 Then
				GoTo lbl_2
			End If
			If state.rstate.stage = 3 Then
				GoTo lbl_3
			End If
			If state.rstate.stage = 4 Then
				GoTo lbl_4
			End If
			If state.rstate.stage = 5 Then
				GoTo lbl_5
			End If
			If state.rstate.stage = 6 Then
				GoTo lbl_6
			End If
			If state.rstate.stage = 7 Then
				GoTo lbl_7
			End If
			If state.rstate.stage = 8 Then
				GoTo lbl_8
			End If
			If state.rstate.stage = 9 Then
				GoTo lbl_9
			End If
			If state.rstate.stage = 10 Then
				GoTo lbl_10
			End If
			If state.rstate.stage = 11 Then
				GoTo lbl_11
			End If
			If state.rstate.stage = 12 Then
				GoTo lbl_12
			End If
			If state.rstate.stage = 13 Then
				GoTo lbl_13
			End If
			If state.rstate.stage = 14 Then
				GoTo lbl_14
			End If
			If state.rstate.stage = 15 Then
				GoTo lbl_15
			End If
			If state.rstate.stage = 16 Then
				GoTo lbl_16
			End If
			If state.rstate.stage = 17 Then
				GoTo lbl_17
			End If
			If state.rstate.stage = 18 Then
				GoTo lbl_18
			End If
			If state.rstate.stage = 19 Then
				GoTo lbl_19
			End If

			'
			' Routine body
			'

			'
			' Prepare
			'
			n = state.n
			state.terminationneeded = False
			state.userterminationneeded = False
			state.repterminationtype = 0
			state.repiterationscount = 0
			state.repvaridx = -1
			state.repnfev = 0
			state.debugrestartscount = 0

			'
			'  Check, that transferred derivative value is right
			'
			clearrequestfields(state)
			If Not (CDbl(state.diffstep) = CDbl(0) AndAlso CDbl(state.teststep) > CDbl(0)) Then
				GoTo lbl_20
			End If
			state.needfg = True
			i = 0
			lbl_22:
			If i > n - 1 Then
				GoTo lbl_24
			End If
			v = state.x(i)
			state.x(i) = v - state.teststep * state.s(i)
			state.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			state.fm1 = state.f
			state.fp1 = state.g(i)
			state.x(i) = v + state.teststep * state.s(i)
			state.rstate.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			state.fm2 = state.f
			state.fp2 = state.g(i)
			state.x(i) = v
			state.rstate.stage = 2
			GoTo lbl_rcomm
			lbl_2:

			'
			' 2*State.TestStep   -   scale parameter
			' width of segment [Xi-TestStep;Xi+TestStep]
			'
			If Not optserv.derivativecheck(state.fm1, state.fp1, state.fm2, state.fp2, state.f, state.g(i), _
				2 * state.teststep) Then
				state.repvaridx = i
				state.repterminationtype = -7
				result = False
				Return result
			End If
			i = i + 1
			GoTo lbl_22
			lbl_24:
			state.needfg = False
			lbl_20:

			'
			' Preparations continue:
			' * set XK
			' * calculate F/G
			' * set DK to -G
			' * powerup algo (it may change preconditioner)
			' * apply preconditioner to DK
			' * report update of X
			' * check stopping conditions for G
			'
			For i_ = 0 To n - 1
				state.xk(i_) = state.x(i_)
			Next
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_25
			End If
			state.needfg = True
			state.rstate.stage = 3
			GoTo lbl_rcomm
			lbl_3:
			state.needfg = False
			GoTo lbl_26
			lbl_25:
			state.needf = True
			state.rstate.stage = 4
			GoTo lbl_rcomm
			lbl_4:
			state.fbase = state.f
			i = 0
			lbl_27:
			If i > n - 1 Then
				GoTo lbl_29
			End If
			v = state.x(i)
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 5
			GoTo lbl_rcomm
			lbl_5:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 6
			GoTo lbl_rcomm
			lbl_6:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 7
			GoTo lbl_rcomm
			lbl_7:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 8
			GoTo lbl_rcomm
			lbl_8:
			state.fp2 = state.f
			state.x(i) = v
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			i = i + 1
			GoTo lbl_27
			lbl_29:
			state.f = state.fbase
			state.needf = False
			lbl_26:
			If Not state.drep Then
				GoTo lbl_30
			End If

			'
			' Report algorithm powerup (if needed)
			'
			clearrequestfields(state)
			state.algpowerup = True
			state.rstate.stage = 9
			GoTo lbl_rcomm
			lbl_9:
			state.algpowerup = False
			lbl_30:
			optserv.trimprepare(state.f, state.trimthreshold)
			For i_ = 0 To n - 1
				state.dk(i_) = -state.g(i_)
			Next
			preconditionedmultiply(state, state.dk, state.work0, state.work1)
			If Not state.xrep Then
				GoTo lbl_32
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 10
			GoTo lbl_rcomm
			lbl_10:
			state.xupdated = False
			lbl_32:
			If state.terminationneeded OrElse state.userterminationneeded Then

				'
				' Combined termination point for "internal" termination by TerminationNeeded flag
				' and for "user" termination by MinCGRequestTermination() (UserTerminationNeeded flag).
				' In this location rules for both of methods are same, thus only one exit point is needed.
				'
				For i_ = 0 To n - 1
					state.xn(i_) = state.xk(i_)
				Next
				state.repterminationtype = 8
				result = False
				Return result
			End If
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.g(i) * state.s(i))
			Next
			If CDbl(System.Math.sqrt(v)) <= CDbl(state.epsg) Then
				For i_ = 0 To n - 1
					state.xn(i_) = state.xk(i_)
				Next
				state.repterminationtype = 4
				result = False
				Return result
			End If
			state.repnfev = 1
			state.k = 0
			state.fold = state.f

			'
			' Choose initial step.
			' Apply preconditioner, if we have something other than default.
			'
			If state.prectype = 2 OrElse state.prectype = 3 Then

				'
				' because we use preconditioner, step length must be equal
				' to the norm of DK
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += state.dk(i_) * state.dk(i_)
				Next
				state.lastgoodstep = System.Math.sqrt(v)
			Else

				'
				' No preconditioner is used, we try to use suggested step
				'
				If CDbl(state.suggestedstep) > CDbl(0) Then
					state.lastgoodstep = state.suggestedstep
				Else
					state.lastgoodstep = 1.0
				End If
			End If

			'
			' Main cycle
			'
			state.rstimer = rscountdownlen
			lbl_34:
			If False Then
				GoTo lbl_35
			End If

			'
			' * clear reset flag
			' * clear termination flag
			' * store G[k] for later calculation of Y[k]
			' * prepare starting point and direction and step length for line search
			'
			state.innerresetneeded = False
			state.terminationneeded = False
			For i_ = 0 To n - 1
				state.yk(i_) = -state.g(i_)
			Next
			For i_ = 0 To n - 1
				state.d(i_) = state.dk(i_)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xk(i_)
			Next
			state.mcstage = 0
			state.stp = 1.0
			linmin.linminnormalized(state.d, state.stp, n)
			If CDbl(state.lastgoodstep) <> CDbl(0) Then
				state.stp = state.lastgoodstep
			End If
			state.curstpmax = state.stpmax

			'
			' Report beginning of line search (if needed)
			' Terminate algorithm, if user request was detected
			'
			If Not state.drep Then
				GoTo lbl_36
			End If
			clearrequestfields(state)
			state.lsstart = True
			state.rstate.stage = 11
			GoTo lbl_rcomm
			lbl_11:
			state.lsstart = False
			lbl_36:
			If state.terminationneeded Then
				For i_ = 0 To n - 1
					state.xn(i_) = state.x(i_)
				Next
				state.repterminationtype = 8
				result = False
				Return result
			End If

			'
			' Minimization along D
			'
			linmin.mcsrch(n, state.x, state.f, state.g, state.d, state.stp, _
				state.curstpmax, gtol, state.mcinfo, state.nfev, state.work0, state.lstate, _
				state.mcstage)
			lbl_38:
			If state.mcstage = 0 Then
				GoTo lbl_39
			End If

			'
			' Calculate function/gradient using either
			' analytical gradient supplied by user
			' or finite difference approximation.
			'
			' "Trim" function in order to handle near-singularity points.
			'
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_40
			End If
			state.needfg = True
			state.rstate.stage = 12
			GoTo lbl_rcomm
			lbl_12:
			state.needfg = False
			GoTo lbl_41
			lbl_40:
			state.needf = True
			state.rstate.stage = 13
			GoTo lbl_rcomm
			lbl_13:
			state.fbase = state.f
			i = 0
			lbl_42:
			If i > n - 1 Then
				GoTo lbl_44
			End If
			v = state.x(i)
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 14
			GoTo lbl_rcomm
			lbl_14:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 15
			GoTo lbl_rcomm
			lbl_15:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 16
			GoTo lbl_rcomm
			lbl_16:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 17
			GoTo lbl_rcomm
			lbl_17:
			state.fp2 = state.f
			state.x(i) = v
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			i = i + 1
			GoTo lbl_42
			lbl_44:
			state.f = state.fbase
			state.needf = False
			lbl_41:
			optserv.trimfunction(state.f, state.g, n, state.trimthreshold)

			'
			' Call MCSRCH again
			'
			linmin.mcsrch(n, state.x, state.f, state.g, state.d, state.stp, _
				state.curstpmax, gtol, state.mcinfo, state.nfev, state.work0, state.lstate, _
				state.mcstage)
			GoTo lbl_38
			lbl_39:

			'
			' * terminate algorithm if "user" request for detected
			' * report end of line search
			' * store current point to XN
			' * report iteration
			' * terminate algorithm if "internal" request was detected
			'
			If state.userterminationneeded Then
				For i_ = 0 To n - 1
					state.xn(i_) = state.xk(i_)
				Next
				state.repterminationtype = 8
				result = False
				Return result
			End If
			If Not state.drep Then
				GoTo lbl_45
			End If

			'
			' Report end of line search (if needed)
			'
			clearrequestfields(state)
			state.lsend = True
			state.rstate.stage = 18
			GoTo lbl_rcomm
			lbl_18:
			state.lsend = False
			lbl_45:
			For i_ = 0 To n - 1
				state.xn(i_) = state.x(i_)
			Next
			If Not state.xrep Then
				GoTo lbl_47
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 19
			GoTo lbl_rcomm
			lbl_19:
			state.xupdated = False
			lbl_47:
			If state.terminationneeded Then
				For i_ = 0 To n - 1
					state.xn(i_) = state.x(i_)
				Next
				state.repterminationtype = 8
				result = False
				Return result
			End If

			'
			' Line search is finished.
			' * calculate BetaK
			' * calculate DN
			' * update timers
			' * calculate step length:
			'   * LastScaledStep is ALWAYS calculated because it is used in the stopping criteria
			'   * LastGoodStep is updated only when MCINFO is equal to 1 (Wolfe conditions hold).
			'     See below for more explanation.
			'
			If state.mcinfo = 1 AndAlso Not state.innerresetneeded Then

				'
				' Standard Wolfe conditions hold
				' Calculate Y[K] and D[K]'*Y[K]
				'
				For i_ = 0 To n - 1
					state.yk(i_) = state.yk(i_) + state.g(i_)
				Next
				vv = 0.0
				For i_ = 0 To n - 1
					vv += state.yk(i_) * state.dk(i_)
				Next

				'
				' Calculate BetaK according to DY formula
				'
				v = preconditionedmultiply2(state, state.g, state.g, state.work0, state.work1)
				state.betady = v / vv

				'
				' Calculate BetaK according to HS formula
				'
				v = preconditionedmultiply2(state, state.g, state.yk, state.work0, state.work1)
				state.betahs = v / vv

				'
				' Choose BetaK
				'
				If state.cgtype = 0 Then
					betak = state.betady
				End If
				If state.cgtype = 1 Then
					betak = System.Math.Max(0, System.Math.Min(state.betady, state.betahs))
				End If
			Else

				'
				' Something is wrong (may be function is too wild or too flat)
				' or we just have to restart algo.
				'
				' We'll set BetaK=0, which will restart CG algorithm.
				' We can stop later (during normal checks) if stopping conditions are met.
				'
				betak = 0
				state.debugrestartscount = state.debugrestartscount + 1
			End If
			If state.repiterationscount > 0 AndAlso state.repiterationscount Mod (3 + n) = 0 Then

				'
				' clear Beta every N iterations
				'
				betak = 0
			End If
			If state.mcinfo = 1 OrElse state.mcinfo = 5 Then
				state.rstimer = rscountdownlen
			Else
				state.rstimer = state.rstimer - 1
			End If
			For i_ = 0 To n - 1
				state.dn(i_) = -state.g(i_)
			Next
			preconditionedmultiply(state, state.dn, state.work0, state.work1)
			For i_ = 0 To n - 1
				state.dn(i_) = state.dn(i_) + betak * state.dk(i_)
			Next
			state.lastscaledstep = 0.0
			For i = 0 To n - 1
				state.lastscaledstep = state.lastscaledstep + Math.sqr(state.d(i) / state.s(i))
			Next
			state.lastscaledstep = state.stp * System.Math.sqrt(state.lastscaledstep)
			If state.mcinfo = 1 Then

				'
				' Step is good (Wolfe conditions hold), update LastGoodStep.
				'
				' This check for MCINFO=1 is essential because sometimes in the
				' constrained optimization setting we may take very short steps
				' (like 1E-15) because we were very close to boundary of the
				' feasible area. Such short step does not mean that we've converged
				' to the solution - it was so short because we were close to the
				' boundary and there was a limit on step length.
				'
				' So having such short step is quite normal situation. However, we
				' should NOT start next iteration from step whose initial length is
				' estimated as 1E-15 because it may lead to the failure of the
				' linear minimizer (step is too short, function does not changes,
				' line search stagnates).
				'
				state.lastgoodstep = 0
				For i = 0 To n - 1
					state.lastgoodstep = state.lastgoodstep + Math.sqr(state.d(i))
				Next
				state.lastgoodstep = state.stp * System.Math.sqrt(state.lastgoodstep)
			End If

			'
			' Update information.
			' Check stopping conditions.
			'
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.g(i) * state.s(i))
			Next
			If Not Math.isfinite(v) OrElse Not Math.isfinite(state.f) Then

				'
				' Abnormal termination - infinities in function/gradient
				'
				state.repterminationtype = -8
				result = False
				Return result
			End If
			state.repnfev = state.repnfev + state.nfev
			state.repiterationscount = state.repiterationscount + 1
			If state.repiterationscount >= state.maxits AndAlso state.maxits > 0 Then

				'
				' Too many iterations
				'
				state.repterminationtype = 5
				result = False
				Return result
			End If
			If CDbl(System.Math.sqrt(v)) <= CDbl(state.epsg) Then

				'
				' Gradient is small enough
				'
				state.repterminationtype = 4
				result = False
				Return result
			End If
			If Not state.innerresetneeded Then

				'
				' These conditions are checked only when no inner reset was requested by user
				'
				If CDbl(state.fold - state.f) <= CDbl(state.epsf * System.Math.Max(System.Math.Abs(state.fold), System.Math.Max(System.Math.Abs(state.f), 1.0))) Then

					'
					' F(k+1)-F(k) is small enough
					'
					state.repterminationtype = 1
					result = False
					Return result
				End If
				If CDbl(state.lastscaledstep) <= CDbl(state.epsx) Then

					'
					' X(k+1)-X(k) is small enough
					'
					state.repterminationtype = 2
					result = False
					Return result
				End If
			End If
			If state.rstimer <= 0 Then

				'
				' Too many subsequent restarts
				'
				state.repterminationtype = 7
				result = False
				Return result
			End If

			'
			' Shift Xk/Dk, update other information
			'
			For i_ = 0 To n - 1
				state.xk(i_) = state.xn(i_)
			Next
			For i_ = 0 To n - 1
				state.dk(i_) = state.dn(i_)
			Next
			state.fold = state.f
			state.k = state.k + 1
			GoTo lbl_34
			lbl_35:
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstate.ia(0) = n
			state.rstate.ia(1) = i
			state.rstate.ra(0) = betak
			state.rstate.ra(1) = v
			state.rstate.ra(2) = vv
			Return result
		End Function


		'************************************************************************
'        Conjugate gradient results
'
'        INPUT PARAMETERS:
'            State   -   algorithm state
'
'        OUTPUT PARAMETERS:
'            X       -   array[0..N-1], solution
'            Rep     -   optimization report:
'                        * Rep.TerminationType completetion code:
'                            * -8    internal integrity control  detected  infinite
'                                    or NAN values in  function/gradient.  Abnormal
'                                    termination signalled.
'                            * -7    gradient verification failed.
'                                    See MinCGSetGradientCheck() for more information.
'                            *  1    relative function improvement is no more than
'                                    EpsF.
'                            *  2    relative step is no more than EpsX.
'                            *  4    gradient norm is no more than EpsG
'                            *  5    MaxIts steps was taken
'                            *  7    stopping conditions are too stringent,
'                                    further improvement is impossible,
'                                    we return best X found so far
'                            *  8    terminated by user
'                        * Rep.IterationsCount contains iterations count
'                        * NFEV countains number of function calculations
'
'          -- ALGLIB --
'             Copyright 20.04.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgresults(state As mincgstate, ByRef x As Double(), rep As mincgreport)
			x = New Double(-1) {}

			mincgresultsbuf(state, x, rep)
		End Sub


		'************************************************************************
'        Conjugate gradient results
'
'        Buffered implementation of MinCGResults(), which uses pre-allocated buffer
'        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
'        intended to be used in the inner cycles of performance critical algorithms
'        where array reallocation penalty is too large to be ignored.
'
'          -- ALGLIB --
'             Copyright 20.04.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgresultsbuf(state As mincgstate, ByRef x As Double(), rep As mincgreport)
			Dim i_ As Integer = 0

			If alglib.ap.len(x) < state.n Then
				x = New Double(state.n - 1) {}
			End If
			For i_ = 0 To state.n - 1
				x(i_) = state.xn(i_)
			Next
			rep.iterationscount = state.repiterationscount
			rep.nfev = state.repnfev
			rep.varidx = state.repvaridx
			rep.terminationtype = state.repterminationtype
		End Sub


		'************************************************************************
'        This  subroutine  restarts  CG  algorithm from new point. All optimization
'        parameters are left unchanged.
'
'        This  function  allows  to  solve multiple  optimization  problems  (which
'        must have same number of dimensions) without object reallocation penalty.
'
'        INPUT PARAMETERS:
'            State   -   structure used to store algorithm state.
'            X       -   new starting point.
'
'          -- ALGLIB --
'             Copyright 30.07.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgrestartfrom(state As mincgstate, x As Double())
			Dim i_ As Integer = 0

			alglib.ap.assert(alglib.ap.len(x) >= state.n, "MinCGRestartFrom: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, state.n), "MinCGCreate: X contains infinite or NaN values!")
			For i_ = 0 To state.n - 1
				state.x(i_) = x(i_)
			Next
			mincgsuggeststep(state, 0.0)
			state.rstate.ia = New Integer(1) {}
			state.rstate.ra = New Double(2) {}
			state.rstate.stage = -1
			clearrequestfields(state)
		End Sub


		'************************************************************************
'        This subroutine submits request for termination of running  optimizer.  It
'        should be called from user-supplied callback when user decides that it  is
'        time to "smoothly" terminate optimization process.  As  result,  optimizer
'        stops at point which was "current accepted" when termination  request  was
'        submitted and returns error code 8 (successful termination).
'
'        INPUT PARAMETERS:
'            State   -   optimizer structure
'
'        NOTE: after  request  for  termination  optimizer  may   perform   several
'              additional calls to user-supplied callbacks. It does  NOT  guarantee
'              to stop immediately - it just guarantees that these additional calls
'              will be discarded later.
'
'        NOTE: calling this function on optimizer which is NOT running will have no
'              effect.
'              
'        NOTE: multiple calls to this function are possible. First call is counted,
'              subsequent calls are silently ignored.
'
'          -- ALGLIB --
'             Copyright 08.10.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgrequesttermination(state As mincgstate)
			state.userterminationneeded = True
		End Sub


		'************************************************************************
'        Faster version of MinCGSetPrecDiag(), for time-critical parts of code,
'        without safety checks.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetprecdiagfast(state As mincgstate, d As Double())
			Dim i As Integer = 0

			apserv.rvectorsetlengthatleast(state.diagh, state.n)
			apserv.rvectorsetlengthatleast(state.diaghl2, state.n)
			state.prectype = 2
			state.vcnt = 0
			state.innerresetneeded = True
			For i = 0 To state.n - 1
				state.diagh(i) = d(i)
				state.diaghl2(i) = 0.0
			Next
		End Sub


		'************************************************************************
'        This function sets low-rank preconditioner for Hessian matrix  H=D+V'*C*V,
'        where:
'        * H is a Hessian matrix, which is approximated by D/V/C
'        * D=D1+D2 is a diagonal matrix, which includes two positive definite terms:
'          * constant term D1 (is not updated or infrequently updated)
'          * variable term D2 (can be cheaply updated from iteration to iteration)
'        * V is a low-rank correction
'        * C is a diagonal factor of low-rank correction
'
'        Preconditioner P is calculated using approximate Woodburry formula:
'            P  = D^(-1) - D^(-1)*V'*(C^(-1)+V*D1^(-1)*V')^(-1)*V*D^(-1)
'               = D^(-1) - D^(-1)*VC'*VC*D^(-1),
'        where
'            VC = sqrt(B)*V
'            B  = (C^(-1)+V*D1^(-1)*V')^(-1)
'            
'        Note that B is calculated using constant term (D1) only,  which  allows us
'        to update D2 without recalculation of B or   VC.  Such  preconditioner  is
'        exact when D2 is zero. When D2 is non-zero, it is only approximation,  but
'        very good and cheap one.
'
'        This function accepts D1, V, C.
'        D2 is set to zero by default.
'
'        Cost of this update is O(N*VCnt*VCnt), but D2 can be updated in just O(N)
'        by MinCGSetPrecVarPart.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetpreclowrankfast(state As mincgstate, d1 As Double(), c As Double(), v As Double(,), vcnt As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim n As Integer = 0
			Dim t As Double = 0
			Dim b As Double(,) = New Double(-1, -1) {}
			Dim i_ As Integer = 0

			If vcnt = 0 Then
				mincgsetprecdiagfast(state, d1)
				Return
			End If
			n = state.n
			b = New Double(vcnt - 1, vcnt - 1) {}
			apserv.rvectorsetlengthatleast(state.diagh, n)
			apserv.rvectorsetlengthatleast(state.diaghl2, n)
			apserv.rmatrixsetlengthatleast(state.vcorr, vcnt, n)
			state.prectype = 2
			state.vcnt = vcnt
			state.innerresetneeded = True
			For i = 0 To n - 1
				state.diagh(i) = d1(i)
				state.diaghl2(i) = 0.0
			Next
			For i = 0 To vcnt - 1
				For j = i To vcnt - 1
					t = 0
					For k = 0 To n - 1
						t = t + v(i, k) * v(j, k) / d1(k)
					Next
					b(i, j) = t
				Next
				b(i, i) = b(i, i) + 1.0 / c(i)
			Next
			If Not trfac.spdmatrixcholeskyrec(b, 0, vcnt, True, state.work0) Then
				state.vcnt = 0
				Return
			End If
			For i = 0 To vcnt - 1
				For i_ = 0 To n - 1
					state.vcorr(i, i_) = v(i, i_)
				Next
				For j = 0 To i - 1
					t = b(j, i)
					For i_ = 0 To n - 1
						state.vcorr(i, i_) = state.vcorr(i, i_) - t * state.vcorr(j, i_)
					Next
				Next
				t = 1 / b(i, i)
				For i_ = 0 To n - 1
					state.vcorr(i, i_) = t * state.vcorr(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        This function updates variable part (diagonal matrix D2)
'        of low-rank preconditioner.
'
'        This update is very cheap and takes just O(N) time.
'
'        It has no effect with default preconditioner.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetprecvarpart(state As mincgstate, d2 As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0

			n = state.n
			For i = 0 To n - 1
				state.diaghl2(i) = d2(i)
			Next
		End Sub


		'************************************************************************
'
'        This  subroutine  turns  on  verification  of  the  user-supplied analytic
'        gradient:
'        * user calls this subroutine before optimization begins
'        * MinCGOptimize() is called
'        * prior to  actual  optimization, for each component  of  parameters being
'          optimized X[i] algorithm performs following steps:
'          * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
'            where X[i] is i-th component of the initial point and S[i] is a  scale
'            of i-th parameter
'          * F(X) is evaluated at these trial points
'          * we perform one more evaluation in the middle point of the interval
'          * we  build  cubic  model using function values and derivatives at trial
'            points and we compare its prediction with actual value in  the  middle
'            point
'          * in case difference between prediction and actual value is higher  than
'            some predetermined threshold, algorithm stops with completion code -7;
'            Rep.VarIdx is set to index of the parameter with incorrect derivative.
'        * after verification is over, algorithm proceeds to the actual optimization.
'
'        NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
'                is very costly and you should use  it  only  for  low  dimensional
'                problems,  when  you  want  to  be  sure  that  you've   correctly
'                calculated  analytic  derivatives.  You  should  not use it in the
'                production code (unless you want to check derivatives provided  by
'                some third party).
'
'        NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
'                (so large that function behaviour is significantly non-cubic) will
'                lead to false alarms. You may use  different  step  for  different
'                parameters by means of setting scale with MinCGSetScale().
'
'        NOTE 3: this function may lead to false positives. In case it reports that
'                I-th  derivative was calculated incorrectly, you may decrease test
'                step  and  try  one  more  time  - maybe your function changes too
'                sharply  and  your  step  is  too  large for such rapidly chanding
'                function.
'
'        INPUT PARAMETERS:
'            State       -   structure used to store algorithm state
'            TestStep    -   verification step:
'                            * TestStep=0 turns verification off
'                            * TestStep>0 activates verification
'
'          -- ALGLIB --
'             Copyright 31.05.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub mincgsetgradientcheck(state As mincgstate, teststep As Double)
			alglib.ap.assert(Math.isfinite(teststep), "MinCGSetGradientCheck: TestStep contains NaN or Infinite")
			alglib.ap.assert(CDbl(teststep) >= CDbl(0), "MinCGSetGradientCheck: invalid argument TestStep(TestStep<0)")
			state.teststep = teststep
		End Sub


		'************************************************************************
'        Clears request fileds (to be sure that we don't forgot to clear something)
'        ************************************************************************

		Private Shared Sub clearrequestfields(state As mincgstate)
			state.needf = False
			state.needfg = False
			state.xupdated = False
			state.lsstart = False
			state.lsend = False
			state.algpowerup = False
		End Sub


		'************************************************************************
'        This function calculates preconditioned product H^(-1)*x and stores result
'        back into X. Work0[] and Work1[] are used as temporaries (size must be at
'        least N; this function doesn't allocate arrays).
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub preconditionedmultiply(state As mincgstate, ByRef x As Double(), ByRef work0 As Double(), ByRef work1 As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0
			Dim vcnt As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = state.n
			vcnt = state.vcnt
			If state.prectype = 0 Then
				Return
			End If
			If state.prectype = 3 Then
				For i = 0 To n - 1
					x(i) = x(i) * state.s(i) * state.s(i)
				Next
				Return
			End If
			alglib.ap.assert(state.prectype = 2, "MinCG: internal error (unexpected PrecType)")

			'
			' handle part common for VCnt=0 and VCnt<>0
			'
			For i = 0 To n - 1
				x(i) = x(i) / (state.diagh(i) + state.diaghl2(i))
			Next

			'
			' if VCnt>0
			'
			If vcnt > 0 Then
				For i = 0 To vcnt - 1
					v = 0.0
					For i_ = 0 To n - 1
						v += state.vcorr(i, i_) * x(i_)
					Next
					work0(i) = v
				Next
				For i = 0 To n - 1
					work1(i) = 0
				Next
				For i = 0 To vcnt - 1
					v = work0(i)
					For i_ = 0 To n - 1
						state.work1(i_) = state.work1(i_) + v * state.vcorr(i, i_)
					Next
				Next
				For i = 0 To n - 1
					x(i) = x(i) - state.work1(i) / (state.diagh(i) + state.diaghl2(i))
				Next
			End If
		End Sub


		'************************************************************************
'        This function calculates preconditioned product x'*H^(-1)*y. Work0[] and
'        Work1[] are used as temporaries (size must be at least N; this function
'        doesn't allocate arrays).
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function preconditionedmultiply2(state As mincgstate, ByRef x As Double(), ByRef y As Double(), ByRef work0 As Double(), ByRef work1 As Double()) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim n As Integer = 0
			Dim vcnt As Integer = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim i_ As Integer = 0

			n = state.n
			vcnt = state.vcnt

			'
			' no preconditioning
			'
			If state.prectype = 0 Then
				v0 = 0.0
				For i_ = 0 To n - 1
					v0 += x(i_) * y(i_)
				Next
				result = v0
				Return result
			End If
			If state.prectype = 3 Then
				result = 0
				For i = 0 To n - 1
					result = result + x(i) * state.s(i) * state.s(i) * y(i)
				Next
				Return result
			End If
			alglib.ap.assert(state.prectype = 2, "MinCG: internal error (unexpected PrecType)")

			'
			' low rank preconditioning
			'
			result = 0.0
			For i = 0 To n - 1
				result = result + x(i) * y(i) / (state.diagh(i) + state.diaghl2(i))
			Next
			If vcnt > 0 Then
				For i = 0 To n - 1
					work0(i) = x(i) / (state.diagh(i) + state.diaghl2(i))
					work1(i) = y(i) / (state.diagh(i) + state.diaghl2(i))
				Next
				For i = 0 To vcnt - 1
					v0 = 0.0
					For i_ = 0 To n - 1
						v0 += work0(i_) * state.vcorr(i, i_)
					Next
					v1 = 0.0
					For i_ = 0 To n - 1
						v1 += work1(i_) * state.vcorr(i, i_)
					Next
					result = result - v0 * v1
				Next
			End If
			Return result
		End Function


		'************************************************************************
'        Internal initialization subroutine
'
'          -- ALGLIB --
'             Copyright 16.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub mincginitinternal(n As Integer, diffstep As Double, state As mincgstate)
			Dim i As Integer = 0


			'
			' Initialize
			'
			state.teststep = 0
			state.n = n
			state.diffstep = diffstep
			state.lastgoodstep = 0
			mincgsetcond(state, 0, 0, 0, 0)
			mincgsetxrep(state, False)
			mincgsetdrep(state, False)
			mincgsetstpmax(state, 0)
			mincgsetcgtype(state, -1)
			mincgsetprecdefault(state)
			state.xk = New Double(n - 1) {}
			state.dk = New Double(n - 1) {}
			state.xn = New Double(n - 1) {}
			state.dn = New Double(n - 1) {}
			state.x = New Double(n - 1) {}
			state.d = New Double(n - 1) {}
			state.g = New Double(n - 1) {}
			state.work0 = New Double(n - 1) {}
			state.work1 = New Double(n - 1) {}
			state.yk = New Double(n - 1) {}
			state.s = New Double(n - 1) {}
			For i = 0 To n - 1
				state.s(i) = 1.0
			Next
		End Sub


	End Class
	Public Class minbleic
		'************************************************************************
'        This object stores nonlinear optimizer state.
'        You should use functions provided by MinBLEIC subpackage to work with this
'        object
'        ************************************************************************

		Public Class minbleicstate
			Inherits apobject
			Public nmain As Integer
			Public nslack As Integer
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public xrep As Boolean
			Public drep As Boolean
			Public stpmax As Double
			Public diffstep As Double
			Public sas As sactivesets.sactiveset
			Public s As Double()
			Public prectype As Integer
			Public diagh As Double()
			Public x As Double()
			Public f As Double
			Public g As Double()
			Public needf As Boolean
			Public needfg As Boolean
			Public xupdated As Boolean
			Public lsstart As Boolean
			Public steepestdescentstep As Boolean
			Public boundedstep As Boolean
			Public userterminationneeded As Boolean
			Public teststep As Double
			Public rstate As rcommstate
			Public ugc As Double()
			Public cgc As Double()
			Public xn As Double()
			Public ugn As Double()
			Public cgn As Double()
			Public xp As Double()
			Public fc As Double
			Public fn As Double
			Public fp As Double
			Public d As Double()
			Public cleic As Double(,)
			Public nec As Integer
			Public nic As Integer
			Public lastgoodstep As Double
			Public lastscaledgoodstep As Double
			Public maxscaledgrad As Double
			Public hasbndl As Boolean()
			Public hasbndu As Boolean()
			Public bndl As Double()
			Public bndu As Double()
			Public repinneriterationscount As Integer
			Public repouteriterationscount As Integer
			Public repnfev As Integer
			Public repvaridx As Integer
			Public repterminationtype As Integer
			Public repdebugeqerr As Double
			Public repdebugfs As Double
			Public repdebugff As Double
			Public repdebugdx As Double
			Public repdebugfeasqpits As Integer
			Public repdebugfeasgpaits As Integer
			Public xstart As Double()
			Public solver As snnls.snnlssolver
			Public fbase As Double
			Public fm2 As Double
			Public fm1 As Double
			Public fp1 As Double
			Public fp2 As Double
			Public xm1 As Double
			Public xp1 As Double
			Public gm1 As Double
			Public gp1 As Double
			Public cidx As Integer
			Public cval As Double
			Public tmpprec As Double()
			Public tmp0 As Double()
			Public nfev As Integer
			Public mcstage As Integer
			Public stp As Double
			Public curstpmax As Double
			Public activationstep As Double
			Public work As Double()
			Public lstate As linmin.linminstate
			Public trimthreshold As Double
			Public nonmonotoniccnt As Integer
			Public bufyk As Double(,)
			Public bufsk As Double(,)
			Public bufrho As Double()
			Public buftheta As Double()
			Public bufsize As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				sas = New sactivesets.sactiveset()
				s = New Double(-1) {}
				diagh = New Double(-1) {}
				x = New Double(-1) {}
				g = New Double(-1) {}
				rstate = New rcommstate()
				ugc = New Double(-1) {}
				cgc = New Double(-1) {}
				xn = New Double(-1) {}
				ugn = New Double(-1) {}
				cgn = New Double(-1) {}
				xp = New Double(-1) {}
				d = New Double(-1) {}
				cleic = New Double(-1, -1) {}
				hasbndl = New Boolean(-1) {}
				hasbndu = New Boolean(-1) {}
				bndl = New Double(-1) {}
				bndu = New Double(-1) {}
				xstart = New Double(-1) {}
				solver = New snnls.snnlssolver()
				tmpprec = New Double(-1) {}
				tmp0 = New Double(-1) {}
				work = New Double(-1) {}
				lstate = New linmin.linminstate()
				bufyk = New Double(-1, -1) {}
				bufsk = New Double(-1, -1) {}
				bufrho = New Double(-1) {}
				buftheta = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minbleicstate()
				_result.nmain = nmain
				_result.nslack = nslack
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				_result.xrep = xrep
				_result.drep = drep
				_result.stpmax = stpmax
				_result.diffstep = diffstep
				_result.sas = DirectCast(sas.make_copy(), sactivesets.sactiveset)
				_result.s = DirectCast(s.Clone(), Double())
				_result.prectype = prectype
				_result.diagh = DirectCast(diagh.Clone(), Double())
				_result.x = DirectCast(x.Clone(), Double())
				_result.f = f
				_result.g = DirectCast(g.Clone(), Double())
				_result.needf = needf
				_result.needfg = needfg
				_result.xupdated = xupdated
				_result.lsstart = lsstart
				_result.steepestdescentstep = steepestdescentstep
				_result.boundedstep = boundedstep
				_result.userterminationneeded = userterminationneeded
				_result.teststep = teststep
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.ugc = DirectCast(ugc.Clone(), Double())
				_result.cgc = DirectCast(cgc.Clone(), Double())
				_result.xn = DirectCast(xn.Clone(), Double())
				_result.ugn = DirectCast(ugn.Clone(), Double())
				_result.cgn = DirectCast(cgn.Clone(), Double())
				_result.xp = DirectCast(xp.Clone(), Double())
				_result.fc = fc
				_result.fn = fn
				_result.fp = fp
				_result.d = DirectCast(d.Clone(), Double())
				_result.cleic = DirectCast(cleic.Clone(), Double(,))
				_result.nec = nec
				_result.nic = nic
				_result.lastgoodstep = lastgoodstep
				_result.lastscaledgoodstep = lastscaledgoodstep
				_result.maxscaledgrad = maxscaledgrad
				_result.hasbndl = DirectCast(hasbndl.Clone(), Boolean())
				_result.hasbndu = DirectCast(hasbndu.Clone(), Boolean())
				_result.bndl = DirectCast(bndl.Clone(), Double())
				_result.bndu = DirectCast(bndu.Clone(), Double())
				_result.repinneriterationscount = repinneriterationscount
				_result.repouteriterationscount = repouteriterationscount
				_result.repnfev = repnfev
				_result.repvaridx = repvaridx
				_result.repterminationtype = repterminationtype
				_result.repdebugeqerr = repdebugeqerr
				_result.repdebugfs = repdebugfs
				_result.repdebugff = repdebugff
				_result.repdebugdx = repdebugdx
				_result.repdebugfeasqpits = repdebugfeasqpits
				_result.repdebugfeasgpaits = repdebugfeasgpaits
				_result.xstart = DirectCast(xstart.Clone(), Double())
				_result.solver = DirectCast(solver.make_copy(), snnls.snnlssolver)
				_result.fbase = fbase
				_result.fm2 = fm2
				_result.fm1 = fm1
				_result.fp1 = fp1
				_result.fp2 = fp2
				_result.xm1 = xm1
				_result.xp1 = xp1
				_result.gm1 = gm1
				_result.gp1 = gp1
				_result.cidx = cidx
				_result.cval = cval
				_result.tmpprec = DirectCast(tmpprec.Clone(), Double())
				_result.tmp0 = DirectCast(tmp0.Clone(), Double())
				_result.nfev = nfev
				_result.mcstage = mcstage
				_result.stp = stp
				_result.curstpmax = curstpmax
				_result.activationstep = activationstep
				_result.work = DirectCast(work.Clone(), Double())
				_result.lstate = DirectCast(lstate.make_copy(), linmin.linminstate)
				_result.trimthreshold = trimthreshold
				_result.nonmonotoniccnt = nonmonotoniccnt
				_result.bufyk = DirectCast(bufyk.Clone(), Double(,))
				_result.bufsk = DirectCast(bufsk.Clone(), Double(,))
				_result.bufrho = DirectCast(bufrho.Clone(), Double())
				_result.buftheta = DirectCast(buftheta.Clone(), Double())
				_result.bufsize = bufsize
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure stores optimization report:
'        * IterationsCount           number of iterations
'        * NFEV                      number of gradient evaluations
'        * TerminationType           termination type (see below)
'
'        TERMINATION CODES
'
'        TerminationType field contains completion code, which can be:
'          -8    internal integrity control detected  infinite  or  NAN  values  in
'                function/gradient. Abnormal termination signalled.
'          -7    gradient verification failed.
'                See MinBLEICSetGradientCheck() for more information.
'          -3    inconsistent constraints. Feasible point is
'                either nonexistent or too hard to find. Try to
'                restart optimizer with better initial approximation
'           1    relative function improvement is no more than EpsF.
'           2    relative step is no more than EpsX.
'           4    gradient norm is no more than EpsG
'           5    MaxIts steps was taken
'           7    stopping conditions are too stringent,
'                further improvement is impossible,
'                X contains best point found so far.
'           8    terminated by user who called minbleicrequesttermination(). X contains
'                point which was "current accepted" when  termination  request  was
'                submitted.
'
'        ADDITIONAL FIELDS
'
'        There are additional fields which can be used for debugging:
'        * DebugEqErr                error in the equality constraints (2-norm)
'        * DebugFS                   f, calculated at projection of initial point
'                                    to the feasible set
'        * DebugFF                   f, calculated at the final point
'        * DebugDX                   |X_start-X_final|
'        ************************************************************************

		Public Class minbleicreport
			Inherits apobject
			Public iterationscount As Integer
			Public nfev As Integer
			Public varidx As Integer
			Public terminationtype As Integer
			Public debugeqerr As Double
			Public debugfs As Double
			Public debugff As Double
			Public debugdx As Double
			Public debugfeasqpits As Integer
			Public debugfeasgpaits As Integer
			Public inneriterationscount As Integer
			Public outeriterationscount As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minbleicreport()
				_result.iterationscount = iterationscount
				_result.nfev = nfev
				_result.varidx = varidx
				_result.terminationtype = terminationtype
				_result.debugeqerr = debugeqerr
				_result.debugfs = debugfs
				_result.debugff = debugff
				_result.debugdx = debugdx
				_result.debugfeasqpits = debugfeasqpits
				_result.debugfeasgpaits = debugfeasgpaits
				_result.inneriterationscount = inneriterationscount
				_result.outeriterationscount = outeriterationscount
				Return _result
			End Function
		End Class




		Public Const gtol As Double = 0.4
		Public Const maxnonmonotoniclen As Double = 1E-05
		Public Const initialdecay As Double = 0.5
		Public Const mindecay As Double = 0.1
		Public Const decaycorrection As Double = 0.8
		Public Const penaltyfactor As Double = 100


		'************************************************************************
'                             BOUND CONSTRAINED OPTIMIZATION
'               WITH ADDITIONAL LINEAR EQUALITY AND INEQUALITY CONSTRAINTS
'
'        DESCRIPTION:
'        The  subroutine  minimizes  function   F(x)  of N arguments subject to any
'        combination of:
'        * bound constraints
'        * linear inequality constraints
'        * linear equality constraints
'
'        REQUIREMENTS:
'        * user must provide function value and gradient
'        * starting point X0 must be feasible or
'          not too far away from the feasible set
'        * grad(f) must be Lipschitz continuous on a level set:
'          L = { x : f(x)<=f(x0) }
'        * function must be defined everywhere on the feasible set F
'
'        USAGE:
'
'        Constrained optimization if far more complex than the unconstrained one.
'        Here we give very brief outline of the BLEIC optimizer. We strongly recommend
'        you to read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
'        on optimization, which is available at http://www.alglib.net/optimization/
'
'        1. User initializes algorithm state with MinBLEICCreate() call
'
'        2. USer adds boundary and/or linear constraints by calling
'           MinBLEICSetBC() and MinBLEICSetLC() functions.
'
'        3. User sets stopping conditions with MinBLEICSetCond().
'
'        4. User calls MinBLEICOptimize() function which takes algorithm  state and
'           pointer (delegate, etc.) to callback function which calculates F/G.
'
'        5. User calls MinBLEICResults() to get solution
'
'        6. Optionally user may call MinBLEICRestartFrom() to solve another problem
'           with same N but another starting point.
'           MinBLEICRestartFrom() allows to reuse already initialized structure.
'
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size ofX
'            X       -   starting point, array[N]:
'                        * it is better to set X to a feasible point
'                        * but X can be infeasible, in which case algorithm will try
'                          to find feasible point first, using X as initial
'                          approximation.
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleiccreate(n As Integer, x As Double(), state As minbleicstate)
			Dim c As Double(,) = New Double(-1, -1) {}
			Dim ct As Integer() = New Integer(-1) {}

			alglib.ap.assert(n >= 1, "MinBLEICCreate: N<1")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinBLEICCreate: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinBLEICCreate: X contains infinite or NaN values!")
			minbleicinitinternal(n, x, 0.0, state)
		End Sub


		'************************************************************************
'        The subroutine is finite difference variant of MinBLEICCreate().  It  uses
'        finite differences in order to differentiate target function.
'
'        Description below contains information which is specific to  this function
'        only. We recommend to read comments on MinBLEICCreate() in  order  to  get
'        more information about creation of BLEIC optimizer.
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size of X
'            X       -   starting point, array[0..N-1].
'            DiffStep-   differentiation step, >0
'
'        OUTPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTES:
'        1. algorithm uses 4-point central formula for differentiation.
'        2. differentiation step along I-th axis is equal to DiffStep*S[I] where
'           S[] is scaling vector which can be set by MinBLEICSetScale() call.
'        3. we recommend you to use moderate values of  differentiation  step.  Too
'           large step will result in too large truncation  errors, while too small
'           step will result in too large numerical  errors.  1.0E-6  can  be  good
'           value to start with.
'        4. Numerical  differentiation  is   very   inefficient  -   one   gradient
'           calculation needs 4*N function evaluations. This function will work for
'           any N - either small (1...10), moderate (10...100) or  large  (100...).
'           However, performance penalty will be too severe for any N's except  for
'           small ones.
'           We should also say that code which relies on numerical  differentiation
'           is  less  robust and precise. CG needs exact gradient values. Imprecise
'           gradient may slow  down  convergence, especially  on  highly  nonlinear
'           problems.
'           Thus  we  recommend to use this function for fast prototyping on small-
'           dimensional problems only, and to implement analytical gradient as soon
'           as possible.
'
'          -- ALGLIB --
'             Copyright 16.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleiccreatef(n As Integer, x As Double(), diffstep As Double, state As minbleicstate)
			Dim c As Double(,) = New Double(-1, -1) {}
			Dim ct As Integer() = New Integer(-1) {}

			alglib.ap.assert(n >= 1, "MinBLEICCreateF: N<1")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinBLEICCreateF: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinBLEICCreateF: X contains infinite or NaN values!")
			alglib.ap.assert(Math.isfinite(diffstep), "MinBLEICCreateF: DiffStep is infinite or NaN!")
			alglib.ap.assert(CDbl(diffstep) > CDbl(0), "MinBLEICCreateF: DiffStep is non-positive!")
			minbleicinitinternal(n, x, diffstep, state)
		End Sub


		'************************************************************************
'        This function sets boundary constraints for BLEIC optimizer.
'
'        Boundary constraints are inactive by default (after initial creation).
'        They are preserved after algorithm restart with MinBLEICRestartFrom().
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            BndL    -   lower bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very small number or -INF.
'            BndU    -   upper bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very large number or +INF.
'
'        NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
'        variable will be "frozen" at X[i]=BndL[i]=BndU[i].
'
'        NOTE 2: this solver has following useful properties:
'        * bound constraints are always satisfied exactly
'        * function is evaluated only INSIDE area specified by  bound  constraints,
'          even  when  numerical  differentiation is used (algorithm adjusts  nodes
'          according to boundary constraints)
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetbc(state As minbleicstate, bndl As Double(), bndu As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0

			n = state.nmain
			alglib.ap.assert(alglib.ap.len(bndl) >= n, "MinBLEICSetBC: Length(BndL)<N")
			alglib.ap.assert(alglib.ap.len(bndu) >= n, "MinBLEICSetBC: Length(BndU)<N")
			For i = 0 To n - 1
				alglib.ap.assert(Math.isfinite(bndl(i)) OrElse [Double].IsNegativeInfinity(bndl(i)), "MinBLEICSetBC: BndL contains NAN or +INF")
				alglib.ap.assert(Math.isfinite(bndu(i)) OrElse [Double].IsPositiveInfinity(bndu(i)), "MinBLEICSetBC: BndL contains NAN or -INF")
				state.bndl(i) = bndl(i)
				state.hasbndl(i) = Math.isfinite(bndl(i))
				state.bndu(i) = bndu(i)
				state.hasbndu(i) = Math.isfinite(bndu(i))
			Next
			sactivesets.sassetbc(state.sas, bndl, bndu)
		End Sub


		'************************************************************************
'        This function sets linear constraints for BLEIC optimizer.
'
'        Linear constraints are inactive by default (after initial creation).
'        They are preserved after algorithm restart with MinBLEICRestartFrom().
'
'        INPUT PARAMETERS:
'            State   -   structure previously allocated with MinBLEICCreate call.
'            C       -   linear constraints, array[K,N+1].
'                        Each row of C represents one constraint, either equality
'                        or inequality (see below):
'                        * first N elements correspond to coefficients,
'                        * last element corresponds to the right part.
'                        All elements of C (including right part) must be finite.
'            CT      -   type of constraints, array[K]:
'                        * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
'                        * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
'                        * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
'            K       -   number of equality/inequality constraints, K>=0:
'                        * if given, only leading K elements of C/CT are used
'                        * if not given, automatically determined from sizes of C/CT
'
'        NOTE 1: linear (non-bound) constraints are satisfied only approximately:
'        * there always exists some minor violation (about Epsilon in magnitude)
'          due to rounding errors
'        * numerical differentiation, if used, may  lead  to  function  evaluations
'          outside  of the feasible  area,   because   algorithm  does  NOT  change
'          numerical differentiation formula according to linear constraints.
'        If you want constraints to be  satisfied  exactly, try to reformulate your
'        problem  in  such  manner  that  all constraints will become boundary ones
'        (this kind of constraints is always satisfied exactly, both in  the  final
'        solution and in all intermediate points).
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetlc(state As minbleicstate, c As Double(,), ct As Integer(), k As Integer)
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = state.nmain

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(k >= 0, "MinBLEICSetLC: K<0")
			alglib.ap.assert(alglib.ap.cols(c) >= n + 1 OrElse k = 0, "MinBLEICSetLC: Cols(C)<N+1")
			alglib.ap.assert(alglib.ap.rows(c) >= k, "MinBLEICSetLC: Rows(C)<K")
			alglib.ap.assert(alglib.ap.len(ct) >= k, "MinBLEICSetLC: Length(CT)<K")
			alglib.ap.assert(apserv.apservisfinitematrix(c, k, n + 1), "MinBLEICSetLC: C contains infinite or NaN values!")

			'
			' Handle zero K
			'
			If k = 0 Then
				state.nec = 0
				state.nic = 0
				Return
			End If

			'
			' Equality constraints are stored first, in the upper
			' NEC rows of State.CLEIC matrix. Inequality constraints
			' are stored in the next NIC rows.
			'
			' NOTE: we convert inequality constraints to the form
			' A*x<=b before copying them.
			'
			apserv.rmatrixsetlengthatleast(state.cleic, k, n + 1)
			state.nec = 0
			state.nic = 0
			For i = 0 To k - 1
				If ct(i) = 0 Then
					For i_ = 0 To n
						state.cleic(state.nec, i_) = c(i, i_)
					Next
					state.nec = state.nec + 1
				End If
			Next
			For i = 0 To k - 1
				If ct(i) <> 0 Then
					If ct(i) > 0 Then
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = -c(i, i_)
						Next
					Else
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = c(i, i_)
						Next
					End If
					state.nic = state.nic + 1
				End If
			Next

			'
			' Normalize rows of State.CLEIC: each row must have unit norm.
			' Norm is calculated using first N elements (i.e. right part is
			' not counted when we calculate norm).
			'
			For i = 0 To k - 1
				v = 0
				For j = 0 To n - 1
					v = v + Math.sqr(state.cleic(i, j))
				Next
				If CDbl(v) = CDbl(0) Then
					Continue For
				End If
				v = 1 / System.Math.sqrt(v)
				For i_ = 0 To n
					state.cleic(i, i_) = v * state.cleic(i, i_)
				Next
			Next
			sactivesets.sassetlc(state.sas, c, ct, k)
		End Sub


		'************************************************************************
'        This function sets stopping conditions for the optimizer.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            EpsG    -   >=0
'                        The  subroutine  finishes  its  work   if   the  condition
'                        |v|<EpsG is satisfied, where:
'                        * |.| means Euclidian norm
'                        * v - scaled gradient vector, v[i]=g[i]*s[i]
'                        * g - gradient
'                        * s - scaling coefficients set by MinBLEICSetScale()
'            EpsF    -   >=0
'                        The  subroutine  finishes  its work if on k+1-th iteration
'                        the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
'                        is satisfied.
'            EpsX    -   >=0
'                        The subroutine finishes its work if  on  k+1-th  iteration
'                        the condition |v|<=EpsX is fulfilled, where:
'                        * |.| means Euclidian norm
'                        * v - scaled step vector, v[i]=dx[i]/s[i]
'                        * dx - step vector, dx=X(k+1)-X(k)
'                        * s - scaling coefficients set by MinBLEICSetScale()
'            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
'                        iterations is unlimited.
'
'        Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
'        to automatic stopping criterion selection.
'
'        NOTE: when SetCond() called with non-zero MaxIts, BLEIC solver may perform
'              slightly more than MaxIts iterations. I.e., MaxIts  sets  non-strict
'              limit on iterations count.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetcond(state As minbleicstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(epsg), "MinBLEICSetCond: EpsG is not finite number")
			alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinBLEICSetCond: negative EpsG")
			alglib.ap.assert(Math.isfinite(epsf), "MinBLEICSetCond: EpsF is not finite number")
			alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinBLEICSetCond: negative EpsF")
			alglib.ap.assert(Math.isfinite(epsx), "MinBLEICSetCond: EpsX is not finite number")
			alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinBLEICSetCond: negative EpsX")
			alglib.ap.assert(maxits >= 0, "MinBLEICSetCond: negative MaxIts!")
			If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
				epsx = 1E-06
			End If
			state.epsg = epsg
			state.epsf = epsf
			state.epsx = epsx
			state.maxits = maxits
		End Sub


		'************************************************************************
'        This function sets scaling coefficients for BLEIC optimizer.
'
'        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'        size and gradient are scaled before comparison with tolerances).  Scale of
'        the I-th variable is a translation invariant measure of:
'        a) "how large" the variable is
'        b) how large the step should be to make significant changes in the function
'
'        Scaling is also used by finite difference variant of the optimizer  - step
'        along I-th axis is equal to DiffStep*S[I].
'
'        In  most  optimizers  (and  in  the  BLEIC  too)  scaling is NOT a form of
'        preconditioning. It just  affects  stopping  conditions.  You  should  set
'        preconditioner  by  separate  call  to  one  of  the  MinBLEICSetPrec...()
'        functions.
'
'        There is a special  preconditioning  mode, however,  which  uses   scaling
'        coefficients to form diagonal preconditioning matrix. You  can  turn  this
'        mode on, if you want.   But  you should understand that scaling is not the
'        same thing as preconditioning - these are two different, although  related
'        forms of tuning solver.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            S       -   array[N], non-zero scaling coefficients
'                        S[i] may be negative, sign doesn't matter.
'
'          -- ALGLIB --
'             Copyright 14.01.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetscale(state As minbleicstate, s As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(s) >= state.nmain, "MinBLEICSetScale: Length(S)<N")
			For i = 0 To state.nmain - 1
				alglib.ap.assert(Math.isfinite(s(i)), "MinBLEICSetScale: S contains infinite or NAN elements")
				alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinBLEICSetScale: S contains zero elements")
				state.s(i) = System.Math.Abs(s(i))
			Next
			sactivesets.sassetscale(state.sas, s)
		End Sub


		'************************************************************************
'        Modification of the preconditioner: preconditioning is turned off.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetprecdefault(state As minbleicstate)
			state.prectype = 0
		End Sub


		'************************************************************************
'        Modification  of  the  preconditioner:  diagonal of approximate Hessian is
'        used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            D       -   diagonal of the approximate Hessian, array[0..N-1],
'                        (if larger, only leading N elements are used).
'
'        NOTE 1: D[i] should be positive. Exception will be thrown otherwise.
'
'        NOTE 2: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetprecdiag(state As minbleicstate, d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(d) >= state.nmain, "MinBLEICSetPrecDiag: D is too short")
			For i = 0 To state.nmain - 1
				alglib.ap.assert(Math.isfinite(d(i)), "MinBLEICSetPrecDiag: D contains infinite or NAN elements")
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "MinBLEICSetPrecDiag: D contains non-positive elements")
			Next
			apserv.rvectorsetlengthatleast(state.diagh, state.nmain)
			state.prectype = 2
			For i = 0 To state.nmain - 1
				state.diagh(i) = d(i)
			Next
		End Sub


		'************************************************************************
'        Modification of the preconditioner: scale-based diagonal preconditioning.
'
'        This preconditioning mode can be useful when you  don't  have  approximate
'        diagonal of Hessian, but you know that your  variables  are  badly  scaled
'        (for  example,  one  variable is in [1,10], and another in [1000,100000]),
'        and most part of the ill-conditioning comes from different scales of vars.
'
'        In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
'        can greatly improve convergence.
'
'        IMPRTANT: you should set scale of your variables  with  MinBLEICSetScale()
'        call  (before  or after MinBLEICSetPrecScale() call). Without knowledge of
'        the scale of your variables scale-based preconditioner will be  just  unit
'        matrix.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetprecscale(state As minbleicstate)
			state.prectype = 3
		End Sub


		'************************************************************************
'        This function turns on/off reporting.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedXRep-   whether iteration reports are needed or not
'
'        If NeedXRep is True, algorithm will call rep() callback function if  it is
'        provided to MinBLEICOptimize().
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetxrep(state As minbleicstate, needxrep As Boolean)
			state.xrep = needxrep
		End Sub


		'************************************************************************
'        This function turns on/off line search reports.
'        These reports are described in more details in developer-only  comments on
'        MinBLEICState object.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedDRep-   whether line search reports are needed or not
'
'        This function is intended for private use only. Turning it on artificially
'        may cause program failure.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetdrep(state As minbleicstate, needdrep As Boolean)
			state.drep = needdrep
		End Sub


		'************************************************************************
'        This function sets maximum step length
'
'        IMPORTANT: this feature is hard to combine with preconditioning. You can't
'        set upper limit on step length, when you solve optimization  problem  with
'        linear (non-boundary) constraints AND preconditioner turned on.
'
'        When  non-boundary  constraints  are  present,  you  have to either a) use
'        preconditioner, or b) use upper limit on step length.  YOU CAN'T USE BOTH!
'        In this case algorithm will terminate with appropriate error code.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
'                        want to limit step length.
'
'        Use this subroutine when you optimize target function which contains exp()
'        or  other  fast  growing  functions,  and optimization algorithm makes too
'        large  steps  which  lead   to overflow. This function allows us to reject
'        steps  that  are  too  large  (and  therefore  expose  us  to the possible
'        overflow) without actually calculating function value at the x+stp*d.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetstpmax(state As minbleicstate, stpmax As Double)
			alglib.ap.assert(Math.isfinite(stpmax), "MinBLEICSetStpMax: StpMax is not finite!")
			alglib.ap.assert(CDbl(stpmax) >= CDbl(0), "MinBLEICSetStpMax: StpMax<0!")
			state.stpmax = stpmax
		End Sub


		'************************************************************************
'        NOTES:
'
'        1. This function has two different implementations: one which  uses  exact
'           (analytical) user-supplied gradient,  and one which uses function value
'           only  and  numerically  differentiates  function  in  order  to  obtain
'           gradient.
'
'           Depending  on  the  specific  function  used to create optimizer object
'           (either  MinBLEICCreate() for analytical gradient or  MinBLEICCreateF()
'           for numerical differentiation) you should choose appropriate variant of
'           MinBLEICOptimize() - one  which  accepts  function  AND gradient or one
'           which accepts function ONLY.
'
'           Be careful to choose variant of MinBLEICOptimize() which corresponds to
'           your optimization scheme! Table below lists different  combinations  of
'           callback (function/gradient) passed to MinBLEICOptimize()  and specific
'           function used to create optimizer.
'
'
'                             |         USER PASSED TO MinBLEICOptimize()
'           CREATED WITH      |  function only   |  function and gradient
'           ------------------------------------------------------------
'           MinBLEICCreateF() |     work                FAIL
'           MinBLEICCreate()  |     FAIL                work
'
'           Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
'           function  and  MinBLEICOptimize()  version.   Attemps   to   use   such
'           combination (for  example,  to  create optimizer with MinBLEICCreateF()
'           and  to  pass  gradient  information  to  MinCGOptimize()) will lead to
'           exception being thrown. Either  you  did  not pass gradient when it WAS
'           needed or you passed gradient when it was NOT needed.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function minbleiciteration(state As minbleicstate) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim m As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim v0 As Double = 0
			Dim b As New Boolean()
			Dim mcinfo As Integer = 0
			Dim actstatus As Integer = 0
			Dim itidx As Integer = 0
			Dim penalty As Double = 0
			Dim ginit As Double = 0
			Dim gdecay As Double = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstate.stage >= 0 Then
				n = state.rstate.ia(0)
				m = state.rstate.ia(1)
				i = state.rstate.ia(2)
				j = state.rstate.ia(3)
				mcinfo = state.rstate.ia(4)
				actstatus = state.rstate.ia(5)
				itidx = state.rstate.ia(6)
				b = state.rstate.ba(0)
				v = state.rstate.ra(0)
				vv = state.rstate.ra(1)
				v0 = state.rstate.ra(2)
				penalty = state.rstate.ra(3)
				ginit = state.rstate.ra(4)
				gdecay = state.rstate.ra(5)
			Else
				n = -983
				m = -989
				i = -834
				j = 900
				mcinfo = -287
				actstatus = 364
				itidx = 214
				b = False
				v = -686
				vv = 912
				v0 = 585
				penalty = 497
				ginit = -271
				gdecay = -581
			End If
			If state.rstate.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstate.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstate.stage = 2 Then
				GoTo lbl_2
			End If
			If state.rstate.stage = 3 Then
				GoTo lbl_3
			End If
			If state.rstate.stage = 4 Then
				GoTo lbl_4
			End If
			If state.rstate.stage = 5 Then
				GoTo lbl_5
			End If
			If state.rstate.stage = 6 Then
				GoTo lbl_6
			End If
			If state.rstate.stage = 7 Then
				GoTo lbl_7
			End If
			If state.rstate.stage = 8 Then
				GoTo lbl_8
			End If
			If state.rstate.stage = 9 Then
				GoTo lbl_9
			End If
			If state.rstate.stage = 10 Then
				GoTo lbl_10
			End If
			If state.rstate.stage = 11 Then
				GoTo lbl_11
			End If
			If state.rstate.stage = 12 Then
				GoTo lbl_12
			End If
			If state.rstate.stage = 13 Then
				GoTo lbl_13
			End If
			If state.rstate.stage = 14 Then
				GoTo lbl_14
			End If
			If state.rstate.stage = 15 Then
				GoTo lbl_15
			End If
			If state.rstate.stage = 16 Then
				GoTo lbl_16
			End If
			If state.rstate.stage = 17 Then
				GoTo lbl_17
			End If
			If state.rstate.stage = 18 Then
				GoTo lbl_18
			End If
			If state.rstate.stage = 19 Then
				GoTo lbl_19
			End If
			If state.rstate.stage = 20 Then
				GoTo lbl_20
			End If
			If state.rstate.stage = 21 Then
				GoTo lbl_21
			End If
			If state.rstate.stage = 22 Then
				GoTo lbl_22
			End If
			If state.rstate.stage = 23 Then
				GoTo lbl_23
			End If

			'
			' Routine body
			'

			'
			' Algorithm parameters:
			' * M          number of L-BFGS corrections.
			'              This coefficient remains fixed during iterations.
			' * GDecay     desired decrease of constrained gradient during L-BFGS iterations.
			'              This coefficient is decreased after each L-BFGS round until
			'              it reaches minimum decay.
			'
			m = System.Math.Min(5, state.nmain)
			gdecay = initialdecay

			'
			' Init
			'
			n = state.nmain
			state.steepestdescentstep = False
			state.userterminationneeded = False
			state.repterminationtype = 0
			state.repinneriterationscount = 0
			state.repouteriterationscount = 0
			state.repnfev = 0
			state.repvaridx = -1
			state.repdebugeqerr = 0.0
			state.repdebugfs = [Double].NaN
			state.repdebugff = [Double].NaN
			state.repdebugdx = [Double].NaN
			If CDbl(state.stpmax) <> CDbl(0) AndAlso state.prectype <> 0 Then
				state.repterminationtype = -10
				result = False
				Return result
			End If
			apserv.rmatrixsetlengthatleast(state.bufyk, m + 1, n)
			apserv.rmatrixsetlengthatleast(state.bufsk, m + 1, n)
			apserv.rvectorsetlengthatleast(state.bufrho, m)
			apserv.rvectorsetlengthatleast(state.buftheta, m)
			apserv.rvectorsetlengthatleast(state.tmp0, n)

			'
			' Fill TmpPrec with current preconditioner
			'
			apserv.rvectorsetlengthatleast(state.tmpprec, n)
			For i = 0 To n - 1
				If state.prectype = 2 Then
					state.tmpprec(i) = state.diagh(i)
					Continue For
				End If
				If state.prectype = 3 Then
					state.tmpprec(i) = 1 / Math.sqr(state.s(i))
					Continue For
				End If
				state.tmpprec(i) = 1
			Next
			sactivesets.sassetprecdiag(state.sas, state.tmpprec)

			'
			' Start optimization
			'
			If Not sactivesets.sasstartoptimization(state.sas, state.xstart) Then
				state.repterminationtype = -3
				result = False
				Return result
			End If

			'
			'  Check correctness of user-supplied gradient
			'
			If Not (CDbl(state.diffstep) = CDbl(0) AndAlso CDbl(state.teststep) > CDbl(0)) Then
				GoTo lbl_24
			End If
			clearrequestfields(state)
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			state.needfg = True
			i = 0
			lbl_26:
			If i > n - 1 Then
				GoTo lbl_28
			End If
			alglib.ap.assert(Not state.hasbndl(i) OrElse CDbl(state.sas.xc(i)) >= CDbl(state.bndl(i)), "MinBLEICIteration: internal error(State.X is out of bounds)")
			alglib.ap.assert(Not state.hasbndu(i) OrElse CDbl(state.sas.xc(i)) <= CDbl(state.bndu(i)), "MinBLEICIteration: internal error(State.X is out of bounds)")
			v = state.x(i)
			state.x(i) = v - state.teststep * state.s(i)
			If state.hasbndl(i) Then
				state.x(i) = System.Math.Max(state.x(i), state.bndl(i))
			End If
			state.xm1 = state.x(i)
			state.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			state.fm1 = state.f
			state.gm1 = state.g(i)
			state.x(i) = v + state.teststep * state.s(i)
			If state.hasbndu(i) Then
				state.x(i) = System.Math.Min(state.x(i), state.bndu(i))
			End If
			state.xp1 = state.x(i)
			state.rstate.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			state.fp1 = state.f
			state.gp1 = state.g(i)
			state.x(i) = (state.xm1 + state.xp1) / 2
			If state.hasbndl(i) Then
				state.x(i) = System.Math.Max(state.x(i), state.bndl(i))
			End If
			If state.hasbndu(i) Then
				state.x(i) = System.Math.Min(state.x(i), state.bndu(i))
			End If
			state.rstate.stage = 2
			GoTo lbl_rcomm
			lbl_2:
			state.x(i) = v
			If Not optserv.derivativecheck(state.fm1, state.gm1, state.fp1, state.gp1, state.f, state.g(i), _
				state.xp1 - state.xm1) Then
				state.repvaridx = i
				state.repterminationtype = -7
				sactivesets.sasstopoptimization(state.sas)
				result = False
				Return result
			End If
			i = i + 1
			GoTo lbl_26
			lbl_28:
			state.needfg = False
			lbl_24:

			'
			' Main cycle of BLEIC-PG algorithm
			'
			state.repterminationtype = 0
			state.lastgoodstep = 0
			state.lastscaledgoodstep = 0
			state.maxscaledgrad = 0
			state.nonmonotoniccnt = CInt(System.Math.Truncate(System.Math.Round(1.5 * (n + state.nic)))) + 5
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_29
			End If
			state.needfg = True
			state.rstate.stage = 3
			GoTo lbl_rcomm
			lbl_3:
			state.needfg = False
			GoTo lbl_30
			lbl_29:
			state.needf = True
			state.rstate.stage = 4
			GoTo lbl_rcomm
			lbl_4:
			state.needf = False
			lbl_30:
			state.fc = state.f
			optserv.trimprepare(state.f, state.trimthreshold)
			state.repnfev = state.repnfev + 1
			If Not state.xrep Then
				GoTo lbl_31
			End If

			'
			' Report current point
			'
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			state.f = state.fc
			state.xupdated = True
			state.rstate.stage = 5
			GoTo lbl_rcomm
			lbl_5:
			state.xupdated = False
			lbl_31:
			If state.userterminationneeded Then

				'
				' User requested termination
				'
				sactivesets.sasstopoptimization(state.sas)
				state.repterminationtype = 8
				result = False
				Return result
			End If
			lbl_33:
			If False Then
				GoTo lbl_34
			End If

			'
			' Preparations
			'
			' (a) calculate unconstrained gradient
			' (b) determine initial active set
			' (c) update MaxScaledGrad
			' (d) check F/G for NAN/INF, abnormally terminate algorithm if needed
			'
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_35
			End If

			'
			' Analytic gradient
			'
			state.needfg = True
			state.rstate.stage = 6
			GoTo lbl_rcomm
			lbl_6:
			state.needfg = False
			GoTo lbl_36
			lbl_35:

			'
			' Numerical differentiation
			'
			state.needf = True
			state.rstate.stage = 7
			GoTo lbl_rcomm
			lbl_7:
			state.fbase = state.f
			i = 0
			lbl_37:
			If i > n - 1 Then
				GoTo lbl_39
			End If
			v = state.x(i)
			b = False
			If state.hasbndl(i) Then
				b = b OrElse CDbl(v - state.diffstep * state.s(i)) < CDbl(state.bndl(i))
			End If
			If state.hasbndu(i) Then
				b = b OrElse CDbl(v + state.diffstep * state.s(i)) > CDbl(state.bndu(i))
			End If
			If b Then
				GoTo lbl_40
			End If
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 8
			GoTo lbl_rcomm
			lbl_8:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 9
			GoTo lbl_rcomm
			lbl_9:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 10
			GoTo lbl_rcomm
			lbl_10:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 11
			GoTo lbl_rcomm
			lbl_11:
			state.fp2 = state.f
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			GoTo lbl_41
			lbl_40:
			state.xm1 = v - state.diffstep * state.s(i)
			state.xp1 = v + state.diffstep * state.s(i)
			If state.hasbndl(i) AndAlso CDbl(state.xm1) < CDbl(state.bndl(i)) Then
				state.xm1 = state.bndl(i)
			End If
			If state.hasbndu(i) AndAlso CDbl(state.xp1) > CDbl(state.bndu(i)) Then
				state.xp1 = state.bndu(i)
			End If
			state.x(i) = state.xm1
			state.rstate.stage = 12
			GoTo lbl_rcomm
			lbl_12:
			state.fm1 = state.f
			state.x(i) = state.xp1
			state.rstate.stage = 13
			GoTo lbl_rcomm
			lbl_13:
			state.fp1 = state.f
			If CDbl(state.xm1) <> CDbl(state.xp1) Then
				state.g(i) = (state.fp1 - state.fm1) / (state.xp1 - state.xm1)
			Else
				state.g(i) = 0
			End If
			lbl_41:
			state.x(i) = v
			i = i + 1
			GoTo lbl_37
			lbl_39:
			state.f = state.fbase
			state.needf = False
			lbl_36:
			state.fc = state.f
			For i_ = 0 To n - 1
				state.ugc(i_) = state.g(i_)
			Next
			For i_ = 0 To n - 1
				state.cgc(i_) = state.g(i_)
			Next
			sactivesets.sasreactivateconstraintsprec(state.sas, state.ugc)
			sactivesets.sasconstraineddirection(state.sas, state.cgc)
			ginit = 0.0
			For i = 0 To n - 1
				ginit = ginit + Math.sqr(state.cgc(i) * state.s(i))
			Next
			ginit = System.Math.sqrt(ginit)
			state.maxscaledgrad = System.Math.Max(state.maxscaledgrad, ginit)
			If Not Math.isfinite(ginit) OrElse Not Math.isfinite(state.fc) Then

				'
				' Abnormal termination - infinities in function/gradient
				'
				sactivesets.sasstopoptimization(state.sas)
				state.repterminationtype = -8
				result = False
				Return result
			End If
			If state.userterminationneeded Then

				'
				' User requested termination
				'
				sactivesets.sasstopoptimization(state.sas)
				state.repterminationtype = 8
				result = False
				Return result
			End If

			'
			' LBFGS stage:
			' * during LBFGS iterations we activate new constraints, but never
			'   deactivate already active ones.
			' * we perform at most N iterations of LBFGS before re-evaluating
			'   active set and restarting LBFGS.
			' * first iteration of LBFGS is a special - it is performed with
			'   minimum set of active constraints, algorithm termination can
			'   be performed only at this state. We call this iteration
			'  "steepest descent step".
			'
			' About termination:
			' * LBFGS iterations can be terminated because of two reasons:
			'   * "termination" - non-zero termination code in RepTerminationType,
			'     which means that optimization is done
			'   * "restart" - zero RepTerminationType, which means that we
			'     have to re-evaluate active set and resume LBFGS stage.
			' * one more option is "refresh" - to continue LBFGS iterations,
			'   but with all BFGS updates (Sk/Yk pairs) being dropped;
			'   it happens after changes in active set
			'
			state.bufsize = 0
			state.steepestdescentstep = True
			itidx = 0
			lbl_42:
			If itidx > n - 1 Then
				GoTo lbl_44
			End If

			'
			' At the beginning of each iteration:
			' * SAS.XC stores current point
			' * FC stores current function value
			' * UGC stores current unconstrained gradient
			' * CGC stores current constrained gradient
			' * D stores constrained step direction (calculated at this block)
			'
			'
			' Check gradient-based stopping criteria
			'
			' This stopping condition is tested only for step which is the
			' first step of LBFGS (subsequent steps may accumulate active
			' constraints thus they should NOT be used for stopping - gradient
			' may be small when constrained, but these constraints may be
			' deactivated by the subsequent steps)
			'
			If state.steepestdescentstep AndAlso CDbl(sactivesets.sasscaledconstrainednorm(state.sas, state.ugc)) <= CDbl(state.epsg) Then

				'
				' Gradient is small enough.
				' Optimization is terminated
				'
				state.repterminationtype = 4
				GoTo lbl_44
			End If

			'
			' 1. Calculate search direction D according to L-BFGS algorithm
			'    using constrained preconditioner to perform inner multiplication.
			' 2. Evaluate scaled length of direction D; restart LBFGS if D is zero
			'    (it may be possible that we found minimum, but it is also possible
			'    that some constraints need deactivation)
			' 3. If D is non-zero, try to use previous scaled step length as initial estimate for new step.
			'
			For i_ = 0 To n - 1
				state.work(i_) = state.cgc(i_)
			Next
			For i = state.bufsize - 1 To 0 Step -1
				v = 0.0
				For i_ = 0 To n - 1
					v += state.bufsk(i, i_) * state.work(i_)
				Next
				state.buftheta(i) = v
				vv = v * state.bufrho(i)
				For i_ = 0 To n - 1
					state.work(i_) = state.work(i_) - vv * state.bufyk(i, i_)
				Next
			Next
			sactivesets.sasconstraineddirectionprec(state.sas, state.work)
			For i = 0 To state.bufsize - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += state.bufyk(i, i_) * state.work(i_)
				Next
				vv = state.bufrho(i) * (-v + state.buftheta(i))
				For i_ = 0 To n - 1
					state.work(i_) = state.work(i_) + vv * state.bufsk(i, i_)
				Next
			Next
			For i_ = 0 To n - 1
				state.d(i_) = -state.work(i_)
			Next
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.d(i) / state.s(i))
			Next
			v = System.Math.sqrt(v)
			If CDbl(v) = CDbl(0) Then

				'
				' Search direction is zero.
				' If we perform "steepest descent step", algorithm is terminated.
				' Otherwise we just restart LBFGS.
				'
				If state.steepestdescentstep Then
					state.repterminationtype = 4
				End If
				GoTo lbl_44
			End If
			alglib.ap.assert(CDbl(v) > CDbl(0), "MinBLEIC: internal error")
			If CDbl(state.lastscaledgoodstep) > CDbl(0) AndAlso CDbl(v) > CDbl(0) Then
				state.stp = state.lastscaledgoodstep / v
			Else
				state.stp = 1.0 / v
			End If

			'
			' Calculate bound on step length.
			' Step direction is stored
			'
			sactivesets.sasexploredirection(state.sas, state.d, state.curstpmax, state.cidx, state.cval)
			state.activationstep = state.curstpmax
			If state.cidx >= 0 AndAlso CDbl(state.activationstep) = CDbl(0) Then

				'
				' We are exactly at the boundary, immediate activation
				' of constraint is required. LBFGS stage is continued
				' with "refreshed" model.
				'
				' ! IMPORTANT: we do not clear SteepestDescent flag here,
				' !            it is very important for correct stopping
				' !            of algorithm.
				'
				sactivesets.sasimmediateactivation(state.sas, state.cidx, state.cval)
				state.bufsize = 0
				GoTo lbl_43
			End If
			If CDbl(state.stpmax) > CDbl(0) Then
				v = 0.0
				For i_ = 0 To n - 1
					v += state.d(i_) * state.d(i_)
				Next
				v = System.Math.sqrt(v)
				If CDbl(v) > CDbl(0) Then
					state.curstpmax = System.Math.Min(state.curstpmax, state.stpmax / v)
				End If
			End If

			'
			' Report beginning of line search (if requested by caller).
			' See description of the MinBLEICState for more information
			' about fields accessible to caller.
			'
			' Caller may do following:
			' * change State.Stp and load better initial estimate of
			'   the step length.
			' Caller may not terminate algorithm.
			'
			If Not state.drep Then
				GoTo lbl_45
			End If
			clearrequestfields(state)
			state.lsstart = True
			state.boundedstep = state.cidx >= 0
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			state.rstate.stage = 14
			GoTo lbl_rcomm
			lbl_14:
			state.lsstart = False
			lbl_45:

			'
			' Minimize F(x+alpha*d)
			'
			For i_ = 0 To n - 1
				state.xn(i_) = state.sas.xc(i_)
			Next
			For i_ = 0 To n - 1
				state.cgn(i_) = state.cgc(i_)
			Next
			For i_ = 0 To n - 1
				state.ugn(i_) = state.ugc(i_)
			Next
			state.fn = state.fc
			state.mcstage = 0
			linmin.mcsrch(n, state.xn, state.fn, state.cgn, state.d, state.stp, _
				state.curstpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			lbl_47:
			If state.mcstage = 0 Then
				GoTo lbl_48
			End If

			'
			' Perform correction (constraints are enforced)
			' Copy XN to X
			'
			sactivesets.sascorrection(state.sas, state.xn, penalty)
			For i = 0 To n - 1
				state.x(i) = state.xn(i)
			Next

			'
			' Gradient, either user-provided or numerical differentiation
			'
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_49
			End If

			'
			' Analytic gradient
			'
			state.needfg = True
			state.rstate.stage = 15
			GoTo lbl_rcomm
			lbl_15:
			state.needfg = False
			state.repnfev = state.repnfev + 1
			GoTo lbl_50
			lbl_49:

			'
			' Numerical differentiation
			'
			state.needf = True
			state.rstate.stage = 16
			GoTo lbl_rcomm
			lbl_16:
			state.fbase = state.f
			i = 0
			lbl_51:
			If i > n - 1 Then
				GoTo lbl_53
			End If
			v = state.x(i)
			b = False
			If state.hasbndl(i) Then
				b = b OrElse CDbl(v - state.diffstep * state.s(i)) < CDbl(state.bndl(i))
			End If
			If state.hasbndu(i) Then
				b = b OrElse CDbl(v + state.diffstep * state.s(i)) > CDbl(state.bndu(i))
			End If
			If b Then
				GoTo lbl_54
			End If
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 17
			GoTo lbl_rcomm
			lbl_17:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 18
			GoTo lbl_rcomm
			lbl_18:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 19
			GoTo lbl_rcomm
			lbl_19:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 20
			GoTo lbl_rcomm
			lbl_20:
			state.fp2 = state.f
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			state.repnfev = state.repnfev + 4
			GoTo lbl_55
			lbl_54:
			state.xm1 = v - state.diffstep * state.s(i)
			state.xp1 = v + state.diffstep * state.s(i)
			If state.hasbndl(i) AndAlso CDbl(state.xm1) < CDbl(state.bndl(i)) Then
				state.xm1 = state.bndl(i)
			End If
			If state.hasbndu(i) AndAlso CDbl(state.xp1) > CDbl(state.bndu(i)) Then
				state.xp1 = state.bndu(i)
			End If
			state.x(i) = state.xm1
			state.rstate.stage = 21
			GoTo lbl_rcomm
			lbl_21:
			state.fm1 = state.f
			state.x(i) = state.xp1
			state.rstate.stage = 22
			GoTo lbl_rcomm
			lbl_22:
			state.fp1 = state.f
			If CDbl(state.xm1) <> CDbl(state.xp1) Then
				state.g(i) = (state.fp1 - state.fm1) / (state.xp1 - state.xm1)
			Else
				state.g(i) = 0
			End If
			state.repnfev = state.repnfev + 2
			lbl_55:
			state.x(i) = v
			i = i + 1
			GoTo lbl_51
			lbl_53:
			state.f = state.fbase
			state.needf = False
			lbl_50:

			'
			' Back to MCSRCH
			'
			' NOTE: penalty term from correction is added to FN in order
			'       to penalize increase in infeasibility.
			'
			state.fn = state.f + penaltyfactor * state.maxscaledgrad * penalty
			For i_ = 0 To n - 1
				state.cgn(i_) = state.g(i_)
			Next
			For i_ = 0 To n - 1
				state.ugn(i_) = state.g(i_)
			Next
			sactivesets.sasconstraineddirection(state.sas, state.cgn)
			optserv.trimfunction(state.fn, state.cgn, n, state.trimthreshold)
			linmin.mcsrch(n, state.xn, state.fn, state.cgn, state.d, state.stp, _
				state.curstpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			GoTo lbl_47
			lbl_48:
			For i_ = 0 To n - 1
				state.bufsk(state.bufsize, i_) = -state.sas.xc(i_)
			Next
			For i_ = 0 To n - 1
				state.bufyk(state.bufsize, i_) = -state.cgc(i_)
			Next
			For i_ = 0 To n - 1
				state.bufsk(state.bufsize, i_) = state.bufsk(state.bufsize, i_) + state.xn(i_)
			Next
			For i_ = 0 To n - 1
				state.bufyk(state.bufsize, i_) = state.bufyk(state.bufsize, i_) + state.cgn(i_)
			Next

			'
			' Check for presence of NAN/INF in function/gradient
			'
			v = state.fn
			For i = 0 To n - 1
				v = 0.1 * v + state.ugn(i)
			Next
			If Not Math.isfinite(v) Then

				'
				' Abnormal termination - infinities in function/gradient
				'
				state.repterminationtype = -8
				GoTo lbl_44
			End If

			'
			' Handle possible failure of the line search or request for termination
			'
			If mcinfo <> 1 AndAlso mcinfo <> 5 Then

				'
				' We can not find step which decreases function value. We have
				' two possibilities:
				' (a) numerical properties of the function do not allow us to
				'     find good step.
				' (b) we are close to activation of some constraint, and it is
				'     so close that step which activates it leads to change in
				'     target function which is smaller than numerical noise.
				'
				' Optimization algorithm must be able to handle case (b), because
				' inability to handle it will cause failure when algorithm
				' started very close to boundary of the feasible area.
				'
				' In order to correctly handle such cases we allow limited amount
				' of small steps which increase function value.
				'
				v = 0.0
				For i = 0 To n - 1
					v = v + Math.sqr(state.d(i) * state.curstpmax / state.s(i))
				Next
				v = System.Math.sqrt(v)
				If (state.cidx >= 0 AndAlso CDbl(v) <= CDbl(maxnonmonotoniclen)) AndAlso state.nonmonotoniccnt > 0 Then

					'
					' We enforce non-monotonic step:
					' * Stp    := CurStpMax
					' * MCINFO := 5
					' * XN     := XC+CurStpMax*D
					' * non-monotonic counter is decreased
					'
					' NOTE: UGN/CGN are not updated because step is so short that we assume that
					'       GN is approximately equal to GC.
					'
					state.stp = state.curstpmax
					mcinfo = 5
					v = state.curstpmax
					For i_ = 0 To n - 1
						state.xn(i_) = state.sas.xc(i_)
					Next
					For i_ = 0 To n - 1
						state.xn(i_) = state.xn(i_) + v * state.d(i_)
					Next
					state.nonmonotoniccnt = state.nonmonotoniccnt - 1
				Else

					'
					' Numerical properties of the function does not allow
					' us to solve problem. Here we have two possibilities:
					' * if it is "steepest descent" step, we can terminate
					'   algorithm because we are close to minimum
					' * if it is NOT "steepest descent" step, we should restart
					'   LBFGS iterations.
					'
					If state.steepestdescentstep Then

						'
						' Algorithm is terminated
						'
						state.repterminationtype = 7
						GoTo lbl_44
					Else

						'
						' Re-evaluate active set and restart LBFGS
						'
						GoTo lbl_44
					End If
				End If
			End If
			If state.userterminationneeded Then
				GoTo lbl_44
			End If

			'
			' Current point is updated:
			' * move XC/FC/GC to XP/FP/GP
			' * change current point remembered by SAS structure
			' * move XN/FN/GN to XC/FC/GC
			' * report current point and update iterations counter
			' * if MCINFO=1, push new pair SK/YK to LBFGS buffer
			'
			state.fp = state.fc
			For i_ = 0 To n - 1
				state.xp(i_) = state.sas.xc(i_)
			Next
			state.fc = state.fn
			For i_ = 0 To n - 1
				state.cgc(i_) = state.cgn(i_)
			Next
			For i_ = 0 To n - 1
				state.ugc(i_) = state.ugn(i_)
			Next
			actstatus = sactivesets.sasmoveto(state.sas, state.xn, state.cidx >= 0 AndAlso CDbl(state.stp) >= CDbl(state.activationstep), state.cidx, state.cval)
			If Not state.xrep Then
				GoTo lbl_56
			End If
			For i_ = 0 To n - 1
				state.x(i_) = state.sas.xc(i_)
			Next
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 23
			GoTo lbl_rcomm
			lbl_23:
			state.xupdated = False
			lbl_56:
			state.repinneriterationscount = state.repinneriterationscount + 1
			If mcinfo = 1 Then

				'
				' Accept new LBFGS update given by Sk,Yk
				'
				If state.bufsize = m Then

					'
					' Buffer is full, shift contents by one row
					'
					For i = 0 To state.bufsize - 1
						For i_ = 0 To n - 1
							state.bufsk(i, i_) = state.bufsk(i + 1, i_)
						Next
						For i_ = 0 To n - 1
							state.bufyk(i, i_) = state.bufyk(i + 1, i_)
						Next
					Next
					For i = 0 To state.bufsize - 2
						state.bufrho(i) = state.bufrho(i + 1)
						state.buftheta(i) = state.buftheta(i + 1)
					Next
				Else

					'
					' Buffer is not full, increase buffer size by 1
					'
					state.bufsize = state.bufsize + 1
				End If
				v = 0.0
				For i_ = 0 To n - 1
					v += state.bufyk(state.bufsize - 1, i_) * state.bufsk(state.bufsize - 1, i_)
				Next
				vv = 0.0
				For i_ = 0 To n - 1
					vv += state.bufyk(state.bufsize - 1, i_) * state.bufyk(state.bufsize - 1, i_)
				Next
				If CDbl(v) = CDbl(0) OrElse CDbl(vv) = CDbl(0) Then

					'
					' Strange internal error in LBFGS - either YK=0
					' (which should not have been) or (SK,YK)=0 (again,
					' unexpected). It should not take place because
					' MCINFO=1, which signals "good" step. But just
					' to be sure we have special branch of code which
					' restarts LBFGS
					'
					GoTo lbl_44
				End If
				state.bufrho(state.bufsize - 1) = 1 / v
				alglib.ap.assert(state.bufsize <= m, "MinBLEIC: internal error")

				'
				' Update length of the good step
				'
				v = 0
				vv = 0
				For i = 0 To n - 1
					v = v + Math.sqr((state.sas.xc(i) - state.xp(i)) / state.s(i))
					vv = vv + Math.sqr(state.sas.xc(i) - state.xp(i))
				Next
				state.lastgoodstep = System.Math.sqrt(vv)
				updateestimateofgoodstep(state.lastscaledgoodstep, System.Math.sqrt(v))
			End If

			'
			' Check stopping criteria
			'
			' Step size and function-based stopping criteria are tested only
			' for step which satisfies Wolfe conditions and is the first step of
			' LBFGS (subsequent steps may accumulate active constraints thus
			' they should NOT be used for stopping; step size or function change
			' may be small when constrained, but these constraints may be
			' deactivated by the subsequent steps).
			'
			' MaxIts-based stopping condition is checked for all kinds of steps.
			'
			If mcinfo = 1 AndAlso state.steepestdescentstep Then

				'
				' Step is small enough
				'
				v = 0
				For i = 0 To n - 1
					v = v + Math.sqr((state.sas.xc(i) - state.xp(i)) / state.s(i))
				Next
				v = System.Math.sqrt(v)
				If CDbl(v) <= CDbl(state.epsx) Then
					state.repterminationtype = 2
					GoTo lbl_44
				End If

				'
				' Function change is small enough
				'
				If CDbl(System.Math.Abs(state.fp - state.fc)) <= CDbl(state.epsf * System.Math.Max(System.Math.Abs(state.fc), System.Math.Max(System.Math.Abs(state.fp), 1.0))) Then
					state.repterminationtype = 1
					GoTo lbl_44
				End If
			End If
			If state.maxits > 0 AndAlso state.repinneriterationscount >= state.maxits Then
				state.repterminationtype = 5
				GoTo lbl_44
			End If

			'
			' Clear "steepest descent" flag.
			'
			state.steepestdescentstep = False

			'
			' Smooth reset (LBFGS memory model is refreshed) or hard restart:
			' * LBFGS model is refreshed, if line search was performed with activation of constraints
			' * algorithm is restarted if scaled gradient decreased below GDecay
			'
			If actstatus >= 0 Then
				state.bufsize = 0
				GoTo lbl_43
			End If
			v = 0.0
			For i = 0 To n - 1
				v = v + Math.sqr(state.cgc(i) * state.s(i))
			Next
			If CDbl(System.Math.sqrt(v)) < CDbl(gdecay * ginit) Then
				GoTo lbl_44
			End If
			lbl_43:
			itidx = itidx + 1
			GoTo lbl_42
			lbl_44:
			If state.userterminationneeded Then

				'
				' User requested termination
				'
				state.repterminationtype = 8
				GoTo lbl_34
			End If
			If state.repterminationtype <> 0 Then

				'
				' Algorithm terminated
				'
				GoTo lbl_34
			End If

			'
			' Decrease decay coefficient. Subsequent L-BFGS stages will
			' have more stringent stopping criteria.
			'
			gdecay = System.Math.Max(gdecay * decaycorrection, mindecay)
			GoTo lbl_33
			lbl_34:
			sactivesets.sasstopoptimization(state.sas)
			state.repouteriterationscount = 1
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstate.ia(0) = n
			state.rstate.ia(1) = m
			state.rstate.ia(2) = i
			state.rstate.ia(3) = j
			state.rstate.ia(4) = mcinfo
			state.rstate.ia(5) = actstatus
			state.rstate.ia(6) = itidx
			state.rstate.ba(0) = b
			state.rstate.ra(0) = v
			state.rstate.ra(1) = vv
			state.rstate.ra(2) = v0
			state.rstate.ra(3) = penalty
			state.rstate.ra(4) = ginit
			state.rstate.ra(5) = gdecay
			Return result
		End Function


		'************************************************************************
'        BLEIC results
'
'        INPUT PARAMETERS:
'            State   -   algorithm state
'
'        OUTPUT PARAMETERS:
'            X       -   array[0..N-1], solution
'            Rep     -   optimization report. You should check Rep.TerminationType
'                        in  order  to  distinguish  successful  termination  from
'                        unsuccessful one:
'                        * -8    internal integrity control  detected  infinite or
'                                NAN   values   in   function/gradient.   Abnormal
'                                termination signalled.
'                        * -7   gradient verification failed.
'                               See MinBLEICSetGradientCheck() for more information.
'                        * -3   inconsistent constraints. Feasible point is
'                               either nonexistent or too hard to find. Try to
'                               restart optimizer with better initial approximation
'                        *  1   relative function improvement is no more than EpsF.
'                        *  2   scaled step is no more than EpsX.
'                        *  4   scaled gradient norm is no more than EpsG.
'                        *  5   MaxIts steps was taken
'                        *  8   terminated by user who called minbleicrequesttermination().
'                               X contains point which was "current accepted"  when
'                               termination request was submitted.
'                        More information about fields of this  structure  can  be
'                        found in the comments on MinBLEICReport datatype.
'           
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicresults(state As minbleicstate, ByRef x As Double(), rep As minbleicreport)
			x = New Double(-1) {}

			minbleicresultsbuf(state, x, rep)
		End Sub


		'************************************************************************
'        BLEIC results
'
'        Buffered implementation of MinBLEICResults() which uses pre-allocated buffer
'        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
'        intended to be used in the inner cycles of performance critical algorithms
'        where array reallocation penalty is too large to be ignored.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicresultsbuf(state As minbleicstate, ByRef x As Double(), rep As minbleicreport)
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			If alglib.ap.len(x) < state.nmain Then
				x = New Double(state.nmain - 1) {}
			End If
			rep.iterationscount = state.repinneriterationscount
			rep.inneriterationscount = state.repinneriterationscount
			rep.outeriterationscount = state.repouteriterationscount
			rep.nfev = state.repnfev
			rep.varidx = state.repvaridx
			rep.terminationtype = state.repterminationtype
			If state.repterminationtype > 0 Then
				For i_ = 0 To state.nmain - 1
					x(i_) = state.sas.xc(i_)
				Next
			Else
				For i = 0 To state.nmain - 1
					x(i) = [Double].NaN
				Next
			End If
			rep.debugeqerr = state.repdebugeqerr
			rep.debugfs = state.repdebugfs
			rep.debugff = state.repdebugff
			rep.debugdx = state.repdebugdx
			rep.debugfeasqpits = state.repdebugfeasqpits
			rep.debugfeasgpaits = state.repdebugfeasgpaits
		End Sub


		'************************************************************************
'        This subroutine restarts algorithm from new point.
'        All optimization parameters (including constraints) are left unchanged.
'
'        This  function  allows  to  solve multiple  optimization  problems  (which
'        must have  same number of dimensions) without object reallocation penalty.
'
'        INPUT PARAMETERS:
'            State   -   structure previously allocated with MinBLEICCreate call.
'            X       -   new starting point.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicrestartfrom(state As minbleicstate, x As Double())
			Dim n As Integer = 0
			Dim i_ As Integer = 0

			n = state.nmain

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinBLEICRestartFrom: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinBLEICRestartFrom: X contains infinite or NaN values!")

			'
			' Set XC
			'
			For i_ = 0 To n - 1
				state.xstart(i_) = x(i_)
			Next

			'
			' prepare RComm facilities
			'
			state.rstate.ia = New Integer(6) {}
			state.rstate.ba = New Boolean(0) {}
			state.rstate.ra = New Double(5) {}
			state.rstate.stage = -1
			clearrequestfields(state)
			sactivesets.sasstopoptimization(state.sas)
		End Sub


		'************************************************************************
'        This subroutine submits request for termination of running  optimizer.  It
'        should be called from user-supplied callback when user decides that it  is
'        time to "smoothly" terminate optimization process.  As  result,  optimizer
'        stops at point which was "current accepted" when termination  request  was
'        submitted and returns error code 8 (successful termination).
'
'        INPUT PARAMETERS:
'            State   -   optimizer structure
'
'        NOTE: after  request  for  termination  optimizer  may   perform   several
'              additional calls to user-supplied callbacks. It does  NOT  guarantee
'              to stop immediately - it just guarantees that these additional calls
'              will be discarded later.
'
'        NOTE: calling this function on optimizer which is NOT running will have no
'              effect.
'              
'        NOTE: multiple calls to this function are possible. First call is counted,
'              subsequent calls are silently ignored.
'
'          -- ALGLIB --
'             Copyright 08.10.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicrequesttermination(state As minbleicstate)
			state.userterminationneeded = True
		End Sub


		'************************************************************************
'        This subroutine finalizes internal structures after emergency  termination
'        from State.LSStart report (see comments on MinBLEICState for more information).
'
'        INPUT PARAMETERS:
'            State   -   structure after exit from LSStart report
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicemergencytermination(state As minbleicstate)
			sactivesets.sasstopoptimization(state.sas)
		End Sub


		'************************************************************************
'        This  subroutine  turns  on  verification  of  the  user-supplied analytic
'        gradient:
'        * user calls this subroutine before optimization begins
'        * MinBLEICOptimize() is called
'        * prior to  actual  optimization, for each component  of  parameters being
'          optimized X[i] algorithm performs following steps:
'          * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
'            where X[i] is i-th component of the initial point and S[i] is a  scale
'            of i-th parameter
'          * if needed, steps are bounded with respect to constraints on X[]
'          * F(X) is evaluated at these trial points
'          * we perform one more evaluation in the middle point of the interval
'          * we  build  cubic  model using function values and derivatives at trial
'            points and we compare its prediction with actual value in  the  middle
'            point
'          * in case difference between prediction and actual value is higher  than
'            some predetermined threshold, algorithm stops with completion code -7;
'            Rep.VarIdx is set to index of the parameter with incorrect derivative.
'        * after verification is over, algorithm proceeds to the actual optimization.
'
'        NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
'                is very costly and you should use  it  only  for  low  dimensional
'                problems,  when  you  want  to  be  sure  that  you've   correctly
'                calculated  analytic  derivatives.  You  should  not use it in the
'                production code (unless you want to check derivatives provided  by
'                some third party).
'
'        NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
'                (so large that function behaviour is significantly non-cubic) will
'                lead to false alarms. You may use  different  step  for  different
'                parameters by means of setting scale with MinBLEICSetScale().
'
'        NOTE 3: this function may lead to false positives. In case it reports that
'                I-th  derivative was calculated incorrectly, you may decrease test
'                step  and  try  one  more  time  - maybe your function changes too
'                sharply  and  your  step  is  too  large for such rapidly chanding
'                function.
'
'        INPUT PARAMETERS:
'            State       -   structure used to store algorithm state
'            TestStep    -   verification step:
'                            * TestStep=0 turns verification off
'                            * TestStep>0 activates verification
'
'          -- ALGLIB --
'             Copyright 15.06.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetgradientcheck(state As minbleicstate, teststep As Double)
			alglib.ap.assert(Math.isfinite(teststep), "MinBLEICSetGradientCheck: TestStep contains NaN or Infinite")
			alglib.ap.assert(CDbl(teststep) >= CDbl(0), "MinBLEICSetGradientCheck: invalid argument TestStep(TestStep<0)")
			state.teststep = teststep
		End Sub


		'************************************************************************
'        Clears request fileds (to be sure that we don't forget to clear something)
'        ************************************************************************

		Private Shared Sub clearrequestfields(state As minbleicstate)
			state.needf = False
			state.needfg = False
			state.xupdated = False
			state.lsstart = False
		End Sub


		'************************************************************************
'        Internal initialization subroutine
'        ************************************************************************

		Private Shared Sub minbleicinitinternal(n As Integer, x As Double(), diffstep As Double, state As minbleicstate)
			Dim i As Integer = 0
			Dim c As Double(,) = New Double(-1, -1) {}
			Dim ct As Integer() = New Integer(-1) {}


			'
			' Initialize
			'
			state.teststep = 0
			state.nmain = n
			state.diffstep = diffstep
			sactivesets.sasinit(n, state.sas)
			state.bndl = New Double(n - 1) {}
			state.hasbndl = New Boolean(n - 1) {}
			state.bndu = New Double(n - 1) {}
			state.hasbndu = New Boolean(n - 1) {}
			state.xstart = New Double(n - 1) {}
			state.cgc = New Double(n - 1) {}
			state.ugc = New Double(n - 1) {}
			state.xn = New Double(n - 1) {}
			state.cgn = New Double(n - 1) {}
			state.ugn = New Double(n - 1) {}
			state.xp = New Double(n - 1) {}
			state.d = New Double(n - 1) {}
			state.s = New Double(n - 1) {}
			state.x = New Double(n - 1) {}
			state.g = New Double(n - 1) {}
			state.work = New Double(n - 1) {}
			For i = 0 To n - 1
				state.bndl(i) = [Double].NegativeInfinity
				state.hasbndl(i) = False
				state.bndu(i) = [Double].PositiveInfinity
				state.hasbndu(i) = False
				state.s(i) = 1.0
			Next
			minbleicsetlc(state, c, ct, 0)
			minbleicsetcond(state, 0.0, 0.0, 0.0, 0)
			minbleicsetxrep(state, False)
			minbleicsetdrep(state, False)
			minbleicsetstpmax(state, 0.0)
			minbleicsetprecdefault(state)
			minbleicrestartfrom(state, x)
		End Sub


		'************************************************************************
'        This subroutine updates estimate of the good step length given:
'        1) previous estimate
'        2) new length of the good step
'
'        It makes sure that estimate does not change too rapidly - ratio of new and
'        old estimates will be at least 0.01, at most 100.0
'
'        In case previous estimate of good step is zero (no estimate), new estimate
'        is used unconditionally.
'
'          -- ALGLIB --
'             Copyright 16.01.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub updateestimateofgoodstep(ByRef estimate As Double, newstep As Double)
			If CDbl(estimate) = CDbl(0) Then
				estimate = newstep
				Return
			End If
			If CDbl(newstep) < CDbl(estimate * 0.01) Then
				estimate = estimate * 0.01
				Return
			End If
			If CDbl(newstep) > CDbl(estimate * 100) Then
				estimate = estimate * 100
				Return
			End If
			estimate = newstep
		End Sub


	End Class
	Public Class minlbfgs
		Public Class minlbfgsstate
			Inherits apobject
			Public n As Integer
			Public m As Integer
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public xrep As Boolean
			Public stpmax As Double
			Public s As Double()
			Public diffstep As Double
			Public nfev As Integer
			Public mcstage As Integer
			Public k As Integer
			Public q As Integer
			Public p As Integer
			Public rho As Double()
			Public yk As Double(,)
			Public sk As Double(,)
			Public xp As Double()
			Public theta As Double()
			Public d As Double()
			Public stp As Double
			Public work As Double()
			Public fold As Double
			Public trimthreshold As Double
			Public prectype As Integer
			Public gammak As Double
			Public denseh As Double(,)
			Public diagh As Double()
			Public precc As Double()
			Public precd As Double()
			Public precw As Double(,)
			Public preck As Integer
			Public precbuf As optserv.precbuflbfgs
			Public lowrankbuf As optserv.precbuflowrank
			Public fbase As Double
			Public fm2 As Double
			Public fm1 As Double
			Public fp1 As Double
			Public fp2 As Double
			Public autobuf As Double()
			Public x As Double()
			Public f As Double
			Public g As Double()
			Public needf As Boolean
			Public needfg As Boolean
			Public xupdated As Boolean
			Public userterminationneeded As Boolean
			Public teststep As Double
			Public rstate As rcommstate
			Public repiterationscount As Integer
			Public repnfev As Integer
			Public repvaridx As Integer
			Public repterminationtype As Integer
			Public lstate As linmin.linminstate
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				s = New Double(-1) {}
				rho = New Double(-1) {}
				yk = New Double(-1, -1) {}
				sk = New Double(-1, -1) {}
				xp = New Double(-1) {}
				theta = New Double(-1) {}
				d = New Double(-1) {}
				work = New Double(-1) {}
				denseh = New Double(-1, -1) {}
				diagh = New Double(-1) {}
				precc = New Double(-1) {}
				precd = New Double(-1) {}
				precw = New Double(-1, -1) {}
				precbuf = New optserv.precbuflbfgs()
				lowrankbuf = New optserv.precbuflowrank()
				autobuf = New Double(-1) {}
				x = New Double(-1) {}
				g = New Double(-1) {}
				rstate = New rcommstate()
				lstate = New linmin.linminstate()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minlbfgsstate()
				_result.n = n
				_result.m = m
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				_result.xrep = xrep
				_result.stpmax = stpmax
				_result.s = DirectCast(s.Clone(), Double())
				_result.diffstep = diffstep
				_result.nfev = nfev
				_result.mcstage = mcstage
				_result.k = k
				_result.q = q
				_result.p = p
				_result.rho = DirectCast(rho.Clone(), Double())
				_result.yk = DirectCast(yk.Clone(), Double(,))
				_result.sk = DirectCast(sk.Clone(), Double(,))
				_result.xp = DirectCast(xp.Clone(), Double())
				_result.theta = DirectCast(theta.Clone(), Double())
				_result.d = DirectCast(d.Clone(), Double())
				_result.stp = stp
				_result.work = DirectCast(work.Clone(), Double())
				_result.fold = fold
				_result.trimthreshold = trimthreshold
				_result.prectype = prectype
				_result.gammak = gammak
				_result.denseh = DirectCast(denseh.Clone(), Double(,))
				_result.diagh = DirectCast(diagh.Clone(), Double())
				_result.precc = DirectCast(precc.Clone(), Double())
				_result.precd = DirectCast(precd.Clone(), Double())
				_result.precw = DirectCast(precw.Clone(), Double(,))
				_result.preck = preck
				_result.precbuf = DirectCast(precbuf.make_copy(), optserv.precbuflbfgs)
				_result.lowrankbuf = DirectCast(lowrankbuf.make_copy(), optserv.precbuflowrank)
				_result.fbase = fbase
				_result.fm2 = fm2
				_result.fm1 = fm1
				_result.fp1 = fp1
				_result.fp2 = fp2
				_result.autobuf = DirectCast(autobuf.Clone(), Double())
				_result.x = DirectCast(x.Clone(), Double())
				_result.f = f
				_result.g = DirectCast(g.Clone(), Double())
				_result.needf = needf
				_result.needfg = needfg
				_result.xupdated = xupdated
				_result.userterminationneeded = userterminationneeded
				_result.teststep = teststep
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.repiterationscount = repiterationscount
				_result.repnfev = repnfev
				_result.repvaridx = repvaridx
				_result.repterminationtype = repterminationtype
				_result.lstate = DirectCast(lstate.make_copy(), linmin.linminstate)
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure stores optimization report:
'        * IterationsCount           total number of inner iterations
'        * NFEV                      number of gradient evaluations
'        * TerminationType           termination type (see below)
'
'        TERMINATION CODES
'
'        TerminationType field contains completion code, which can be:
'          -8    internal integrity control detected  infinite  or  NAN  values  in
'                function/gradient. Abnormal termination signalled.
'          -7    gradient verification failed.
'                See MinLBFGSSetGradientCheck() for more information.
'           1    relative function improvement is no more than EpsF.
'           2    relative step is no more than EpsX.
'           4    gradient norm is no more than EpsG
'           5    MaxIts steps was taken
'           7    stopping conditions are too stringent,
'                further improvement is impossible,
'                X contains best point found so far.
'           8    terminated    by  user  who  called  minlbfgsrequesttermination().
'                X contains point which was   "current accepted"  when  termination
'                request was submitted.
'                
'        Other fields of this structure are not documented and should not be used!
'        ************************************************************************

		Public Class minlbfgsreport
			Inherits apobject
			Public iterationscount As Integer
			Public nfev As Integer
			Public varidx As Integer
			Public terminationtype As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minlbfgsreport()
				_result.iterationscount = iterationscount
				_result.nfev = nfev
				_result.varidx = varidx
				_result.terminationtype = terminationtype
				Return _result
			End Function
		End Class




		Public Const gtol As Double = 0.4


		'************************************************************************
'                LIMITED MEMORY BFGS METHOD FOR LARGE SCALE OPTIMIZATION
'
'        DESCRIPTION:
'        The subroutine minimizes function F(x) of N arguments by  using  a  quasi-
'        Newton method (LBFGS scheme) which is optimized to use  a  minimum  amount
'        of memory.
'        The subroutine generates the approximation of an inverse Hessian matrix by
'        using information about the last M steps of the algorithm  (instead of N).
'        It lessens a required amount of memory from a value  of  order  N^2  to  a
'        value of order 2*N*M.
'
'
'        REQUIREMENTS:
'        Algorithm will request following information during its operation:
'        * function value F and its gradient G (simultaneously) at given point X
'
'
'        USAGE:
'        1. User initializes algorithm state with MinLBFGSCreate() call
'        2. User tunes solver parameters with MinLBFGSSetCond() MinLBFGSSetStpMax()
'           and other functions
'        3. User calls MinLBFGSOptimize() function which takes algorithm  state and
'           pointer (delegate, etc.) to callback function which calculates F/G.
'        4. User calls MinLBFGSResults() to get solution
'        5. Optionally user may call MinLBFGSRestartFrom() to solve another problem
'           with same N/M but another starting point and/or another function.
'           MinLBFGSRestartFrom() allows to reuse already initialized structure.
'
'
'        INPUT PARAMETERS:
'            N       -   problem dimension. N>0
'            M       -   number of corrections in the BFGS scheme of Hessian
'                        approximation update. Recommended value:  3<=M<=7. The smaller
'                        value causes worse convergence, the bigger will  not  cause  a
'                        considerably better convergence, but will cause a fall in  the
'                        performance. M<=N.
'            X       -   initial solution approximation, array[0..N-1].
'
'
'        OUTPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            
'
'        NOTES:
'        1. you may tune stopping conditions with MinLBFGSSetCond() function
'        2. if target function contains exp() or other fast growing functions,  and
'           optimization algorithm makes too large steps which leads  to  overflow,
'           use MinLBFGSSetStpMax() function to bound algorithm's  steps.  However,
'           L-BFGS rarely needs such a tuning.
'
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgscreate(n As Integer, m As Integer, x As Double(), state As minlbfgsstate)
			alglib.ap.assert(n >= 1, "MinLBFGSCreate: N<1!")
			alglib.ap.assert(m >= 1, "MinLBFGSCreate: M<1")
			alglib.ap.assert(m <= n, "MinLBFGSCreate: M>N")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinLBFGSCreate: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinLBFGSCreate: X contains infinite or NaN values!")
			minlbfgscreatex(n, m, x, 0, 0.0, state)
		End Sub


		'************************************************************************
'        The subroutine is finite difference variant of MinLBFGSCreate().  It  uses
'        finite differences in order to differentiate target function.
'
'        Description below contains information which is specific to  this function
'        only. We recommend to read comments on MinLBFGSCreate() in  order  to  get
'        more information about creation of LBFGS optimizer.
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size of X
'            M       -   number of corrections in the BFGS scheme of Hessian
'                        approximation update. Recommended value:  3<=M<=7. The smaller
'                        value causes worse convergence, the bigger will  not  cause  a
'                        considerably better convergence, but will cause a fall in  the
'                        performance. M<=N.
'            X       -   starting point, array[0..N-1].
'            DiffStep-   differentiation step, >0
'
'        OUTPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTES:
'        1. algorithm uses 4-point central formula for differentiation.
'        2. differentiation step along I-th axis is equal to DiffStep*S[I] where
'           S[] is scaling vector which can be set by MinLBFGSSetScale() call.
'        3. we recommend you to use moderate values of  differentiation  step.  Too
'           large step will result in too large truncation  errors, while too small
'           step will result in too large numerical  errors.  1.0E-6  can  be  good
'           value to start with.
'        4. Numerical  differentiation  is   very   inefficient  -   one   gradient
'           calculation needs 4*N function evaluations. This function will work for
'           any N - either small (1...10), moderate (10...100) or  large  (100...).
'           However, performance penalty will be too severe for any N's except  for
'           small ones.
'           We should also say that code which relies on numerical  differentiation
'           is   less  robust  and  precise.  LBFGS  needs  exact  gradient values.
'           Imprecise gradient may slow  down  convergence,  especially  on  highly
'           nonlinear problems.
'           Thus  we  recommend to use this function for fast prototyping on small-
'           dimensional problems only, and to implement analytical gradient as soon
'           as possible.
'
'          -- ALGLIB --
'             Copyright 16.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgscreatef(n As Integer, m As Integer, x As Double(), diffstep As Double, state As minlbfgsstate)
			alglib.ap.assert(n >= 1, "MinLBFGSCreateF: N too small!")
			alglib.ap.assert(m >= 1, "MinLBFGSCreateF: M<1")
			alglib.ap.assert(m <= n, "MinLBFGSCreateF: M>N")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinLBFGSCreateF: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinLBFGSCreateF: X contains infinite or NaN values!")
			alglib.ap.assert(Math.isfinite(diffstep), "MinLBFGSCreateF: DiffStep is infinite or NaN!")
			alglib.ap.assert(CDbl(diffstep) > CDbl(0), "MinLBFGSCreateF: DiffStep is non-positive!")
			minlbfgscreatex(n, m, x, 0, diffstep, state)
		End Sub


		'************************************************************************
'        This function sets stopping conditions for L-BFGS optimization algorithm.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            EpsG    -   >=0
'                        The  subroutine  finishes  its  work   if   the  condition
'                        |v|<EpsG is satisfied, where:
'                        * |.| means Euclidian norm
'                        * v - scaled gradient vector, v[i]=g[i]*s[i]
'                        * g - gradient
'                        * s - scaling coefficients set by MinLBFGSSetScale()
'            EpsF    -   >=0
'                        The  subroutine  finishes  its work if on k+1-th iteration
'                        the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
'                        is satisfied.
'            EpsX    -   >=0
'                        The subroutine finishes its work if  on  k+1-th  iteration
'                        the condition |v|<=EpsX is fulfilled, where:
'                        * |.| means Euclidian norm
'                        * v - scaled step vector, v[i]=dx[i]/s[i]
'                        * dx - ste pvector, dx=X(k+1)-X(k)
'                        * s - scaling coefficients set by MinLBFGSSetScale()
'            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
'                        iterations is unlimited.
'
'        Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
'        automatic stopping criterion selection (small EpsX).
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetcond(state As minlbfgsstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(epsg), "MinLBFGSSetCond: EpsG is not finite number!")
			alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinLBFGSSetCond: negative EpsG!")
			alglib.ap.assert(Math.isfinite(epsf), "MinLBFGSSetCond: EpsF is not finite number!")
			alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinLBFGSSetCond: negative EpsF!")
			alglib.ap.assert(Math.isfinite(epsx), "MinLBFGSSetCond: EpsX is not finite number!")
			alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinLBFGSSetCond: negative EpsX!")
			alglib.ap.assert(maxits >= 0, "MinLBFGSSetCond: negative MaxIts!")
			If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
				epsx = 1E-06
			End If
			state.epsg = epsg
			state.epsf = epsf
			state.epsx = epsx
			state.maxits = maxits
		End Sub


		'************************************************************************
'        This function turns on/off reporting.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedXRep-   whether iteration reports are needed or not
'
'        If NeedXRep is True, algorithm will call rep() callback function if  it is
'        provided to MinLBFGSOptimize().
'
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetxrep(state As minlbfgsstate, needxrep As Boolean)
			state.xrep = needxrep
		End Sub


		'************************************************************************
'        This function sets maximum step length
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            StpMax  -   maximum step length, >=0. Set StpMax to 0.0 (default),  if
'                        you don't want to limit step length.
'
'        Use this subroutine when you optimize target function which contains exp()
'        or  other  fast  growing  functions,  and optimization algorithm makes too
'        large  steps  which  leads  to overflow. This function allows us to reject
'        steps  that  are  too  large  (and  therefore  expose  us  to the possible
'        overflow) without actually calculating function value at the x+stp*d.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetstpmax(state As minlbfgsstate, stpmax As Double)
			alglib.ap.assert(Math.isfinite(stpmax), "MinLBFGSSetStpMax: StpMax is not finite!")
			alglib.ap.assert(CDbl(stpmax) >= CDbl(0), "MinLBFGSSetStpMax: StpMax<0!")
			state.stpmax = stpmax
		End Sub


		'************************************************************************
'        This function sets scaling coefficients for LBFGS optimizer.
'
'        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'        size and gradient are scaled before comparison with tolerances).  Scale of
'        the I-th variable is a translation invariant measure of:
'        a) "how large" the variable is
'        b) how large the step should be to make significant changes in the function
'
'        Scaling is also used by finite difference variant of the optimizer  - step
'        along I-th axis is equal to DiffStep*S[I].
'
'        In  most  optimizers  (and  in  the  LBFGS  too)  scaling is NOT a form of
'        preconditioning. It just  affects  stopping  conditions.  You  should  set
'        preconditioner  by  separate  call  to  one  of  the  MinLBFGSSetPrec...()
'        functions.
'
'        There  is  special  preconditioning  mode, however,  which  uses   scaling
'        coefficients to form diagonal preconditioning matrix. You  can  turn  this
'        mode on, if you want.   But  you should understand that scaling is not the
'        same thing as preconditioning - these are two different, although  related
'        forms of tuning solver.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            S       -   array[N], non-zero scaling coefficients
'                        S[i] may be negative, sign doesn't matter.
'
'          -- ALGLIB --
'             Copyright 14.01.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetscale(state As minlbfgsstate, s As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(s) >= state.n, "MinLBFGSSetScale: Length(S)<N")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(s(i)), "MinLBFGSSetScale: S contains infinite or NAN elements")
				alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinLBFGSSetScale: S contains zero elements")
				state.s(i) = System.Math.Abs(s(i))
			Next
		End Sub


		'************************************************************************
'        Extended subroutine for internal use only.
'
'        Accepts additional parameters:
'
'            Flags - additional settings:
'                    * Flags = 0     means no additional settings
'                    * Flags = 1     "do not allocate memory". used when solving
'                                    a many subsequent tasks with  same N/M  values.
'                                    First  call MUST  be without this flag bit set,
'                                    subsequent  calls   of   MinLBFGS   with   same
'                                    MinLBFGSState structure can set Flags to 1.
'            DiffStep - numerical differentiation step
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgscreatex(n As Integer, m As Integer, x As Double(), flags As Integer, diffstep As Double, state As minlbfgsstate)
			Dim allocatemem As New Boolean()
			Dim i As Integer = 0

			alglib.ap.assert(n >= 1, "MinLBFGS: N too small!")
			alglib.ap.assert(m >= 1, "MinLBFGS: M too small!")
			alglib.ap.assert(m <= n, "MinLBFGS: M too large!")

			'
			' Initialize
			'
			state.teststep = 0
			state.diffstep = diffstep
			state.n = n
			state.m = m
			allocatemem = flags Mod 2 = 0
			flags = flags \ 2
			If allocatemem Then
				state.rho = New Double(m - 1) {}
				state.theta = New Double(m - 1) {}
				state.yk = New Double(m - 1, n - 1) {}
				state.sk = New Double(m - 1, n - 1) {}
				state.d = New Double(n - 1) {}
				state.xp = New Double(n - 1) {}
				state.x = New Double(n - 1) {}
				state.s = New Double(n - 1) {}
				state.g = New Double(n - 1) {}
				state.work = New Double(n - 1) {}
			End If
			minlbfgssetcond(state, 0, 0, 0, 0)
			minlbfgssetxrep(state, False)
			minlbfgssetstpmax(state, 0)
			minlbfgsrestartfrom(state, x)
			For i = 0 To n - 1
				state.s(i) = 1.0
			Next
			state.prectype = 0
		End Sub


		'************************************************************************
'        Modification  of  the  preconditioner:  default  preconditioner    (simple
'        scaling, same for all elements of X) is used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetprecdefault(state As minlbfgsstate)
			state.prectype = 0
		End Sub


		'************************************************************************
'        Modification of the preconditioner: Cholesky factorization of  approximate
'        Hessian is used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            P       -   triangular preconditioner, Cholesky factorization of
'                        the approximate Hessian. array[0..N-1,0..N-1],
'                        (if larger, only leading N elements are used).
'            IsUpper -   whether upper or lower triangle of P is given
'                        (other triangle is not referenced)
'
'        After call to this function preconditioner is changed to P  (P  is  copied
'        into the internal buffer).
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'        NOTE 2:  P  should  be nonsingular. Exception will be thrown otherwise.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetpreccholesky(state As minlbfgsstate, p As Double(,), isupper As Boolean)
			Dim i As Integer = 0
			Dim mx As Double = 0

			alglib.ap.assert(apserv.isfinitertrmatrix(p, state.n, isupper), "MinLBFGSSetPrecCholesky: P contains infinite or NAN values!")
			mx = 0
			For i = 0 To state.n - 1
				mx = System.Math.Max(mx, System.Math.Abs(p(i, i)))
			Next
			alglib.ap.assert(CDbl(mx) > CDbl(0), "MinLBFGSSetPrecCholesky: P is strictly singular!")
			If alglib.ap.rows(state.denseh) < state.n OrElse alglib.ap.cols(state.denseh) < state.n Then
				state.denseh = New Double(state.n - 1, state.n - 1) {}
			End If
			state.prectype = 1
			If isupper Then
				ablas.rmatrixcopy(state.n, state.n, p, 0, 0, state.denseh, _
					0, 0)
			Else
				ablas.rmatrixtranspose(state.n, state.n, p, 0, 0, state.denseh, _
					0, 0)
			End If
		End Sub


		'************************************************************************
'        Modification  of  the  preconditioner:  diagonal of approximate Hessian is
'        used.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            D       -   diagonal of the approximate Hessian, array[0..N-1],
'                        (if larger, only leading N elements are used).
'
'        NOTE:  you  can  change  preconditioner  "on  the  fly",  during algorithm
'        iterations.
'
'        NOTE 2: D[i] should be positive. Exception will be thrown otherwise.
'
'        NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetprecdiag(state As minlbfgsstate, d As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(d) >= state.n, "MinLBFGSSetPrecDiag: D is too short")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(d(i)), "MinLBFGSSetPrecDiag: D contains infinite or NAN elements")
				alglib.ap.assert(CDbl(d(i)) > CDbl(0), "MinLBFGSSetPrecDiag: D contains non-positive elements")
			Next
			apserv.rvectorsetlengthatleast(state.diagh, state.n)
			state.prectype = 2
			For i = 0 To state.n - 1
				state.diagh(i) = d(i)
			Next
		End Sub


		'************************************************************************
'        Modification of the preconditioner: scale-based diagonal preconditioning.
'
'        This preconditioning mode can be useful when you  don't  have  approximate
'        diagonal of Hessian, but you know that your  variables  are  badly  scaled
'        (for  example,  one  variable is in [1,10], and another in [1000,100000]),
'        and most part of the ill-conditioning comes from different scales of vars.
'
'        In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
'        can greatly improve convergence.
'
'        IMPRTANT: you should set scale of your variables  with  MinLBFGSSetScale()
'        call  (before  or after MinLBFGSSetPrecScale() call). Without knowledge of
'        the scale of your variables scale-based preconditioner will be  just  unit
'        matrix.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetprecscale(state As minlbfgsstate)
			state.prectype = 3
		End Sub


		'************************************************************************
'        This function sets low-rank preconditioner for Hessian matrix  H=D+W'*C*W,
'        where:
'        * H is a Hessian matrix, which is approximated by D/W/C
'        * D is a NxN diagonal positive definite matrix
'        * W is a KxN low-rank correction
'        * C is a KxK positive definite diagonal factor of low-rank correction
'
'        This preconditioner is inexact but fast - it requires O(N*K)  time  to  be
'        applied. Preconditioner P is calculated by artificially constructing a set
'        of BFGS updates which tries to reproduce behavior of H:
'        * Sk = Wk (k-th row of W)
'        * Yk = (D+Wk'*Ck*Wk)*Sk
'        * Yk/Sk are reordered by ascending of C[k]*norm(Wk)^2
'
'        Here we assume that rows of Wk are orthogonal or nearly orthogonal,  which
'        allows us to have O(N*K+K^2) update instead of O(N*K^2) one. Reordering of
'        updates is essential for having good performance on non-orthogonal problems
'        (updates which do not add much of curvature are added first,  and  updates
'        which add very large eigenvalues are added last and override effect of the
'        first updates).
'
'        In practice, this preconditioner is perfect when ortogonal  correction  is
'        applied; on non-orthogonal problems sometimes  it  allows  to  achieve  5x
'        speedup (when compared to non-preconditioned solver).
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetprecrankklbfgsfast(state As minlbfgsstate, d As Double(), c As Double(), w As Double(,), cnt As Integer)
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim n As Integer = 0

			n = state.n
			state.prectype = 4
			state.preck = cnt
			apserv.rvectorsetlengthatleast(state.precc, cnt)
			apserv.rvectorsetlengthatleast(state.precd, n)
			apserv.rmatrixsetlengthatleast(state.precw, cnt, n)
			For i = 0 To n - 1
				state.precd(i) = d(i)
			Next
			For i = 0 To cnt - 1
				state.precc(i) = c(i)
				For j = 0 To n - 1
					state.precw(i, j) = w(i, j)
				Next
			Next
		End Sub


		'************************************************************************
'        This function  sets  exact  low-rank  preconditioner  for  Hessian  matrix
'        H=D+W'*C*W, where:
'        * H is a Hessian matrix, which is approximated by D/W/C
'        * D is a NxN diagonal positive definite matrix
'        * W is a KxN low-rank correction
'        * C is a KxK semidefinite diagonal factor of low-rank correction
'
'        This preconditioner is exact but slow - it requires O(N*K^2)  time  to  be
'        built and O(N*K) time to be applied. Woodbury matrix identity is  used  to
'        build inverse matrix.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetpreclowrankexact(state As minlbfgsstate, d As Double(), c As Double(), w As Double(,), cnt As Integer)
			state.prectype = 5
			optserv.preparelowrankpreconditioner(d, c, w, state.n, cnt, state.lowrankbuf)
		End Sub


		'************************************************************************
'        NOTES:
'
'        1. This function has two different implementations: one which  uses  exact
'           (analytical) user-supplied gradient,  and one which uses function value
'           only  and  numerically  differentiates  function  in  order  to  obtain
'           gradient.
'
'           Depending  on  the  specific  function  used to create optimizer object
'           (either MinLBFGSCreate() for analytical gradient  or  MinLBFGSCreateF()
'           for numerical differentiation) you should choose appropriate variant of
'           MinLBFGSOptimize() - one  which  accepts  function  AND gradient or one
'           which accepts function ONLY.
'
'           Be careful to choose variant of MinLBFGSOptimize() which corresponds to
'           your optimization scheme! Table below lists different  combinations  of
'           callback (function/gradient) passed to MinLBFGSOptimize()  and specific
'           function used to create optimizer.
'
'
'                             |         USER PASSED TO MinLBFGSOptimize()
'           CREATED WITH      |  function only   |  function and gradient
'           ------------------------------------------------------------
'           MinLBFGSCreateF() |     work                FAIL
'           MinLBFGSCreate()  |     FAIL                work
'
'           Here "FAIL" denotes inappropriate combinations  of  optimizer  creation
'           function  and  MinLBFGSOptimize()  version.   Attemps   to   use   such
'           combination (for example, to create optimizer with MinLBFGSCreateF() and
'           to pass gradient information to MinCGOptimize()) will lead to exception
'           being thrown. Either  you  did  not pass gradient when it WAS needed or
'           you passed gradient when it was NOT needed.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function minlbfgsiteration(state As minlbfgsstate) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim m As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim ic As Integer = 0
			Dim mcinfo As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstate.stage >= 0 Then
				n = state.rstate.ia(0)
				m = state.rstate.ia(1)
				i = state.rstate.ia(2)
				j = state.rstate.ia(3)
				ic = state.rstate.ia(4)
				mcinfo = state.rstate.ia(5)
				v = state.rstate.ra(0)
				vv = state.rstate.ra(1)
			Else
				n = -983
				m = -989
				i = -834
				j = 900
				ic = -287
				mcinfo = 364
				v = 214
				vv = -338
			End If
			If state.rstate.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstate.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstate.stage = 2 Then
				GoTo lbl_2
			End If
			If state.rstate.stage = 3 Then
				GoTo lbl_3
			End If
			If state.rstate.stage = 4 Then
				GoTo lbl_4
			End If
			If state.rstate.stage = 5 Then
				GoTo lbl_5
			End If
			If state.rstate.stage = 6 Then
				GoTo lbl_6
			End If
			If state.rstate.stage = 7 Then
				GoTo lbl_7
			End If
			If state.rstate.stage = 8 Then
				GoTo lbl_8
			End If
			If state.rstate.stage = 9 Then
				GoTo lbl_9
			End If
			If state.rstate.stage = 10 Then
				GoTo lbl_10
			End If
			If state.rstate.stage = 11 Then
				GoTo lbl_11
			End If
			If state.rstate.stage = 12 Then
				GoTo lbl_12
			End If
			If state.rstate.stage = 13 Then
				GoTo lbl_13
			End If
			If state.rstate.stage = 14 Then
				GoTo lbl_14
			End If
			If state.rstate.stage = 15 Then
				GoTo lbl_15
			End If
			If state.rstate.stage = 16 Then
				GoTo lbl_16
			End If

			'
			' Routine body
			'

			'
			' Unload frequently used variables from State structure
			' (just for typing convinience)
			'
			n = state.n
			m = state.m
			state.userterminationneeded = False
			state.repterminationtype = 0
			state.repiterationscount = 0
			state.repvaridx = -1
			state.repnfev = 0

			'
			'  Check, that transferred derivative value is right
			'
			clearrequestfields(state)
			If Not (CDbl(state.diffstep) = CDbl(0) AndAlso CDbl(state.teststep) > CDbl(0)) Then
				GoTo lbl_17
			End If
			state.needfg = True
			i = 0
			lbl_19:
			If i > n - 1 Then
				GoTo lbl_21
			End If
			v = state.x(i)
			state.x(i) = v - state.teststep * state.s(i)
			state.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			state.fm1 = state.f
			state.fp1 = state.g(i)
			state.x(i) = v + state.teststep * state.s(i)
			state.rstate.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			state.fm2 = state.f
			state.fp2 = state.g(i)
			state.x(i) = v
			state.rstate.stage = 2
			GoTo lbl_rcomm
			lbl_2:

			'
			' 2*State.TestStep   -   scale parameter
			' width of segment [Xi-TestStep;Xi+TestStep]
			'
			If Not optserv.derivativecheck(state.fm1, state.fp1, state.fm2, state.fp2, state.f, state.g(i), _
				2 * state.teststep) Then
				state.repvaridx = i
				state.repterminationtype = -7
				result = False
				Return result
			End If
			i = i + 1
			GoTo lbl_19
			lbl_21:
			state.needfg = False
			lbl_17:

			'
			' Calculate F/G at the initial point
			'
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_22
			End If
			state.needfg = True
			state.rstate.stage = 3
			GoTo lbl_rcomm
			lbl_3:
			state.needfg = False
			GoTo lbl_23
			lbl_22:
			state.needf = True
			state.rstate.stage = 4
			GoTo lbl_rcomm
			lbl_4:
			state.fbase = state.f
			i = 0
			lbl_24:
			If i > n - 1 Then
				GoTo lbl_26
			End If
			v = state.x(i)
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 5
			GoTo lbl_rcomm
			lbl_5:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 6
			GoTo lbl_rcomm
			lbl_6:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 7
			GoTo lbl_rcomm
			lbl_7:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 8
			GoTo lbl_rcomm
			lbl_8:
			state.fp2 = state.f
			state.x(i) = v
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			i = i + 1
			GoTo lbl_24
			lbl_26:
			state.f = state.fbase
			state.needf = False
			lbl_23:
			optserv.trimprepare(state.f, state.trimthreshold)
			If Not state.xrep Then
				GoTo lbl_27
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 9
			GoTo lbl_rcomm
			lbl_9:
			state.xupdated = False
			lbl_27:
			If state.userterminationneeded Then

				'
				' User requested termination
				'
				state.repterminationtype = 8
				result = False
				Return result
			End If
			state.repnfev = 1
			state.fold = state.f
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.g(i) * state.s(i))
			Next
			If CDbl(System.Math.sqrt(v)) <= CDbl(state.epsg) Then
				state.repterminationtype = 4
				result = False
				Return result
			End If

			'
			' Choose initial step and direction.
			' Apply preconditioner, if we have something other than default.
			'
			For i_ = 0 To n - 1
				state.d(i_) = -state.g(i_)
			Next
			If state.prectype = 0 Then

				'
				' Default preconditioner is used, but we can't use it before iterations will start
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += state.g(i_) * state.g(i_)
				Next
				v = System.Math.sqrt(v)
				If CDbl(state.stpmax) = CDbl(0) Then
					state.stp = System.Math.Min(1.0 / v, 1)
				Else
					state.stp = System.Math.Min(1.0 / v, state.stpmax)
				End If
			End If
			If state.prectype = 1 Then

				'
				' Cholesky preconditioner is used
				'
				fbls.fblscholeskysolve(state.denseh, 1.0, n, True, state.d, state.autobuf)
				state.stp = 1
			End If
			If state.prectype = 2 Then

				'
				' diagonal approximation is used
				'
				For i = 0 To n - 1
					state.d(i) = state.d(i) / state.diagh(i)
				Next
				state.stp = 1
			End If
			If state.prectype = 3 Then

				'
				' scale-based preconditioner is used
				'
				For i = 0 To n - 1
					state.d(i) = state.d(i) * state.s(i) * state.s(i)
				Next
				state.stp = 1
			End If
			If state.prectype = 4 Then

				'
				' rank-k BFGS-based preconditioner is used
				'
				optserv.inexactlbfgspreconditioner(state.d, n, state.precd, state.precc, state.precw, state.preck, _
					state.precbuf)
				state.stp = 1
			End If
			If state.prectype = 5 Then

				'
				' exact low-rank preconditioner is used
				'
				optserv.applylowrankpreconditioner(state.d, state.lowrankbuf)
				state.stp = 1
			End If

			'
			' Main cycle
			'
			state.k = 0
			lbl_29:
			If False Then
				GoTo lbl_30
			End If

			'
			' Main cycle: prepare to 1-D line search
			'
			state.p = state.k Mod m
			state.q = System.Math.Min(state.k, m - 1)

			'
			' Store X[k], G[k]
			'
			For i_ = 0 To n - 1
				state.xp(i_) = state.x(i_)
			Next
			For i_ = 0 To n - 1
				state.sk(state.p, i_) = -state.x(i_)
			Next
			For i_ = 0 To n - 1
				state.yk(state.p, i_) = -state.g(i_)
			Next

			'
			' Minimize F(x+alpha*d)
			' Calculate S[k], Y[k]
			'
			state.mcstage = 0
			If state.k <> 0 Then
				state.stp = 1.0
			End If
			linmin.linminnormalized(state.d, state.stp, n)
			linmin.mcsrch(n, state.x, state.f, state.g, state.d, state.stp, _
				state.stpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			lbl_31:
			If state.mcstage = 0 Then
				GoTo lbl_32
			End If
			clearrequestfields(state)
			If CDbl(state.diffstep) <> CDbl(0) Then
				GoTo lbl_33
			End If
			state.needfg = True
			state.rstate.stage = 10
			GoTo lbl_rcomm
			lbl_10:
			state.needfg = False
			GoTo lbl_34
			lbl_33:
			state.needf = True
			state.rstate.stage = 11
			GoTo lbl_rcomm
			lbl_11:
			state.fbase = state.f
			i = 0
			lbl_35:
			If i > n - 1 Then
				GoTo lbl_37
			End If
			v = state.x(i)
			state.x(i) = v - state.diffstep * state.s(i)
			state.rstate.stage = 12
			GoTo lbl_rcomm
			lbl_12:
			state.fm2 = state.f
			state.x(i) = v - 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 13
			GoTo lbl_rcomm
			lbl_13:
			state.fm1 = state.f
			state.x(i) = v + 0.5 * state.diffstep * state.s(i)
			state.rstate.stage = 14
			GoTo lbl_rcomm
			lbl_14:
			state.fp1 = state.f
			state.x(i) = v + state.diffstep * state.s(i)
			state.rstate.stage = 15
			GoTo lbl_rcomm
			lbl_15:
			state.fp2 = state.f
			state.x(i) = v
			state.g(i) = (8 * (state.fp1 - state.fm1) - (state.fp2 - state.fm2)) / (6 * state.diffstep * state.s(i))
			i = i + 1
			GoTo lbl_35
			lbl_37:
			state.f = state.fbase
			state.needf = False
			lbl_34:
			optserv.trimfunction(state.f, state.g, n, state.trimthreshold)
			linmin.mcsrch(n, state.x, state.f, state.g, state.d, state.stp, _
				state.stpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			GoTo lbl_31
			lbl_32:
			If state.userterminationneeded Then

				'
				' User requested termination.
				' Restore previous point and return.
				'
				For i_ = 0 To n - 1
					state.x(i_) = state.xp(i_)
				Next
				state.repterminationtype = 8
				result = False
				Return result
			End If
			If Not state.xrep Then
				GoTo lbl_38
			End If

			'
			' report
			'
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 16
			GoTo lbl_rcomm
			lbl_16:
			state.xupdated = False
			lbl_38:
			state.repnfev = state.repnfev + state.nfev
			state.repiterationscount = state.repiterationscount + 1
			For i_ = 0 To n - 1
				state.sk(state.p, i_) = state.sk(state.p, i_) + state.x(i_)
			Next
			For i_ = 0 To n - 1
				state.yk(state.p, i_) = state.yk(state.p, i_) + state.g(i_)
			Next

			'
			' Stopping conditions
			'
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.g(i) * state.s(i))
			Next
			If Not Math.isfinite(v) OrElse Not Math.isfinite(state.f) Then

				'
				' Abnormal termination - infinities in function/gradient
				'
				state.repterminationtype = -8
				result = False
				Return result
			End If
			If state.repiterationscount >= state.maxits AndAlso state.maxits > 0 Then

				'
				' Too many iterations
				'
				state.repterminationtype = 5
				result = False
				Return result
			End If
			If CDbl(System.Math.sqrt(v)) <= CDbl(state.epsg) Then

				'
				' Gradient is small enough
				'
				state.repterminationtype = 4
				result = False
				Return result
			End If
			If CDbl(state.fold - state.f) <= CDbl(state.epsf * System.Math.Max(System.Math.Abs(state.fold), System.Math.Max(System.Math.Abs(state.f), 1.0))) Then

				'
				' F(k+1)-F(k) is small enough
				'
				state.repterminationtype = 1
				result = False
				Return result
			End If
			v = 0
			For i = 0 To n - 1
				v = v + Math.sqr(state.sk(state.p, i) / state.s(i))
			Next
			If CDbl(System.Math.sqrt(v)) <= CDbl(state.epsx) Then

				'
				' X(k+1)-X(k) is small enough
				'
				state.repterminationtype = 2
				result = False
				Return result
			End If

			'
			' If Wolfe conditions are satisfied, we can update
			' limited memory model.
			'
			' However, if conditions are not satisfied (NFEV limit is met,
			' function is too wild, ...), we'll skip L-BFGS update
			'
			If mcinfo <> 1 Then

				'
				' Skip update.
				'
				' In such cases we'll initialize search direction by
				' antigradient vector, because it  leads to more
				' transparent code with less number of special cases
				'
				state.fold = state.f
				For i_ = 0 To n - 1
					state.d(i_) = -state.g(i_)
				Next
			Else

				'
				' Calculate Rho[k], GammaK
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += state.yk(state.p, i_) * state.sk(state.p, i_)
				Next
				vv = 0.0
				For i_ = 0 To n - 1
					vv += state.yk(state.p, i_) * state.yk(state.p, i_)
				Next
				If CDbl(v) = CDbl(0) OrElse CDbl(vv) = CDbl(0) Then

					'
					' Rounding errors make further iterations impossible.
					'
					state.repterminationtype = -2
					result = False
					Return result
				End If
				state.rho(state.p) = 1 / v
				state.gammak = v / vv

				'
				'  Calculate d(k+1) = -H(k+1)*g(k+1)
				'
				'  for I:=K downto K-Q do
				'      V = s(i)^T * work(iteration:I)
				'      theta(i) = V
				'      work(iteration:I+1) = work(iteration:I) - V*Rho(i)*y(i)
				'  work(last iteration) = H0*work(last iteration) - preconditioner
				'  for I:=K-Q to K do
				'      V = y(i)^T*work(iteration:I)
				'      work(iteration:I+1) = work(iteration:I) +(-V+theta(i))*Rho(i)*s(i)
				'
				'  NOW WORK CONTAINS d(k+1)
				'
				For i_ = 0 To n - 1
					state.work(i_) = state.g(i_)
				Next
				For i = state.k To state.k - state.q Step -1
					ic = i Mod m
					v = 0.0
					For i_ = 0 To n - 1
						v += state.sk(ic, i_) * state.work(i_)
					Next
					state.theta(ic) = v
					vv = v * state.rho(ic)
					For i_ = 0 To n - 1
						state.work(i_) = state.work(i_) - vv * state.yk(ic, i_)
					Next
				Next
				If state.prectype = 0 Then

					'
					' Simple preconditioner is used
					'
					v = state.gammak
					For i_ = 0 To n - 1
						state.work(i_) = v * state.work(i_)
					Next
				End If
				If state.prectype = 1 Then

					'
					' Cholesky preconditioner is used
					'
					fbls.fblscholeskysolve(state.denseh, 1, n, True, state.work, state.autobuf)
				End If
				If state.prectype = 2 Then

					'
					' diagonal approximation is used
					'
					For i = 0 To n - 1
						state.work(i) = state.work(i) / state.diagh(i)
					Next
				End If
				If state.prectype = 3 Then

					'
					' scale-based preconditioner is used
					'
					For i = 0 To n - 1
						state.work(i) = state.work(i) * state.s(i) * state.s(i)
					Next
				End If
				If state.prectype = 4 Then

					'
					' Rank-K BFGS-based preconditioner is used
					'
					optserv.inexactlbfgspreconditioner(state.work, n, state.precd, state.precc, state.precw, state.preck, _
						state.precbuf)
				End If
				If state.prectype = 5 Then

					'
					' Exact low-rank preconditioner is used
					'
					optserv.applylowrankpreconditioner(state.work, state.lowrankbuf)
				End If
				For i = state.k - state.q To state.k
					ic = i Mod m
					v = 0.0
					For i_ = 0 To n - 1
						v += state.yk(ic, i_) * state.work(i_)
					Next
					vv = state.rho(ic) * (-v + state.theta(ic))
					For i_ = 0 To n - 1
						state.work(i_) = state.work(i_) + vv * state.sk(ic, i_)
					Next
				Next
				For i_ = 0 To n - 1
					state.d(i_) = -state.work(i_)
				Next

				'
				' Next step
				'
				state.fold = state.f
				state.k = state.k + 1
			End If
			GoTo lbl_29
			lbl_30:
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstate.ia(0) = n
			state.rstate.ia(1) = m
			state.rstate.ia(2) = i
			state.rstate.ia(3) = j
			state.rstate.ia(4) = ic
			state.rstate.ia(5) = mcinfo
			state.rstate.ra(0) = v
			state.rstate.ra(1) = vv
			Return result
		End Function


		'************************************************************************
'        L-BFGS algorithm results
'
'        INPUT PARAMETERS:
'            State   -   algorithm state
'
'        OUTPUT PARAMETERS:
'            X       -   array[0..N-1], solution
'            Rep     -   optimization report:
'                        * Rep.TerminationType completetion code:
'                            * -8    internal integrity control  detected  infinite
'                                    or NAN values in  function/gradient.  Abnormal
'                                    termination signalled.
'                            * -7    gradient verification failed.
'                                    See MinLBFGSSetGradientCheck() for more information.
'                            * -2    rounding errors prevent further improvement.
'                                    X contains best point found.
'                            * -1    incorrect parameters were specified
'                            *  1    relative function improvement is no more than
'                                    EpsF.
'                            *  2    relative step is no more than EpsX.
'                            *  4    gradient norm is no more than EpsG
'                            *  5    MaxIts steps was taken
'                            *  7    stopping conditions are too stringent,
'                                    further improvement is impossible
'                            *  8    terminated by user who called minlbfgsrequesttermination().
'                                    X contains point which was "current accepted" when
'                                    termination request was submitted.
'                        * Rep.IterationsCount contains iterations count
'                        * NFEV countains number of function calculations
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgsresults(state As minlbfgsstate, ByRef x As Double(), rep As minlbfgsreport)
			x = New Double(-1) {}

			minlbfgsresultsbuf(state, x, rep)
		End Sub


		'************************************************************************
'        L-BFGS algorithm results
'
'        Buffered implementation of MinLBFGSResults which uses pre-allocated buffer
'        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
'        intended to be used in the inner cycles of performance critical algorithms
'        where array reallocation penalty is too large to be ignored.
'
'          -- ALGLIB --
'             Copyright 20.08.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgsresultsbuf(state As minlbfgsstate, ByRef x As Double(), rep As minlbfgsreport)
			Dim i_ As Integer = 0

			If alglib.ap.len(x) < state.n Then
				x = New Double(state.n - 1) {}
			End If
			For i_ = 0 To state.n - 1
				x(i_) = state.x(i_)
			Next
			rep.iterationscount = state.repiterationscount
			rep.nfev = state.repnfev
			rep.varidx = state.repvaridx
			rep.terminationtype = state.repterminationtype
		End Sub


		'************************************************************************
'        This  subroutine restarts LBFGS algorithm from new point. All optimization
'        parameters are left unchanged.
'
'        This  function  allows  to  solve multiple  optimization  problems  (which
'        must have same number of dimensions) without object reallocation penalty.
'
'        INPUT PARAMETERS:
'            State   -   structure used to store algorithm state
'            X       -   new starting point.
'
'          -- ALGLIB --
'             Copyright 30.07.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgsrestartfrom(state As minlbfgsstate, x As Double())
			Dim i_ As Integer = 0

			alglib.ap.assert(alglib.ap.len(x) >= state.n, "MinLBFGSRestartFrom: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, state.n), "MinLBFGSRestartFrom: X contains infinite or NaN values!")
			For i_ = 0 To state.n - 1
				state.x(i_) = x(i_)
			Next
			state.rstate.ia = New Integer(5) {}
			state.rstate.ra = New Double(1) {}
			state.rstate.stage = -1
			clearrequestfields(state)
		End Sub


		'************************************************************************
'        This subroutine submits request for termination of running  optimizer.  It
'        should be called from user-supplied callback when user decides that it  is
'        time to "smoothly" terminate optimization process.  As  result,  optimizer
'        stops at point which was "current accepted" when termination  request  was
'        submitted and returns error code 8 (successful termination).
'
'        INPUT PARAMETERS:
'            State   -   optimizer structure
'
'        NOTE: after  request  for  termination  optimizer  may   perform   several
'              additional calls to user-supplied callbacks. It does  NOT  guarantee
'              to stop immediately - it just guarantees that these additional calls
'              will be discarded later.
'
'        NOTE: calling this function on optimizer which is NOT running will have no
'              effect.
'              
'        NOTE: multiple calls to this function are possible. First call is counted,
'              subsequent calls are silently ignored.
'
'          -- ALGLIB --
'             Copyright 08.10.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgsrequesttermination(state As minlbfgsstate)
			state.userterminationneeded = True
		End Sub


		'************************************************************************
'        This  subroutine  turns  on  verification  of  the  user-supplied analytic
'        gradient:
'        * user calls this subroutine before optimization begins
'        * MinLBFGSOptimize() is called
'        * prior to  actual  optimization, for each component  of  parameters being
'          optimized X[i] algorithm performs following steps:
'          * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
'            where X[i] is i-th component of the initial point and S[i] is a  scale
'            of i-th parameter
'          * if needed, steps are bounded with respect to constraints on X[]
'          * F(X) is evaluated at these trial points
'          * we perform one more evaluation in the middle point of the interval
'          * we  build  cubic  model using function values and derivatives at trial
'            points and we compare its prediction with actual value in  the  middle
'            point
'          * in case difference between prediction and actual value is higher  than
'            some predetermined threshold, algorithm stops with completion code -7;
'            Rep.VarIdx is set to index of the parameter with incorrect derivative.
'        * after verification is over, algorithm proceeds to the actual optimization.
'
'        NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
'                is very costly and you should use  it  only  for  low  dimensional
'                problems,  when  you  want  to  be  sure  that  you've   correctly
'                calculated  analytic  derivatives.  You  should  not use it in the
'                production code (unless you want to check derivatives provided  by
'                some third party).
'
'        NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
'                (so large that function behaviour is significantly non-cubic) will
'                lead to false alarms. You may use  different  step  for  different
'                parameters by means of setting scale with MinLBFGSSetScale().
'
'        NOTE 3: this function may lead to false positives. In case it reports that
'                I-th  derivative was calculated incorrectly, you may decrease test
'                step  and  try  one  more  time  - maybe your function changes too
'                sharply  and  your  step  is  too  large for such rapidly chanding
'                function.
'
'        INPUT PARAMETERS:
'            State       -   structure used to store algorithm state
'            TestStep    -   verification step:
'                            * TestStep=0 turns verification off
'                            * TestStep>0 activates verification
'
'          -- ALGLIB --
'             Copyright 24.05.2012 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetgradientcheck(state As minlbfgsstate, teststep As Double)
			alglib.ap.assert(Math.isfinite(teststep), "MinLBFGSSetGradientCheck: TestStep contains NaN or Infinite")
			alglib.ap.assert(CDbl(teststep) >= CDbl(0), "MinLBFGSSetGradientCheck: invalid argument TestStep(TestStep<0)")
			state.teststep = teststep
		End Sub


		'************************************************************************
'        Clears request fileds (to be sure that we don't forgot to clear something)
'        ************************************************************************

		Private Shared Sub clearrequestfields(state As minlbfgsstate)
			state.needf = False
			state.needfg = False
			state.xupdated = False
		End Sub


	End Class
	Public Class qqpsolver
		'************************************************************************
'        This object stores settings for QQP solver.
'        It must be initialized with QQPLoadDefaults().
'        After initialization you may change settings.
'        ************************************************************************

		Public Class qqpsettings
			Inherits apobject
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxouterits As Integer
			Public cgphase As Boolean
			Public cnphase As Boolean
			Public cgminits As Integer
			Public cgmaxits As Integer
			Public cnmaxupdates As Integer
			Public sparsesolver As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New qqpsettings()
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxouterits = maxouterits
				_result.cgphase = cgphase
				_result.cnphase = cnphase
				_result.cgminits = cgminits
				_result.cgmaxits = cgmaxits
				_result.cnmaxupdates = cnmaxupdates
				_result.sparsesolver = sparsesolver
				Return _result
			End Function
		End Class


		'************************************************************************
'        This object stores temporaries used by QuickQP solver.
'        ************************************************************************

		Public Class qqpbuffers
			Inherits apobject
			Public n As Integer
			Public nmain As Integer
			Public nslack As Integer
			Public nec As Integer
			Public nic As Integer
			Public akind As Integer
			Public densea As Double(,)
			Public sparsea As sparse.sparsematrix
			Public sparseupper As Boolean
			Public absamax As Double
			Public absasum As Double
			Public absasum2 As Double
			Public b As Double()
			Public bndl As Double()
			Public bndu As Double()
			Public havebndl As Boolean()
			Public havebndu As Boolean()
			Public cleic As Double(,)
			Public xs As Double()
			Public gc As Double()
			Public xp As Double()
			Public dc As Double()
			Public dp As Double()
			Public cgc As Double()
			Public cgp As Double()
			Public sas As sactivesets.sactiveset
			Public activated As Boolean()
			Public nfree As Integer
			Public cnmodelage As Integer
			Public densez As Double(,)
			Public sparsecca As sparse.sparsematrix
			Public yidx As Integer()
			Public regdiag As Double()
			Public regx0 As Double()
			Public tmpcn As Double()
			Public tmpcni As Integer()
			Public tmpcnb As Boolean()
			Public tmp0 As Double()
			Public stpbuf As Double()
			Public sbuf As sparse.sparsebuffers
			Public repinneriterationscount As Integer
			Public repouteriterationscount As Integer
			Public repncholesky As Integer
			Public repncupdates As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				densea = New Double(-1, -1) {}
				sparsea = New sparse.sparsematrix()
				b = New Double(-1) {}
				bndl = New Double(-1) {}
				bndu = New Double(-1) {}
				havebndl = New Boolean(-1) {}
				havebndu = New Boolean(-1) {}
				cleic = New Double(-1, -1) {}
				xs = New Double(-1) {}
				gc = New Double(-1) {}
				xp = New Double(-1) {}
				dc = New Double(-1) {}
				dp = New Double(-1) {}
				cgc = New Double(-1) {}
				cgp = New Double(-1) {}
				sas = New sactivesets.sactiveset()
				activated = New Boolean(-1) {}
				densez = New Double(-1, -1) {}
				sparsecca = New sparse.sparsematrix()
				yidx = New Integer(-1) {}
				regdiag = New Double(-1) {}
				regx0 = New Double(-1) {}
				tmpcn = New Double(-1) {}
				tmpcni = New Integer(-1) {}
				tmpcnb = New Boolean(-1) {}
				tmp0 = New Double(-1) {}
				stpbuf = New Double(-1) {}
				sbuf = New sparse.sparsebuffers()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New qqpbuffers()
				_result.n = n
				_result.nmain = nmain
				_result.nslack = nslack
				_result.nec = nec
				_result.nic = nic
				_result.akind = akind
				_result.densea = DirectCast(densea.Clone(), Double(,))
				_result.sparsea = DirectCast(sparsea.make_copy(), sparse.sparsematrix)
				_result.sparseupper = sparseupper
				_result.absamax = absamax
				_result.absasum = absasum
				_result.absasum2 = absasum2
				_result.b = DirectCast(b.Clone(), Double())
				_result.bndl = DirectCast(bndl.Clone(), Double())
				_result.bndu = DirectCast(bndu.Clone(), Double())
				_result.havebndl = DirectCast(havebndl.Clone(), Boolean())
				_result.havebndu = DirectCast(havebndu.Clone(), Boolean())
				_result.cleic = DirectCast(cleic.Clone(), Double(,))
				_result.xs = DirectCast(xs.Clone(), Double())
				_result.gc = DirectCast(gc.Clone(), Double())
				_result.xp = DirectCast(xp.Clone(), Double())
				_result.dc = DirectCast(dc.Clone(), Double())
				_result.dp = DirectCast(dp.Clone(), Double())
				_result.cgc = DirectCast(cgc.Clone(), Double())
				_result.cgp = DirectCast(cgp.Clone(), Double())
				_result.sas = DirectCast(sas.make_copy(), sactivesets.sactiveset)
				_result.activated = DirectCast(activated.Clone(), Boolean())
				_result.nfree = nfree
				_result.cnmodelage = cnmodelage
				_result.densez = DirectCast(densez.Clone(), Double(,))
				_result.sparsecca = DirectCast(sparsecca.make_copy(), sparse.sparsematrix)
				_result.yidx = DirectCast(yidx.Clone(), Integer())
				_result.regdiag = DirectCast(regdiag.Clone(), Double())
				_result.regx0 = DirectCast(regx0.Clone(), Double())
				_result.tmpcn = DirectCast(tmpcn.Clone(), Double())
				_result.tmpcni = DirectCast(tmpcni.Clone(), Integer())
				_result.tmpcnb = DirectCast(tmpcnb.Clone(), Boolean())
				_result.tmp0 = DirectCast(tmp0.Clone(), Double())
				_result.stpbuf = DirectCast(stpbuf.Clone(), Double())
				_result.sbuf = DirectCast(sbuf.make_copy(), sparse.sparsebuffers)
				_result.repinneriterationscount = repinneriterationscount
				_result.repouteriterationscount = repouteriterationscount
				_result.repncholesky = repncholesky
				_result.repncupdates = repncupdates
				Return _result
			End Function
		End Class




		Public Const quickqprestartcg As Integer = 50
		Public Const penaltyfactor As Double = 50.0
		Public Const regz As Double = 1E-09


		'************************************************************************
'        This function initializes QQPSettings structure with default settings.
'
'        Newly created structure MUST be initialized by default settings  -  or  by
'        copy of the already initialized structure.
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qqploaddefaults(nmain As Integer, s As qqpsettings)
			s.epsg = 0.0
			s.epsf = 0.0
			s.epsx = 1E-06
			s.maxouterits = 0
			s.cgphase = True
			s.cnphase = True
			s.cgminits = 5
			s.cgmaxits = System.Math.Max(s.cgminits, CInt(System.Math.Truncate(System.Math.Round(1 + 0.33 * nmain))))
			s.sparsesolver = 0
			s.cnmaxupdates = CInt(System.Math.Truncate(System.Math.Round(1 + 0.1 * nmain)))
		End Sub


		'************************************************************************
'        This function initializes QQPSettings  structure  with  copy  of  another,
'        already initialized structure.
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qqpcopysettings(src As qqpsettings, dst As qqpsettings)
			dst.epsg = src.epsg
			dst.epsf = src.epsf
			dst.epsx = src.epsx
			dst.maxouterits = src.maxouterits
			dst.cgphase = src.cgphase
			dst.cnphase = src.cnphase
			dst.cgminits = src.cgminits
			dst.cgmaxits = src.cgmaxits
			dst.sparsesolver = src.sparsesolver
			dst.cnmaxupdates = src.cnmaxupdates
		End Sub


		'************************************************************************
'        This function runs QQP solver; it returns after optimization  process  was
'        completed. Following QP problem is solved:
'
'            min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))
'            
'        subject to boundary constraints.
'
'        IMPORTANT: UNLIKE MANY OTHER SOLVERS, THIS FUNCTION DOES NOT  REQUIRE  YOU
'                   TO INITIALIZE STATE OBJECT. IT CAN BE AUTOMATICALLY INITIALIZED
'                   DURING SOLUTION PROCESS.
'
'        INPUT PARAMETERS:
'            AC          -   for dense problems (AKind=0) A-term of CQM object
'                            contains system matrix. Other terms are unspecified
'                            and should not be referenced.
'            SparseAC    -   for sparse problems (AKind=1
'            AKind       -   sparse matrix format:
'                            * 0 for dense matrix
'                            * 1 for sparse matrix
'            SparseUpper -   which triangle of SparseAC stores matrix  -  upper  or
'                            lower one (for dense matrices this  parameter  is  not
'                            actual).
'            BC          -   linear term, array[NC]
'            BndLC       -   lower bound, array[NC]
'            BndUC       -   upper bound, array[NC]
'            SC          -   scale vector, array[NC]:
'                            * I-th element contains scale of I-th variable,
'                            * SC[I]>0
'            XOriginC    -   origin term, array[NC]. Can be zero.
'            NC          -   number of variables in the  original  formulation  (no
'                            slack variables).
'            CLEICC      -   linear equality/inequality constraints. Present version
'                            of this function does NOT provide  publicly  available
'                            support for linear constraints. This feature  will  be
'                            introduced in the future versions of the function.
'            NEC, NIC    -   number of equality/inequality constraints.
'                            MUST BE ZERO IN THE CURRENT VERSION!!!
'            Settings    -   QQPSettings object initialized by one of the initialization
'                            functions.
'            SState      -   object which stores temporaries:
'                            * uninitialized object is automatically initialized
'                            * previously allocated memory is reused as much
'                              as possible
'            XS          -   initial point, array[NC]
'            
'            
'        OUTPUT PARAMETERS:
'            XS          -   last point
'            TerminationType-termination type:
'                            *
'                            *
'                            *
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qqpoptimize(ac As cqmodels.convexquadraticmodel, sparseac As sparse.sparsematrix, akind As Integer, sparseupper As Boolean, bc As Double(), bndlc As Double(), _
			bnduc As Double(), sc As Double(), xoriginc As Double(), nc As Integer, cleicc As Double(,), nec As Integer, _
			nic As Integer, settings As qqpsettings, sstate As qqpbuffers, xs As Double(), ByRef terminationtype As Integer)
			Dim n As Integer = 0
			Dim nmain As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim d2 As Double = 0
			Dim d1 As Double = 0
			Dim d1est As Integer = 0
			Dim d2est As Integer = 0
			Dim needact As New Boolean()
			Dim reststp As Double = 0
			Dim fullstp As Double = 0
			Dim stpmax As Double = 0
			Dim stp As Double = 0
			Dim stpcnt As Integer = 0
			Dim cidx As Integer = 0
			Dim cval As Double = 0
			Dim cgcnt As Integer = 0
			Dim cgmax As Integer = 0
			Dim newtcnt As Integer = 0
			Dim sparsesolver As Integer = 0
			Dim beta As Double = 0
			Dim b As New Boolean()
			Dim fprev As Double = 0
			Dim fcur As Double = 0
			Dim i_ As Integer = 0

			terminationtype = 0


			'
			' Primary checks
			'
			alglib.ap.assert(akind = 0 OrElse akind = 1, "QQPOptimize: incorrect AKind")
			sstate.nmain = nc
			sstate.nslack = nic
			sstate.n = nc + nic
			sstate.nec = nec
			sstate.nic = nic
			sstate.akind = akind
			n = sstate.n
			nmain = sstate.nmain
			terminationtype = 0
			sstate.repinneriterationscount = 0
			sstate.repouteriterationscount = 0
			sstate.repncholesky = 0
			sstate.repncupdates = 0

			'
			' Several checks
			' * matrix size
			' * scale vector
			' * consistency of bound constraints
			' * consistency of settings
			'
			If akind = 1 Then
				alglib.ap.assert(sparse.sparsegetnrows(sparseac) = nmain, "QQPOptimize: rows(SparseAC)<>NMain")
				alglib.ap.assert(sparse.sparsegetncols(sparseac) = nmain, "QQPOptimize: cols(SparseAC)<>NMain")
			End If
			For i = 0 To nmain - 1
				alglib.ap.assert(Math.isfinite(sc(i)) AndAlso CDbl(sc(i)) > CDbl(0), "QQPOptimize: incorrect scale")
			Next
			For i = 0 To nmain - 1
				If Math.isfinite(bndlc(i)) AndAlso Math.isfinite(bnduc(i)) Then
					If CDbl(bndlc(i)) > CDbl(bnduc(i)) Then
						terminationtype = -3
						Return
					End If
				End If
			Next
			alglib.ap.assert(settings.cgphase OrElse settings.cnphase, "QQPOptimize: both phases (CG and Newton) are inactive")

			'
			' Allocate data structures
			'
			apserv.rvectorsetlengthatleast(sstate.bndl, n)
			apserv.rvectorsetlengthatleast(sstate.bndu, n)
			apserv.bvectorsetlengthatleast(sstate.havebndl, n)
			apserv.bvectorsetlengthatleast(sstate.havebndu, n)
			apserv.rvectorsetlengthatleast(sstate.xs, n)
			apserv.rvectorsetlengthatleast(sstate.xp, n)
			apserv.rvectorsetlengthatleast(sstate.gc, n)
			apserv.rvectorsetlengthatleast(sstate.cgc, n)
			apserv.rvectorsetlengthatleast(sstate.cgp, n)
			apserv.rvectorsetlengthatleast(sstate.dc, n)
			apserv.rvectorsetlengthatleast(sstate.dp, n)
			apserv.rvectorsetlengthatleast(sstate.tmp0, n)
			apserv.rvectorsetlengthatleast(sstate.stpbuf, 15)
			sactivesets.sasinit(n, sstate.sas)

			'
			' Scale/shift problem coefficients:
			'
			'     min { 0.5*(x-x0)'*A*(x-x0) + b'*(x-x0) }
			'
			' becomes (after transformation "x = S*y+x0")
			'
			'     min { 0.5*y'*(S*A*S)*y + (S*b)'*y
			'
			' Modified A_mod=S*A*S and b_mod=S*(b+A*x0) are
			' stored into SState.DenseA and SState.B.
			'
			' NOTE: DenseA/DenseB are arrays whose lengths are
			'       NMain, not N=NMain+NSlack! We store reduced 
			'       matrix and vector because extend parts (last
			'       NSlack rows/columns) are exactly zero.
			'       
			'
			apserv.rvectorsetlengthatleast(sstate.b, nmain)
			For i = 0 To nmain - 1
				sstate.b(i) = sc(i) * bc(i)
			Next
			If akind = 0 Then

				'
				' Dense QP problem - just copy and scale.
				'
				apserv.rmatrixsetlengthatleast(sstate.densea, nmain, nmain)
				cqmodels.cqmgeta(ac, sstate.densea)
				sstate.absamax = 0
				sstate.absasum = 0
				sstate.absasum2 = 0
				For i = 0 To nmain - 1
					For j = 0 To nmain - 1
						v = sc(i) * sstate.densea(i, j) * sc(j)
						sstate.densea(i, j) = v
						sstate.absamax = System.Math.Max(sstate.absamax, v)
						sstate.absasum = sstate.absasum + v
						sstate.absasum2 = sstate.absasum2 + v * v
					Next
				Next
			Else

				'
				' Sparse QP problem - a bit tricky. Depending on format of the
				' input we use different strategies for copying matrix:
				' * SKS matrices are copied to SKS format
				' * anything else is copied to CRS format
				'
				alglib.ap.assert(akind = 1, "QQPOptimize: unexpected AKind (internal error)")
				sparse.sparsecopytosksbuf(sparseac, sstate.sparsea)
				If sparseupper Then
					sparse.sparsetransposesks(sstate.sparsea)
				End If
				sstate.sparseupper = False
				sstate.absamax = 0
				sstate.absasum = 0
				sstate.absasum2 = 0
				For i = 0 To n - 1
					k = sstate.sparsea.ridx(i)
					For j = i - sstate.sparsea.didx(i) To i
						v = sc(i) * sstate.sparsea.vals(k) * sc(j)
						sstate.sparsea.vals(k) = v
						If i = j Then

							'
							' Diagonal terms are counted only once
							'
							sstate.absamax = System.Math.Max(sstate.absamax, v)
							sstate.absasum = sstate.absasum + v
							sstate.absasum2 = sstate.absasum2 + v * v
						Else

							'
							' Offdiagonal terms are counted twice
							'
							sstate.absamax = System.Math.Max(sstate.absamax, v)
							sstate.absasum = sstate.absasum + 2 * v
							sstate.absasum2 = sstate.absasum2 + 2 * v * v
						End If
						k = k + 1
					Next
				Next
			End If

			'
			' Load box constraints into State structure.
			'
			' We apply transformation to variables: y=(x-x_origin)/s,
			' each of the constraints is appropriately shifted/scaled.
			'
			For i = 0 To nmain - 1
				sstate.havebndl(i) = Math.isfinite(bndlc(i))
				If sstate.havebndl(i) Then
					sstate.bndl(i) = (bndlc(i) - xoriginc(i)) / sc(i)
				Else
					alglib.ap.assert([Double].IsNegativeInfinity(bndlc(i)), "QQPOptimize: incorrect lower bound")
					sstate.bndl(i) = [Double].NegativeInfinity
				End If
				sstate.havebndu(i) = Math.isfinite(bnduc(i))
				If sstate.havebndu(i) Then
					sstate.bndu(i) = (bnduc(i) - xoriginc(i)) / sc(i)
				Else
					alglib.ap.assert([Double].IsPositiveInfinity(bnduc(i)), "QQPOptimize: incorrect upper bound")
					sstate.bndu(i) = [Double].PositiveInfinity
				End If
			Next
			For i = nmain To n - 1
				sstate.havebndl(i) = True
				sstate.bndl(i) = 0.0
				sstate.havebndu(i) = False
				sstate.bndu(i) = [Double].PositiveInfinity
			Next

			'
			' Shift/scale linear constraints with transformation y=(x-x_origin)/s:
			' * constraint "c[i]'*x = b[i]" becomes "(s[i]*c[i])'*x = b[i]-c[i]'*x_origin".
			' * after constraint is loaded into SState.CLEIC, it is additionally normalized
			'
			apserv.rmatrixsetlengthatleast(sstate.cleic, nec + nic, n + 1)
			For i = 0 To nec + nic - 1
				v = 0
				vv = 0
				For j = 0 To nmain - 1
					sstate.cleic(i, j) = cleicc(i, j) * sc(j)
					vv = vv + Math.sqr(sstate.cleic(i, j))
					v = v + cleicc(i, j) * xoriginc(j)
				Next
				vv = System.Math.sqrt(vv)
				For j = nmain To n - 1
					sstate.cleic(i, j) = 0.0
				Next
				sstate.cleic(i, n) = cleicc(i, nmain) - v
				If i >= nec Then
					sstate.cleic(i, nmain + i - nec) = 1.0
				End If
				If CDbl(vv) > CDbl(0) Then
					For j = 0 To n
						sstate.cleic(i, j) = sstate.cleic(i, j) / vv
					Next
				End If
			Next

			'
			' Process initial point:
			' * first NMain components are equal to XS-XOriginC
			' * last NIC components are deduced from linear constraints
			' * make sure that boundary constraints are preserved by transformation
			'
			For i = 0 To nmain - 1
				sstate.xs(i) = (xs(i) - xoriginc(i)) / sc(i)
				If sstate.havebndl(i) AndAlso CDbl(sstate.xs(i)) < CDbl(sstate.bndl(i)) Then
					sstate.xs(i) = sstate.bndl(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(sstate.xs(i)) > CDbl(sstate.bndu(i)) Then
					sstate.xs(i) = sstate.bndu(i)
				End If
				If sstate.havebndl(i) AndAlso CDbl(xs(i)) = CDbl(bndlc(i)) Then
					sstate.xs(i) = sstate.bndl(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(xs(i)) = CDbl(bnduc(i)) Then
					sstate.xs(i) = sstate.bndu(i)
				End If
			Next
			For i = 0 To nic - 1
				v = 0.0
				For i_ = 0 To nmain - 1
					v += sstate.xs(i_) * sstate.cleic(nec + i, i_)
				Next
				sstate.xs(nmain + i) = System.Math.Max(sstate.cleic(nec + i, n) - v, 0.0)
			Next

			'
			' Prepare "active set" structure
			'
			sactivesets.sassetbc(sstate.sas, sstate.bndl, sstate.bndu)
			sactivesets.sassetlcx(sstate.sas, sstate.cleic, 0, 0)
			If Not sactivesets.sasstartoptimization(sstate.sas, sstate.xs) Then
				terminationtype = -3
				Return
			End If

			'
			' Select sparse direct solver
			'
			If akind = 1 Then
				sparsesolver = settings.sparsesolver
				If sparsesolver = 0 Then
					sparsesolver = 1
				End If
				If sparse.sparseissks(sstate.sparsea) Then
					sparsesolver = 2
				End If
				sparsesolver = 2
				alglib.ap.assert(sparsesolver = 1 OrElse sparsesolver = 2, "QQPOptimize: incorrect SparseSolver")
			Else
				sparsesolver = 0
			End If

			'
			' Main loop.
			'
			' Following variables are used:
			' * GC stores current gradient (unconstrained)
			' * CGC stores current gradient (constrained)
			' * DC stores current search direction
			' * CGP stores constrained gradient at previous point
			'   (zero on initial entry)
			' * DP stores previous search direction
			'   (zero on initial entry)
			'
			cgmax = settings.cgminits
			sstate.repinneriterationscount = 0
			sstate.repouteriterationscount = 0
			While True
				If settings.maxouterits > 0 AndAlso sstate.repouteriterationscount >= settings.maxouterits Then
					terminationtype = 5
					Exit While
				End If
				If sstate.repouteriterationscount > 0 Then

					'
					' Check EpsF- and EpsX-based stopping criteria.
					' Because problem was already scaled, we do not scale step before checking its length.
					' NOTE: these checks are performed only after at least one outer iteration was made.
					'
					If CDbl(settings.epsf) > CDbl(0) Then

						'
						' NOTE 1: here we rely on the fact that ProjectedTargetFunction() ignore D when Stp=0
						' NOTE 2: code below handles situation when update increases function value instead
						'         of decreasing it.
						'
						fprev = projectedtargetfunction(sstate, sstate.xp, sstate.dc, 0.0, sstate.tmp0)
						fcur = projectedtargetfunction(sstate, sstate.sas.xc, sstate.dc, 0.0, sstate.tmp0)
						If CDbl(fprev - fcur) <= CDbl(settings.epsf * System.Math.Max(System.Math.Abs(fprev), System.Math.Max(System.Math.Abs(fcur), 1.0))) Then
							terminationtype = 1
							Exit While
						End If
					End If
					If CDbl(settings.epsx) > CDbl(0) Then
						v = 0.0
						For i = 0 To n - 1
							v = v + Math.sqr(sstate.xp(i) - sstate.sas.xc(i))
						Next
						If CDbl(System.Math.sqrt(v)) <= CDbl(settings.epsx) Then
							terminationtype = 2
							Exit While
						End If
					End If
				End If
				apserv.inc(sstate.repouteriterationscount)
				For i_ = 0 To n - 1
					sstate.xp(i_) = sstate.sas.xc(i_)
				Next
				If Not settings.cgphase Then
					cgmax = 0
				End If
				For i = 0 To n - 1
					sstate.cgp(i) = 0.0
					sstate.dp(i) = 0.0
				Next
				For cgcnt = 0 To cgmax - 1

					'
					' Calculate unconstrained gradient GC for "extended" QP problem
					' Determine active set, current constrained gradient CGC.
					' Check gradient-based stopping condition.
					'
					' NOTE: because problem was scaled, we do not have to apply scaling
					'       to gradient before checking stopping condition.
					'
					targetgradient(sstate, sstate.sas.xc, sstate.gc)
					sactivesets.sasreactivateconstraints(sstate.sas, sstate.gc)
					For i_ = 0 To n - 1
						sstate.cgc(i_) = sstate.gc(i_)
					Next
					sactivesets.sasconstraineddirection(sstate.sas, sstate.cgc)
					v = 0.0
					For i_ = 0 To n - 1
						v += sstate.cgc(i_) * sstate.cgc(i_)
					Next
					If CDbl(System.Math.sqrt(v)) <= CDbl(settings.epsg) Then
						terminationtype = 4
						Exit For
					End If

					'
					' Prepare search direction DC and explore it.
					'
					' We try to use CGP/DP to prepare conjugate gradient step,
					' but we resort to steepest descent step (Beta=0) in case
					' we are at I-th boundary, but DP[I]<>0.
					'
					' Such approach allows us to ALWAYS have feasible DC, with
					' guaranteed compatibility with both feasible area and current
					' active set.
					'
					' Automatic CG reset performed every time DP is incompatible
					' with current active set and/or feasible area. We also
					' perform reset every QuickQPRestartCG iterations.
					'
					For i_ = 0 To n - 1
						sstate.dc(i_) = -sstate.cgc(i_)
					Next
					v = 0.0
					vv = 0.0
					b = False
					For i = 0 To n - 1
						v = v + sstate.cgc(i) * sstate.cgc(i)
						vv = vv + sstate.cgp(i) * sstate.cgp(i)
						b = b OrElse ((sstate.havebndl(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndl(i))) AndAlso CDbl(sstate.dp(i)) <> CDbl(0))
						b = b OrElse ((sstate.havebndu(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndu(i))) AndAlso CDbl(sstate.dp(i)) <> CDbl(0))
					Next
					b = b OrElse CDbl(vv) = CDbl(0)
					b = b OrElse cgcnt Mod quickqprestartcg = 0
					If Not b Then
						beta = v / vv
					Else
						beta = 0.0
					End If
					For i_ = 0 To n - 1
						sstate.dc(i_) = sstate.dc(i_) + beta * sstate.dp(i_)
					Next
					sactivesets.sasconstraineddirection(sstate.sas, sstate.dc)
					sactivesets.sasexploredirection(sstate.sas, sstate.dc, stpmax, cidx, cval)

					'
					' Build quadratic model of F along descent direction:
					'
					'     F(xc+alpha*D) = D2*alpha^2 + D1*alpha
					'
					' Terminate algorithm if needed.
					'
					' NOTE: we do not maintain constant term D0
					'
					quadraticmodel(sstate, sstate.sas.xc, sstate.dc, sstate.gc, d1, d1est, _
						d2, d2est)
					If CDbl(d1) = CDbl(0) AndAlso CDbl(d2) = CDbl(0) Then

						'
						' D1 and D2 are exactly zero, success.
						' After this if-then we assume that D is non-zero.
						'
						terminationtype = 4
						Exit For
					End If
					If d1est >= 0 Then

						'
						' Numerical noise is too large, it means that we are close
						' to minimum - and that further improvement is impossible.
						'
						' After this if-then we assume that D1 is definitely negative
						' (even under presence of numerical errors).
						'
						terminationtype = 7
						Exit For
					End If
					If d2est <= 0 AndAlso cidx < 0 Then

						'
						' Function is unbounded from below:
						' * D1<0 (verified by previous block)
						' * D2Est<=0, which means that either D2<0 - or it can not
						'   be reliably distinguished from zero.
						' * step is unconstrained
						'
						' If these conditions are true, we abnormally terminate QP
						' algorithm with return code -4
						'
						terminationtype = -4
						Exit For
					End If

					'
					' Perform step along DC.
					'
					' In this block of code we maintain two step length:
					' * RestStp -  restricted step, maximum step length along DC which does
					'              not violate constraints
					' * FullStp -  step length along DC which minimizes quadratic function
					'              without taking constraints into account. If problem is
					'              unbounded from below without constraints, FullStp is
					'              forced to be RestStp.
					'
					' So, if function is convex (D2>0):
					' * FullStp = -D1/(2*D2)
					' * RestStp = restricted FullStp
					' * 0<=RestStp<=FullStp
					'
					' If function is non-convex, but bounded from below under constraints:
					' * RestStp = step length subject to constraints
					' * FullStp = RestStp
					'
					' After RestStp and FullStp are initialized, we generate several trial
					' steps which are different multiples of RestStp and FullStp.
					'
					If d2est > 0 Then
						alglib.ap.assert(CDbl(d1) < CDbl(0), "QQPOptimize: internal error")
						fullstp = -(d1 / (2 * d2))
						needact = CDbl(fullstp) >= CDbl(stpmax)
						If needact Then
							alglib.ap.assert(alglib.ap.len(sstate.stpbuf) >= 3, "QQPOptimize: StpBuf overflow")
							reststp = stpmax
							stp = reststp
							sstate.stpbuf(0) = reststp * 4
							sstate.stpbuf(1) = fullstp
							sstate.stpbuf(2) = fullstp / 4
							stpcnt = 3
						Else
							reststp = fullstp
							stp = fullstp
							stpcnt = 0
						End If
					Else
						alglib.ap.assert(cidx >= 0, "QQPOptimize: internal error")
						alglib.ap.assert(alglib.ap.len(sstate.stpbuf) >= 2, "QQPOptimize: StpBuf overflow")
						reststp = stpmax
						fullstp = stpmax
						stp = reststp
						needact = True
						sstate.stpbuf(0) = 4 * reststp
						stpcnt = 1
					End If
					findbeststepandmove(sstate, sstate.sas, sstate.dc, stp, needact, cidx, _
						cval, sstate.stpbuf, stpcnt, sstate.activated, sstate.tmp0)

					'
					' Update CG information.
					'
					For i_ = 0 To n - 1
						sstate.dp(i_) = sstate.dc(i_)
					Next
					For i_ = 0 To n - 1
						sstate.cgp(i_) = sstate.cgc(i_)
					Next

					'
					' Update iterations counter
					'
					sstate.repinneriterationscount = sstate.repinneriterationscount + 1
				Next
				If terminationtype <> 0 Then
					Exit While
				End If
				cgmax = settings.cgmaxits

				'
				' Generate YIdx - reordering of variables for constrained Newton phase.
				' Free variables come first, fixed are last ones.
				'
				newtcnt = 0
				While True

					'
					' Skip iteration if constrained Newton is turned off.
					'
					If Not settings.cnphase Then
						Exit While
					End If

					'
					' At the first iteration   - build Cholesky decomposition of Hessian.
					' At subsequent iterations - refine Hessian by adding new constraints.
					'
					' Loop is terminated in following cases:
					' * Hessian is not positive definite subject to current constraints
					'   (termination during initial decomposition)
					' * there were no new constraints being activated
					'   (termination during update)
					' * all constraints were activated during last step
					'   (termination during update)
					' * CNMaxUpdates were performed on matrix
					'   (termination during update)
					'
					If newtcnt = 0 Then

						'
						' Perform initial Newton step. If Cholesky decomposition fails,
						' increase number of CG iterations to CGMaxIts - it should help
						' us to find set of constraints which will make matrix positive
						' definite.
						'
						b = cnewtonbuild(sstate, sparsesolver, sstate.repncholesky)
						If b Then
							cgmax = settings.cgminits
						End If
					Else
						b = cnewtonupdate(sstate, settings, sstate.repncupdates)
					End If
					If Not b Then
						Exit While
					End If
					apserv.inc(newtcnt)

					'
					' Calculate gradient GC.
					'
					targetgradient(sstate, sstate.sas.xc, sstate.gc)

					'
					' Bound-constrained Newton step
					'
					For i = 0 To n - 1
						sstate.dc(i) = sstate.gc(i)
					Next
					If Not cnewtonstep(sstate, settings, sstate.dc) Then
						Exit While
					End If
					quadraticmodel(sstate, sstate.sas.xc, sstate.dc, sstate.gc, d1, d1est, _
						d2, d2est)
					If d1est >= 0 OrElse d2est <= 0 Then
						Exit While
					End If
					alglib.ap.assert(CDbl(d1) < CDbl(0), "QQPOptimize: internal error")
					fullstp = -(d1 / (2 * d2))
					sactivesets.sasexploredirection(sstate.sas, sstate.dc, stpmax, cidx, cval)
					needact = CDbl(fullstp) >= CDbl(stpmax)
					If needact Then
						alglib.ap.assert(alglib.ap.len(sstate.stpbuf) >= 3, "QQPOptimize: StpBuf overflow")
						reststp = stpmax
						stp = reststp
						sstate.stpbuf(0) = reststp * 4
						sstate.stpbuf(1) = fullstp
						sstate.stpbuf(2) = fullstp / 4
						stpcnt = 3
					Else
						reststp = fullstp
						stp = fullstp
						stpcnt = 0
					End If
					findbeststepandmove(sstate, sstate.sas, sstate.dc, stp, needact, cidx, _
						cval, sstate.stpbuf, stpcnt, sstate.activated, sstate.tmp0)
				End While
				If terminationtype <> 0 Then
					Exit While
				End If
			End While

			'
			' Stop optimization and unpack results.
			'
			' Add XOriginC to XS and make sure that boundary constraints are
			' both (a) satisfied, (b) preserved. Former means that "shifted"
			' point is feasible, while latter means that point which was exactly
			' at the boundary before shift will be exactly at the boundary
			' after shift.
			'
			sactivesets.sasstopoptimization(sstate.sas)
			For i = 0 To nmain - 1
				xs(i) = sc(i) * sstate.sas.xc(i) + xoriginc(i)
				If sstate.havebndl(i) AndAlso CDbl(xs(i)) < CDbl(bndlc(i)) Then
					xs(i) = bndlc(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(xs(i)) > CDbl(bnduc(i)) Then
					xs(i) = bnduc(i)
				End If
				If sstate.havebndl(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndl(i)) Then
					xs(i) = bndlc(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndu(i)) Then
					xs(i) = bnduc(i)
				End If
			Next
		End Sub


		'************************************************************************
'        Target function at point PROJ(X+Stp*D), where PROJ(.) is a projection into
'        feasible set.
'
'        NOTE: if Stp=0, D is not referenced at all. Thus,  there  is  no  need  to
'              fill it by some meaningful values for Stp=0.
'
'        This subroutine uses temporary buffer Tmp, which is automatically  resized
'        if needed.
'
'          -- ALGLIB --
'             Copyright 21.12.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function projectedtargetfunction(sstate As qqpbuffers, x As Double(), d As Double(), stp As Double, ByRef tmp0 As Double()) As Double
			Dim result As Double = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim n As Integer = 0
			Dim nmain As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim i_ As Integer = 0

			n = sstate.n
			nmain = sstate.nmain
			nec = sstate.nec
			nic = sstate.nic
			apserv.rvectorsetlengthatleast(tmp0, n)

			'
			' Calculate projected point
			'
			For i = 0 To n - 1
				If CDbl(stp) <> CDbl(0) Then
					v = x(i) + stp * d(i)
				Else
					v = x(i)
				End If
				If sstate.havebndl(i) AndAlso CDbl(v) < CDbl(sstate.bndl(i)) Then
					v = sstate.bndl(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(v) > CDbl(sstate.bndu(i)) Then
					v = sstate.bndu(i)
				End If
				tmp0(i) = v
			Next

			'
			' Function value at the Tmp0:
			'
			' f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
			'
			result = 0.0
			For i = 0 To nmain - 1
				result = result + sstate.b(i) * tmp0(i)
			Next
			If sstate.akind = 0 Then

				'
				' Dense matrix A
				'
				For i = 0 To nmain - 1
					v = tmp0(i)
					result = result + 0.5 * v * v * sstate.densea(i, i)
					vv = 0.0
					For j = i + 1 To nmain - 1
						vv = vv + sstate.densea(i, j) * tmp0(j)
					Next
					result = result + v * vv
				Next
			Else

				'
				' sparse matrix A
				'
				alglib.ap.assert(sstate.akind = 1, "QQPOptimize: unexpected AKind in ProjectedTargetFunction")
				result = result + 0.5 * sparse.sparsevsmv(sstate.sparsea, sstate.sparseupper, tmp0)
			End If
			For i = 0 To nec + nic - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += sstate.cleic(i, i_) * tmp0(i_)
				Next
				result = result + 0.5 * penaltyfactor * Math.sqr(v - sstate.cleic(i, n))
			Next
			Return result
		End Function


		'************************************************************************
'        Gradient of "extended" (N>=NMain variables, original + slack ones)  target
'        function:
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        which is equal to
'
'            grad = A*x + b + penaltyfactor*C'*(C*x-b)
'
'        Here:
'        * x is array[N] (that's why problem is called extended - N>=NMain)
'        * A is array[NMain,NMain]
'        * b is array[NMain]
'        * C is array[NEC+NIC,N]
'
'        INPUT PARAMETERS:
'            SState  -   structure which stores function terms (not modified)
'            X       -   location
'            G       -   possibly preallocated buffer
'
'        OUTPUT PARAMETERS:
'            G       -   array[N], gradient
'                    
'          -- ALGLIB --
'             Copyright 21.12.2013 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub targetgradient(sstate As qqpbuffers, x As Double(), ByRef g As Double())
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim n As Integer = 0
			Dim nmain As Integer = 0
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			n = sstate.n
			nmain = sstate.nmain
			nec = sstate.nec
			nic = sstate.nic
			apserv.rvectorsetlengthatleast(g, n)
			If sstate.akind = 0 Then

				'
				' Dense matrix A
				'
				ablas.rmatrixmv(nmain, nmain, sstate.densea, 0, 0, 0, _
					x, 0, g, 0)
			Else

				'
				' Sparse matrix A
				'
				alglib.ap.assert(sstate.akind = 1, "QQPOptimize: unexpected AKind in TargetGradient")
				sparse.sparsesmv(sstate.sparsea, sstate.sparseupper, x, g)
			End If
			For i_ = 0 To nmain - 1
				g(i_) = g(i_) + sstate.b(i_)
			Next
			For i = nmain To n - 1
				g(i) = 0.0
			Next
			For i = 0 To nec + nic - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += sstate.cleic(i, i_) * x(i_)
				Next
				v = v - sstate.cleic(i, n)
				v = v * penaltyfactor
				For i_ = 0 To n - 1
					g(i_) = g(i_) + v * sstate.cleic(i, i_)
				Next
			Next
		End Sub


		'************************************************************************
'        First and second derivatives  of  the  "extended"  target  function  along
'        specified direction. Target  function  is  called  "extended"  because  of
'        additional slack variables and has form:
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        with gradient
'
'            grad = A*x + b + penaltyfactor*C'*(C*x-b)
'            
'        Quadratic model has form
'
'            F(x0+alpha*D) = D2*alpha^2 + D1*alpha
'
'        INPUT PARAMETERS:
'            SState  -   structure which is used to obtain quadratic term of the model
'            X       -   current point, array[N]
'            D       -   direction across which derivatives are calculated, array[N]
'            G       -   gradient at current point (pre-calculated by caller), array[N]
'
'        OUTPUT PARAMETERS:
'            D1      -   linear coefficient
'            D1Est   -   estimate of D1 sign,  accounting  for  possible  numerical
'                        errors:
'                        * >0    means "almost surely positive"
'                        * <0    means "almost surely negative"
'                        * =0    means "pessimistic estimate  of  numerical  errors
'                                in D1 is larger than magnitude of D1 itself; it is
'                                impossible to reliably distinguish D1 from zero".
'            D2      -   quadratic coefficient
'            D2Est   -   estimate of D2 sign,  accounting  for  possible  numerical
'                        errors:
'                        * >0    means "almost surely positive"
'                        * <0    means "almost surely negative"
'                        * =0    means "pessimistic estimate  of  numerical  errors
'                                in D2 is larger than magnitude of D2 itself; it is
'                                impossible to reliably distinguish D2 from zero".
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub quadraticmodel(sstate As qqpbuffers, x As Double(), d As Double(), g As Double(), ByRef d1 As Double, ByRef d1est As Integer, _
			ByRef d2 As Double, ByRef d2est As Integer)
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim n As Integer = 0
			Dim nmain As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim mx As Double = 0
			Dim mb As Double = 0
			Dim md As Double = 0
			Dim i_ As Integer = 0

			d1 = 0
			d1est = 0
			d2 = 0
			d2est = 0

			n = sstate.n
			nmain = sstate.nmain
			nec = sstate.nec
			nic = sstate.nic

			'
			' Maximums
			'
			mx = 0.0
			md = 0.0
			mb = 0.0
			For i = 0 To n - 1
				mx = System.Math.Max(mx, System.Math.Abs(x(i)))
				md = System.Math.Max(md, System.Math.Abs(d(i)))
			Next
			For i = 0 To nmain - 1
				mb = System.Math.Max(mb, System.Math.Abs(sstate.b(i)))
			Next

			'
			' D2
			'
			If sstate.akind = 0 Then

				'
				' Dense matrix A
				'
				d2 = 0.0
				For i = 0 To nmain - 1
					v = d(i)
					vv = 0.0
					d2 = d2 + 0.5 * v * v * sstate.densea(i, i)
					For j = i + 1 To nmain - 1
						vv = vv + sstate.densea(i, j) * d(j)
					Next
					d2 = d2 + v * vv
				Next
			Else

				'
				' Sparse matrix A
				'
				alglib.ap.assert(sstate.akind = 1, "QQPOptimize: unexpected AKind in TargetGradient")
				d2 = 0.5 * sparse.sparsevsmv(sstate.sparsea, sstate.sparseupper, d)
			End If
			For i = 0 To nec + nic - 1

				'
				' NOTE: there is no "V:=V-CLEIC[I,N]" line, and it is not an error!
				'       We estimate curvature information here, which is not dependent
				'       on right part.
				'
				v = 0.0
				For i_ = 0 To n - 1
					v += sstate.cleic(i, i_) * d(i_)
				Next
				d2 = d2 + v * v * penaltyfactor * 0.5
			Next
			v = 0.0
			For i_ = 0 To n - 1
				v += d(i_) * g(i_)
			Next
			d1 = v

			'
			' Error estimates
			'
			optserv.estimateparabolicmodel(sstate.absasum, sstate.absasum2, mx, mb, md, d1, _
				d2, d1est, d2est)
		End Sub


		'************************************************************************
'        This function accepts quadratic model of the form
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        and list of possible steps along direction D. It chooses  best  step  (one
'        which achieves minimum value of the target  function)  and  moves  current
'        point (given by SAS object) to the new location. Step is  bounded  subject
'        to boundary constraints.
'
'        Candidate steps are divided into two groups:
'        * "default" step, which is always performed when no candidate steps LONGER
'          THAN THE DEFAULT  ONE  is  given.  This  candidate  MUST  reduce  target
'          function value; it is  responsibility  of  caller  to   provide  default
'          candidate which reduces target function.
'        * "additional candidates", which may be shorter or longer than the default
'          step. Candidates which are shorter that the default  step  are  ignored;
'          candidates which are longer than the "default" step are tested.
'          
'        The idea is that we ALWAYS try "default" step, and it is responsibility of
'        the caller to provide us with something which is worth trying.  This  step
'        may activate some constraint - that's why we  stopped  at  "default"  step
'        size. However, we may also try longer steps which may activate  additional
'        constraints and further reduce function value.
'
'        INPUT PARAMETERS:
'            SState  -   structure which stores model
'            SAS     -   active set structure which stores current point in SAS.XC
'            D       -   direction for step
'            Stp     -   step length for "default" candidate
'            NeedAct -   whether   default  candidate  activates  some  constraint;
'                        if NeedAct  is True,  constraint  given  by  CIdc/CVal  is
'                        GUARANTEED to be activated in the final point.
'            CIdx    -   if NeedAct is True, stores index of the constraint to activate
'            CVal    -   if NeedAct is True, stores constrained value;
'                        SAS.XC[CIdx] is forced to be equal to CVal.
'            AddSteps-   array[AddStepsCnt] of additional steps:
'                        * AddSteps[]<=Stp are ignored
'                        * AddSteps[]>Stp are tried
'            Activated-  possibly preallocated buffer; previously allocated memory
'                        will be reused.
'            Tmp0    -  possibly preallocated buffer; previously allocated memory
'                        will be reused.
'            
'        OUTPUT PARAMETERS:
'            SAS     -   SAS.XC is set to new point;  if  there  was  a  constraint
'                        specified  by  NeedAct/CIdx/CVal,  it  will  be  activated
'                        (other constraints may be activated too, but this  one  is
'                        guaranteed to be active in the final point).
'            Activated-  elements of this array are set to True, if I-th constraint
'                        as inactive at previous point, but become  active  in  the
'                        new one.
'                        Situations when we deactivate xi>=0 and activate xi<=1 are
'                        considered as activation of previously inactive constraint
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub findbeststepandmove(sstate As qqpbuffers, sas As sactivesets.sactiveset, d As Double(), stp As Double, needact As Boolean, cidx As Integer, _
			cval As Double, addsteps As Double(), addstepscnt As Integer, ByRef activated As Boolean(), ByRef tmp0 As Double())
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim stpbest As Double = 0
			Dim fbest As Double = 0
			Dim fcand As Double = 0

			n = sstate.n
			apserv.rvectorsetlengthatleast(tmp0, n)
			apserv.bvectorsetlengthatleast(activated, n)

			'
			' Calculate initial step, store to Tmp0
			'
			' NOTE: Tmp0 is guaranteed to be feasible w.r.t. boundary constraints
			'
			For i = 0 To n - 1
				v = sas.xc(i) + stp * d(i)
				If sstate.havebndl(i) AndAlso CDbl(v) < CDbl(sstate.bndl(i)) Then
					v = sstate.bndl(i)
				End If
				If sstate.havebndu(i) AndAlso CDbl(v) > CDbl(sstate.bndu(i)) Then
					v = sstate.bndu(i)
				End If
				tmp0(i) = v
			Next
			If needact Then
				tmp0(cidx) = cval
			End If

			'
			' Try additional steps, if AddStepsCnt>0
			'
			If addstepscnt > 0 Then

				'
				' Find best step
				'
				stpbest = stp
				fbest = projectedtargetfunction(sstate, sas.xc, d, stpbest, tmp0)
				For k = 0 To addstepscnt - 1
					If CDbl(addsteps(k)) > CDbl(stp) Then
						fcand = projectedtargetfunction(sstate, sas.xc, d, addsteps(k), tmp0)
						If CDbl(fcand) < CDbl(fbest) Then
							fbest = fcand
							stpbest = addsteps(k)
						End If
					End If
				Next

				'
				' Prepare best step
				'
				' NOTE: because only AddSteps[]>Stp were checked,
				'       this step will activate constraint CIdx.
				'
				For i = 0 To n - 1
					v = sas.xc(i) + stpbest * d(i)
					If sstate.havebndl(i) AndAlso CDbl(v) < CDbl(sstate.bndl(i)) Then
						v = sstate.bndl(i)
					End If
					If sstate.havebndu(i) AndAlso CDbl(v) > CDbl(sstate.bndu(i)) Then
						v = sstate.bndu(i)
					End If
					tmp0(i) = v
				Next
				If needact Then
					tmp0(cidx) = cval
				End If
			End If

			'
			' Fill Activated array by information about activated constraints.
			' Perform step
			'
			For i = 0 To n - 1
				activated(i) = False
				v = tmp0(i)
				If CDbl(v) = CDbl(sas.xc(i)) Then
					Continue For
				End If
				If sstate.havebndl(i) AndAlso CDbl(v) = CDbl(sstate.bndl(i)) Then
					activated(i) = True
				End If
				If sstate.havebndu(i) AndAlso CDbl(v) = CDbl(sstate.bndu(i)) Then
					activated(i) = True
				End If
			Next
			sactivesets.sasmoveto(sas, tmp0, needact, cidx, cval)
		End Sub


		'************************************************************************
'        This function prepares data for  constrained  Newton  step  for  penalized
'        quadratic model of the form
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        where A can be dense or sparse, and model is considered subject to equality
'        constraints specified by SState.SAS.XC  object.  Constraint  is considered
'        active if XC[i] is exactly BndL[i] or BndU[i],  i.e.  we  ignore  internal
'        list of constraints monitored by SAS object. Our own  set  of  constraints
'        includes all  constraints  stored  by  SAS,  but  also  may  include  some
'        constraints which are inactive in SAS.
'
'        "Preparation" means that Cholesky decomposition of  the  effective  system
'        matrix is performed, and we can  perform  constrained  Newton  step.
'
'        This function works as black box. It uses fields of SState which are marked
'        as "Variables for constrained Newton phase", and only  this  function  and
'        its friends know about these variables. Everyone else should use:
'        * CNewtonBuild() to prepare initial Cholesky decomposition for step
'        * CNewtonStep() to perform constrained Newton step
'        * CNewtonUpdate() to update Cholesky matrix  after  point  was  moved  and
'          constraints were updated. In some cases it  is  possible to  efficiently
'          re-calculate Cholesky decomposition if you know which  constraints  were
'          activated. If efficient  re-calculation  is  impossible,  this  function
'          returns False.
'
'        INPUT PARAMETERS:
'            SState  -   structure which stores model and temporaries for CN phase;
'                        in particular, SAS.XC stores current point.
'            SparseSolver-which sparse solver to use for sparse model; ignored  for
'                        dense QP. Can be:
'                        * 2 -   SKS-based Cholesky
'            NCholesky-  counter which is incremented after Cholesky (successful or
'                        failed one)
'            
'        OUTPUT PARAMETERS:
'            NCholesky-  possibly updated counter
'            
'        RESULT:
'            True, if Cholesky decomposition was successfully performed.
'            False, if a) matrix was semi-definite or indefinite, or b)  particular
'            combination of matrix type (sparse) and constraints  (general  linear)
'            is not supported.
'            
'        NOTE: this function may routinely return False, for indefinite matrices or
'              for sparse problems with general linear constraints. You  should  be
'              able to handle such situations.
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function cnewtonbuild(sstate As qqpbuffers, sparsesolver As Integer, ByRef ncholesky As Integer) As Boolean
			Dim result As New Boolean()
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim n As Integer = 0
			Dim nmain As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim k As Integer = 0
			Dim v As Double = 0
			Dim b As New Boolean()
			Dim ridx0 As Integer = 0
			Dim ridx1 As Integer = 0
			Dim nfree As Integer = 0
			Dim i_ As Integer = 0

			result = False

			'
			' Fetch often used fields
			'
			n = sstate.n
			nmain = sstate.nmain
			nec = sstate.nec
			nic = sstate.nic

			'
			' Check problem properties.
			' Sparse problems with general linear constraints are not supported yet.
			'
			If sstate.akind = 1 AndAlso nec + nic > 0 Then
				Return result
			End If

			'
			' 1. Set CNModelAge to zero
			' 2. Generate YIdx - reordering of variables such that free variables
			'    come first and are ordered by ascending, fixed are last ones and
			'    have no particular ordering.
			'
			' This step is same for dense and sparse problems.
			'
			sstate.cnmodelage = 0
			apserv.ivectorsetlengthatleast(sstate.yidx, n)
			ridx0 = 0
			ridx1 = n - 1
			For i = 0 To n - 1
				sstate.yidx(i) = -1
			Next
			For i = 0 To n - 1
				alglib.ap.assert(Not sstate.havebndl(i) OrElse CDbl(sstate.sas.xc(i)) >= CDbl(sstate.bndl(i)), "CNewtonBuild: internal error")
				alglib.ap.assert(Not sstate.havebndu(i) OrElse CDbl(sstate.sas.xc(i)) <= CDbl(sstate.bndu(i)), "CNewtonBuild: internal error")
				b = False
				b = b OrElse (sstate.havebndl(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndl(i)))
				b = b OrElse (sstate.havebndu(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndu(i)))
				If b Then
					sstate.yidx(ridx1) = i
					ridx1 = ridx1 - 1
				Else
					sstate.yidx(ridx0) = i
					ridx0 = ridx0 + 1
				End If
			Next
			alglib.ap.assert(ridx0 = ridx1 + 1, "CNewtonBuild: internal error")
			nfree = ridx0
			sstate.nfree = nfree
			If nfree = 0 Then
				Return result
			End If

			'
			' Constrained Newton matrix: dense version
			'
			If sstate.akind = 0 Then
				apserv.rmatrixsetlengthatleast(sstate.densez, n, n)
				apserv.rvectorsetlengthatleast(sstate.tmpcn, n)
				If nec + nic > 0 Then

					'
					' Initialize Z with C'*C, add A
					'
					ablas.rmatrixsyrk(n, nec + nic, penaltyfactor, sstate.cleic, 0, 0, _
						2, 0.0, sstate.densez, 0, 0, True)
					For i = 0 To nmain - 1
						For j = i To nmain - 1
							sstate.densez(i, j) = sstate.densez(i, j) + sstate.densea(i, j)
						Next
					Next
				Else

					'
					' No linear constraints, just set Z to A
					'
					alglib.ap.assert(n = nmain, "CNewtonBuild: integrity check failed")
					For i = 0 To nmain - 1
						For j = i To nmain - 1
							sstate.densez(i, j) = sstate.densea(i, j)
						Next
					Next
				End If
				For i = 1 To nfree - 1
					alglib.ap.assert(sstate.yidx(i) > sstate.yidx(i - 1), "CNewtonBuild: integrity check failed")
				Next
				For i = 0 To nfree - 1
					k = sstate.yidx(i)
					For j = i To nfree - 1
						sstate.densez(i, j) = sstate.densez(k, sstate.yidx(j))
					Next
				Next
				apserv.rvectorsetlengthatleast(sstate.regdiag, n)
				For i = 0 To nfree - 1
					v = 0.0
					For j = 0 To i - 1
						v = v + System.Math.Abs(sstate.densez(j, i))
					Next
					For j = i To nfree - 1
						v = v + System.Math.Abs(sstate.densez(i, j))
					Next
					If CDbl(v) = CDbl(0) Then
						v = 1.0
					End If
					sstate.regdiag(i) = regz * v
				Next
				For i = 0 To nfree - 1
					sstate.densez(i, i) = sstate.densez(i, i) + sstate.regdiag(i)
				Next
				apserv.inc(ncholesky)
				If Not trfac.spdmatrixcholeskyrec(sstate.densez, 0, nfree, True, sstate.tmpcn) Then
					Return result
				End If
				For i = nfree - 1 To 0 Step -1
					For i_ = i To nfree - 1
						sstate.tmpcn(i_) = sstate.densez(i, i_)
					Next
					k = sstate.yidx(i)
					For j = k To n - 1
						sstate.densez(k, j) = 0
					Next
					For j = i To nfree - 1
						sstate.densez(k, sstate.yidx(j)) = sstate.tmpcn(j)
					Next
				Next
				For i = nfree To n - 1
					k = sstate.yidx(i)
					sstate.densez(k, k) = 1.0
					For j = k + 1 To n - 1
						sstate.densez(k, j) = 0
					Next
				Next
				result = True
				Return result
			End If

			'
			' Constrained Newton matrix: sparse version
			'
			If sstate.akind = 1 Then
				alglib.ap.assert(nec + nic = 0, "CNewtonBuild: internal error")
				alglib.ap.assert(sparsesolver = 2, "CNewtonBuild: internal error")

				'
				' Copy sparse A to Z and fill rows/columns corresponding to active
				' constraints by zeros. Diagonal elements corresponding to active
				' constraints are filled by unit values.
				'
				sparse.sparsecopytosksbuf(sstate.sparsea, sstate.sparsecca)
				apserv.rvectorsetlengthatleast(sstate.tmpcn, n)
				For i = 0 To n - 1
					sstate.tmpcn(i) = 0
				Next
				For i = nfree To n - 1
					sstate.tmpcn(sstate.yidx(i)) = 1
				Next
				For i = 0 To n - 1
					k = sstate.sparsecca.ridx(i)
					For j = i - sstate.sparsecca.didx(i) To i
						If CDbl(sstate.tmpcn(i)) <> CDbl(0) OrElse CDbl(sstate.tmpcn(j)) <> CDbl(0) Then

							'
							' I-th or J-th variable is in active set (constrained)
							'
							If i = j Then
								sstate.sparsecca.vals(k) = 1.0
							Else
								sstate.sparsecca.vals(k) = 0.0
							End If
						End If
						k = k + 1
					Next
				Next

				'
				' Perform sparse Cholesky
				'
				apserv.inc(ncholesky)
				If Not trfac.sparsecholeskyskyline(sstate.sparsecca, nmain, sstate.sparseupper) Then
					Return result
				End If
				result = True
				Return result
			End If

			'
			' Unexpected :)
			'
			alglib.ap.assert(False, "CNewtonBuild: internal error")
			Return result
		End Function


		'************************************************************************
'        This   function  updates  equality-constrained   Cholesky   matrix   after
'        activation of the  new  equality  constraints.  Matrix  being  updated  is
'        quadratic term of the function below
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        where A can be dense or sparse.
'
'        This  function  uses  YIdx[]  array  (set by CNewtonBuild()  function)  to
'        distinguish between active and inactive constraints.
'
'        This function works as black box. It uses fields of SState which are marked
'        as "Variables for constrained Newton phase", and only  this  function  and
'        its friends know about these variables. Everyone else should use:
'        * CNewtonBuild() to prepare initial Cholesky decomposition for step
'        * CNewtonStep() to perform constrained Newton step
'        * CNewtonUpdate() to update Cholesky matrix  after  point  was  moved  and
'          constraints were updated. In some cases it  is  possible to  efficiently
'          re-calculate Cholesky decomposition if you know which  constraints  were
'          activated. If efficient  re-calculation  is  impossible,  this  function
'          returns False.
'
'        INPUT PARAMETERS:
'            SState  -   structure which stores model and temporaries for CN phase;
'                        in particular, SAS.XC stores current point.
'            Settings -  QQPSettings object which was  initialized  by  appropriate
'                        construction function.
'            NCUpdates-  counter which is incremented after each update (one update
'                        means one variable being fixed)
'            
'        OUTPUT PARAMETERS:
'            NCUpdates-  possibly updated counter
'            
'        RESULT:
'            True, if Cholesky decomposition was successfully performed.
'            False, if a) model age was too high, or b) particular  combination  of
'            matrix type (sparse) and constraints (general linear) is not supported
'            
'        NOTE: this function may routinely return False.
'              You should be able to handle such situations.
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function cnewtonupdate(sstate As qqpbuffers, settings As qqpsettings, ByRef ncupdates As Integer) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim ntofix As Integer = 0
			Dim b As New Boolean()
			Dim ridx0 As Integer = 0
			Dim ridx1 As Integer = 0
			Dim i As Integer = 0
			Dim k As Integer = 0

			result = False

			'
			' Cholesky updates for sparse problems are not supported
			'
			If sstate.akind = 1 Then
				Return result
			End If

			'
			' Fetch often used fields
			'
			n = sstate.n
			nfree = sstate.nfree

			'
			' Determine variables to fix and move them to YIdx[NFree-NToFix:NFree-1]
			' Exit if CNModelAge increased too much.
			'
			apserv.ivectorsetlengthatleast(sstate.tmpcni, n)
			ridx0 = 0
			ridx1 = nfree - 1
			For i = 0 To nfree - 1
				sstate.tmpcni(i) = -1
			Next
			For k = 0 To nfree - 1
				i = sstate.yidx(k)
				alglib.ap.assert(Not sstate.havebndl(i) OrElse CDbl(sstate.sas.xc(i)) >= CDbl(sstate.bndl(i)), "CNewtonUpdate: internal error")
				alglib.ap.assert(Not sstate.havebndu(i) OrElse CDbl(sstate.sas.xc(i)) <= CDbl(sstate.bndu(i)), "CNewtonUpdate: internal error")
				b = False
				b = b OrElse (sstate.havebndl(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndl(i)))
				b = b OrElse (sstate.havebndu(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.bndu(i)))
				If b Then
					sstate.tmpcni(ridx1) = i
					ridx1 = ridx1 - 1
				Else
					sstate.tmpcni(ridx0) = i
					ridx0 = ridx0 + 1
				End If
			Next
			alglib.ap.assert(ridx0 = ridx1 + 1, "CNewtonUpdate: internal error")
			ntofix = nfree - ridx0
			If ntofix = 0 OrElse ntofix = nfree Then
				Return result
			End If
			If sstate.cnmodelage + ntofix > settings.cnmaxupdates Then
				Return result
			End If
			For i = 0 To nfree - 1
				sstate.yidx(i) = sstate.tmpcni(i)
			Next

			'
			' Constrained Newton matrix: dense version.
			'
			If sstate.akind = 0 Then

				'
				' Update Cholesky matrix with SPDMatrixCholeskyUpdateFixBuf()
				'
				apserv.bvectorsetlengthatleast(sstate.tmpcnb, n)
				For i = 0 To n - 1
					sstate.tmpcnb(i) = False
				Next
				For i = nfree - ntofix To nfree - 1
					sstate.tmpcnb(sstate.yidx(i)) = True
				Next
				trfac.spdmatrixcholeskyupdatefixbuf(sstate.densez, n, True, sstate.tmpcnb, sstate.tmpcn)

				'
				' Update information stored in State and exit
				'
				sstate.nfree = nfree - ntofix
				sstate.cnmodelage = sstate.cnmodelage + ntofix
				ncupdates = ncupdates + ntofix
				result = True
				Return result
			End If

			'
			' Unexpected :)
			'
			alglib.ap.assert(False, "CNewtonUpdate: internal error")
			Return result
		End Function


		'************************************************************************
'        This   function prepares equality-constrained Newton step using previously
'        calculated constrained Cholesky matrix of the problem
'
'            f(x) = 0.5*x'*A*x + b'*x + penaltyfactor*0.5*(C*x-b)'*(C*x-b)
'            
'        where A can be dense or sparse.
'
'        As  input,  this  function  accepts  gradient  at the current location. As
'        output, it returns step vector (replaces gradient).
'
'        This function works as black box. It uses fields of SState which are marked
'        as "Variables for constrained Newton phase", and only  this  function  and
'        its friends know about these variables. Everyone else should use:
'        * CNewtonBuild() to prepare initial Cholesky decomposition for step
'        * CNewtonStep() to perform constrained Newton step
'        * CNewtonUpdate() to update Cholesky matrix  after  point  was  moved  and
'          constraints were updated. In some cases it  is  possible to  efficiently
'          re-calculate Cholesky decomposition if you know which  constraints  were
'          activated. If efficient  re-calculation  is  impossible,  this  function
'          returns False.
'
'        INPUT PARAMETERS:
'            SState  -   structure which stores model and temporaries for CN phase;
'                        in particular, SAS.XC stores current point.
'            Settings -  QQPSettings object which was  initialized  by  appropriate
'                        construction function.
'            GC       -  array[NMain+NSlack], gradient of the target function
'            
'        OUTPUT PARAMETERS:
'            GC       -  array[NMain+NSlack], step vector (on success)
'            
'        RESULT:
'            True, if step was successfully calculated.
'            False, if step calculation failed:
'            a) gradient was exactly zero,
'            b) gradient norm was smaller than EpsG (stopping condition)
'            c) all variables were equality-constrained
'            
'        NOTE: this function may routinely return False.
'              You should be able to handle such situations.
'                    
'          -- ALGLIB --
'             Copyright 14.05.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function cnewtonstep(sstate As qqpbuffers, settings As qqpsettings, gc As Double()) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim n As Integer = 0
			Dim nfree As Integer = 0
			Dim v As Double = 0
			Dim i_ As Integer = 0

			result = False
			n = sstate.n
			nfree = sstate.nfree
			For i = nfree To n - 1
				gc(sstate.yidx(i)) = 0.0
			Next
			v = 0.0
			For i_ = 0 To n - 1
				v += gc(i_) * gc(i_)
			Next
			If CDbl(System.Math.sqrt(v)) <= CDbl(settings.epsg) Then
				Return result
			End If
			For i = 0 To n - 1
				gc(i) = -gc(i)
			Next
			If sstate.akind = 0 Then

				'
				' Dense Newton step.
				' Use straightforward Cholesky solver.
				'
				fbls.fblscholeskysolve(sstate.densez, 1.0, n, True, gc, sstate.tmpcn)
				result = True
				Return result
			End If
			If sstate.akind = 1 Then

				'
				' Sparse Newton step.
				'
				' We have T*T' = L*L' = U'*U (depending on specific triangle stored in SparseCCA).
				'
				If sstate.sparseupper Then
					sparse.sparsetrsv(sstate.sparsecca, sstate.sparseupper, False, 1, gc)
					sparse.sparsetrsv(sstate.sparsecca, sstate.sparseupper, False, 0, gc)
				Else
					sparse.sparsetrsv(sstate.sparsecca, sstate.sparseupper, False, 0, gc)
					sparse.sparsetrsv(sstate.sparsecca, sstate.sparseupper, False, 1, gc)
				End If
				result = True
				Return result
			End If
			alglib.ap.assert(False, "CNewtonStep: internal error")
			Return result
		End Function


	End Class
	Public Class qpbleicsolver
		'************************************************************************
'        This object stores settings for QPBLEIC solver.
'        It must be initialized with QPBLEICLoadDefaults().
'        After initialization you may change settings.
'        ************************************************************************

		Public Class qpbleicsettings
			Inherits apobject
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New qpbleicsettings()
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				Return _result
			End Function
		End Class


		'************************************************************************
'        This object stores temporaries used by QuickQP solver.
'        ************************************************************************

		Public Class qpbleicbuffers
			Inherits apobject
			Public solver As minbleic.minbleicstate
			Public solverrep As minbleic.minbleicreport
			Public tmp0 As Double()
			Public tmp1 As Double()
			Public tmpi As Integer()
			Public repinneriterationscount As Integer
			Public repouteriterationscount As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				solver = New minbleic.minbleicstate()
				solverrep = New minbleic.minbleicreport()
				tmp0 = New Double(-1) {}
				tmp1 = New Double(-1) {}
				tmpi = New Integer(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New qpbleicbuffers()
				_result.solver = DirectCast(solver.make_copy(), minbleic.minbleicstate)
				_result.solverrep = DirectCast(solverrep.make_copy(), minbleic.minbleicreport)
				_result.tmp0 = DirectCast(tmp0.Clone(), Double())
				_result.tmp1 = DirectCast(tmp1.Clone(), Double())
				_result.tmpi = DirectCast(tmpi.Clone(), Integer())
				_result.repinneriterationscount = repinneriterationscount
				_result.repouteriterationscount = repouteriterationscount
				Return _result
			End Function
		End Class




		'************************************************************************
'        This function initializes QPBLEICSettings structure with default settings.
'
'        Newly created structure MUST be initialized by default settings  -  or  by
'        copy of the already initialized structure.
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qpbleicloaddefaults(nmain As Integer, s As qpbleicsettings)
			s.epsg = 0.0
			s.epsf = 0.0
			s.epsx = 1E-06
			s.maxits = 0
		End Sub


		'************************************************************************
'        This function initializes QPBLEICSettings  structure  with  copy  of  another,
'        already initialized structure.
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qpbleiccopysettings(src As qpbleicsettings, dst As qpbleicsettings)
			dst.epsg = src.epsg
			dst.epsf = src.epsf
			dst.epsx = src.epsx
			dst.maxits = src.maxits
		End Sub


		'************************************************************************
'        This function runs QPBLEIC solver; it returns after optimization   process
'        was completed. Following QP problem is solved:
'
'            min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))
'            
'        subject to boundary constraints.
'
'        INPUT PARAMETERS:
'            AC          -   for dense problems (AKind=0), A-term of CQM object
'                            contains system matrix. Other terms are unspecified
'                            and should not be referenced.
'            SparseAC    -   for sparse problems (AKind=1
'            AKind       -   sparse matrix format:
'                            * 0 for dense matrix
'                            * 1 for sparse matrix
'            SparseUpper -   which triangle of SparseAC stores matrix  -  upper  or
'                            lower one (for dense matrices this  parameter  is  not
'                            actual).
'            AbsASum     -   SUM(|A[i,j]|)
'            AbsASum2    -   SUM(A[i,j]^2)
'            BC          -   linear term, array[NC]
'            BndLC       -   lower bound, array[NC]
'            BndUC       -   upper bound, array[NC]
'            SC          -   scale vector, array[NC]:
'                            * I-th element contains scale of I-th variable,
'                            * SC[I]>0
'            XOriginC    -   origin term, array[NC]. Can be zero.
'            NC          -   number of variables in the  original  formulation  (no
'                            slack variables).
'            CLEICC      -   linear equality/inequality constraints. Present version
'                            of this function does NOT provide  publicly  available
'                            support for linear constraints. This feature  will  be
'                            introduced in the future versions of the function.
'            NEC, NIC    -   number of equality/inequality constraints.
'                            MUST BE ZERO IN THE CURRENT VERSION!!!
'            Settings    -   QPBLEICSettings object initialized by one of the initialization
'                            functions.
'            SState      -   object which stores temporaries:
'                            * if uninitialized object was passed, FirstCall parameter MUST
'                              be set to True; object will be automatically initialized by the
'                              function, and FirstCall will be set to False.
'                            * if FirstCall=False, it is assumed that this parameter was already
'                              initialized by previous call to this function with same
'                              problem dimensions (variable count N).
'            FirstCall   -   whether it is first call of this function for this specific
'                            instance of SState, with this number of variables N specified.
'            XS          -   initial point, array[NC]
'            
'            
'        OUTPUT PARAMETERS:
'            XS          -   last point
'            FirstCall   -   uncondtionally set to False
'            TerminationType-termination type:
'                            *
'                            *
'                            *
'
'          -- ALGLIB --
'             Copyright 14.05.2011 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub qpbleicoptimize(a As cqmodels.convexquadraticmodel, sparsea As sparse.sparsematrix, akind As Integer, sparseaupper As Boolean, absasum As Double, absasum2 As Double, _
			b As Double(), bndl As Double(), bndu As Double(), s As Double(), xorigin As Double(), n As Integer, _
			cleic As Double(,), nec As Integer, nic As Integer, settings As qpbleicsettings, sstate As qpbleicbuffers, ByRef firstcall As Boolean, _
			ByRef xs As Double(), ByRef terminationtype As Integer)
			Dim i As Integer = 0
			Dim d2 As Double = 0
			Dim d1 As Double = 0
			Dim d0 As Double = 0
			Dim v As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim md As Double = 0
			Dim mx As Double = 0
			Dim mb As Double = 0
			Dim d1est As Integer = 0
			Dim d2est As Integer = 0
			Dim i_ As Integer = 0

			terminationtype = 0

			alglib.ap.assert(akind = 0 OrElse akind = 1, "QPBLEICOptimize: unexpected AKind")
			sstate.repinneriterationscount = 0
			sstate.repouteriterationscount = 0
			terminationtype = 0

			'
			' Prepare solver object, if needed
			'
			If firstcall Then
				minbleic.minbleiccreate(n, xs, sstate.solver)
				firstcall = False
			End If

			'
			' Prepare max(|B|)
			'
			mb = 0.0
			For i = 0 To n - 1
				mb = System.Math.Max(mb, System.Math.Abs(b(i)))
			Next

			'
			' Temporaries
			'
			apserv.ivectorsetlengthatleast(sstate.tmpi, nec + nic)
			apserv.rvectorsetlengthatleast(sstate.tmp0, n)
			apserv.rvectorsetlengthatleast(sstate.tmp1, n)
			For i = 0 To nec - 1
				sstate.tmpi(i) = 0
			Next
			For i = 0 To nic - 1
				sstate.tmpi(nec + i) = -1
			Next
			minbleic.minbleicsetlc(sstate.solver, cleic, sstate.tmpi, nec + nic)
			minbleic.minbleicsetbc(sstate.solver, bndl, bndu)
			minbleic.minbleicsetdrep(sstate.solver, True)
            minbleic.minbleicsetcond(sstate.solver, Math.minrealnumber, 0.0, 0.0, settings.maxits)
            minbleic.minbleicsetscale(sstate.solver, s)
            minbleic.minbleicsetprecscale(sstate.solver)
            minbleic.minbleicrestartfrom(sstate.solver, xs)
            While minbleic.minbleiciteration(sstate.solver)

                '
                ' Line search started
                '
                If sstate.solver.lsstart Then

                    '
                    ' Iteration counters:
                    ' * inner iterations count is increased on every line search
                    ' * outer iterations count is increased only at steepest descent line search
                    '
                    apserv.inc(sstate.repinneriterationscount)
                    If sstate.solver.steepestdescentstep Then
                        apserv.inc(sstate.repouteriterationscount)
                    End If

                    '
                    ' Build quadratic model of F along descent direction:
                    '
                    '     F(x+alpha*d) = D2*alpha^2 + D1*alpha + D0
                    '
                    ' Calculate estimates of linear and quadratic term
                    ' (term magnitude is compared with magnitude of numerical errors)
                    '
                    d0 = sstate.solver.f
                    d1 = 0.0
                    For i_ = 0 To n - 1
                        d1 += sstate.solver.d(i_) * sstate.solver.g(i_)
                    Next
                    d2 = 0
                    If akind = 0 Then
                        d2 = cqmodels.cqmxtadx2(a, sstate.solver.d)
                    End If
                    If akind = 1 Then
                        sparse.sparsesmv(sparsea, sparseaupper, sstate.solver.d, sstate.tmp0)
                        d2 = 0.0
                        For i = 0 To n - 1
                            d2 = d2 + sstate.solver.d(i) * sstate.tmp0(i)
                        Next
                        d2 = 0.5 * d2
                    End If
                    mx = 0.0
                    md = 0.0
                    For i = 0 To n - 1
                        mx = System.Math.Max(mx, System.Math.Abs(sstate.solver.x(i)))
                        md = System.Math.Max(md, System.Math.Abs(sstate.solver.d(i)))
                    Next
                    optserv.estimateparabolicmodel(absasum, absasum2, mx, mb, md, d1, _
                        d2, d1est, d2est)

                    '
                    ' Tests for "normal" convergence.
                    '
                    ' This line search may be started from steepest descent
                    ' stage (stage 2) or from L-BFGS stage (stage 3) of the
                    ' BLEIC algorithm. Depending on stage type, different
                    ' checks are performed.
                    '
                    ' Say, L-BFGS stage is an equality-constrained refinement
                    ' stage of BLEIC. This stage refines current iterate
                    ' under "frozen" equality constraints. We can terminate
                    ' iterations at this stage only when we encounter
                    ' unconstrained direction of negative curvature. In all
                    ' other cases (say, when constrained gradient is zero)
                    ' we should not terminate algorithm because everything may
                    ' change after de-activating presently active constraints.
                    '
                    ' Tests for convergence are performed only at "steepest descent" stage
                    ' of the BLEIC algorithm, and only when function is non-concave
                    ' (D2 is positive or approximately zero) along direction D.
                    '
                    ' NOTE: we do not test iteration count (MaxIts) here, because
                    '       this stopping condition is tested by BLEIC itself.
                    '
                    If sstate.solver.steepestdescentstep AndAlso d2est >= 0 Then
                        If d1est >= 0 Then

                            '
                            ' "Emergency" stopping condition: D is non-descent direction.
                            ' Sometimes it is possible because of numerical noise in the
                            ' target function.
                            '
                            terminationtype = 4
                            For i = 0 To n - 1
                                xs(i) = sstate.solver.x(i)
                            Next
                            Exit While
                        End If
                        If d2est > 0 Then

                            '
                            ' Stopping condition #4 - gradient norm is small:
                            '
                            ' 1. rescale State.Solver.D and State.Solver.G according to
                            '    current scaling, store results to Tmp0 and Tmp1.
                            ' 2. Normalize Tmp0 (scaled direction vector).
                            ' 3. compute directional derivative (in scaled variables),
                            '    which is equal to DOTPRODUCT(Tmp0,Tmp1).
                            '
                            v = 0
                            For i = 0 To n - 1
                                sstate.tmp0(i) = sstate.solver.d(i) / s(i)
                                sstate.tmp1(i) = sstate.solver.g(i) * s(i)
                                v = v + Math.sqr(sstate.tmp0(i))
                            Next
                            alglib.ap.assert(CDbl(v) > CDbl(0), "QPBLEICOptimize: inernal errror (scaled direction is zero)")
                            v = 1 / System.Math.sqrt(v)
                            For i_ = 0 To n - 1
                                sstate.tmp0(i_) = v * sstate.tmp0(i_)
                            Next
                            v = 0.0
                            For i_ = 0 To n - 1
                                v += sstate.tmp0(i_) * sstate.tmp1(i_)
                            Next
                            If CDbl(System.Math.Abs(v)) <= CDbl(settings.epsg) Then
                                terminationtype = 4
                                For i = 0 To n - 1
                                    xs(i) = sstate.solver.x(i)
                                Next
                                Exit While
                            End If

                            '
                            ' Stopping condition #1 - relative function improvement is small:
                            '
                            ' 1. calculate steepest descent step:   V = -D1/(2*D2)
                            ' 2. calculate function change:         V1= D2*V^2 + D1*V
                            ' 3. stop if function change is small enough
                            '
                            v = -(d1 / (2 * d2))
                            v1 = d2 * v * v + d1 * v
                            If CDbl(System.Math.Abs(v1)) <= CDbl(settings.epsf * System.Math.Max(d0, 1.0)) Then
                                terminationtype = 1
                                For i = 0 To n - 1
                                    xs(i) = sstate.solver.x(i)
                                Next
                                Exit While
                            End If

                            '
                            ' Stopping condition #2 - scaled step is small:
                            '
                            ' 1. calculate step multiplier V0 (step itself is D*V0)
                            ' 2. calculate scaled step length V
                            ' 3. stop if step is small enough
                            '
                            v0 = -(d1 / (2 * d2))
                            v = 0
                            For i = 0 To n - 1
                                v = v + Math.sqr(v0 * sstate.solver.d(i) / s(i))
                            Next
                            If CDbl(System.Math.sqrt(v)) <= CDbl(settings.epsx) Then
                                terminationtype = 2
                                For i = 0 To n - 1
                                    xs(i) = sstate.solver.x(i)
                                Next
                                Exit While
                            End If
                        End If
                    End If

                    '
                    ' Test for unconstrained direction of negative curvature
                    '
                    If (d2est < 0 OrElse (d2est = 0 AndAlso d1est < 0)) AndAlso Not sstate.solver.boundedstep Then

                        '
                        ' Function is unbounded from below:
                        ' * function will decrease along D, i.e. either:
                        '   * D2<0
                        '   * D2=0 and D1<0
                        ' * step is unconstrained
                        '
                        ' If these conditions are true, we abnormally terminate QP
                        ' algorithm with return code -4 (we can do so at any stage
                        ' of BLEIC - whether it is L-BFGS or steepest descent one).
                        '
                        terminationtype = -4
                        For i = 0 To n - 1
                            xs(i) = sstate.solver.x(i)
                        Next
                        Exit While
                    End If

                    '
                    ' Suggest new step (only if D1 is negative far away from zero,
                    ' D2 is positive far away from zero).
                    '
                    If d1est < 0 AndAlso d2est > 0 Then
                        sstate.solver.stp = apserv.safeminposrv(-d1, 2 * d2, sstate.solver.curstpmax)
                    End If
                End If

                '
                ' Gradient evaluation
                '
                If sstate.solver.needfg Then
                    For i = 0 To n - 1
                        sstate.tmp0(i) = sstate.solver.x(i) - xorigin(i)
                    Next
                    If akind = 0 Then
                        cqmodels.cqmadx(a, sstate.tmp0, sstate.tmp1)
                    End If
                    If akind = 1 Then
                        sparse.sparsesmv(sparsea, sparseaupper, sstate.tmp0, sstate.tmp1)
                    End If
                    v0 = 0.0
                    For i_ = 0 To n - 1
                        v0 += sstate.tmp0(i_) * sstate.tmp1(i_)
                    Next
                    v1 = 0.0
                    For i_ = 0 To n - 1
                        v1 += sstate.tmp0(i_) * b(i_)
                    Next
                    sstate.solver.f = 0.5 * v0 + v1
                    For i_ = 0 To n - 1
                        sstate.solver.g(i_) = sstate.tmp1(i_)
                    Next
                    For i_ = 0 To n - 1
                        sstate.solver.g(i_) = sstate.solver.g(i_) + b(i_)
                    Next
                End If
            End While
            If terminationtype = 0 Then

                '
                ' BLEIC optimizer was terminated by one of its inner stopping
                ' conditions. Usually it is iteration counter (if such
                ' stopping condition was specified by user).
                '
                minbleic.minbleicresultsbuf(sstate.solver, xs, sstate.solverrep)
                terminationtype = sstate.solverrep.terminationtype
            Else

                '
                ' BLEIC optimizer was terminated in "emergency" mode by QP
                ' solver.
                '
                ' NOTE: such termination is "emergency" only when viewed from
                '       BLEIC's position. QP solver sees such termination as
                '       routine one, triggered by QP's stopping criteria.
                '
                minbleic.minbleicemergencytermination(sstate.solver)
            End If
        End Sub


    End Class
    Public Class qpcholeskysolver
        '************************************************************************
        '        This object stores settings for QPCholesky solver.
        '        It must be initialized with QPCholeskyLoadDefaults().
        '        After initialization you may change settings.
        '        ************************************************************************

        Public Class qpcholeskysettings
            Inherits apobject
            Public epsg As Double
            Public epsf As Double
            Public epsx As Double
            Public maxits As Integer
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New qpcholeskysettings()
                _result.epsg = epsg
                _result.epsf = epsf
                _result.epsx = epsx
                _result.maxits = maxits
                Return _result
            End Function
        End Class


        '************************************************************************
        '        This object stores temporaries used by QuickQP solver.
        '        ************************************************************************

        Public Class qpcholeskybuffers
            Inherits apobject
            Public sas As sactivesets.sactiveset
            Public pg As Double()
            Public gc As Double()
            Public xs As Double()
            Public xn As Double()
            Public workbndl As Double()
            Public workbndu As Double()
            Public havebndl As Boolean()
            Public havebndu As Boolean()
            Public workcleic As Double(,)
            Public rctmpg As Double()
            Public tmp0 As Double()
            Public tmp1 As Double()
            Public tmpb As Boolean()
            Public repinneriterationscount As Integer
            Public repouteriterationscount As Integer
            Public repncholesky As Integer
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
                sas = New sactivesets.sactiveset()
                pg = New Double(-1) {}
                gc = New Double(-1) {}
                xs = New Double(-1) {}
                xn = New Double(-1) {}
                workbndl = New Double(-1) {}
                workbndu = New Double(-1) {}
                havebndl = New Boolean(-1) {}
                havebndu = New Boolean(-1) {}
                workcleic = New Double(-1, -1) {}
                rctmpg = New Double(-1) {}
                tmp0 = New Double(-1) {}
                tmp1 = New Double(-1) {}
                tmpb = New Boolean(-1) {}
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New qpcholeskybuffers()
                _result.sas = DirectCast(sas.make_copy(), sactivesets.sactiveset)
                _result.pg = DirectCast(pg.Clone(), Double())
                _result.gc = DirectCast(gc.Clone(), Double())
                _result.xs = DirectCast(xs.Clone(), Double())
                _result.xn = DirectCast(xn.Clone(), Double())
                _result.workbndl = DirectCast(workbndl.Clone(), Double())
                _result.workbndu = DirectCast(workbndu.Clone(), Double())
                _result.havebndl = DirectCast(havebndl.Clone(), Boolean())
                _result.havebndu = DirectCast(havebndu.Clone(), Boolean())
                _result.workcleic = DirectCast(workcleic.Clone(), Double(,))
                _result.rctmpg = DirectCast(rctmpg.Clone(), Double())
                _result.tmp0 = DirectCast(tmp0.Clone(), Double())
                _result.tmp1 = DirectCast(tmp1.Clone(), Double())
                _result.tmpb = DirectCast(tmpb.Clone(), Boolean())
                _result.repinneriterationscount = repinneriterationscount
                _result.repouteriterationscount = repouteriterationscount
                _result.repncholesky = repncholesky
                Return _result
            End Function
        End Class




        Public Const maxlagrangeits As Integer = 10
        Public Const maxbadnewtonits As Integer = 7
        Public Const penaltyfactor As Double = 100.0


        '************************************************************************
        '        This function initializes QPCholeskySettings structure with default settings.
        '
        '        Newly created structure MUST be initialized by default settings  -  or  by
        '        copy of the already initialized structure.
        '
        '          -- ALGLIB --
        '             Copyright 14.05.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub qpcholeskyloaddefaults(nmain As Integer, s As qpcholeskysettings)
            s.epsg = 0.0
            s.epsf = 0.0
            s.epsx = 0.000001
            s.maxits = 0
        End Sub


        '************************************************************************
        '        This function initializes QPCholeskySettings  structure  with  copy  of  another,
        '        already initialized structure.
        '
        '          -- ALGLIB --
        '             Copyright 14.05.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub qpcholeskycopysettings(src As qpcholeskysettings, dst As qpcholeskysettings)
            dst.epsg = src.epsg
            dst.epsf = src.epsf
            dst.epsx = src.epsx
            dst.maxits = src.maxits
        End Sub


        '************************************************************************
        '        This function runs QPCholesky solver; it returns after optimization   process
        '        was completed. Following QP problem is solved:
        '
        '            min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))
        '            
        '        subject to boundary constraints.
        '
        '        INPUT PARAMETERS:
        '            AC          -   for dense problems (AKind=0) contains system matrix in
        '                            the A-term of CQM object.  OTHER  TERMS  ARE  ACTIVELY
        '                            USED AND MODIFIED BY THE SOLVER!
        '            SparseAC    -   for sparse problems (AKind=1
        '            AKind       -   sparse matrix format:
        '                            * 0 for dense matrix
        '                            * 1 for sparse matrix
        '            SparseUpper -   which triangle of SparseAC stores matrix  -  upper  or
        '                            lower one (for dense matrices this  parameter  is  not
        '                            actual).
        '            BC          -   linear term, array[NC]
        '            BndLC       -   lower bound, array[NC]
        '            BndUC       -   upper bound, array[NC]
        '            SC          -   scale vector, array[NC]:
        '                            * I-th element contains scale of I-th variable,
        '                            * SC[I]>0
        '            XOriginC    -   origin term, array[NC]. Can be zero.
        '            NC          -   number of variables in the  original  formulation  (no
        '                            slack variables).
        '            CLEICC      -   linear equality/inequality constraints. Present version
        '                            of this function does NOT provide  publicly  available
        '                            support for linear constraints. This feature  will  be
        '                            introduced in the future versions of the function.
        '            NEC, NIC    -   number of equality/inequality constraints.
        '                            MUST BE ZERO IN THE CURRENT VERSION!!!
        '            Settings    -   QPCholeskySettings object initialized by one of the initialization
        '                            functions.
        '            SState      -   object which stores temporaries:
        '                            * if uninitialized object was passed, FirstCall parameter MUST
        '                              be set to True; object will be automatically initialized by the
        '                              function, and FirstCall will be set to False.
        '                            * if FirstCall=False, it is assumed that this parameter was already
        '                              initialized by previous call to this function with same
        '                              problem dimensions (variable count N).
        '            XS          -   initial point, array[NC]
        '            
        '            
        '        OUTPUT PARAMETERS:
        '            XS          -   last point
        '            TerminationType-termination type:
        '                            *
        '                            *
        '                            *
        '
        '          -- ALGLIB --
        '             Copyright 14.05.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub qpcholeskyoptimize(a As cqmodels.convexquadraticmodel, anorm As Double, b As Double(), bndl As Double(), bndu As Double(), s As Double(), _
            xorigin As Double(), n As Integer, cleic As Double(,), nec As Integer, nic As Integer, sstate As qpcholeskybuffers, _
            ByRef xsc As Double(), ByRef terminationtype As Integer)
            Dim i As Integer = 0
            Dim noisetolerance As Double = 0
            Dim havebc As New Boolean()
            Dim v As Double = 0
            Dim badnewtonits As Integer = 0
            Dim maxscaledgrad As Double = 0
            Dim v0 As Double = 0
            Dim v1 As Double = 0
            Dim nextaction As Integer = 0
            Dim fprev As Double = 0
            Dim fcur As Double = 0
            Dim fcand As Double = 0
            Dim noiselevel As Double = 0
            Dim d0 As Double = 0
            Dim d1 As Double = 0
            Dim d2 As Double = 0
            Dim actstatus As Integer = 0
            Dim i_ As Integer = 0

            terminationtype = 0


            '
            ' Allocate storage and prepare fields
            '
            apserv.rvectorsetlengthatleast(sstate.rctmpg, n)
            apserv.rvectorsetlengthatleast(sstate.tmp0, n)
            apserv.rvectorsetlengthatleast(sstate.tmp1, n)
            apserv.rvectorsetlengthatleast(sstate.gc, n)
            apserv.rvectorsetlengthatleast(sstate.pg, n)
            apserv.rvectorsetlengthatleast(sstate.xs, n)
            apserv.rvectorsetlengthatleast(sstate.xn, n)
            apserv.rvectorsetlengthatleast(sstate.workbndl, n)
            apserv.rvectorsetlengthatleast(sstate.workbndu, n)
            apserv.bvectorsetlengthatleast(sstate.havebndl, n)
            apserv.bvectorsetlengthatleast(sstate.havebndu, n)
            sstate.repinneriterationscount = 0
            sstate.repouteriterationscount = 0
            sstate.repncholesky = 0
            noisetolerance = 10

            '
            ' Our formulation of quadratic problem includes origin point,
            ' i.e. we have F(x-x_origin) which is minimized subject to
            ' constraints on x, instead of having simply F(x).
            '
            ' Here we make transition from non-zero origin to zero one.
            ' In order to make such transition we have to:
            ' 1. subtract x_origin from x_start
            ' 2. modify constraints
            ' 3. solve problem
            ' 4. add x_origin to solution
            '
            ' There is alternate solution - to modify quadratic function
            ' by expansion of multipliers containing (x-x_origin), but
            ' we prefer to modify constraints, because it is a) more precise
            ' and b) easier to to.
            '
            ' Parts (1)-(2) are done here. After this block is over,
            ' we have:
            ' * XS, which stores shifted XStart (if we don't have XStart,
            '   value of XS will be ignored later)
            ' * WorkBndL, WorkBndU, which store modified boundary constraints.
            '
            havebc = False
            For i = 0 To n - 1
                sstate.havebndl(i) = Math.isfinite(bndl(i))
                sstate.havebndu(i) = Math.isfinite(bndu(i))
                havebc = (havebc OrElse sstate.havebndl(i)) OrElse sstate.havebndu(i)
                If sstate.havebndl(i) Then
                    sstate.workbndl(i) = bndl(i) - xorigin(i)
                Else
                    sstate.workbndl(i) = [Double].NegativeInfinity
                End If
                If sstate.havebndu(i) Then
                    sstate.workbndu(i) = bndu(i) - xorigin(i)
                Else
                    sstate.workbndu(i) = [Double].PositiveInfinity
                End If
            Next
            apserv.rmatrixsetlengthatleast(sstate.workcleic, nec + nic, n + 1)
            For i = 0 To nec + nic - 1
                v = 0.0
                For i_ = 0 To n - 1
                    v += cleic(i, i_) * xorigin(i_)
                Next
                For i_ = 0 To n - 1
                    sstate.workcleic(i, i_) = cleic(i, i_)
                Next
                sstate.workcleic(i, n) = cleic(i, n) - v
            Next

            '
            ' We have starting point in StartX, so we just have to shift and bound it
            '
            For i = 0 To n - 1
                sstate.xs(i) = xsc(i) - xorigin(i)
                If sstate.havebndl(i) Then
                    If CDbl(sstate.xs(i)) < CDbl(sstate.workbndl(i)) Then
                        sstate.xs(i) = sstate.workbndl(i)
                    End If
                End If
                If sstate.havebndu(i) Then
                    If CDbl(sstate.xs(i)) > CDbl(sstate.workbndu(i)) Then
                        sstate.xs(i) = sstate.workbndu(i)
                    End If
                End If
            Next

            '
            ' Handle special case - no constraints
            '
            If Not havebc AndAlso nec + nic = 0 Then

                '
                ' "Simple" unconstrained Cholesky
                '
                apserv.bvectorsetlengthatleast(sstate.tmpb, n)
                For i = 0 To n - 1
                    sstate.tmpb(i) = False
                Next
                sstate.repncholesky = sstate.repncholesky + 1
                cqmodels.cqmsetb(a, b)
                cqmodels.cqmsetactiveset(a, sstate.xs, sstate.tmpb)
                If Not cqmodels.cqmconstrainedoptimum(a, sstate.xn) Then
                    terminationtype = -5
                    Return
                End If
                For i_ = 0 To n - 1
                    xsc(i_) = sstate.xn(i_)
                Next
                For i_ = 0 To n - 1
                    xsc(i_) = xsc(i_) + xorigin(i_)
                Next
                sstate.repinneriterationscount = 1
                sstate.repouteriterationscount = 1
                terminationtype = 4
                Return
            End If

            '
            ' Prepare "active set" structure
            '
            sactivesets.sasinit(n, sstate.sas)
            sactivesets.sassetbc(sstate.sas, sstate.workbndl, sstate.workbndu)
            sactivesets.sassetlcx(sstate.sas, sstate.workcleic, nec, nic)
            sactivesets.sassetscale(sstate.sas, s)
            If Not sactivesets.sasstartoptimization(sstate.sas, sstate.xs) Then
                terminationtype = -3
                Return
            End If

            '
            ' Main cycle of CQP algorithm
            '
            terminationtype = 4
            badnewtonits = 0
            maxscaledgrad = 0.0
            While True

                '
                ' Update iterations count
                '
                apserv.inc(sstate.repouteriterationscount)
                apserv.inc(sstate.repinneriterationscount)

                '
                ' Phase 1.
                '
                ' Determine active set.
                ' Update MaxScaledGrad.
                '
                cqmodels.cqmadx(a, sstate.sas.xc, sstate.rctmpg)
                For i_ = 0 To n - 1
                    sstate.rctmpg(i_) = sstate.rctmpg(i_) + b(i_)
                Next
                sactivesets.sasreactivateconstraints(sstate.sas, sstate.rctmpg)
                v = 0.0
                For i = 0 To n - 1
                    v = v + Math.sqr(sstate.rctmpg(i) * s(i))
                Next
                maxscaledgrad = System.Math.Max(maxscaledgrad, System.Math.sqrt(v))

                '
                ' Phase 2: perform penalized steepest descent step.
                '
                ' NextAction control variable is set on exit from this loop:
                ' * NextAction>0 in case we have to proceed to Phase 3 (Newton step)
                ' * NextAction<0 in case we have to proceed to Phase 1 (recalculate active set)
                ' * NextAction=0 in case we found solution (step along projected gradient is small enough)
                '
                While True

                    '
                    ' Calculate constrained descent direction, store to PG.
                    ' Successful termination if PG is zero.
                    '
                    cqmodels.cqmadx(a, sstate.sas.xc, sstate.gc)
                    For i_ = 0 To n - 1
                        sstate.gc(i_) = sstate.gc(i_) + b(i_)
                    Next
                    sactivesets.sasconstraineddescent(sstate.sas, sstate.gc, sstate.pg)
                    v0 = 0.0
                    For i_ = 0 To n - 1
                        v0 += sstate.pg(i_) * sstate.pg(i_)
                    Next
                    If CDbl(v0) = CDbl(0) Then

                        '
                        ' Constrained derivative is zero.
                        ' Solution found.
                        '
                        nextaction = 0
                        Exit While
                    End If

                    '
                    ' Build quadratic model of F along descent direction:
                    '     F(xc+alpha*pg) = D2*alpha^2 + D1*alpha + D0
                    ' Store noise level in the XC (noise level is used to classify
                    ' step as singificant or insignificant).
                    '
                    ' In case function curvature is negative or product of descent
                    ' direction and gradient is non-negative, iterations are terminated.
                    '
                    ' NOTE: D0 is not actually used, but we prefer to maintain it.
                    '
                    fprev = modelvalue(a, b, sstate.sas.xc, n, sstate.tmp0)
                    fprev = fprev + penaltyfactor * maxscaledgrad * sactivesets.sasactivelcpenalty1(sstate.sas, sstate.sas.xc)
                    cqmodels.cqmevalx(a, sstate.sas.xc, v, noiselevel)
                    v0 = cqmodels.cqmxtadx2(a, sstate.pg)
                    d2 = v0
                    v1 = 0.0
                    For i_ = 0 To n - 1
                        v1 += sstate.pg(i_) * sstate.gc(i_)
                    Next
                    d1 = v1
                    d0 = fprev
                    If CDbl(d2) <= CDbl(0) Then

                        '
                        ' Second derivative is non-positive, function is non-convex.
                        '
                        terminationtype = -5
                        nextaction = 0
                        Exit While
                    End If
                    If CDbl(d1) >= CDbl(0) Then

                        '
                        ' Second derivative is positive, first derivative is non-negative.
                        ' Solution found.
                        '
                        nextaction = 0
                        Exit While
                    End If

                    '
                    ' Modify quadratic model - add penalty for violation of the active
                    ' constraints.
                    '
                    ' Boundary constraints are always satisfied exactly, so we do not
                    ' add penalty term for them. General equality constraint of the
                    ' form a'*(xc+alpha*d)=b adds penalty term:
                    '     P(alpha) = (a'*(xc+alpha*d)-b)^2
                    '              = (alpha*(a'*d) + (a'*xc-b))^2
                    '              = alpha^2*(a'*d)^2 + alpha*2*(a'*d)*(a'*xc-b) + (a'*xc-b)^2
                    ' Each penalty term is multiplied by 100*Anorm before adding it to
                    ' the 1-dimensional quadratic model.
                    '
                    ' Penalization of the quadratic model improves behavior of the
                    ' algorithm in the presense of the multiple degenerate constraints.
                    ' In particular, it prevents algorithm from making large steps in
                    ' directions which violate equality constraints.
                    '
                    For i = 0 To nec + nic - 1
                        If sstate.sas.activeset(n + i) > 0 Then
                            v0 = 0.0
                            For i_ = 0 To n - 1
                                v0 += sstate.workcleic(i, i_) * sstate.pg(i_)
                            Next
                            v1 = 0.0
                            For i_ = 0 To n - 1
                                v1 += sstate.workcleic(i, i_) * sstate.sas.xc(i_)
                            Next
                            v1 = v1 - sstate.workcleic(i, n)
                            v = 100 * anorm
                            d2 = d2 + v * Math.sqr(v0)
                            d1 = d1 + v * 2 * v0 * v1
                            d0 = d0 + v * Math.sqr(v1)
                        End If
                    Next

                    '
                    ' Try unbounded step.
                    ' In case function change is dominated by noise or function actually increased
                    ' instead of decreasing, we terminate iterations.
                    '
                    v = -(d1 / (2 * d2))
                    For i_ = 0 To n - 1
                        sstate.xn(i_) = sstate.sas.xc(i_)
                    Next
                    For i_ = 0 To n - 1
                        sstate.xn(i_) = sstate.xn(i_) + v * sstate.pg(i_)
                    Next
                    fcand = modelvalue(a, b, sstate.xn, n, sstate.tmp0)
                    fcand = fcand + penaltyfactor * maxscaledgrad * sactivesets.sasactivelcpenalty1(sstate.sas, sstate.xn)
                    If CDbl(fcand) >= CDbl(fprev - noiselevel * noisetolerance) Then
                        nextaction = 0
                        Exit While
                    End If

                    '
                    ' Save active set
                    ' Perform bounded step with (possible) activation
                    '
                    actstatus = boundedstepandactivation(sstate.sas, sstate.xn, n, sstate.tmp0)
                    fcur = modelvalue(a, b, sstate.sas.xc, n, sstate.tmp0)

                    '
                    ' Depending on results, decide what to do:
                    ' 1. In case step was performed without activation of constraints,
                    '    we proceed to Newton method
                    ' 2. In case there was activated at least one constraint with ActiveSet[I]<0,
                    '    we proceed to Phase 1 and re-evaluate active set.
                    ' 3. Otherwise (activation of the constraints with ActiveSet[I]=0)
                    '    we try Phase 2 one more time.
                    '
                    If actstatus < 0 Then

                        '
                        ' Step without activation, proceed to Newton
                        '
                        nextaction = 1
                        Exit While
                    End If
                    If actstatus = 0 Then

                        '
                        ' No new constraints added during last activation - only
                        ' ones which were at the boundary (ActiveSet[I]=0), but
                        ' inactive due to numerical noise.
                        '
                        ' Now, these constraints are added to the active set, and
                        ' we try to perform steepest descent (Phase 2) one more time.
                        '
                        Continue While
                    Else

                        '
                        ' Last step activated at least one significantly new
                        ' constraint (ActiveSet[I]<0), we have to re-evaluate
                        ' active set (Phase 1).
                        '
                        nextaction = -1
                        Exit While
                    End If
                End While
                If nextaction < 0 Then
                    Continue While
                End If
                If nextaction = 0 Then
                    Exit While
                End If

                '
                ' Phase 3: fast equality-constrained solver
                '
                ' NOTE: this solver uses Augmented Lagrangian algorithm to solve
                '       equality-constrained subproblems. This algorithm may
                '       perform steps which increase function values instead of
                '       decreasing it (in hard cases, like overconstrained problems).
                '
                '       Such non-monononic steps may create a loop, when Augmented
                '       Lagrangian algorithm performs uphill step, and steepest
                '       descent algorithm (Phase 2) performs downhill step in the
                '       opposite direction.
                '
                '       In order to prevent iterations to continue forever we
                '       count iterations when AL algorithm increased function
                '       value instead of decreasing it. When number of such "bad"
                '       iterations will increase beyong MaxBadNewtonIts, we will
                '       terminate algorithm.
                '
                fprev = modelvalue(a, b, sstate.sas.xc, n, sstate.tmp0)
                While True

                    '
                    ' Calculate optimum subject to presently active constraints
                    '
                    sstate.repncholesky = sstate.repncholesky + 1
                    If Not constrainedoptimum(sstate.sas, a, anorm, b, sstate.xn, n, _
                        sstate.tmp0, sstate.tmpb, sstate.tmp1) Then
                        terminationtype = -5
                        sactivesets.sasstopoptimization(sstate.sas)
                        Return
                    End If

                    '
                    ' Add constraints.
                    ' If no constraints was added, accept candidate point XN and move to next phase.
                    '
                    If boundedstepandactivation(sstate.sas, sstate.xn, n, sstate.tmp0) < 0 Then
                        Exit While
                    End If
                End While
                fcur = modelvalue(a, b, sstate.sas.xc, n, sstate.tmp0)
                If CDbl(fcur) >= CDbl(fprev) Then
                    badnewtonits = badnewtonits + 1
                End If
                If badnewtonits >= maxbadnewtonits Then

                    '
                    ' Algorithm found solution, but keeps iterating because Newton
                    ' algorithm performs uphill steps (noise in the Augmented Lagrangian
                    ' algorithm). We terminate algorithm; it is considered normal
                    ' termination.
                    '
                    Exit While
                End If
            End While
            sactivesets.sasstopoptimization(sstate.sas)

            '
            ' Post-process: add XOrigin to XC
            '
            For i = 0 To n - 1
                If sstate.havebndl(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.workbndl(i)) Then
                    xsc(i) = bndl(i)
                    Continue For
                End If
                If sstate.havebndu(i) AndAlso CDbl(sstate.sas.xc(i)) = CDbl(sstate.workbndu(i)) Then
                    xsc(i) = bndu(i)
                    Continue For
                End If
                xsc(i) = apserv.boundval(sstate.sas.xc(i) + xorigin(i), bndl(i), bndu(i))
            Next
        End Sub


        '************************************************************************
        '        Model value: f = 0.5*x'*A*x + b'*x
        '
        '        INPUT PARAMETERS:
        '            A       -   convex quadratic model; only main quadratic term is used,
        '                        other parts of the model (D/Q/linear term) are ignored.
        '                        This function does not modify model state.
        '            B       -   right part
        '            XC      -   evaluation point
        '            Tmp     -   temporary buffer, automatically resized if needed
        '
        '          -- ALGLIB --
        '             Copyright 20.06.2012 by Bochkanov Sergey
        '        ************************************************************************

        Private Shared Function modelvalue(a As cqmodels.convexquadraticmodel, b As Double(), xc As Double(), n As Integer, ByRef tmp As Double()) As Double
            Dim result As Double = 0
            Dim v0 As Double = 0
            Dim v1 As Double = 0
            Dim i_ As Integer = 0

            apserv.rvectorsetlengthatleast(tmp, n)
            cqmodels.cqmadx(a, xc, tmp)
            v0 = 0.0
            For i_ = 0 To n - 1
                v0 += xc(i_) * tmp(i_)
            Next
            v1 = 0.0
            For i_ = 0 To n - 1
                v1 += xc(i_) * b(i_)
            Next
            result = 0.5 * v0 + v1
            Return result
        End Function


        '************************************************************************
        '        Having feasible current point XC and possibly infeasible candidate   point
        '        XN,  this  function  performs  longest  step  from  XC to XN which retains
        '        feasibility. In case XN is found to be infeasible, at least one constraint
        '        is activated.
        '
        '        For example, if we have:
        '          XC=0.5
        '          XN=1.2
        '          x>=0, x<=1
        '        then this function will move us to X=1.0 and activate constraint "x<=1".
        '
        '        INPUT PARAMETERS:
        '            State   -   MinQP state.
        '            XC      -   current point, must be feasible with respect to
        '                        all constraints
        '            XN      -   candidate point, can be infeasible with respect to some
        '                        constraints. Must be located in the subspace of current
        '                        active set, i.e. it is feasible with respect to already
        '                        active constraints.
        '            Buf     -   temporary buffer, automatically resized if needed
        '
        '        OUTPUT PARAMETERS:
        '            State   -   this function changes following fields of State:
        '                        * State.ActiveSet
        '                        * State.ActiveC     -   active linear constraints
        '            XC      -   new position
        '
        '        RESULT:
        '            >0, in case at least one inactive non-candidate constraint was activated
        '            =0, in case only "candidate" constraints were activated
        '            <0, in case no constraints were activated by the step
        '
        '
        '          -- ALGLIB --
        '             Copyright 29.02.2012 by Bochkanov Sergey
        '        ************************************************************************

        Private Shared Function boundedstepandactivation(sas As sactivesets.sactiveset, xn As Double(), n As Integer, ByRef buf As Double()) As Integer
            Dim result As Integer = 0
            Dim stpmax As Double = 0
            Dim cidx As Integer = 0
            Dim cval As Double = 0
            Dim needact As New Boolean()
            Dim v As Double = 0
            Dim i_ As Integer = 0

            apserv.rvectorsetlengthatleast(buf, n)
            For i_ = 0 To n - 1
                buf(i_) = xn(i_)
            Next
            For i_ = 0 To n - 1
                buf(i_) = buf(i_) - sas.xc(i_)
            Next
            sactivesets.sasexploredirection(sas, buf, stpmax, cidx, cval)
            needact = CDbl(stpmax) <= CDbl(1)
            v = System.Math.Min(stpmax, 1.0)
            For i_ = 0 To n - 1
                buf(i_) = v * buf(i_)
            Next
            For i_ = 0 To n - 1
                buf(i_) = buf(i_) + sas.xc(i_)
            Next
            result = sactivesets.sasmoveto(sas, buf, needact, cidx, cval)
            Return result
        End Function


        '************************************************************************
        '        Optimum of A subject to:
        '        a) active boundary constraints (given by ActiveSet[] and corresponding
        '           elements of XC)
        '        b) active linear constraints (given by C, R, LagrangeC)
        '
        '        INPUT PARAMETERS:
        '            A       -   main quadratic term of the model;
        '                        although structure may  store  linear  and  rank-K  terms,
        '                        these terms are ignored and rewritten  by  this  function.
        '            ANorm   -   estimate of ||A|| (2-norm is used)
        '            B       -   array[N], linear term of the model
        '            XN      -   possibly preallocated buffer
        '            Tmp     -   temporary buffer (automatically resized)
        '            Tmp1    -   temporary buffer (automatically resized)
        '
        '        OUTPUT PARAMETERS:
        '            A       -   modified quadratic model (this function changes rank-K
        '                        term and linear term of the model)
        '            LagrangeC-  current estimate of the Lagrange coefficients
        '            XN      -   solution
        '
        '        RESULT:
        '            True on success, False on failure (non-SPD model)
        '
        '          -- ALGLIB --
        '             Copyright 20.06.2012 by Bochkanov Sergey
        '        ************************************************************************

        Private Shared Function constrainedoptimum(sas As sactivesets.sactiveset, a As cqmodels.convexquadraticmodel, anorm As Double, b As Double(), ByRef xn As Double(), n As Integer, _
            ByRef tmp As Double(), ByRef tmpb As Boolean(), ByRef lagrangec As Double()) As Boolean
            Dim result As New Boolean()
            Dim itidx As Integer = 0
            Dim i As Integer = 0
            Dim v As Double = 0
            Dim feaserrold As Double = 0
            Dim feaserrnew As Double = 0
            Dim theta As Double = 0
            Dim i_ As Integer = 0


            '
            ' Rebuild basis accroding to current active set.
            ' We call SASRebuildBasis() to make sure that fields of SAS
            ' store up to date values.
            '
            sactivesets.sasrebuildbasis(sas)

            '
            ' Allocate temporaries.
            '
            apserv.rvectorsetlengthatleast(tmp, System.Math.Max(n, sas.basissize))
            apserv.bvectorsetlengthatleast(tmpb, n)
            apserv.rvectorsetlengthatleast(lagrangec, sas.basissize)

            '
            ' Prepare model
            '
            For i = 0 To sas.basissize - 1
                tmp(i) = sas.pbasis(i, n)
            Next
            theta = 100.0 * anorm
            For i = 0 To n - 1
                If sas.activeset(i) > 0 Then
                    tmpb(i) = True
                Else
                    tmpb(i) = False
                End If
            Next
            cqmodels.cqmsetactiveset(a, sas.xc, tmpb)
            cqmodels.cqmsetq(a, sas.pbasis, tmp, sas.basissize, theta)

            '
            ' Iterate until optimal values of Lagrange multipliers are found
            '
            For i = 0 To sas.basissize - 1
                lagrangec(i) = 0
            Next
            feaserrnew = Math.maxrealnumber
            result = True
            For itidx = 1 To maxlagrangeits

                '
                ' Generate right part B using linear term and current
                ' estimate of the Lagrange multipliers.
                '
                For i_ = 0 To n - 1
                    tmp(i_) = b(i_)
                Next
                For i = 0 To sas.basissize - 1
                    v = lagrangec(i)
                    For i_ = 0 To n - 1
                        tmp(i_) = tmp(i_) - v * sas.pbasis(i, i_)
                    Next
                Next
                cqmodels.cqmsetb(a, tmp)

                '
                ' Solve
                '
                result = cqmodels.cqmconstrainedoptimum(a, xn)
                If Not result Then
                    Return result
                End If

                '
                ' Compare feasibility errors.
                ' Terminate if error decreased too slowly.
                '
                feaserrold = feaserrnew
                feaserrnew = 0
                For i = 0 To sas.basissize - 1
                    v = 0.0
                    For i_ = 0 To n - 1
                        v += sas.pbasis(i, i_) * xn(i_)
                    Next
                    feaserrnew = feaserrnew + Math.sqr(v - sas.pbasis(i, n))
                Next
                feaserrnew = System.Math.sqrt(feaserrnew)
                If CDbl(feaserrnew) >= CDbl(0.2 * feaserrold) Then
                    Exit For
                End If

                '
                ' Update Lagrange multipliers
                '
                For i = 0 To sas.basissize - 1
                    v = 0.0
                    For i_ = 0 To n - 1
                        v += sas.pbasis(i, i_) * xn(i_)
                    Next
                    lagrangec(i) = lagrangec(i) - theta * (v - sas.pbasis(i, n))
                Next
            Next
            Return result
        End Function


    End Class
    Public Class minqp
        '************************************************************************
        '        This object stores nonlinear optimizer state.
        '        You should use functions provided by MinQP subpackage to work with this
        '        object
        '        ************************************************************************

        Public Class minqpstate
            Inherits apobject
            Public n As Integer
            Public qqpsettingsuser As qqpsolver.qqpsettings
            Public qqpsettingscurrent As qqpsolver.qqpsettings
            Public qpbleicsettingsuser As qpbleicsolver.qpbleicsettings
            Public qpbleicsettingscurrent As qpbleicsolver.qpbleicsettings
            Public algokind As Integer
            Public akind As Integer
            Public a As cqmodels.convexquadraticmodel
            Public sparsea As sparse.sparsematrix
            Public sparseaupper As Boolean
            Public absamax As Double
            Public absasum As Double
            Public absasum2 As Double
            Public b As Double()
            Public bndl As Double()
            Public bndu As Double()
            Public s As Double()
            Public havebndl As Boolean()
            Public havebndu As Boolean()
            Public xorigin As Double()
            Public startx As Double()
            Public havex As Boolean
            Public cleic As Double(,)
            Public nec As Integer
            Public nic As Integer
            Public xs As Double()
            Public repinneriterationscount As Integer
            Public repouteriterationscount As Integer
            Public repncholesky As Integer
            Public repnmv As Integer
            Public repterminationtype As Integer
            Public tmp0 As Double()
            Public qpbleicfirstcall As Boolean
            Public qpbleicbuf As qpbleicsolver.qpbleicbuffers
            Public qqpbuf As qqpsolver.qqpbuffers
            Public qpcholeskybuf As qpcholeskysolver.qpcholeskybuffers
            Public estimator As normestimator.normestimatorstate
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
                qqpsettingsuser = New qqpsolver.qqpsettings()
                qqpsettingscurrent = New qqpsolver.qqpsettings()
                qpbleicsettingsuser = New qpbleicsolver.qpbleicsettings()
                qpbleicsettingscurrent = New qpbleicsolver.qpbleicsettings()
                a = New cqmodels.convexquadraticmodel()
                sparsea = New sparse.sparsematrix()
                b = New Double(-1) {}
                bndl = New Double(-1) {}
                bndu = New Double(-1) {}
                s = New Double(-1) {}
                havebndl = New Boolean(-1) {}
                havebndu = New Boolean(-1) {}
                xorigin = New Double(-1) {}
                startx = New Double(-1) {}
                cleic = New Double(-1, -1) {}
                xs = New Double(-1) {}
                tmp0 = New Double(-1) {}
                qpbleicbuf = New qpbleicsolver.qpbleicbuffers()
                qqpbuf = New qqpsolver.qqpbuffers()
                qpcholeskybuf = New qpcholeskysolver.qpcholeskybuffers()
                estimator = New normestimator.normestimatorstate()
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New minqpstate()
                _result.n = n
                _result.qqpsettingsuser = DirectCast(qqpsettingsuser.make_copy(), qqpsolver.qqpsettings)
                _result.qqpsettingscurrent = DirectCast(qqpsettingscurrent.make_copy(), qqpsolver.qqpsettings)
                _result.qpbleicsettingsuser = DirectCast(qpbleicsettingsuser.make_copy(), qpbleicsolver.qpbleicsettings)
                _result.qpbleicsettingscurrent = DirectCast(qpbleicsettingscurrent.make_copy(), qpbleicsolver.qpbleicsettings)
                _result.algokind = algokind
                _result.akind = akind
                _result.a = DirectCast(a.make_copy(), cqmodels.convexquadraticmodel)
                _result.sparsea = DirectCast(sparsea.make_copy(), sparse.sparsematrix)
                _result.sparseaupper = sparseaupper
                _result.absamax = absamax
                _result.absasum = absasum
                _result.absasum2 = absasum2
                _result.b = DirectCast(b.Clone(), Double())
                _result.bndl = DirectCast(bndl.Clone(), Double())
                _result.bndu = DirectCast(bndu.Clone(), Double())
                _result.s = DirectCast(s.Clone(), Double())
                _result.havebndl = DirectCast(havebndl.Clone(), Boolean())
                _result.havebndu = DirectCast(havebndu.Clone(), Boolean())
                _result.xorigin = DirectCast(xorigin.Clone(), Double())
                _result.startx = DirectCast(startx.Clone(), Double())
                _result.havex = havex
                _result.cleic = DirectCast(cleic.Clone(), Double(,))
                _result.nec = nec
                _result.nic = nic
                _result.xs = DirectCast(xs.Clone(), Double())
                _result.repinneriterationscount = repinneriterationscount
                _result.repouteriterationscount = repouteriterationscount
                _result.repncholesky = repncholesky
                _result.repnmv = repnmv
                _result.repterminationtype = repterminationtype
                _result.tmp0 = DirectCast(tmp0.Clone(), Double())
                _result.qpbleicfirstcall = qpbleicfirstcall
                _result.qpbleicbuf = DirectCast(qpbleicbuf.make_copy(), qpbleicsolver.qpbleicbuffers)
                _result.qqpbuf = DirectCast(qqpbuf.make_copy(), qqpsolver.qqpbuffers)
                _result.qpcholeskybuf = DirectCast(qpcholeskybuf.make_copy(), qpcholeskysolver.qpcholeskybuffers)
                _result.estimator = DirectCast(estimator.make_copy(), normestimator.normestimatorstate)
                Return _result
            End Function
        End Class


        '************************************************************************
        '        This structure stores optimization report:
        '        * InnerIterationsCount      number of inner iterations
        '        * OuterIterationsCount      number of outer iterations
        '        * NCholesky                 number of Cholesky decomposition
        '        * NMV                       number of matrix-vector products
        '                                    (only products calculated as part of iterative
        '                                    process are counted)
        '        * TerminationType           completion code (see below)
        '
        '        Completion codes:
        '        * -5    inappropriate solver was used:
        '                * QuickQP solver for problem with general linear constraints
        '                * Cholesky solver for semidefinite or indefinite problems
        '                * Cholesky solver for problems with non-boundary constraints
        '        * -4    BLEIC-QP or QuickQP solver found unconstrained direction
        '                of negative curvature (function is unbounded from
        '                below  even  under  constraints),  no  meaningful
        '                minimum can be found.
        '        * -3    inconsistent constraints (or, maybe, feasible point is
        '                too hard to find). If you are sure that constraints are feasible,
        '                try to restart optimizer with better initial approximation.
        '        * -1    solver error
        '        *  1..4 successful completion
        '        *  5    MaxIts steps was taken
        '        *  7    stopping conditions are too stringent,
        '                further improvement is impossible,
        '                X contains best point found so far.
        '        ************************************************************************

        Public Class minqpreport
            Inherits apobject
            Public inneriterationscount As Integer
            Public outeriterationscount As Integer
            Public nmv As Integer
            Public ncholesky As Integer
            Public terminationtype As Integer
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New minqpreport()
                _result.inneriterationscount = inneriterationscount
                _result.outeriterationscount = outeriterationscount
                _result.nmv = nmv
                _result.ncholesky = ncholesky
                _result.terminationtype = terminationtype
                Return _result
            End Function
        End Class




        '************************************************************************
        '                            CONSTRAINED QUADRATIC PROGRAMMING
        '
        '        The subroutine creates QP optimizer. After initial creation,  it  contains
        '        default optimization problem with zero quadratic and linear terms  and  no
        '        constraints. You should set quadratic/linear terms with calls to functions
        '        provided by MinQP subpackage.
        '
        '        You should also choose appropriate QP solver and set it  and  its stopping
        '        criteria by means of MinQPSetAlgo??????() function. Then, you should start
        '        solution process by means of MinQPOptimize() call. Solution itself can  be
        '        obtained with MinQPResults() function.
        '
        '        INPUT PARAMETERS:
        '            N       -   problem size
        '            
        '        OUTPUT PARAMETERS:
        '            State   -   optimizer with zero quadratic/linear terms
        '                        and no constraints
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpcreate(n As Integer, state As minqpstate)
            Dim i As Integer = 0

            alglib.ap.assert(n >= 1, "MinQPCreate: N<1")

            '
            ' initialize QP solver
            '
            state.n = n
            state.nec = 0
            state.nic = 0
            state.repterminationtype = 0
            state.absamax = 1
            state.absasum = 1
            state.absasum2 = 1
            state.akind = 0
            state.sparseaupper = False
            cqmodels.cqminit(n, state.a)
            state.b = New Double(n - 1) {}
            state.bndl = New Double(n - 1) {}
            state.bndu = New Double(n - 1) {}
            state.havebndl = New Boolean(n - 1) {}
            state.havebndu = New Boolean(n - 1) {}
            state.s = New Double(n - 1) {}
            state.startx = New Double(n - 1) {}
            state.xorigin = New Double(n - 1) {}
            state.xs = New Double(n - 1) {}
            For i = 0 To n - 1
                state.bndl(i) = [Double].NegativeInfinity
                state.bndu(i) = [Double].PositiveInfinity
                state.havebndl(i) = False
                state.havebndu(i) = False
                state.b(i) = 0.0
                state.startx(i) = 0.0
                state.xorigin(i) = 0.0
                state.s(i) = 1.0
            Next
            state.havex = False
            minqpsetalgocholesky(state)
            normestimator.normestimatorcreate(n, n, 5, 5, state.estimator)
            qqpsolver.qqploaddefaults(n, state.qqpsettingsuser)
            qpbleicsolver.qpbleicloaddefaults(n, state.qpbleicsettingsuser)
            state.qpbleicfirstcall = True
        End Sub


        '************************************************************************
        '        This function sets linear term for QP solver.
        '
        '        By default, linear term is zero.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            B       -   linear term, array[N].
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetlinearterm(state As minqpstate, b As Double())
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.len(b) >= n, "MinQPSetLinearTerm: Length(B)<N")
            alglib.ap.assert(apserv.isfinitevector(b, n), "MinQPSetLinearTerm: B contains infinite or NaN elements")
            minqpsetlineartermfast(state, b)
        End Sub


        '************************************************************************
        '        This  function  sets  dense  quadratic  term  for  QP solver. By  default,
        '        quadratic term is zero.
        '
        '        SUPPORT BY ALGLIB QP ALGORITHMS:
        '
        '        Dense quadratic term can be handled by any of the QP algorithms  supported
        '        by ALGLIB QP Solver.
        '
        '        IMPORTANT:
        '
        '        This solver minimizes following  function:
        '            f(x) = 0.5*x'*A*x + b'*x.
        '        Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
        '            f(x) = x^2 + x
        '        you should rewrite your problem as follows:
        '            f(x) = 0.5*(2*x^2) + x
        '        and your matrix A will be equal to [[2.0]], not to [[1.0]]
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            A       -   matrix, array[N,N]
        '            IsUpper -   (optional) storage type:
        '                        * if True, symmetric matrix  A  is  given  by  its  upper
        '                          triangle, and the lower triangle isn used
        '                        * if False, symmetric matrix  A  is  given  by  its lower
        '                          triangle, and the upper triangle isn used
        '                        * if not given, both lower and upper  triangles  must  be
        '                          filled.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetquadraticterm(state As minqpstate, a As Double(,), isupper As Boolean)
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.rows(a) >= n, "MinQPSetQuadraticTerm: Rows(A)<N")
            alglib.ap.assert(alglib.ap.cols(a) >= n, "MinQPSetQuadraticTerm: Cols(A)<N")
            alglib.ap.assert(apserv.isfinitertrmatrix(a, n, isupper), "MinQPSetQuadraticTerm: A contains infinite or NaN elements")
            minqpsetquadratictermfast(state, a, isupper, 0.0)
        End Sub


        '************************************************************************
        '        This  function  sets  sparse  quadratic  term  for  QP solver. By default,
        '        quadratic term is zero.
        '
        '        IMPORTANT:
        '
        '        This solver minimizes following  function:
        '            f(x) = 0.5*x'*A*x + b'*x.
        '        Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
        '            f(x) = x^2 + x
        '        you should rewrite your problem as follows:
        '            f(x) = 0.5*(2*x^2) + x
        '        and your matrix A will be equal to [[2.0]], not to [[1.0]]
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            A       -   matrix, array[N,N]
        '            IsUpper -   (optional) storage type:
        '                        * if True, symmetric matrix  A  is  given  by  its  upper
        '                          triangle, and the lower triangle isn used
        '                        * if False, symmetric matrix  A  is  given  by  its lower
        '                          triangle, and the upper triangle isn used
        '                        * if not given, both lower and upper  triangles  must  be
        '                          filled.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetquadratictermsparse(state As minqpstate, a As sparse.sparsematrix, isupper As Boolean)
            Dim n As Integer = 0
            Dim t0 As Integer = 0
            Dim t1 As Integer = 0
            Dim i As Integer = 0
            Dim j As Integer = 0
            Dim v As Double = 0

            n = state.n
            alglib.ap.assert(sparse.sparsegetnrows(a) = n, "MinQPSetQuadraticTermSparse: Rows(A)<>N")
            alglib.ap.assert(sparse.sparsegetncols(a) = n, "MinQPSetQuadraticTermSparse: Cols(A)<>N")
            sparse.sparsecopytocrsbuf(a, state.sparsea)
            state.sparseaupper = isupper
            state.akind = 1

            '
            ' Estimate norm of A
            ' (it will be used later in the quadratic penalty function)
            '
            state.absamax = 0
            state.absasum = 0
            state.absasum2 = 0
            t0 = 0
            t1 = 0
            While sparse.sparseenumerate(a, t0, t1, i, j, v)
                If i = j Then

                    '
                    ' Diagonal terms are counted only once
                    '
                    state.absamax = System.Math.Max(state.absamax, v)
                    state.absasum = state.absasum + v
                    state.absasum2 = state.absasum2 + v * v
                End If
                If (j > i AndAlso isupper) OrElse (j < i AndAlso Not isupper) Then

                    '
                    ' Offdiagonal terms are counted twice
                    '
                    state.absamax = System.Math.Max(state.absamax, v)
                    state.absasum = state.absasum + 2 * v
                    state.absasum2 = state.absasum2 + 2 * v * v
                End If
            End While
        End Sub


        '************************************************************************
        '        This function sets starting point for QP solver. It is useful to have
        '        good initial approximation to the solution, because it will increase
        '        speed of convergence and identification of active constraints.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            X       -   starting point, array[N].
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetstartingpoint(state As minqpstate, x As Double())
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.len(x) >= n, "MinQPSetStartingPoint: Length(B)<N")
            alglib.ap.assert(apserv.isfinitevector(x, n), "MinQPSetStartingPoint: X contains infinite or NaN elements")
            minqpsetstartingpointfast(state, x)
        End Sub


        '************************************************************************
        '        This  function sets origin for QP solver. By default, following QP program
        '        is solved:
        '
        '            min(0.5*x'*A*x+b'*x)
        '            
        '        This function allows to solve different problem:
        '
        '            min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))
        '            
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            XOrigin -   origin, array[N].
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetorigin(state As minqpstate, xorigin As Double())
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.len(xorigin) >= n, "MinQPSetOrigin: Length(B)<N")
            alglib.ap.assert(apserv.isfinitevector(xorigin, n), "MinQPSetOrigin: B contains infinite or NaN elements")
            minqpsetoriginfast(state, xorigin)
        End Sub


        '************************************************************************
        '        This function sets scaling coefficients.
        '
        '        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
        '        size and gradient are scaled before comparison with tolerances).  Scale of
        '        the I-th variable is a translation invariant measure of:
        '        a) "how large" the variable is
        '        b) how large the step should be to make significant changes in the function
        '
        '        BLEIC-based QP solver uses scale for two purposes:
        '        * to evaluate stopping conditions
        '        * for preconditioning of the underlying BLEIC solver
        '
        '        INPUT PARAMETERS:
        '            State   -   structure stores algorithm state
        '            S       -   array[N], non-zero scaling coefficients
        '                        S[i] may be negative, sign doesn't matter.
        '
        '          -- ALGLIB --
        '             Copyright 14.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetscale(state As minqpstate, s As Double())
            Dim i As Integer = 0

            alglib.ap.assert(alglib.ap.len(s) >= state.n, "MinQPSetScale: Length(S)<N")
            For i = 0 To state.n - 1
                alglib.ap.assert(Math.isfinite(s(i)), "MinQPSetScale: S contains infinite or NAN elements")
                alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinQPSetScale: S contains zero elements")
                state.s(i) = System.Math.Abs(s(i))
            Next
        End Sub


        '************************************************************************
        '        This function tells solver to use Cholesky-based algorithm. This algorithm
        '        was deprecated in ALGLIB 3.9.0 because its performance is inferior to that
        '        of BLEIC-QP or  QuickQP  on  high-dimensional  problems.  Furthermore,  it
        '        supports only dense convex QP problems.
        '
        '        This solver is no longer active by default.
        '
        '        We recommend you to switch to BLEIC-QP or QuickQP solver.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetalgocholesky(state As minqpstate)
            state.algokind = 1
        End Sub


        '************************************************************************
        '        This function tells solver to use BLEIC-based algorithm and sets  stopping
        '        criteria for the algorithm.
        '
        '        ALGORITHM FEATURES:
        '
        '        * supports dense and sparse QP problems
        '        * supports boundary and general linear equality/inequality constraints
        '        * can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
        '          long as they are bounded from below under constraints.
        '          Say, it is possible to solve "min{-x^2} subject to -1<=x<=+1".
        '          Of course, global  minimum  is found only  for  positive  definite   and
        '          semidefinite  problems.  As  for indefinite ones - only local minimum is
        '          found.
        '
        '        ALGORITHM OUTLINE:
        '
        '        * BLEIC-QP solver is just a driver function for MinBLEIC solver; it solves
        '          quadratic  programming   problem   as   general   linearly   constrained
        '          optimization problem, which is solved by means of BLEIC solver  (part of
        '          ALGLIB, active set method).
        '          
        '        ALGORITHM LIMITATIONS:
        '
        '        * unlike QuickQP solver, this algorithm does not perform Newton steps  and
        '          does not use Level 3 BLAS. Being general-purpose active set  method,  it
        '          can activate constraints only one-by-one. Thus, its performance is lower
        '          than that of QuickQP.
        '        * its precision is also a bit  inferior  to  that  of   QuickQP.  BLEIC-QP
        '          performs only LBFGS steps (no Newton steps), which are good at detecting
        '          neighborhood of the solution, buy need many iterations to find  solution
        '          with more than 6 digits of precision.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            EpsG    -   >=0
        '                        The  subroutine  finishes  its  work   if   the  condition
        '                        |v|<EpsG is satisfied, where:
        '                        * |.| means Euclidian norm
        '                        * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
        '                        * g - gradient
        '                        * s - scaling coefficients set by MinQPSetScale()
        '            EpsF    -   >=0
        '                        The  subroutine  finishes its work if exploratory steepest
        '                        descent  step  on  k+1-th iteration  satisfies   following
        '                        condition:  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
        '            EpsX    -   >=0
        '                        The  subroutine  finishes its work if exploratory steepest
        '                        descent  step  on  k+1-th iteration  satisfies   following
        '                        condition:  
        '                        * |.| means Euclidian norm
        '                        * v - scaled step vector, v[i]=dx[i]/s[i]
        '                        * dx - step vector, dx=X(k+1)-X(k)
        '                        * s - scaling coefficients set by MinQPSetScale()
        '            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
        '                        iterations is unlimited. NOTE: this  algorithm uses  LBFGS
        '                        iterations,  which  are  relatively  cheap,  but   improve
        '                        function value only a bit. So you will need many iterations
        '                        to converge - from 0.1*N to 10*N, depending  on  problem's
        '                        condition number.
        '
        '        IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
        '        BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!
        '
        '        Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
        '        to automatic stopping criterion selection (presently it is  small    step
        '        length, but it may change in the future versions of ALGLIB).
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetalgobleic(state As minqpstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
            alglib.ap.assert(Math.isfinite(epsg), "MinQPSetAlgoBLEIC: EpsG is not finite number")
            alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinQPSetAlgoBLEIC: negative EpsG")
            alglib.ap.assert(Math.isfinite(epsf), "MinQPSetAlgoBLEIC: EpsF is not finite number")
            alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinQPSetAlgoBLEIC: negative EpsF")
            alglib.ap.assert(Math.isfinite(epsx), "MinQPSetAlgoBLEIC: EpsX is not finite number")
            alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinQPSetAlgoBLEIC: negative EpsX")
            alglib.ap.assert(maxits >= 0, "MinQPSetAlgoBLEIC: negative MaxIts!")
            state.algokind = 2
            If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
                epsx = 0.000001
            End If
            state.qpbleicsettingsuser.epsg = epsg
            state.qpbleicsettingsuser.epsf = epsf
            state.qpbleicsettingsuser.epsx = epsx
            state.qpbleicsettingsuser.maxits = maxits
        End Sub


        '************************************************************************
        '        This function tells solver to use QuickQP  algorithm:  special  extra-fast
        '        algorithm   for   problems  with  boundary-only constrants. It  may  solve
        '        non-convex  problems  as  long  as  they  are  bounded  from  below  under
        '        constraints.
        '
        '        ALGORITHM FEATURES:
        '        * many times (from 5x to 50x!) faster than BLEIC-based QP solver; utilizes
        '          accelerated methods for activation of constraints.
        '        * supports dense and sparse QP problems
        '        * supports ONLY boundary constraints; general linear constraints  are  NOT
        '          supported by this solver
        '        * can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
        '          long as they are bounded from below under constraints.
        '          Say, it is possible to solve "min{-x^2} subject to -1<=x<=+1".
        '          In convex/semidefinite case global minimum  is  returned,  in  nonconvex
        '          case - algorithm returns one of the local minimums.
        '
        '        ALGORITHM OUTLINE:
        '
        '        * algorithm  performs  two kinds of iterations: constrained CG  iterations
        '          and constrained Newton iterations
        '        * initially it performs small number of constrained CG  iterations,  which
        '          can efficiently activate/deactivate multiple constraints
        '        * after CG phase algorithm tries to calculate Cholesky  decomposition  and
        '          to perform several constrained Newton steps. If  Cholesky  decomposition
        '          failed (matrix is indefinite even under constraints),  we  perform  more
        '          CG iterations until we converge to such set of constraints  that  system
        '          matrix becomes  positive  definite.  Constrained  Newton  steps  greatly
        '          increase convergence speed and precision.
        '        * algorithm interleaves CG and Newton iterations which  allows  to  handle
        '          indefinite matrices (CG phase) and quickly converge after final  set  of
        '          constraints is found (Newton phase). Combination of CG and Newton phases
        '          is called "outer iteration".
        '        * it is possible to turn off Newton  phase  (beneficial  for  semidefinite
        '          problems - Cholesky decomposition will fail too often)
        '          
        '        ALGORITHM LIMITATIONS:
        '
        '        * algorithm does not support general  linear  constraints;  only  boundary
        '          ones are supported
        '        * Cholesky decomposition for sparse problems  is  performed  with  Skyline
        '          Cholesky solver, which is intended for low-profile matrices. No profile-
        '          reducing reordering of variables is performed in this version of ALGLIB.
        '        * problems with near-zero negative eigenvalues (or exacty zero  ones)  may
        '          experience about 2-3x performance penalty. The reason is  that  Cholesky
        '          decomposition can not be performed until we identify directions of  zero
        '          and negative curvature and activate corresponding boundary constraints -
        '          but we need a lot of trial and errors because these directions  are hard
        '          to notice in the matrix spectrum.
        '          In this case you may turn off Newton phase of algorithm.
        '          Large negative eigenvalues  are  not  an  issue,  so  highly  non-convex
        '          problems can be solved very efficiently.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            EpsG    -   >=0
        '                        The  subroutine  finishes  its  work   if   the  condition
        '                        |v|<EpsG is satisfied, where:
        '                        * |.| means Euclidian norm
        '                        * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
        '                        * g - gradient
        '                        * s - scaling coefficients set by MinQPSetScale()
        '            EpsF    -   >=0
        '                        The  subroutine  finishes its work if exploratory steepest
        '                        descent  step  on  k+1-th iteration  satisfies   following
        '                        condition:  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
        '            EpsX    -   >=0
        '                        The  subroutine  finishes its work if exploratory steepest
        '                        descent  step  on  k+1-th iteration  satisfies   following
        '                        condition:  
        '                        * |.| means Euclidian norm
        '                        * v - scaled step vector, v[i]=dx[i]/s[i]
        '                        * dx - step vector, dx=X(k+1)-X(k)
        '                        * s - scaling coefficients set by MinQPSetScale()
        '            MaxOuterIts-maximum number of OUTER iterations.  One  outer  iteration
        '                        includes some amount of CG iterations (from 5 to  ~N)  and
        '                        one or several (usually small amount) Newton steps.  Thus,
        '                        one outer iteration has high cost, but can greatly  reduce
        '                        funcation value.
        '            UseNewton-  use Newton phase or not:
        '                        * Newton phase improves performance of  positive  definite
        '                          dense problems (about 2 times improvement can be observed)
        '                        * can result in some performance penalty  on  semidefinite
        '                          or slightly negative definite  problems  -  each  Newton
        '                          phase will bring no improvement (Cholesky failure),  but
        '                          still will require computational time.
        '                        * if you doubt, you can turn off this  phase  -  optimizer
        '                          will retain its most of its high speed.
        '
        '        IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
        '        BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!
        '
        '        Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
        '        to automatic stopping criterion selection (presently it is  small    step
        '        length, but it may change in the future versions of ALGLIB).
        '
        '          -- ALGLIB --
        '             Copyright 22.05.2014 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetalgoquickqp(state As minqpstate, epsg As Double, epsf As Double, epsx As Double, maxouterits As Integer, usenewton As Boolean)
            alglib.ap.assert(Math.isfinite(epsg), "MinQPSetAlgoQuickQP: EpsG is not finite number")
            alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinQPSetAlgoQuickQP: negative EpsG")
            alglib.ap.assert(Math.isfinite(epsf), "MinQPSetAlgoQuickQP: EpsF is not finite number")
            alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinQPSetAlgoQuickQP: negative EpsF")
            alglib.ap.assert(Math.isfinite(epsx), "MinQPSetAlgoQuickQP: EpsX is not finite number")
            alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinQPSetAlgoQuickQP: negative EpsX")
            alglib.ap.assert(maxouterits >= 0, "MinQPSetAlgoQuickQP: negative MaxOuterIts!")
            state.algokind = 3
            If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxouterits = 0 Then
                epsx = 0.000001
            End If
            state.qqpsettingsuser.maxouterits = maxouterits
            state.qqpsettingsuser.epsg = epsg
            state.qqpsettingsuser.epsf = epsf
            state.qqpsettingsuser.epsx = epsx
            state.qqpsettingsuser.cnphase = usenewton
        End Sub


        '************************************************************************
        '        This function sets boundary constraints for QP solver
        '
        '        Boundary constraints are inactive by default (after initial creation).
        '        After  being  set,  they  are  preserved  until explicitly turned off with
        '        another SetBC() call.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure stores algorithm state
        '            BndL    -   lower bounds, array[N].
        '                        If some (all) variables are unbounded, you may specify
        '                        very small number or -INF (latter is recommended because
        '                        it will allow solver to use better algorithm).
        '            BndU    -   upper bounds, array[N].
        '                        If some (all) variables are unbounded, you may specify
        '                        very large number or +INF (latter is recommended because
        '                        it will allow solver to use better algorithm).
        '                        
        '        NOTE: it is possible to specify BndL[i]=BndU[i]. In this case I-th
        '        variable will be "frozen" at X[i]=BndL[i]=BndU[i].
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetbc(state As minqpstate, bndl As Double(), bndu As Double())
            Dim i As Integer = 0
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.len(bndl) >= n, "MinQPSetBC: Length(BndL)<N")
            alglib.ap.assert(alglib.ap.len(bndu) >= n, "MinQPSetBC: Length(BndU)<N")
            For i = 0 To n - 1
                alglib.ap.assert(Math.isfinite(bndl(i)) OrElse [Double].IsNegativeInfinity(bndl(i)), "MinQPSetBC: BndL contains NAN or +INF")
                alglib.ap.assert(Math.isfinite(bndu(i)) OrElse [Double].IsPositiveInfinity(bndu(i)), "MinQPSetBC: BndU contains NAN or -INF")
                state.bndl(i) = bndl(i)
                state.havebndl(i) = Math.isfinite(bndl(i))
                state.bndu(i) = bndu(i)
                state.havebndu(i) = Math.isfinite(bndu(i))
            Next
        End Sub


        '************************************************************************
        '        This function sets linear constraints for QP optimizer.
        '
        '        Linear constraints are inactive by default (after initial creation).
        '
        '        INPUT PARAMETERS:
        '            State   -   structure previously allocated with MinQPCreate call.
        '            C       -   linear constraints, array[K,N+1].
        '                        Each row of C represents one constraint, either equality
        '                        or inequality (see below):
        '                        * first N elements correspond to coefficients,
        '                        * last element corresponds to the right part.
        '                        All elements of C (including right part) must be finite.
        '            CT      -   type of constraints, array[K]:
        '                        * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
        '                        * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
        '                        * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
        '            K       -   number of equality/inequality constraints, K>=0:
        '                        * if given, only leading K elements of C/CT are used
        '                        * if not given, automatically determined from sizes of C/CT
        '
        '        NOTE 1: linear (non-bound) constraints are satisfied only approximately  -
        '                there always exists some minor violation (about 10^-10...10^-13)
        '                due to numerical errors.
        '
        '          -- ALGLIB --
        '             Copyright 19.06.2012 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetlc(state As minqpstate, c As Double(,), ct As Integer(), k As Integer)
            Dim n As Integer = 0
            Dim i As Integer = 0
            Dim j As Integer = 0
            Dim v As Double = 0
            Dim i_ As Integer = 0

            n = state.n

            '
            ' First, check for errors in the inputs
            '
            alglib.ap.assert(k >= 0, "MinQPSetLC: K<0")
            alglib.ap.assert(alglib.ap.cols(c) >= n + 1 OrElse k = 0, "MinQPSetLC: Cols(C)<N+1")
            alglib.ap.assert(alglib.ap.rows(c) >= k, "MinQPSetLC: Rows(C)<K")
            alglib.ap.assert(alglib.ap.len(ct) >= k, "MinQPSetLC: Length(CT)<K")
            alglib.ap.assert(apserv.apservisfinitematrix(c, k, n + 1), "MinQPSetLC: C contains infinite or NaN values!")

            '
            ' Handle zero K
            '
            If k = 0 Then
                state.nec = 0
                state.nic = 0
                Return
            End If

            '
            ' Equality constraints are stored first, in the upper
            ' NEC rows of State.CLEIC matrix. Inequality constraints
            ' are stored in the next NIC rows.
            '
            ' NOTE: we convert inequality constraints to the form
            ' A*x<=b before copying them.
            '
            apserv.rmatrixsetlengthatleast(state.cleic, k, n + 1)
            state.nec = 0
            state.nic = 0
            For i = 0 To k - 1
                If ct(i) = 0 Then
                    For i_ = 0 To n
                        state.cleic(state.nec, i_) = c(i, i_)
                    Next
                    state.nec = state.nec + 1
                End If
            Next
            For i = 0 To k - 1
                If ct(i) <> 0 Then
                    If ct(i) > 0 Then
                        For i_ = 0 To n
                            state.cleic(state.nec + state.nic, i_) = -c(i, i_)
                        Next
                    Else
                        For i_ = 0 To n
                            state.cleic(state.nec + state.nic, i_) = c(i, i_)
                        Next
                    End If
                    state.nic = state.nic + 1
                End If
            Next

            '
            ' Normalize rows of State.CLEIC: each row must have unit norm.
            ' Norm is calculated using first N elements (i.e. right part is
            ' not counted when we calculate norm).
            '
            For i = 0 To k - 1
                v = 0
                For j = 0 To n - 1
                    v = v + Math.sqr(state.cleic(i, j))
                Next
                If CDbl(v) = CDbl(0) Then
                    Continue For
                End If
                v = 1 / System.Math.sqrt(v)
                For i_ = 0 To n
                    state.cleic(i, i_) = v * state.cleic(i, i_)
                Next
            Next
        End Sub


        '************************************************************************
        '        This function solves quadratic programming problem.
        '
        '        Prior to calling this function you should choose solver by means of one of
        '        the following functions:
        '
        '        * MinQPSetAlgoQuickQP() - for QuickQP solver
        '        * MinQPSetAlgoBLEIC() - for BLEIC-QP solver
        '
        '        These functions also allow you to control stopping criteria of the solver.
        '        If you did not set solver,  MinQP  subpackage  will  automatically  select
        '        solver for your problem and will run it with default stopping criteria.
        '
        '        However, it is better to set explicitly solver and its stopping criteria.
        '
        '        INPUT PARAMETERS:
        '            State   -   algorithm state
        '
        '        You should use MinQPResults() function to access results after calls
        '        to this function.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey.
        '             Special thanks to Elvira Illarionova  for  important  suggestions  on
        '             the linearly constrained QP algorithm.
        '        ************************************************************************

        Public Shared Sub minqpoptimize(state As minqpstate)
            Dim n As Integer = 0
            Dim i As Integer = 0
            Dim nbc As Integer = 0
            Dim currentsolver As Integer = 0

            n = state.n
            state.repterminationtype = -5
            state.repinneriterationscount = 0
            state.repouteriterationscount = 0
            state.repncholesky = 0
            state.repnmv = 0

            '
            ' check correctness of constraints
            '
            For i = 0 To n - 1
                If state.havebndl(i) AndAlso state.havebndu(i) Then
                    If CDbl(state.bndl(i)) > CDbl(state.bndu(i)) Then
                        state.repterminationtype = -3
                        Return
                    End If
                End If
            Next

            '
            ' count number of bound and linear constraints
            '
            nbc = 0
            For i = 0 To n - 1
                If state.havebndl(i) Then
                    nbc = nbc + 1
                End If
                If state.havebndu(i) Then
                    nbc = nbc + 1
                End If
            Next

            '
            ' Initial point:
            ' * if we have starting point in StartX, we just have to bound it
            ' * if we do not have StartX, deduce initial point from boundary constraints
            '
            If state.havex Then
                For i = 0 To n - 1
                    state.xs(i) = state.startx(i)
                    If state.havebndl(i) AndAlso CDbl(state.xs(i)) < CDbl(state.bndl(i)) Then
                        state.xs(i) = state.bndl(i)
                    End If
                    If state.havebndu(i) AndAlso CDbl(state.xs(i)) > CDbl(state.bndu(i)) Then
                        state.xs(i) = state.bndu(i)
                    End If
                Next
            Else
                For i = 0 To n - 1
                    If state.havebndl(i) AndAlso state.havebndu(i) Then
                        state.xs(i) = 0.5 * (state.bndl(i) + state.bndu(i))
                        Continue For
                    End If
                    If state.havebndl(i) Then
                        state.xs(i) = state.bndl(i)
                        Continue For
                    End If
                    If state.havebndu(i) Then
                        state.xs(i) = state.bndu(i)
                        Continue For
                    End If
                    state.xs(i) = 0
                Next
            End If

            '
            ' Choose solver - user-specified or default one.
            '
            currentsolver = state.algokind
            If currentsolver = 0 Then

                '
                ' Choose solver automatically
                '
                If state.nec + state.nic = 0 Then

                    '
                    ' QQP solver is used for problems without linear constraints
                    '
                    currentsolver = 3
                    qqpsolver.qqploaddefaults(n, state.qqpsettingscurrent)
                Else

                    '
                    ' QP-BLEIC solver is used for problems without linear constraints
                    '
                    currentsolver = 2
                    qpbleicsolver.qpbleicloaddefaults(n, state.qpbleicsettingscurrent)
                End If
            Else
                alglib.ap.assert((currentsolver = 1 OrElse currentsolver = 2) OrElse currentsolver = 3, "MinQPOptimize: internal error")
                If currentsolver = 1 Then
                End If
                If currentsolver = 2 Then

                    '
                    ' QP-BLEIC solver is chosen by user
                    '
                    qpbleicsolver.qpbleiccopysettings(state.qpbleicsettingsuser, state.qpbleicsettingscurrent)
                End If
                If currentsolver = 3 Then

                    '
                    ' QQP solver is chosen by user
                    '
                    qqpsolver.qqpcopysettings(state.qqpsettingsuser, state.qqpsettingscurrent)
                End If
            End If

            '
            ' QP-BLEIC solver
            '
            If currentsolver = 2 Then
                qpbleicsolver.qpbleicoptimize(state.a, state.sparsea, state.akind, state.sparseaupper, state.absasum, state.absasum2, _
                    state.b, state.bndl, state.bndu, state.s, state.xorigin, n, _
                    state.cleic, state.nec, state.nic, state.qpbleicsettingscurrent, state.qpbleicbuf, state.qpbleicfirstcall, _
                    state.xs, state.repterminationtype)
                state.repinneriterationscount = state.qpbleicbuf.repinneriterationscount
                state.repouteriterationscount = state.qpbleicbuf.repouteriterationscount
                Return
            End If

            '
            ' QuickQP solver
            '
            If currentsolver = 3 Then
                If state.nec + state.nic > 0 Then
                    state.repterminationtype = -5
                    Return
                End If
                qqpsolver.qqpoptimize(state.a, state.sparsea, state.akind, state.sparseaupper, state.b, state.bndl, _
                    state.bndu, state.s, state.xorigin, n, state.cleic, state.nec, _
                    state.nic, state.qqpsettingscurrent, state.qqpbuf, state.xs, state.repterminationtype)
                state.repinneriterationscount = state.qqpbuf.repinneriterationscount
                state.repouteriterationscount = state.qqpbuf.repouteriterationscount
                state.repncholesky = state.qqpbuf.repncholesky
                Return
            End If

            '
            ' Cholesky solver.
            '
            If currentsolver = 1 Then

                '
                ' Check matrix type.
                ' Cholesky solver supports only dense matrices.
                '
                If state.akind <> 0 Then
                    state.repterminationtype = -5
                    Return
                End If
                qpcholeskysolver.qpcholeskyoptimize(state.a, state.absamax * n, state.b, state.bndl, state.bndu, state.s, _
                    state.xorigin, n, state.cleic, state.nec, state.nic, state.qpcholeskybuf, _
                    state.xs, state.repterminationtype)
                state.repinneriterationscount = state.qqpbuf.repinneriterationscount
                state.repouteriterationscount = state.qqpbuf.repouteriterationscount
                state.repncholesky = state.qqpbuf.repncholesky
                Return
            End If
        End Sub


        '************************************************************************
        '        QP solver results
        '
        '        INPUT PARAMETERS:
        '            State   -   algorithm state
        '
        '        OUTPUT PARAMETERS:
        '            X       -   array[0..N-1], solution.
        '                        This array is allocated and initialized only when
        '                        Rep.TerminationType parameter is positive (success).
        '            Rep     -   optimization report. You should check Rep.TerminationType,
        '                        which contains completion code, and you may check  another
        '                        fields which contain another information  about  algorithm
        '                        functioning.
        '                        
        '                        Failure codes returned by algorithm are:
        '                        * -5    inappropriate solver was used:
        '                                * Cholesky solver for (semi)indefinite problems
        '                                * Cholesky solver for problems with sparse matrix
        '                                * QuickQP solver for problem with  general  linear
        '                                  constraints
        '                        * -4    BLEIC-QP/QuickQP   solver    found   unconstrained
        '                                direction  of   negative  curvature  (function  is
        '                                unbounded from below even under constraints),   no
        '                                meaningful minimum can be found.
        '                        * -3    inconsistent constraints (or maybe  feasible point
        '                                is too  hard  to  find).  If  you  are  sure  that
        '                                constraints are feasible, try to restart optimizer
        '                                with better initial approximation.
        '                                
        '                        Completion codes specific for Cholesky algorithm:
        '                        *  4   successful completion
        '                        
        '                        Completion codes specific for BLEIC/QuickQP algorithms:
        '                        *  1   relative function improvement is no more than EpsF.
        '                        *  2   scaled step is no more than EpsX.
        '                        *  4   scaled gradient norm is no more than EpsG.
        '                        *  5   MaxIts steps was taken
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpresults(state As minqpstate, ByRef x As Double(), rep As minqpreport)
            x = New Double(-1) {}

            minqpresultsbuf(state, x, rep)
        End Sub


        '************************************************************************
        '        QP results
        '
        '        Buffered implementation of MinQPResults() which uses pre-allocated  buffer
        '        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
        '        intended to be used in the inner cycles of performance critical algorithms
        '        where array reallocation penalty is too large to be ignored.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpresultsbuf(state As minqpstate, ByRef x As Double(), rep As minqpreport)
            Dim i_ As Integer = 0

            If alglib.ap.len(x) < state.n Then
                x = New Double(state.n - 1) {}
            End If
            For i_ = 0 To state.n - 1
                x(i_) = state.xs(i_)
            Next
            rep.inneriterationscount = state.repinneriterationscount
            rep.outeriterationscount = state.repouteriterationscount
            rep.nmv = state.repnmv
            rep.ncholesky = state.repncholesky
            rep.terminationtype = state.repterminationtype
        End Sub


        '************************************************************************
        '        Fast version of MinQPSetLinearTerm(), which doesn't check its arguments.
        '        For internal use only.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetlineartermfast(state As minqpstate, b As Double())
            Dim i_ As Integer = 0

            For i_ = 0 To state.n - 1
                state.b(i_) = b(i_)
            Next
        End Sub


        '************************************************************************
        '        Fast version of MinQPSetQuadraticTerm(), which doesn't check its arguments.
        '
        '        It accepts additional parameter - shift S, which allows to "shift"  matrix
        '        A by adding s*I to A. S must be positive (although it is not checked).
        '
        '        For internal use only.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetquadratictermfast(state As minqpstate, a As Double(,), isupper As Boolean, s As Double)
            Dim i As Integer = 0
            Dim j As Integer = 0
            Dim n As Integer = 0
            Dim v As Double = 0
            Dim j0 As Integer = 0
            Dim j1 As Integer = 0

            n = state.n
            state.akind = 0
            cqmodels.cqmseta(state.a, a, isupper, 1.0)
            If CDbl(s) > CDbl(0) Then
                apserv.rvectorsetlengthatleast(state.tmp0, n)
                For i = 0 To n - 1
                    state.tmp0(i) = a(i, i) + s
                Next
                cqmodels.cqmrewritedensediagonal(state.a, state.tmp0)
            End If

            '
            ' Estimate norm of A
            ' (it will be used later in the quadratic penalty function)
            '
            state.absamax = 0
            state.absasum = 0
            state.absasum2 = 0
            For i = 0 To n - 1
                If isupper Then
                    j0 = i
                    j1 = n - 1
                Else
                    j0 = 0
                    j1 = i
                End If
                For j = j0 To j1
                    v = System.Math.Abs(a(i, j))
                    state.absamax = System.Math.Max(state.absamax, v)
                    state.absasum = state.absasum + v
                    state.absasum2 = state.absasum2 + v * v
                Next
            Next
        End Sub


        '************************************************************************
        '        Internal function which allows to rewrite diagonal of quadratic term.
        '        For internal use only.
        '
        '        This function can be used only when you have dense A and already made
        '        MinQPSetQuadraticTerm(Fast) call.
        '
        '          -- ALGLIB --
        '             Copyright 16.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqprewritediagonal(state As minqpstate, s As Double())
            cqmodels.cqmrewritedensediagonal(state.a, s)
        End Sub


        '************************************************************************
        '        Fast version of MinQPSetStartingPoint(), which doesn't check its arguments.
        '        For internal use only.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetstartingpointfast(state As minqpstate, x As Double())
            Dim n As Integer = 0
            Dim i_ As Integer = 0

            n = state.n
            For i_ = 0 To n - 1
                state.startx(i_) = x(i_)
            Next
            state.havex = True
        End Sub


        '************************************************************************
        '        Fast version of MinQPSetOrigin(), which doesn't check its arguments.
        '        For internal use only.
        '
        '          -- ALGLIB --
        '             Copyright 11.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minqpsetoriginfast(state As minqpstate, xorigin As Double())
            Dim n As Integer = 0
            Dim i_ As Integer = 0

            n = state.n
            For i_ = 0 To n - 1
                state.xorigin(i_) = xorigin(i_)
            Next
        End Sub


    End Class
    Public Class minlm
        '************************************************************************
        '        Levenberg-Marquardt optimizer.
        '
        '        This structure should be created using one of the MinLMCreate???()
        '        functions. You should not access its fields directly; use ALGLIB functions
        '        to work with it.
        '        ************************************************************************

        Public Class minlmstate
            Inherits apobject
            Public n As Integer
            Public m As Integer
            Public diffstep As Double
            Public epsg As Double
            Public epsf As Double
            Public epsx As Double
            Public maxits As Integer
            Public xrep As Boolean
            Public stpmax As Double
            Public maxmodelage As Integer
            Public makeadditers As Boolean
            Public x As Double()
            Public f As Double
            Public fi As Double()
            Public j As Double(,)
            Public h As Double(,)
            Public g As Double()
            Public needf As Boolean
            Public needfg As Boolean
            Public needfgh As Boolean
            Public needfij As Boolean
            Public needfi As Boolean
            Public xupdated As Boolean
            Public userterminationneeded As Boolean
            Public algomode As Integer
            Public hasf As Boolean
            Public hasfi As Boolean
            Public hasg As Boolean
            Public xbase As Double()
            Public fbase As Double
            Public fibase As Double()
            Public gbase As Double()
            Public quadraticmodel As Double(,)
            Public bndl As Double()
            Public bndu As Double()
            Public havebndl As Boolean()
            Public havebndu As Boolean()
            Public s As Double()
            Public lambdav As Double
            Public nu As Double
            Public modelage As Integer
            Public xdir As Double()
            Public deltax As Double()
            Public deltaf As Double()
            Public deltaxready As Boolean
            Public deltafready As Boolean
            Public teststep As Double
            Public repiterationscount As Integer
            Public repterminationtype As Integer
            Public repfuncidx As Integer
            Public repvaridx As Integer
            Public repnfunc As Integer
            Public repnjac As Integer
            Public repngrad As Integer
            Public repnhess As Integer
            Public repncholesky As Integer
            Public rstate As rcommstate
            Public choleskybuf As Double()
            Public tmp0 As Double()
            Public actualdecrease As Double
            Public predicteddecrease As Double
            Public xm1 As Double
            Public xp1 As Double
            Public fm1 As Double()
            Public fp1 As Double()
            Public fc1 As Double()
            Public gm1 As Double()
            Public gp1 As Double()
            Public gc1 As Double()
            Public internalstate As minlbfgs.minlbfgsstate
            Public internalrep As minlbfgs.minlbfgsreport
            Public qpstate As minqp.minqpstate
            Public qprep As minqp.minqpreport
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
                x = New Double(-1) {}
                fi = New Double(-1) {}
                j = New Double(-1, -1) {}
                h = New Double(-1, -1) {}
                g = New Double(-1) {}
                xbase = New Double(-1) {}
                fibase = New Double(-1) {}
                gbase = New Double(-1) {}
                quadraticmodel = New Double(-1, -1) {}
                bndl = New Double(-1) {}
                bndu = New Double(-1) {}
                havebndl = New Boolean(-1) {}
                havebndu = New Boolean(-1) {}
                s = New Double(-1) {}
                xdir = New Double(-1) {}
                deltax = New Double(-1) {}
                deltaf = New Double(-1) {}
                rstate = New rcommstate()
                choleskybuf = New Double(-1) {}
                tmp0 = New Double(-1) {}
                fm1 = New Double(-1) {}
                fp1 = New Double(-1) {}
                fc1 = New Double(-1) {}
                gm1 = New Double(-1) {}
                gp1 = New Double(-1) {}
                gc1 = New Double(-1) {}
                internalstate = New minlbfgs.minlbfgsstate()
                internalrep = New minlbfgs.minlbfgsreport()
                qpstate = New minqp.minqpstate()
                qprep = New minqp.minqpreport()
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New minlmstate()
                _result.n = n
                _result.m = m
                _result.diffstep = diffstep
                _result.epsg = epsg
                _result.epsf = epsf
                _result.epsx = epsx
                _result.maxits = maxits
                _result.xrep = xrep
                _result.stpmax = stpmax
                _result.maxmodelage = maxmodelage
                _result.makeadditers = makeadditers
                _result.x = DirectCast(x.Clone(), Double())
                _result.f = f
                _result.fi = DirectCast(fi.Clone(), Double())
                _result.j = DirectCast(j.Clone(), Double(,))
                _result.h = DirectCast(h.Clone(), Double(,))
                _result.g = DirectCast(g.Clone(), Double())
                _result.needf = needf
                _result.needfg = needfg
                _result.needfgh = needfgh
                _result.needfij = needfij
                _result.needfi = needfi
                _result.xupdated = xupdated
                _result.userterminationneeded = userterminationneeded
                _result.algomode = algomode
                _result.hasf = hasf
                _result.hasfi = hasfi
                _result.hasg = hasg
                _result.xbase = DirectCast(xbase.Clone(), Double())
                _result.fbase = fbase
                _result.fibase = DirectCast(fibase.Clone(), Double())
                _result.gbase = DirectCast(gbase.Clone(), Double())
                _result.quadraticmodel = DirectCast(quadraticmodel.Clone(), Double(,))
                _result.bndl = DirectCast(bndl.Clone(), Double())
                _result.bndu = DirectCast(bndu.Clone(), Double())
                _result.havebndl = DirectCast(havebndl.Clone(), Boolean())
                _result.havebndu = DirectCast(havebndu.Clone(), Boolean())
                _result.s = DirectCast(s.Clone(), Double())
                _result.lambdav = lambdav
                _result.nu = nu
                _result.modelage = modelage
                _result.xdir = DirectCast(xdir.Clone(), Double())
                _result.deltax = DirectCast(deltax.Clone(), Double())
                _result.deltaf = DirectCast(deltaf.Clone(), Double())
                _result.deltaxready = deltaxready
                _result.deltafready = deltafready
                _result.teststep = teststep
                _result.repiterationscount = repiterationscount
                _result.repterminationtype = repterminationtype
                _result.repfuncidx = repfuncidx
                _result.repvaridx = repvaridx
                _result.repnfunc = repnfunc
                _result.repnjac = repnjac
                _result.repngrad = repngrad
                _result.repnhess = repnhess
                _result.repncholesky = repncholesky
                _result.rstate = DirectCast(rstate.make_copy(), rcommstate)
                _result.choleskybuf = DirectCast(choleskybuf.Clone(), Double())
                _result.tmp0 = DirectCast(tmp0.Clone(), Double())
                _result.actualdecrease = actualdecrease
                _result.predicteddecrease = predicteddecrease
                _result.xm1 = xm1
                _result.xp1 = xp1
                _result.fm1 = DirectCast(fm1.Clone(), Double())
                _result.fp1 = DirectCast(fp1.Clone(), Double())
                _result.fc1 = DirectCast(fc1.Clone(), Double())
                _result.gm1 = DirectCast(gm1.Clone(), Double())
                _result.gp1 = DirectCast(gp1.Clone(), Double())
                _result.gc1 = DirectCast(gc1.Clone(), Double())
                _result.internalstate = DirectCast(internalstate.make_copy(), minlbfgs.minlbfgsstate)
                _result.internalrep = DirectCast(internalrep.make_copy(), minlbfgs.minlbfgsreport)
                _result.qpstate = DirectCast(qpstate.make_copy(), minqp.minqpstate)
                _result.qprep = DirectCast(qprep.make_copy(), minqp.minqpreport)
                Return _result
            End Function
        End Class


        '************************************************************************
        '        Optimization report, filled by MinLMResults() function
        '
        '        FIELDS:
        '        * TerminationType, completetion code:
        '            * -7    derivative correctness check failed;
        '                    see Rep.WrongNum, Rep.WrongI, Rep.WrongJ for
        '                    more information.
        '            * -3    constraints are inconsistent
        '            *  1    relative function improvement is no more than
        '                    EpsF.
        '            *  2    relative step is no more than EpsX.
        '            *  4    gradient is no more than EpsG.
        '            *  5    MaxIts steps was taken
        '            *  7    stopping conditions are too stringent,
        '                    further improvement is impossible
        '            *  8    terminated   by  user  who  called  MinLMRequestTermination().
        '                    X contains point which was "current accepted" when termination
        '                    request was submitted.
        '        * IterationsCount, contains iterations count
        '        * NFunc, number of function calculations
        '        * NJac, number of Jacobi matrix calculations
        '        * NGrad, number of gradient calculations
        '        * NHess, number of Hessian calculations
        '        * NCholesky, number of Cholesky decomposition calculations
        '        ************************************************************************

        Public Class minlmreport
            Inherits apobject
            Public iterationscount As Integer
            Public terminationtype As Integer
            Public funcidx As Integer
            Public varidx As Integer
            Public nfunc As Integer
            Public njac As Integer
            Public ngrad As Integer
            Public nhess As Integer
            Public ncholesky As Integer
            Public Sub New()
                init()
            End Sub
            Public Overrides Sub init()
            End Sub
            Public Overrides Function make_copy() As alglib.apobject
                Dim _result As New minlmreport()
                _result.iterationscount = iterationscount
                _result.terminationtype = terminationtype
                _result.funcidx = funcidx
                _result.varidx = varidx
                _result.nfunc = nfunc
                _result.njac = njac
                _result.ngrad = ngrad
                _result.nhess = nhess
                _result.ncholesky = ncholesky
                Return _result
            End Function
        End Class




        Public Const lambdaup As Double = 2.0
        Public Const lambdadown As Double = 0.33
        Public Const suspiciousnu As Double = 16
        Public Const smallmodelage As Integer = 3
        Public Const additers As Integer = 5


        '************************************************************************
        '                        IMPROVED LEVENBERG-MARQUARDT METHOD FOR
        '                         NON-LINEAR LEAST SQUARES OPTIMIZATION
        '
        '        DESCRIPTION:
        '        This function is used to find minimum of function which is represented  as
        '        sum of squares:
        '            F(x) = f[0]^2(x[0],...,x[n-1]) + ... + f[m-1]^2(x[0],...,x[n-1])
        '        using value of function vector f[] and Jacobian of f[].
        '
        '
        '        REQUIREMENTS:
        '        This algorithm will request following information during its operation:
        '
        '        * function vector f[] at given point X
        '        * function vector f[] and Jacobian of f[] (simultaneously) at given point
        '
        '        There are several overloaded versions of  MinLMOptimize()  function  which
        '        correspond  to  different LM-like optimization algorithms provided by this
        '        unit. You should choose version which accepts fvec()  and jac() callbacks.
        '        First  one  is used to calculate f[] at given point, second one calculates
        '        f[] and Jacobian df[i]/dx[j].
        '
        '        You can try to initialize MinLMState structure with VJ  function and  then
        '        use incorrect version  of  MinLMOptimize()  (for  example,  version  which
        '        works  with  general  form function and does not provide Jacobian), but it
        '        will  lead  to  exception  being  thrown  after first attempt to calculate
        '        Jacobian.
        '
        '
        '        USAGE:
        '        1. User initializes algorithm state with MinLMCreateVJ() call
        '        2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
        '           other functions
        '        3. User calls MinLMOptimize() function which  takes algorithm  state   and
        '           callback functions.
        '        4. User calls MinLMResults() to get solution
        '        5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
        '           with same N/M but another starting point and/or another function.
        '           MinLMRestartFrom() allows to reuse already initialized structure.
        '
        '
        '        INPUT PARAMETERS:
        '            N       -   dimension, N>1
        '                        * if given, only leading N elements of X are used
        '                        * if not given, automatically determined from size of X
        '            M       -   number of functions f[i]
        '            X       -   initial solution, array[0..N-1]
        '
        '        OUTPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '
        '        NOTES:
        '        1. you may tune stopping conditions with MinLMSetCond() function
        '        2. if target function contains exp() or other fast growing functions,  and
        '           optimization algorithm makes too large steps which leads  to  overflow,
        '           use MinLMSetStpMax() function to bound algorithm's steps.
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatevj(n As Integer, m As Integer, x As Double(), state As minlmstate)
            alglib.ap.assert(n >= 1, "MinLMCreateVJ: N<1!")
            alglib.ap.assert(m >= 1, "MinLMCreateVJ: M<1!")
            alglib.ap.assert(alglib.ap.len(x) >= n, "MinLMCreateVJ: Length(X)<N!")
            alglib.ap.assert(apserv.isfinitevector(x, n), "MinLMCreateVJ: X contains infinite or NaN values!")

            '
            ' initialize, check parameters
            '
            state.teststep = 0
            state.n = n
            state.m = m
            state.algomode = 1
            state.hasf = False
            state.hasfi = True
            state.hasg = False

            '
            ' second stage of initialization
            '
            lmprepare(n, m, False, state)
            minlmsetacctype(state, 0)
            minlmsetcond(state, 0, 0, 0, 0)
            minlmsetxrep(state, False)
            minlmsetstpmax(state, 0)
            minlmrestartfrom(state, x)
        End Sub


        '************************************************************************
        '                        IMPROVED LEVENBERG-MARQUARDT METHOD FOR
        '                         NON-LINEAR LEAST SQUARES OPTIMIZATION
        '
        '        DESCRIPTION:
        '        This function is used to find minimum of function which is represented  as
        '        sum of squares:
        '            F(x) = f[0]^2(x[0],...,x[n-1]) + ... + f[m-1]^2(x[0],...,x[n-1])
        '        using value of function vector f[] only. Finite differences  are  used  to
        '        calculate Jacobian.
        '
        '
        '        REQUIREMENTS:
        '        This algorithm will request following information during its operation:
        '        * function vector f[] at given point X
        '
        '        There are several overloaded versions of  MinLMOptimize()  function  which
        '        correspond  to  different LM-like optimization algorithms provided by this
        '        unit. You should choose version which accepts fvec() callback.
        '
        '        You can try to initialize MinLMState structure with VJ  function and  then
        '        use incorrect version  of  MinLMOptimize()  (for  example,  version  which
        '        works with general form function and does not accept function vector), but
        '        it will  lead  to  exception being thrown after first attempt to calculate
        '        Jacobian.
        '
        '
        '        USAGE:
        '        1. User initializes algorithm state with MinLMCreateV() call
        '        2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
        '           other functions
        '        3. User calls MinLMOptimize() function which  takes algorithm  state   and
        '           callback functions.
        '        4. User calls MinLMResults() to get solution
        '        5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
        '           with same N/M but another starting point and/or another function.
        '           MinLMRestartFrom() allows to reuse already initialized structure.
        '
        '
        '        INPUT PARAMETERS:
        '            N       -   dimension, N>1
        '                        * if given, only leading N elements of X are used
        '                        * if not given, automatically determined from size of X
        '            M       -   number of functions f[i]
        '            X       -   initial solution, array[0..N-1]
        '            DiffStep-   differentiation step, >0
        '
        '        OUTPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '
        '        See also MinLMIteration, MinLMResults.
        '
        '        NOTES:
        '        1. you may tune stopping conditions with MinLMSetCond() function
        '        2. if target function contains exp() or other fast growing functions,  and
        '           optimization algorithm makes too large steps which leads  to  overflow,
        '           use MinLMSetStpMax() function to bound algorithm's steps.
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatev(n As Integer, m As Integer, x As Double(), diffstep As Double, state As minlmstate)
            alglib.ap.assert(Math.isfinite(diffstep), "MinLMCreateV: DiffStep is not finite!")
            alglib.ap.assert(CDbl(diffstep) > CDbl(0), "MinLMCreateV: DiffStep<=0!")
            alglib.ap.assert(n >= 1, "MinLMCreateV: N<1!")
            alglib.ap.assert(m >= 1, "MinLMCreateV: M<1!")
            alglib.ap.assert(alglib.ap.len(x) >= n, "MinLMCreateV: Length(X)<N!")
            alglib.ap.assert(apserv.isfinitevector(x, n), "MinLMCreateV: X contains infinite or NaN values!")

            '
            ' Initialize
            '
            state.teststep = 0
            state.n = n
            state.m = m
            state.algomode = 0
            state.hasf = False
            state.hasfi = True
            state.hasg = False
            state.diffstep = diffstep

            '
            ' Second stage of initialization
            '
            lmprepare(n, m, False, state)
            minlmsetacctype(state, 1)
            minlmsetcond(state, 0, 0, 0, 0)
            minlmsetxrep(state, False)
            minlmsetstpmax(state, 0)
            minlmrestartfrom(state, x)
        End Sub


        '************************************************************************
        '            LEVENBERG-MARQUARDT-LIKE METHOD FOR NON-LINEAR OPTIMIZATION
        '
        '        DESCRIPTION:
        '        This  function  is  used  to  find  minimum  of general form (not "sum-of-
        '        -squares") function
        '            F = F(x[0], ..., x[n-1])
        '        using  its  gradient  and  Hessian.  Levenberg-Marquardt modification with
        '        L-BFGS pre-optimization and internal pre-conditioned  L-BFGS  optimization
        '        after each Levenberg-Marquardt step is used.
        '
        '
        '        REQUIREMENTS:
        '        This algorithm will request following information during its operation:
        '
        '        * function value F at given point X
        '        * F and gradient G (simultaneously) at given point X
        '        * F, G and Hessian H (simultaneously) at given point X
        '
        '        There are several overloaded versions of  MinLMOptimize()  function  which
        '        correspond  to  different LM-like optimization algorithms provided by this
        '        unit. You should choose version which accepts func(),  grad()  and  hess()
        '        function pointers. First pointer is used to calculate F  at  given  point,
        '        second  one  calculates  F(x)  and  grad F(x),  third one calculates F(x),
        '        grad F(x), hess F(x).
        '
        '        You can try to initialize MinLMState structure with FGH-function and  then
        '        use incorrect version of MinLMOptimize() (for example, version which  does
        '        not provide Hessian matrix), but it will lead to  exception  being  thrown
        '        after first attempt to calculate Hessian.
        '
        '
        '        USAGE:
        '        1. User initializes algorithm state with MinLMCreateFGH() call
        '        2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
        '           other functions
        '        3. User calls MinLMOptimize() function which  takes algorithm  state   and
        '           pointers (delegates, etc.) to callback functions.
        '        4. User calls MinLMResults() to get solution
        '        5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
        '           with same N but another starting point and/or another function.
        '           MinLMRestartFrom() allows to reuse already initialized structure.
        '
        '
        '        INPUT PARAMETERS:
        '            N       -   dimension, N>1
        '                        * if given, only leading N elements of X are used
        '                        * if not given, automatically determined from size of X
        '            X       -   initial solution, array[0..N-1]
        '
        '        OUTPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '
        '        NOTES:
        '        1. you may tune stopping conditions with MinLMSetCond() function
        '        2. if target function contains exp() or other fast growing functions,  and
        '           optimization algorithm makes too large steps which leads  to  overflow,
        '           use MinLMSetStpMax() function to bound algorithm's steps.
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatefgh(n As Integer, x As Double(), state As minlmstate)
            alglib.ap.assert(n >= 1, "MinLMCreateFGH: N<1!")
            alglib.ap.assert(alglib.ap.len(x) >= n, "MinLMCreateFGH: Length(X)<N!")
            alglib.ap.assert(apserv.isfinitevector(x, n), "MinLMCreateFGH: X contains infinite or NaN values!")

            '
            ' initialize
            '
            state.teststep = 0
            state.n = n
            state.m = 0
            state.algomode = 2
            state.hasf = True
            state.hasfi = False
            state.hasg = True

            '
            ' init2
            '
            lmprepare(n, 0, True, state)
            minlmsetacctype(state, 2)
            minlmsetcond(state, 0, 0, 0, 0)
            minlmsetxrep(state, False)
            minlmsetstpmax(state, 0)
            minlmrestartfrom(state, x)
        End Sub


        '************************************************************************
        '        This function sets stopping conditions for Levenberg-Marquardt optimization
        '        algorithm.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            EpsG    -   >=0
        '                        The  subroutine  finishes  its  work   if   the  condition
        '                        |v|<EpsG is satisfied, where:
        '                        * |.| means Euclidian norm
        '                        * v - scaled gradient vector, v[i]=g[i]*s[i]
        '                        * g - gradient
        '                        * s - scaling coefficients set by MinLMSetScale()
        '            EpsF    -   >=0
        '                        The  subroutine  finishes  its work if on k+1-th iteration
        '                        the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
        '                        is satisfied.
        '            EpsX    -   >=0
        '                        The subroutine finishes its work if  on  k+1-th  iteration
        '                        the condition |v|<=EpsX is fulfilled, where:
        '                        * |.| means Euclidian norm
        '                        * v - scaled step vector, v[i]=dx[i]/s[i]
        '                        * dx - ste pvector, dx=X(k+1)-X(k)
        '                        * s - scaling coefficients set by MinLMSetScale()
        '            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
        '                        iterations   is    unlimited.   Only   Levenberg-Marquardt
        '                        iterations  are  counted  (L-BFGS/CG  iterations  are  NOT
        '                        counted because their cost is very low compared to that of
        '                        LM).
        '
        '        Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
        '        automatic stopping criterion selection (small EpsX).
        '
        '          -- ALGLIB --
        '             Copyright 02.04.2010 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetcond(state As minlmstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
            alglib.ap.assert(Math.isfinite(epsg), "MinLMSetCond: EpsG is not finite number!")
            alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinLMSetCond: negative EpsG!")
            alglib.ap.assert(Math.isfinite(epsf), "MinLMSetCond: EpsF is not finite number!")
            alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinLMSetCond: negative EpsF!")
            alglib.ap.assert(Math.isfinite(epsx), "MinLMSetCond: EpsX is not finite number!")
            alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinLMSetCond: negative EpsX!")
            alglib.ap.assert(maxits >= 0, "MinLMSetCond: negative MaxIts!")
            If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
                epsx = 0.000001
            End If
            state.epsg = epsg
            state.epsf = epsf
            state.epsx = epsx
            state.maxits = maxits
        End Sub


        '************************************************************************
        '        This function turns on/off reporting.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            NeedXRep-   whether iteration reports are needed or not
        '
        '        If NeedXRep is True, algorithm will call rep() callback function if  it is
        '        provided to MinLMOptimize(). Both Levenberg-Marquardt and internal  L-BFGS
        '        iterations are reported.
        '
        '          -- ALGLIB --
        '             Copyright 02.04.2010 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetxrep(state As minlmstate, needxrep As Boolean)
            state.xrep = needxrep
        End Sub


        '************************************************************************
        '        This function sets maximum step length
        '
        '        INPUT PARAMETERS:
        '            State   -   structure which stores algorithm state
        '            StpMax  -   maximum step length, >=0. Set StpMax to 0.0,  if you don't
        '                        want to limit step length.
        '
        '        Use this subroutine when you optimize target function which contains exp()
        '        or  other  fast  growing  functions,  and optimization algorithm makes too
        '        large  steps  which  leads  to overflow. This function allows us to reject
        '        steps  that  are  too  large  (and  therefore  expose  us  to the possible
        '        overflow) without actually calculating function value at the x+stp*d.
        '
        '        NOTE: non-zero StpMax leads to moderate  performance  degradation  because
        '        intermediate  step  of  preconditioned L-BFGS optimization is incompatible
        '        with limits on step size.
        '
        '          -- ALGLIB --
        '             Copyright 02.04.2010 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetstpmax(state As minlmstate, stpmax As Double)
            alglib.ap.assert(Math.isfinite(stpmax), "MinLMSetStpMax: StpMax is not finite!")
            alglib.ap.assert(CDbl(stpmax) >= CDbl(0), "MinLMSetStpMax: StpMax<0!")
            state.stpmax = stpmax
        End Sub


        '************************************************************************
        '        This function sets scaling coefficients for LM optimizer.
        '
        '        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
        '        size and gradient are scaled before comparison with tolerances).  Scale of
        '        the I-th variable is a translation invariant measure of:
        '        a) "how large" the variable is
        '        b) how large the step should be to make significant changes in the function
        '
        '        Generally, scale is NOT considered to be a form of preconditioner.  But LM
        '        optimizer is unique in that it uses scaling matrix both  in  the  stopping
        '        condition tests and as Marquardt damping factor.
        '
        '        Proper scaling is very important for the algorithm performance. It is less
        '        important for the quality of results, but still has some influence (it  is
        '        easier  to  converge  when  variables  are  properly  scaled, so premature
        '        stopping is possible when very badly scalled variables are  combined  with
        '        relaxed stopping conditions).
        '
        '        INPUT PARAMETERS:
        '            State   -   structure stores algorithm state
        '            S       -   array[N], non-zero scaling coefficients
        '                        S[i] may be negative, sign doesn't matter.
        '
        '          -- ALGLIB --
        '             Copyright 14.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetscale(state As minlmstate, s As Double())
            Dim i As Integer = 0

            alglib.ap.assert(alglib.ap.len(s) >= state.n, "MinLMSetScale: Length(S)<N")
            For i = 0 To state.n - 1
                alglib.ap.assert(Math.isfinite(s(i)), "MinLMSetScale: S contains infinite or NAN elements")
                alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinLMSetScale: S contains zero elements")
                state.s(i) = System.Math.Abs(s(i))
            Next
        End Sub


        '************************************************************************
        '        This function sets boundary constraints for LM optimizer
        '
        '        Boundary constraints are inactive by default (after initial creation).
        '        They are preserved until explicitly turned off with another SetBC() call.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure stores algorithm state
        '            BndL    -   lower bounds, array[N].
        '                        If some (all) variables are unbounded, you may specify
        '                        very small number or -INF (latter is recommended because
        '                        it will allow solver to use better algorithm).
        '            BndU    -   upper bounds, array[N].
        '                        If some (all) variables are unbounded, you may specify
        '                        very large number or +INF (latter is recommended because
        '                        it will allow solver to use better algorithm).
        '
        '        NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
        '        variable will be "frozen" at X[i]=BndL[i]=BndU[i].
        '
        '        NOTE 2: this solver has following useful properties:
        '        * bound constraints are always satisfied exactly
        '        * function is evaluated only INSIDE area specified by bound constraints
        '          or at its boundary
        '
        '          -- ALGLIB --
        '             Copyright 14.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetbc(state As minlmstate, bndl As Double(), bndu As Double())
            Dim i As Integer = 0
            Dim n As Integer = 0

            n = state.n
            alglib.ap.assert(alglib.ap.len(bndl) >= n, "MinLMSetBC: Length(BndL)<N")
            alglib.ap.assert(alglib.ap.len(bndu) >= n, "MinLMSetBC: Length(BndU)<N")
            For i = 0 To n - 1
                alglib.ap.assert(Math.isfinite(bndl(i)) OrElse [Double].IsNegativeInfinity(bndl(i)), "MinLMSetBC: BndL contains NAN or +INF")
                alglib.ap.assert(Math.isfinite(bndu(i)) OrElse [Double].IsPositiveInfinity(bndu(i)), "MinLMSetBC: BndU contains NAN or -INF")
                state.bndl(i) = bndl(i)
                state.havebndl(i) = Math.isfinite(bndl(i))
                state.bndu(i) = bndu(i)
                state.havebndu(i) = Math.isfinite(bndu(i))
            Next
        End Sub


        '************************************************************************
        '        This function is used to change acceleration settings
        '
        '        You can choose between three acceleration strategies:
        '        * AccType=0, no acceleration.
        '        * AccType=1, secant updates are used to update quadratic model after  each
        '          iteration. After fixed number of iterations (or after  model  breakdown)
        '          we  recalculate  quadratic  model  using  analytic  Jacobian  or  finite
        '          differences. Number of secant-based iterations depends  on  optimization
        '          settings: about 3 iterations - when we have analytic Jacobian, up to 2*N
        '          iterations - when we use finite differences to calculate Jacobian.
        '
        '        AccType=1 is recommended when Jacobian  calculation  cost  is  prohibitive
        '        high (several Mx1 function vector calculations  followed  by  several  NxN
        '        Cholesky factorizations are faster than calculation of one M*N  Jacobian).
        '        It should also be used when we have no Jacobian, because finite difference
        '        approximation takes too much time to compute.
        '
        '        Table below list  optimization  protocols  (XYZ  protocol  corresponds  to
        '        MinLMCreateXYZ) and acceleration types they support (and use by  default).
        '
        '        ACCELERATION TYPES SUPPORTED BY OPTIMIZATION PROTOCOLS:
        '
        '        protocol    0   1   comment
        '        V           +   +
        '        VJ          +   +
        '        FGH         +
        '
        '        DAFAULT VALUES:
        '
        '        protocol    0   1   comment
        '        V               x   without acceleration it is so slooooooooow
        '        VJ          x
        '        FGH         x
        '
        '        NOTE: this  function should be called before optimization. Attempt to call
        '        it during algorithm iterations may result in unexpected behavior.
        '
        '        NOTE: attempt to call this function with unsupported protocol/acceleration
        '        combination will result in exception being thrown.
        '
        '          -- ALGLIB --
        '             Copyright 14.10.2010 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetacctype(state As minlmstate, acctype As Integer)
            alglib.ap.assert((acctype = 0 OrElse acctype = 1) OrElse acctype = 2, "MinLMSetAccType: incorrect AccType!")
            If acctype = 2 Then
                acctype = 0
            End If
            If acctype = 0 Then
                state.maxmodelage = 0
                state.makeadditers = False
                Return
            End If
            If acctype = 1 Then
                alglib.ap.assert(state.hasfi, "MinLMSetAccType: AccType=1 is incompatible with current protocol!")
                If state.algomode = 0 Then
                    state.maxmodelage = 2 * state.n
                Else
                    state.maxmodelage = smallmodelage
                End If
                state.makeadditers = False
                Return
            End If
        End Sub


        '************************************************************************
        '        NOTES:
        '
        '        1. Depending on function used to create state  structure,  this  algorithm
        '           may accept Jacobian and/or Hessian and/or gradient.  According  to  the
        '           said above, there ase several versions of this function,  which  accept
        '           different sets of callbacks.
        '
        '           This flexibility opens way to subtle errors - you may create state with
        '           MinLMCreateFGH() (optimization using Hessian), but call function  which
        '           does not accept Hessian. So when algorithm will request Hessian,  there
        '           will be no callback to call. In this case exception will be thrown.
        '
        '           Be careful to avoid such errors because there is no way to find them at
        '           compile time - you can see them at runtime only.
        '
        '          -- ALGLIB --
        '             Copyright 10.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Function minlmiteration(state As minlmstate) As Boolean
            Dim result As New Boolean()
            Dim n As Integer = 0
            Dim m As Integer = 0
            Dim bflag As New Boolean()
            Dim iflag As Integer = 0
            Dim v As Double = 0
            Dim s As Double = 0
            Dim t As Double = 0
            Dim i As Integer = 0
            Dim k As Integer = 0
            Dim i_ As Integer = 0


            '
            ' Reverse communication preparations
            ' I know it looks ugly, but it works the same way
            ' anywhere from C++ to Python.
            '
            ' This code initializes locals by:
            ' * random values determined during code
            '   generation - on first subroutine call
            ' * values from previous call - on subsequent calls
            '
            If state.rstate.stage >= 0 Then
                n = state.rstate.ia(0)
                m = state.rstate.ia(1)
                iflag = state.rstate.ia(2)
                i = state.rstate.ia(3)
                k = state.rstate.ia(4)
                bflag = state.rstate.ba(0)
                v = state.rstate.ra(0)
                s = state.rstate.ra(1)
                t = state.rstate.ra(2)
            Else
                n = -983
                m = -989
                iflag = -834
                i = 900
                k = -287
                bflag = False
                v = 214
                s = -338
                t = -686
            End If
            If state.rstate.stage = 0 Then
                GoTo lbl_0
            End If
            If state.rstate.stage = 1 Then
                GoTo lbl_1
            End If
            If state.rstate.stage = 2 Then
                GoTo lbl_2
            End If
            If state.rstate.stage = 3 Then
                GoTo lbl_3
            End If
            If state.rstate.stage = 4 Then
                GoTo lbl_4
            End If
            If state.rstate.stage = 5 Then
                GoTo lbl_5
            End If
            If state.rstate.stage = 6 Then
                GoTo lbl_6
            End If
            If state.rstate.stage = 7 Then
                GoTo lbl_7
            End If
            If state.rstate.stage = 8 Then
                GoTo lbl_8
            End If
            If state.rstate.stage = 9 Then
                GoTo lbl_9
            End If
            If state.rstate.stage = 10 Then
                GoTo lbl_10
            End If
            If state.rstate.stage = 11 Then
                GoTo lbl_11
            End If
            If state.rstate.stage = 12 Then
                GoTo lbl_12
            End If
            If state.rstate.stage = 13 Then
                GoTo lbl_13
            End If
            If state.rstate.stage = 14 Then
                GoTo lbl_14
            End If
            If state.rstate.stage = 15 Then
                GoTo lbl_15
            End If
            If state.rstate.stage = 16 Then
                GoTo lbl_16
            End If
            If state.rstate.stage = 17 Then
                GoTo lbl_17
            End If
            If state.rstate.stage = 18 Then
                GoTo lbl_18
            End If

            '
            ' Routine body
            '

            '
            ' prepare
            '
            n = state.n
            m = state.m
            state.repiterationscount = 0
            state.repterminationtype = 0
            state.repfuncidx = -1
            state.repvaridx = -1
            state.repnfunc = 0
            state.repnjac = 0
            state.repngrad = 0
            state.repnhess = 0
            state.repncholesky = 0
            state.userterminationneeded = False

            '
            ' check consistency of constraints,
            ' enforce feasibility of the solution
            ' set constraints
            '
            If Not optserv.enforceboundaryconstraints(state.xbase, state.bndl, state.havebndl, state.bndu, state.havebndu, n, _
                0) Then
                state.repterminationtype = -3
                result = False
                Return result
            End If
            minqp.minqpsetbc(state.qpstate, state.bndl, state.bndu)

            '
            '  Check, that transferred derivative value is right
            '
            clearrequestfields(state)
            If Not (state.algomode = 1 AndAlso CDbl(state.teststep) > CDbl(0)) Then
                GoTo lbl_19
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.needfij = True
            i = 0
lbl_21:
            If i > n - 1 Then
                GoTo lbl_23
            End If
            alglib.ap.assert((state.havebndl(i) AndAlso CDbl(state.bndl(i)) <= CDbl(state.x(i))) OrElse Not state.havebndl(i), "MinLM: internal error(State.X is out of bounds)")
            alglib.ap.assert((state.havebndu(i) AndAlso CDbl(state.x(i)) <= CDbl(state.bndu(i))) OrElse Not state.havebndu(i), "MinLMIteration: internal error(State.X is out of bounds)")
            v = state.x(i)
            state.x(i) = v - state.teststep * state.s(i)
            If state.havebndl(i) Then
                state.x(i) = System.Math.Max(state.x(i), state.bndl(i))
            End If
            state.xm1 = state.x(i)
            state.rstate.stage = 0
            GoTo lbl_rcomm
lbl_0:
            For i_ = 0 To m - 1
                state.fm1(i_) = state.fi(i_)
            Next
            For i_ = 0 To m - 1
                state.gm1(i_) = state.j(i_, i)
            Next
            state.x(i) = v + state.teststep * state.s(i)
            If state.havebndu(i) Then
                state.x(i) = System.Math.Min(state.x(i), state.bndu(i))
            End If
            state.xp1 = state.x(i)
            state.rstate.stage = 1
            GoTo lbl_rcomm
lbl_1:
            For i_ = 0 To m - 1
                state.fp1(i_) = state.fi(i_)
            Next
            For i_ = 0 To m - 1
                state.gp1(i_) = state.j(i_, i)
            Next
            state.x(i) = (state.xm1 + state.xp1) / 2
            If state.havebndl(i) Then
                state.x(i) = System.Math.Max(state.x(i), state.bndl(i))
            End If
            If state.havebndu(i) Then
                state.x(i) = System.Math.Min(state.x(i), state.bndu(i))
            End If
            state.rstate.stage = 2
            GoTo lbl_rcomm
lbl_2:
            For i_ = 0 To m - 1
                state.fc1(i_) = state.fi(i_)
            Next
            For i_ = 0 To m - 1
                state.gc1(i_) = state.j(i_, i)
            Next
            state.x(i) = v
            For k = 0 To m - 1
                If Not optserv.derivativecheck(state.fm1(k), state.gm1(k), state.fp1(k), state.gp1(k), state.fc1(k), state.gc1(k), _
                    state.xp1 - state.xm1) Then
                    state.repfuncidx = k
                    state.repvaridx = i
                    state.repterminationtype = -7
                    result = False
                    Return result
                End If
            Next
            i = i + 1
            GoTo lbl_21
lbl_23:
            state.needfij = False
lbl_19:

            '
            ' Initial report of current point
            '
            ' Note 1: we rewrite State.X twice because
            ' user may accidentally change it after first call.
            '
            ' Note 2: we set NeedF or NeedFI depending on what
            ' information about function we have.
            '
            If Not state.xrep Then
                GoTo lbl_24
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            If Not state.hasf Then
                GoTo lbl_26
            End If
            state.needf = True
            state.rstate.stage = 3
            GoTo lbl_rcomm
lbl_3:
            state.needf = False
            GoTo lbl_27
lbl_26:
            alglib.ap.assert(state.hasfi, "MinLM: internal error 2!")
            state.needfi = True
            state.rstate.stage = 4
            GoTo lbl_rcomm
lbl_4:
            state.needfi = False
            v = 0.0
            For i_ = 0 To m - 1
                v += state.fi(i_) * state.fi(i_)
            Next
            state.f = v
lbl_27:
            state.repnfunc = state.repnfunc + 1
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 5
            GoTo lbl_rcomm
lbl_5:
            state.xupdated = False
lbl_24:
            If state.userterminationneeded Then

                '
                ' User requested termination
                '
                For i_ = 0 To n - 1
                    state.x(i_) = state.xbase(i_)
                Next
                state.repterminationtype = 8
                result = False
                Return result
            End If

            '
            ' Prepare control variables
            '
            state.nu = 1
            state.lambdav = -Math.maxrealnumber
            state.modelage = state.maxmodelage + 1
            state.deltaxready = False
            state.deltafready = False
lbl_28:

            '
            ' Main cycle.
            '
            ' We move through it until either:
            ' * one of the stopping conditions is met
            ' * we decide that stopping conditions are too stringent
            '   and break from cycle
            '
            '
            If False Then
                GoTo lbl_29
            End If

            '
            ' First, we have to prepare quadratic model for our function.
            ' We use BFlag to ensure that model is prepared;
            ' if it is false at the end of this block, something went wrong.
            '
            ' We may either calculate brand new model or update old one.
            '
            ' Before this block we have:
            ' * State.XBase            - current position.
            ' * State.DeltaX           - if DeltaXReady is True
            ' * State.DeltaF           - if DeltaFReady is True
            '
            ' After this block is over, we will have:
            ' * State.XBase            - base point (unchanged)
            ' * State.FBase            - F(XBase)
            ' * State.GBase            - linear term
            ' * State.QuadraticModel   - quadratic term
            ' * State.LambdaV          - current estimate for lambda
            '
            ' We also clear DeltaXReady/DeltaFReady flags
            ' after initialization is done.
            '
            bflag = False
            If Not (state.algomode = 0 OrElse state.algomode = 1) Then
                GoTo lbl_30
            End If

            '
            ' Calculate f[] and Jacobian
            '
            If Not (state.modelage > state.maxmodelage OrElse Not (state.deltaxready AndAlso state.deltafready)) Then
                GoTo lbl_32
            End If

            '
            ' Refresh model (using either finite differences or analytic Jacobian)
            '
            If state.algomode <> 0 Then
                GoTo lbl_34
            End If

            '
            ' Optimization using F values only.
            ' Use finite differences to estimate Jacobian.
            '
            alglib.ap.assert(state.hasfi, "MinLMIteration: internal error when estimating Jacobian (no f[])")
            k = 0
lbl_36:
            If k > n - 1 Then
                GoTo lbl_38
            End If

            '
            ' We guard X[k] from leaving [BndL,BndU].
            ' In case BndL=BndU, we assume that derivative in this direction is zero.
            '
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.x(k) = state.x(k) - state.s(k) * state.diffstep
            If state.havebndl(k) Then
                state.x(k) = System.Math.Max(state.x(k), state.bndl(k))
            End If
            If state.havebndu(k) Then
                state.x(k) = System.Math.Min(state.x(k), state.bndu(k))
            End If
            state.xm1 = state.x(k)
            clearrequestfields(state)
            state.needfi = True
            state.rstate.stage = 6
            GoTo lbl_rcomm
lbl_6:
            state.repnfunc = state.repnfunc + 1
            For i_ = 0 To m - 1
                state.fm1(i_) = state.fi(i_)
            Next
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.x(k) = state.x(k) + state.s(k) * state.diffstep
            If state.havebndl(k) Then
                state.x(k) = System.Math.Max(state.x(k), state.bndl(k))
            End If
            If state.havebndu(k) Then
                state.x(k) = System.Math.Min(state.x(k), state.bndu(k))
            End If
            state.xp1 = state.x(k)
            clearrequestfields(state)
            state.needfi = True
            state.rstate.stage = 7
            GoTo lbl_rcomm
lbl_7:
            state.repnfunc = state.repnfunc + 1
            For i_ = 0 To m - 1
                state.fp1(i_) = state.fi(i_)
            Next
            v = state.xp1 - state.xm1
            If CDbl(v) <> CDbl(0) Then
                v = 1 / v
                For i_ = 0 To m - 1
                    state.j(i_, k) = v * state.fp1(i_)
                Next
                For i_ = 0 To m - 1
                    state.j(i_, k) = state.j(i_, k) - v * state.fm1(i_)
                Next
            Else
                For i = 0 To m - 1
                    state.j(i, k) = 0
                Next
            End If
            k = k + 1
            GoTo lbl_36
lbl_38:

            '
            ' Calculate F(XBase)
            '
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.needfi = True
            state.rstate.stage = 8
            GoTo lbl_rcomm
lbl_8:
            state.needfi = False
            state.repnfunc = state.repnfunc + 1
            state.repnjac = state.repnjac + 1

            '
            ' New model
            '
            state.modelage = 0
            GoTo lbl_35
lbl_34:

            '
            ' Obtain f[] and Jacobian
            '
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.needfij = True
            state.rstate.stage = 9
            GoTo lbl_rcomm
lbl_9:
            state.needfij = False
            state.repnfunc = state.repnfunc + 1
            state.repnjac = state.repnjac + 1

            '
            ' New model
            '
            state.modelage = 0
lbl_35:
            GoTo lbl_33
lbl_32:

            '
            ' State.J contains Jacobian or its current approximation;
            ' refresh it using secant updates:
            '
            ' f(x0+dx) = f(x0) + J*dx,
            ' J_new = J_old + u*h'
            ' h = x_new-x_old
            ' u = (f_new - f_old - J_old*h)/(h'h)
            '
            ' We can explicitly generate h and u, but it is
            ' preferential to do in-place calculations. Only
            ' I-th row of J_old is needed to calculate u[I],
            ' so we can update J row by row in one pass.
            '
            ' NOTE: we expect that State.XBase contains new point,
            ' State.FBase contains old point, State.DeltaX and
            ' State.DeltaY contain updates from last step.
            '
            alglib.ap.assert(state.deltaxready AndAlso state.deltafready, "MinLMIteration: uninitialized DeltaX/DeltaF")
            t = 0.0
            For i_ = 0 To n - 1
                t += state.deltax(i_) * state.deltax(i_)
            Next
            alglib.ap.assert(CDbl(t) <> CDbl(0), "MinLM: internal error (T=0)")
            For i = 0 To m - 1
                v = 0.0
                For i_ = 0 To n - 1
                    v += state.j(i, i_) * state.deltax(i_)
                Next
                v = (state.deltaf(i) - v) / t
                For i_ = 0 To n - 1
                    state.j(i, i_) = state.j(i, i_) + v * state.deltax(i_)
                Next
            Next
            For i_ = 0 To m - 1
                state.fi(i_) = state.fibase(i_)
            Next
            For i_ = 0 To m - 1
                state.fi(i_) = state.fi(i_) + state.deltaf(i_)
            Next

            '
            ' Increase model age
            '
            state.modelage = state.modelage + 1
lbl_33:

            '
            ' Generate quadratic model:
            '     f(xbase+dx) =
            '       = (f0 + J*dx)'(f0 + J*dx)
            '       = f0^2 + dx'J'f0 + f0*J*dx + dx'J'J*dx
            '       = f0^2 + 2*f0*J*dx + dx'J'J*dx
            '
            ' Note that we calculate 2*(J'J) instead of J'J because
            ' our quadratic model is based on Tailor decomposition,
            ' i.e. it has 0.5 before quadratic term.
            '
            ablas.rmatrixgemm(n, n, m, 2.0, state.j, 0, _
                0, 1, state.j, 0, 0, 0, _
                0.0, state.quadraticmodel, 0, 0)
            ablas.rmatrixmv(n, m, state.j, 0, 0, 1, _
                state.fi, 0, state.gbase, 0)
            For i_ = 0 To n - 1
                state.gbase(i_) = 2 * state.gbase(i_)
            Next
            v = 0.0
            For i_ = 0 To m - 1
                v += state.fi(i_) * state.fi(i_)
            Next
            state.fbase = v
            For i_ = 0 To m - 1
                state.fibase(i_) = state.fi(i_)
            Next

            '
            ' set control variables
            '
            bflag = True
lbl_30:
            If state.algomode <> 2 Then
                GoTo lbl_39
            End If
            alglib.ap.assert(Not state.hasfi, "MinLMIteration: internal error (HasFI is True in Hessian-based mode)")

            '
            ' Obtain F, G, H
            '
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.needfgh = True
            state.rstate.stage = 10
            GoTo lbl_rcomm
lbl_10:
            state.needfgh = False
            state.repnfunc = state.repnfunc + 1
            state.repngrad = state.repngrad + 1
            state.repnhess = state.repnhess + 1
            ablas.rmatrixcopy(n, n, state.h, 0, 0, state.quadraticmodel, _
                0, 0)
            For i_ = 0 To n - 1
                state.gbase(i_) = state.g(i_)
            Next
            state.fbase = state.f

            '
            ' set control variables
            '
            bflag = True
            state.modelage = 0
lbl_39:
            alglib.ap.assert(bflag, "MinLM: internal integrity check failed!")
            state.deltaxready = False
            state.deltafready = False

            '
            ' If Lambda is not initialized, initialize it using quadratic model
            '
            If CDbl(state.lambdav) < CDbl(0) Then
                state.lambdav = 0
                For i = 0 To n - 1
                    state.lambdav = System.Math.Max(state.lambdav, System.Math.Abs(state.quadraticmodel(i, i)) * Math.sqr(state.s(i)))
                Next
                state.lambdav = 0.001 * state.lambdav
                If CDbl(state.lambdav) = CDbl(0) Then
                    state.lambdav = 1
                End If
            End If

            '
            ' Test stopping conditions for function gradient
            '
            If CDbl(boundedscaledantigradnorm(state, state.xbase, state.gbase)) > CDbl(state.epsg) Then
                GoTo lbl_41
            End If
            If state.modelage <> 0 Then
                GoTo lbl_43
            End If

            '
            ' Model is fresh, we can rely on it and terminate algorithm
            '
            state.repterminationtype = 4
            If Not state.xrep Then
                GoTo lbl_45
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.f = state.fbase
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 11
            GoTo lbl_rcomm
lbl_11:
            state.xupdated = False
lbl_45:
            result = False
            Return result
            GoTo lbl_44
lbl_43:

            '
            ' Model is not fresh, we should refresh it and test
            ' conditions once more
            '
            state.modelage = state.maxmodelage + 1
            GoTo lbl_28
lbl_44:
lbl_41:

            '
            ' Find value of Levenberg-Marquardt damping parameter which:
            ' * leads to positive definite damped model
            ' * within bounds specified by StpMax
            ' * generates step which decreases function value
            '
            ' After this block IFlag is set to:
            ' * -3, if constraints are infeasible
            ' * -2, if model update is needed (either Lambda growth is too large
            '       or step is too short, but we can't rely on model and stop iterations)
            ' * -1, if model is fresh, Lambda have grown too large, termination is needed
            ' *  0, if everything is OK, continue iterations
            '
            ' State.Nu can have any value on enter, but after exit it is set to 1.0
            '
            iflag = -99
lbl_47:
            If False Then
                GoTo lbl_48
            End If

            '
            ' Do we need model update?
            '
            If state.modelage > 0 AndAlso CDbl(state.nu) >= CDbl(suspiciousnu) Then
                iflag = -2
                GoTo lbl_48
            End If

            '
            ' Setup quadratic solver and solve quadratic programming problem.
            ' After problem is solved we'll try to bound step by StpMax
            ' (Lambda will be increased if step size is too large).
            '
            ' We use BFlag variable to indicate that we have to increase Lambda.
            ' If it is False, we will try to increase Lambda and move to new iteration.
            '
            bflag = True
            minqp.minqpsetstartingpointfast(state.qpstate, state.xbase)
            minqp.minqpsetoriginfast(state.qpstate, state.xbase)
            minqp.minqpsetlineartermfast(state.qpstate, state.gbase)
            minqp.minqpsetquadratictermfast(state.qpstate, state.quadraticmodel, True, 0.0)
            For i = 0 To n - 1
                state.tmp0(i) = state.quadraticmodel(i, i) + state.lambdav / Math.sqr(state.s(i))
            Next
            minqp.minqprewritediagonal(state.qpstate, state.tmp0)
            minqp.minqpoptimize(state.qpstate)
            minqp.minqpresultsbuf(state.qpstate, state.xdir, state.qprep)
            If state.qprep.terminationtype > 0 Then

                '
                ' successful solution of QP problem
                '
                For i_ = 0 To n - 1
                    state.xdir(i_) = state.xdir(i_) - state.xbase(i_)
                Next
                v = 0.0
                For i_ = 0 To n - 1
                    v += state.xdir(i_) * state.xdir(i_)
                Next
                If Math.isfinite(v) Then
                    v = System.Math.sqrt(v)
                    If CDbl(state.stpmax) > CDbl(0) AndAlso CDbl(v) > CDbl(state.stpmax) Then
                        bflag = False
                    End If
                Else
                    bflag = False
                End If
            Else

                '
                ' Either problem is non-convex (increase LambdaV) or constraints are inconsistent
                '
                alglib.ap.assert(state.qprep.terminationtype = -3 OrElse state.qprep.terminationtype = -5, "MinLM: unexpected completion code from QP solver")
                If state.qprep.terminationtype = -3 Then
                    iflag = -3
                    GoTo lbl_48
                End If
                bflag = False
            End If
            If Not bflag Then

                '
                ' Solution failed:
                ' try to increase lambda to make matrix positive definite and continue.
                '
                If Not increaselambda(state.lambdav, state.nu) Then
                    iflag = -1
                    GoTo lbl_48
                End If
                GoTo lbl_47
            End If

            '
            ' Step in State.XDir and it is bounded by StpMax.
            '
            ' We should check stopping conditions on step size here.
            ' DeltaX, which is used for secant updates, is initialized here.
            '
            ' This code is a bit tricky because sometimes XDir<>0, but
            ' it is so small that XDir+XBase==XBase (in finite precision
            ' arithmetics). So we set DeltaX to XBase, then
            ' add XDir, and then subtract XBase to get exact value of
            ' DeltaX.
            '
            ' Step length is estimated using DeltaX.
            '
            ' NOTE: stopping conditions are tested
            ' for fresh models only (ModelAge=0)
            '
            For i_ = 0 To n - 1
                state.deltax(i_) = state.xbase(i_)
            Next
            For i_ = 0 To n - 1
                state.deltax(i_) = state.deltax(i_) + state.xdir(i_)
            Next
            For i_ = 0 To n - 1
                state.deltax(i_) = state.deltax(i_) - state.xbase(i_)
            Next
            state.deltaxready = True
            v = 0.0
            For i = 0 To n - 1
                v = v + Math.sqr(state.deltax(i) / state.s(i))
            Next
            v = System.Math.sqrt(v)
            If CDbl(v) > CDbl(state.epsx) Then
                GoTo lbl_49
            End If
            If state.modelage <> 0 Then
                GoTo lbl_51
            End If

            '
            ' Step is too short, model is fresh and we can rely on it.
            ' Terminating.
            '
            state.repterminationtype = 2
            If Not state.xrep Then
                GoTo lbl_53
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.f = state.fbase
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 12
            GoTo lbl_rcomm
lbl_12:
            state.xupdated = False
lbl_53:
            result = False
            Return result
            GoTo lbl_52
lbl_51:

            '
            ' Step is suspiciously short, but model is not fresh
            ' and we can't rely on it.
            '
            iflag = -2
            GoTo lbl_48
lbl_52:
lbl_49:

            '
            ' Let's evaluate new step:
            ' a) if we have Fi vector, we evaluate it using rcomm, and
            '    then we manually calculate State.F as sum of squares of Fi[]
            ' b) if we have F value, we just evaluate it through rcomm interface
            '
            ' We prefer (a) because we may need Fi vector for additional
            ' iterations
            '
            alglib.ap.assert(state.hasfi OrElse state.hasf, "MinLM: internal error 2!")
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            For i_ = 0 To n - 1
                state.x(i_) = state.x(i_) + state.xdir(i_)
            Next
            clearrequestfields(state)
            If Not state.hasfi Then
                GoTo lbl_55
            End If
            state.needfi = True
            state.rstate.stage = 13
            GoTo lbl_rcomm
lbl_13:
            state.needfi = False
            v = 0.0
            For i_ = 0 To m - 1
                v += state.fi(i_) * state.fi(i_)
            Next
            state.f = v
            For i_ = 0 To m - 1
                state.deltaf(i_) = state.fi(i_)
            Next
            For i_ = 0 To m - 1
                state.deltaf(i_) = state.deltaf(i_) - state.fibase(i_)
            Next
            state.deltafready = True
            GoTo lbl_56
lbl_55:
            state.needf = True
            state.rstate.stage = 14
            GoTo lbl_rcomm
lbl_14:
            state.needf = False
lbl_56:
            state.repnfunc = state.repnfunc + 1
            If CDbl(state.f) >= CDbl(state.fbase) Then

                '
                ' Increase lambda and continue
                '
                If Not increaselambda(state.lambdav, state.nu) Then
                    iflag = -1
                    GoTo lbl_48
                End If
                GoTo lbl_47
            End If

            '
            ' We've found our step!
            '
            iflag = 0
            GoTo lbl_48
            GoTo lbl_47
lbl_48:
            If state.userterminationneeded Then

                '
                ' User requested termination
                '
                For i_ = 0 To n - 1
                    state.x(i_) = state.xbase(i_)
                Next
                state.repterminationtype = 8
                result = False
                Return result
            End If
            state.nu = 1
            alglib.ap.assert(iflag >= -3 AndAlso iflag <= 0, "MinLM: internal integrity check failed!")
            If iflag = -3 Then
                state.repterminationtype = -3
                result = False
                Return result
            End If
            If iflag = -2 Then
                state.modelage = state.maxmodelage + 1
                GoTo lbl_28
            End If
            If iflag = -1 Then
                GoTo lbl_29
            End If

            '
            ' Levenberg-Marquardt step is ready.
            ' Compare predicted vs. actual decrease and decide what to do with lambda.
            '
            ' NOTE: we expect that State.DeltaX contains direction of step,
            ' State.F contains function value at new point.
            '
            alglib.ap.assert(state.deltaxready, "MinLM: deltaX is not ready")
            t = 0
            For i = 0 To n - 1
                v = 0.0
                For i_ = 0 To n - 1
                    v += state.quadraticmodel(i, i_) * state.deltax(i_)
                Next
                t = t + state.deltax(i) * state.gbase(i) + 0.5 * state.deltax(i) * v
            Next
            state.predicteddecrease = -t
            state.actualdecrease = -(state.f - state.fbase)
            If CDbl(state.predicteddecrease) <= CDbl(0) Then
                GoTo lbl_29
            End If
            v = state.actualdecrease / state.predicteddecrease
            If CDbl(v) >= CDbl(0.1) Then
                GoTo lbl_57
            End If
            If increaselambda(state.lambdav, state.nu) Then
                GoTo lbl_59
            End If

            '
            ' Lambda is too large, we have to break iterations.
            '
            state.repterminationtype = 7
            If Not state.xrep Then
                GoTo lbl_61
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.f = state.fbase
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 15
            GoTo lbl_rcomm
lbl_15:
            state.xupdated = False
lbl_61:
            result = False
            Return result
lbl_59:
lbl_57:
            If CDbl(v) > CDbl(0.5) Then
                decreaselambda(state.lambdav, state.nu)
            End If

            '
            ' Accept step, report it and
            ' test stopping conditions on iterations count and function decrease.
            '
            ' NOTE: we expect that State.DeltaX contains direction of step,
            ' State.F contains function value at new point.
            '
            ' NOTE2: we should update XBase ONLY. In the beginning of the next
            ' iteration we expect that State.FIBase is NOT updated and
            ' contains old value of a function vector.
            '
            For i_ = 0 To n - 1
                state.xbase(i_) = state.xbase(i_) + state.deltax(i_)
            Next
            If Not state.xrep Then
                GoTo lbl_63
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 16
            GoTo lbl_rcomm
lbl_16:
            state.xupdated = False
lbl_63:
            state.repiterationscount = state.repiterationscount + 1
            If state.repiterationscount >= state.maxits AndAlso state.maxits > 0 Then
                state.repterminationtype = 5
            End If
            If state.modelage = 0 Then
                If CDbl(System.Math.Abs(state.f - state.fbase)) <= CDbl(state.epsf * System.Math.Max(1, System.Math.Max(System.Math.Abs(state.f), System.Math.Abs(state.fbase)))) Then
                    state.repterminationtype = 1
                End If
            End If
            If state.repterminationtype <= 0 Then
                GoTo lbl_65
            End If
            If Not state.xrep Then
                GoTo lbl_67
            End If

            '
            ' Report: XBase contains new point, F contains function value at new point
            '
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 17
            GoTo lbl_rcomm
lbl_17:
            state.xupdated = False
lbl_67:
            result = False
            Return result
lbl_65:
            state.modelage = state.modelage + 1
            GoTo lbl_28
lbl_29:

            '
            ' Lambda is too large, we have to break iterations.
            '
            state.repterminationtype = 7
            If Not state.xrep Then
                GoTo lbl_69
            End If
            For i_ = 0 To n - 1
                state.x(i_) = state.xbase(i_)
            Next
            state.f = state.fbase
            clearrequestfields(state)
            state.xupdated = True
            state.rstate.stage = 18
            GoTo lbl_rcomm
lbl_18:
            state.xupdated = False
lbl_69:
            result = False
            Return result
lbl_rcomm:

            '
            ' Saving state
            '
            result = True
            state.rstate.ia(0) = n
            state.rstate.ia(1) = m
            state.rstate.ia(2) = iflag
            state.rstate.ia(3) = i
            state.rstate.ia(4) = k
            state.rstate.ba(0) = bflag
            state.rstate.ra(0) = v
            state.rstate.ra(1) = s
            state.rstate.ra(2) = t
            Return result
        End Function


        '************************************************************************
        '        Levenberg-Marquardt algorithm results
        '
        '        INPUT PARAMETERS:
        '            State   -   algorithm state
        '
        '        OUTPUT PARAMETERS:
        '            X       -   array[0..N-1], solution
        '            Rep     -   optimization  report;  includes  termination   codes   and
        '                        additional information. Termination codes are listed below,
        '                        see comments for this structure for more info.
        '                        Termination code is stored in rep.terminationtype field:
        '                        * -7    derivative correctness check failed;
        '                                see rep.wrongnum, rep.wrongi, rep.wrongj for
        '                                more information.
        '                        * -3    constraints are inconsistent
        '                        *  1    relative function improvement is no more than
        '                                EpsF.
        '                        *  2    relative step is no more than EpsX.
        '                        *  4    gradient is no more than EpsG.
        '                        *  5    MaxIts steps was taken
        '                        *  7    stopping conditions are too stringent,
        '                                further improvement is impossible
        '                        *  8    terminated by user who called minlmrequesttermination().
        '                                X contains point which was "current accepted" when
        '                                termination request was submitted.
        '
        '          -- ALGLIB --
        '             Copyright 10.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmresults(state As minlmstate, ByRef x As Double(), rep As minlmreport)
            x = New Double(-1) {}

            minlmresultsbuf(state, x, rep)
        End Sub


        '************************************************************************
        '        Levenberg-Marquardt algorithm results
        '
        '        Buffered implementation of MinLMResults(), which uses pre-allocated buffer
        '        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
        '        intended to be used in the inner cycles of performance critical algorithms
        '        where array reallocation penalty is too large to be ignored.
        '
        '          -- ALGLIB --
        '             Copyright 10.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmresultsbuf(state As minlmstate, ByRef x As Double(), rep As minlmreport)
            Dim i_ As Integer = 0

            If alglib.ap.len(x) < state.n Then
                x = New Double(state.n - 1) {}
            End If
            For i_ = 0 To state.n - 1
                x(i_) = state.x(i_)
            Next
            rep.iterationscount = state.repiterationscount
            rep.terminationtype = state.repterminationtype
            rep.funcidx = state.repfuncidx
            rep.varidx = state.repvaridx
            rep.nfunc = state.repnfunc
            rep.njac = state.repnjac
            rep.ngrad = state.repngrad
            rep.nhess = state.repnhess
            rep.ncholesky = state.repncholesky
        End Sub


        '************************************************************************
        '        This  subroutine  restarts  LM  algorithm from new point. All optimization
        '        parameters are left unchanged.
        '
        '        This  function  allows  to  solve multiple  optimization  problems  (which
        '        must have same number of dimensions) without object reallocation penalty.
        '
        '        INPUT PARAMETERS:
        '            State   -   structure used for reverse communication previously
        '                        allocated with MinLMCreateXXX call.
        '            X       -   new starting point.
        '
        '          -- ALGLIB --
        '             Copyright 30.07.2010 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmrestartfrom(state As minlmstate, x As Double())
            Dim i_ As Integer = 0

            alglib.ap.assert(alglib.ap.len(x) >= state.n, "MinLMRestartFrom: Length(X)<N!")
            alglib.ap.assert(apserv.isfinitevector(x, state.n), "MinLMRestartFrom: X contains infinite or NaN values!")
            For i_ = 0 To state.n - 1
                state.xbase(i_) = x(i_)
            Next
            state.rstate.ia = New Integer(4) {}
            state.rstate.ba = New Boolean(0) {}
            state.rstate.ra = New Double(2) {}
            state.rstate.stage = -1
            clearrequestfields(state)
        End Sub


        '************************************************************************
        '        This subroutine submits request for termination of running  optimizer.  It
        '        should be called from user-supplied callback when user decides that it  is
        '        time to "smoothly" terminate optimization process.  As  result,  optimizer
        '        stops at point which was "current accepted" when termination  request  was
        '        submitted and returns error code 8 (successful termination).
        '
        '        INPUT PARAMETERS:
        '            State   -   optimizer structure
        '
        '        NOTE: after  request  for  termination  optimizer  may   perform   several
        '              additional calls to user-supplied callbacks. It does  NOT  guarantee
        '              to stop immediately - it just guarantees that these additional calls
        '              will be discarded later.
        '
        '        NOTE: calling this function on optimizer which is NOT running will have no
        '              effect.
        '              
        '        NOTE: multiple calls to this function are possible. First call is counted,
        '              subsequent calls are silently ignored.
        '
        '          -- ALGLIB --
        '             Copyright 08.10.2014 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmrequesttermination(state As minlmstate)
            state.userterminationneeded = True
        End Sub


        '************************************************************************
        '        This is obsolete function.
        '
        '        Since ALGLIB 3.3 it is equivalent to MinLMCreateVJ().
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatevgj(n As Integer, m As Integer, x As Double(), state As minlmstate)
            minlmcreatevj(n, m, x, state)
        End Sub


        '************************************************************************
        '        This is obsolete function.
        '
        '        Since ALGLIB 3.3 it is equivalent to MinLMCreateFJ().
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatefgj(n As Integer, m As Integer, x As Double(), state As minlmstate)
            minlmcreatefj(n, m, x, state)
        End Sub


        '************************************************************************
        '        This function is considered obsolete since ALGLIB 3.1.0 and is present for
        '        backward  compatibility  only.  We  recommend  to use MinLMCreateVJ, which
        '        provides similar, but more consistent and feature-rich interface.
        '
        '          -- ALGLIB --
        '             Copyright 30.03.2009 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmcreatefj(n As Integer, m As Integer, x As Double(), state As minlmstate)
            alglib.ap.assert(n >= 1, "MinLMCreateFJ: N<1!")
            alglib.ap.assert(m >= 1, "MinLMCreateFJ: M<1!")
            alglib.ap.assert(alglib.ap.len(x) >= n, "MinLMCreateFJ: Length(X)<N!")
            alglib.ap.assert(apserv.isfinitevector(x, n), "MinLMCreateFJ: X contains infinite or NaN values!")

            '
            ' initialize
            '
            state.teststep = 0
            state.n = n
            state.m = m
            state.algomode = 1
            state.hasf = True
            state.hasfi = False
            state.hasg = False

            '
            ' init 2
            '
            lmprepare(n, m, True, state)
            minlmsetacctype(state, 0)
            minlmsetcond(state, 0, 0, 0, 0)
            minlmsetxrep(state, False)
            minlmsetstpmax(state, 0)
            minlmrestartfrom(state, x)
        End Sub


        '************************************************************************
        '        This  subroutine  turns  on  verification  of  the  user-supplied analytic
        '        gradient:
        '        * user calls this subroutine before optimization begins
        '        * MinLMOptimize() is called
        '        * prior to actual optimization, for  each  function Fi and each  component
        '          of parameters  being  optimized X[j] algorithm performs following steps:
        '          * two trial steps are made to X[j]-TestStep*S[j] and X[j]+TestStep*S[j],
        '            where X[j] is j-th parameter and S[j] is a scale of j-th parameter
        '          * if needed, steps are bounded with respect to constraints on X[]
        '          * Fi(X) is evaluated at these trial points
        '          * we perform one more evaluation in the middle point of the interval
        '          * we  build  cubic  model using function values and derivatives at trial
        '            points and we compare its prediction with actual value in  the  middle
        '            point
        '          * in case difference between prediction and actual value is higher  than
        '            some predetermined threshold, algorithm stops with completion code -7;
        '            Rep.VarIdx is set to index of the parameter with incorrect derivative,
        '            Rep.FuncIdx is set to index of the function.
        '        * after verification is over, algorithm proceeds to the actual optimization.
        '
        '        NOTE 1: verification  needs  N (parameters count) Jacobian evaluations. It
        '                is  very  costly  and  you  should use it only for low dimensional
        '                problems,  when  you  want  to  be  sure  that  you've   correctly
        '                calculated  analytic  derivatives.  You should not  use  it in the
        '                production code  (unless  you  want  to check derivatives provided
        '                by some third party).
        '
        '        NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
        '                (so large that function behaviour is significantly non-cubic) will
        '                lead to false alarms. You may use  different  step  for  different
        '                parameters by means of setting scale with MinLMSetScale().
        '
        '        NOTE 3: this function may lead to false positives. In case it reports that
        '                I-th  derivative was calculated incorrectly, you may decrease test
        '                step  and  try  one  more  time  - maybe your function changes too
        '                sharply  and  your  step  is  too  large for such rapidly chanding
        '                function.
        '
        '        INPUT PARAMETERS:
        '            State       -   structure used to store algorithm state
        '            TestStep    -   verification step:
        '                            * TestStep=0 turns verification off
        '                            * TestStep>0 activates verification
        '
        '          -- ALGLIB --
        '             Copyright 15.06.2012 by Bochkanov Sergey
        '        ************************************************************************

        Public Shared Sub minlmsetgradientcheck(state As minlmstate, teststep As Double)
            alglib.ap.assert(Math.isfinite(teststep), "MinLMSetGradientCheck: TestStep contains NaN or Infinite")
            alglib.ap.assert(CDbl(teststep) >= CDbl(0), "MinLMSetGradientCheck: invalid argument TestStep(TestStep<0)")
            state.teststep = teststep
        End Sub


        '************************************************************************
        '        Prepare internal structures (except for RComm).
        '
        '        Note: M must be zero for FGH mode, non-zero for V/VJ/FJ/FGJ mode.
        '        ************************************************************************

        Private Shared Sub lmprepare(n As Integer, m As Integer, havegrad As Boolean, state As minlmstate)
            Dim i As Integer = 0

            If n <= 0 OrElse m < 0 Then
                Return
            End If
            If havegrad Then
                state.g = New Double(n - 1) {}
            End If
            If m <> 0 Then
                state.j = New Double(m - 1, n - 1) {}
                state.fi = New Double(m - 1) {}
                state.fibase = New Double(m - 1) {}
                state.deltaf = New Double(m - 1) {}
                state.fm1 = New Double(m - 1) {}
                state.fp1 = New Double(m - 1) {}
                state.fc1 = New Double(m - 1) {}
                state.gm1 = New Double(m - 1) {}
                state.gp1 = New Double(m - 1) {}
                state.gc1 = New Double(m - 1) {}
            Else
                state.h = New Double(n - 1, n - 1) {}
            End If
            state.x = New Double(n - 1) {}
            state.deltax = New Double(n - 1) {}
            state.quadraticmodel = New Double(n - 1, n - 1) {}
            state.xbase = New Double(n - 1) {}
            state.gbase = New Double(n - 1) {}
            state.xdir = New Double(n - 1) {}
            state.tmp0 = New Double(n - 1) {}

            '
            ' prepare internal L-BFGS
            '
            For i = 0 To n - 1
                state.x(i) = 0
            Next
            minlbfgs.minlbfgscreate(n, System.Math.Min(additers, n), state.x, state.internalstate)
            minlbfgs.minlbfgssetcond(state.internalstate, 0.0, 0.0, 0.0, System.Math.Min(additers, n))

            '
            ' Prepare internal QP solver
            '
            minqp.minqpcreate(n, state.qpstate)
            minqp.minqpsetalgocholesky(state.qpstate)

            '
            ' Prepare boundary constraints
            '
            state.bndl = New Double(n - 1) {}
            state.bndu = New Double(n - 1) {}
            state.havebndl = New Boolean(n - 1) {}
            state.havebndu = New Boolean(n - 1) {}
            For i = 0 To n - 1
                state.bndl(i) = [Double].NegativeInfinity
                state.havebndl(i) = False
                state.bndu(i) = [Double].PositiveInfinity
                state.havebndu(i) = False
            Next

            '
            ' Prepare scaling matrix
            '
            state.s = New Double(n - 1) {}
            For i = 0 To n - 1
                state.s(i) = 1.0
            Next
        End Sub


        '************************************************************************
        '        Clears request fileds (to be sure that we don't forgot to clear something)
        '        ************************************************************************

        Private Shared Sub clearrequestfields(state As minlmstate)
            state.needf = False
            state.needfg = False
            state.needfgh = False
            state.needfij = False
            state.needfi = False
            state.xupdated = False
        End Sub


        '************************************************************************
        '        Increases lambda, returns False when there is a danger of overflow
        '        ************************************************************************

        Private Shared Function increaselambda(ByRef lambdav As Double, ByRef nu As Double) As Boolean
            Dim result As New Boolean()
            Dim lnlambda As Double = 0
            Dim lnnu As Double = 0
            Dim lnlambdaup As Double = 0
            Dim lnmax As Double = 0

            result = False
            lnlambda = System.Math.Log(lambdav)
            lnlambdaup = System.Math.Log(lambdaup)
            lnnu = System.Math.Log(nu)
            lnmax = System.Math.Log(Math.maxrealnumber)
            If CDbl(lnlambda + lnlambdaup + lnnu) > CDbl(0.25 * lnmax) Then
                Return result
            End If
            If CDbl(lnnu + System.Math.Log(2)) > CDbl(lnmax) Then
                Return result
            End If
            lambdav = lambdav * lambdaup * nu
            nu = nu * 2
            result = True
            Return result
        End Function


        '************************************************************************
        '        Decreases lambda, but leaves it unchanged when there is danger of underflow.
        '        ************************************************************************

        Private Shared Sub decreaselambda(ByRef lambdav As Double, ByRef nu As Double)
            nu = 1
            If CDbl(System.Math.Log(lambdav) + System.Math.Log(lambdadown)) < CDbl(System.Math.Log(Math.minrealnumber)) Then
                lambdav = Math.minrealnumber
            Else
                lambdav = lambdav * lambdadown
            End If
        End Sub


        '************************************************************************
        '        Returns norm of bounded scaled anti-gradient.
        '
        '        Bounded antigradient is a vector obtained from  anti-gradient  by  zeroing
        '        components which point outwards:
        '            result = norm(v)
        '            v[i]=0     if ((-g[i]<0)and(x[i]=bndl[i])) or
        '                          ((-g[i]>0)and(x[i]=bndu[i]))
        '            v[i]=-g[i]*s[i] otherwise, where s[i] is a scale for I-th variable
        '
        '        This function may be used to check a stopping criterion.
        '
        '          -- ALGLIB --
        '             Copyright 14.01.2011 by Bochkanov Sergey
        '        ************************************************************************

        Private Shared Function boundedscaledantigradnorm(state As minlmstate, x As Double(), g As Double()) As Double
            Dim result As Double = 0
            Dim n As Integer = 0
            Dim i As Integer = 0
            Dim v As Double = 0

            result = 0
            n = state.n
            For i = 0 To n - 1
                v = -(g(i) * state.s(i))
                If state.havebndl(i) Then
                    If CDbl(x(i)) <= CDbl(state.bndl(i)) AndAlso CDbl(-g(i)) < CDbl(0) Then
                        v = 0
                    End If
                End If
                If state.havebndu(i) Then
                    If CDbl(x(i)) >= CDbl(state.bndu(i)) AndAlso CDbl(-g(i)) > CDbl(0) Then
                        v = 0
                    End If
                End If
                result = result + Math.sqr(v)
            Next
            result = System.Math.sqrt(result)
            Return result
        End Function


    End Class
	Public Class mincomp
		Public Class minasastate
			Inherits apobject
			Public n As Integer
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public xrep As Boolean
			Public stpmax As Double
			Public cgtype As Integer
			Public k As Integer
			Public nfev As Integer
			Public mcstage As Integer
			Public bndl As Double()
			Public bndu As Double()
			Public curalgo As Integer
			Public acount As Integer
			Public mu As Double
			Public finit As Double
			Public dginit As Double
			Public ak As Double()
			Public xk As Double()
			Public dk As Double()
			Public an As Double()
			Public xn As Double()
			Public dn As Double()
			Public d As Double()
			Public fold As Double
			Public stp As Double
			Public work As Double()
			Public yk As Double()
			Public gc As Double()
			Public laststep As Double
			Public x As Double()
			Public f As Double
			Public g As Double()
			Public needfg As Boolean
			Public xupdated As Boolean
			Public rstate As rcommstate
			Public repiterationscount As Integer
			Public repnfev As Integer
			Public repterminationtype As Integer
			Public debugrestartscount As Integer
			Public lstate As linmin.linminstate
			Public betahs As Double
			Public betady As Double
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				bndl = New Double(-1) {}
				bndu = New Double(-1) {}
				ak = New Double(-1) {}
				xk = New Double(-1) {}
				dk = New Double(-1) {}
				an = New Double(-1) {}
				xn = New Double(-1) {}
				dn = New Double(-1) {}
				d = New Double(-1) {}
				work = New Double(-1) {}
				yk = New Double(-1) {}
				gc = New Double(-1) {}
				x = New Double(-1) {}
				g = New Double(-1) {}
				rstate = New rcommstate()
				lstate = New linmin.linminstate()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minasastate()
				_result.n = n
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				_result.xrep = xrep
				_result.stpmax = stpmax
				_result.cgtype = cgtype
				_result.k = k
				_result.nfev = nfev
				_result.mcstage = mcstage
				_result.bndl = DirectCast(bndl.Clone(), Double())
				_result.bndu = DirectCast(bndu.Clone(), Double())
				_result.curalgo = curalgo
				_result.acount = acount
				_result.mu = mu
				_result.finit = finit
				_result.dginit = dginit
				_result.ak = DirectCast(ak.Clone(), Double())
				_result.xk = DirectCast(xk.Clone(), Double())
				_result.dk = DirectCast(dk.Clone(), Double())
				_result.an = DirectCast(an.Clone(), Double())
				_result.xn = DirectCast(xn.Clone(), Double())
				_result.dn = DirectCast(dn.Clone(), Double())
				_result.d = DirectCast(d.Clone(), Double())
				_result.fold = fold
				_result.stp = stp
				_result.work = DirectCast(work.Clone(), Double())
				_result.yk = DirectCast(yk.Clone(), Double())
				_result.gc = DirectCast(gc.Clone(), Double())
				_result.laststep = laststep
				_result.x = DirectCast(x.Clone(), Double())
				_result.f = f
				_result.g = DirectCast(g.Clone(), Double())
				_result.needfg = needfg
				_result.xupdated = xupdated
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.repiterationscount = repiterationscount
				_result.repnfev = repnfev
				_result.repterminationtype = repterminationtype
				_result.debugrestartscount = debugrestartscount
				_result.lstate = DirectCast(lstate.make_copy(), linmin.linminstate)
				_result.betahs = betahs
				_result.betady = betady
				Return _result
			End Function
		End Class


		Public Class minasareport
			Inherits apobject
			Public iterationscount As Integer
			Public nfev As Integer
			Public terminationtype As Integer
			Public activeconstraints As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minasareport()
				_result.iterationscount = iterationscount
				_result.nfev = nfev
				_result.terminationtype = terminationtype
				_result.activeconstraints = activeconstraints
				Return _result
			End Function
		End Class




		Public Const n1 As Integer = 2
		Public Const n2 As Integer = 2
		Public Const stpmin As Double = 1E-300
		Public Const gtol As Double = 0.3
		Public Const gpaftol As Double = 0.0001
		Public Const gpadecay As Double = 0.5
		Public Const asarho As Double = 0.5


		'************************************************************************
'        Obsolete function, use MinLBFGSSetPrecDefault() instead.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetdefaultpreconditioner(state As minlbfgs.minlbfgsstate)
			minlbfgs.minlbfgssetprecdefault(state)
		End Sub


		'************************************************************************
'        Obsolete function, use MinLBFGSSetCholeskyPreconditioner() instead.
'
'          -- ALGLIB --
'             Copyright 13.10.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minlbfgssetcholeskypreconditioner(state As minlbfgs.minlbfgsstate, p As Double(,), isupper As Boolean)
			minlbfgs.minlbfgssetpreccholesky(state, p, isupper)
		End Sub


		'************************************************************************
'        This is obsolete function which was used by previous version of the  BLEIC
'        optimizer. It does nothing in the current version of BLEIC.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetbarrierwidth(state As minbleic.minbleicstate, mu As Double)
		End Sub


		'************************************************************************
'        This is obsolete function which was used by previous version of the  BLEIC
'        optimizer. It does nothing in the current version of BLEIC.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minbleicsetbarrierdecay(state As minbleic.minbleicstate, mudecay As Double)
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 25.03.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasacreate(n As Integer, x As Double(), bndl As Double(), bndu As Double(), state As minasastate)
			Dim i As Integer = 0

			alglib.ap.assert(n >= 1, "MinASA: N too small!")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinCGCreate: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinCGCreate: X contains infinite or NaN values!")
			alglib.ap.assert(alglib.ap.len(bndl) >= n, "MinCGCreate: Length(BndL)<N!")
			alglib.ap.assert(apserv.isfinitevector(bndl, n), "MinCGCreate: BndL contains infinite or NaN values!")
			alglib.ap.assert(alglib.ap.len(bndu) >= n, "MinCGCreate: Length(BndU)<N!")
			alglib.ap.assert(apserv.isfinitevector(bndu, n), "MinCGCreate: BndU contains infinite or NaN values!")
			For i = 0 To n - 1
				alglib.ap.assert(CDbl(bndl(i)) <= CDbl(bndu(i)), "MinASA: inconsistent bounds!")
				alglib.ap.assert(CDbl(bndl(i)) <= CDbl(x(i)), "MinASA: infeasible X!")
				alglib.ap.assert(CDbl(x(i)) <= CDbl(bndu(i)), "MinASA: infeasible X!")
			Next

			'
			' Initialize
			'
			state.n = n
			minasasetcond(state, 0, 0, 0, 0)
			minasasetxrep(state, False)
			minasasetstpmax(state, 0)
			minasasetalgorithm(state, -1)
			state.bndl = New Double(n - 1) {}
			state.bndu = New Double(n - 1) {}
			state.ak = New Double(n - 1) {}
			state.xk = New Double(n - 1) {}
			state.dk = New Double(n - 1) {}
			state.an = New Double(n - 1) {}
			state.xn = New Double(n - 1) {}
			state.dn = New Double(n - 1) {}
			state.x = New Double(n - 1) {}
			state.d = New Double(n - 1) {}
			state.g = New Double(n - 1) {}
			state.gc = New Double(n - 1) {}
			state.work = New Double(n - 1) {}
			state.yk = New Double(n - 1) {}
			minasarestartfrom(state, x, bndl, bndu)
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasasetcond(state As minasastate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(epsg), "MinASASetCond: EpsG is not finite number!")
			alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinASASetCond: negative EpsG!")
			alglib.ap.assert(Math.isfinite(epsf), "MinASASetCond: EpsF is not finite number!")
			alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinASASetCond: negative EpsF!")
			alglib.ap.assert(Math.isfinite(epsx), "MinASASetCond: EpsX is not finite number!")
			alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinASASetCond: negative EpsX!")
			alglib.ap.assert(maxits >= 0, "MinASASetCond: negative MaxIts!")
			If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
				epsx = 1E-06
			End If
			state.epsg = epsg
			state.epsf = epsf
			state.epsx = epsx
			state.maxits = maxits
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasasetxrep(state As minasastate, needxrep As Boolean)
			state.xrep = needxrep
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasasetalgorithm(state As minasastate, algotype As Integer)
			alglib.ap.assert(algotype >= -1 AndAlso algotype <= 1, "MinASASetAlgorithm: incorrect AlgoType!")
			If algotype = -1 Then
				algotype = 1
			End If
			state.cgtype = algotype
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 02.04.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasasetstpmax(state As minasastate, stpmax As Double)
			alglib.ap.assert(Math.isfinite(stpmax), "MinASASetStpMax: StpMax is not finite!")
			alglib.ap.assert(CDbl(stpmax) >= CDbl(0), "MinASASetStpMax: StpMax<0!")
			state.stpmax = stpmax
		End Sub


		'************************************************************************
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function minasaiteration(state As minasastate) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim betak As Double = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim mcinfo As Integer = 0
			Dim b As New Boolean()
			Dim stepfound As New Boolean()
			Dim diffcnt As Integer = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstate.stage >= 0 Then
				n = state.rstate.ia(0)
				i = state.rstate.ia(1)
				mcinfo = state.rstate.ia(2)
				diffcnt = state.rstate.ia(3)
				b = state.rstate.ba(0)
				stepfound = state.rstate.ba(1)
				betak = state.rstate.ra(0)
				v = state.rstate.ra(1)
				vv = state.rstate.ra(2)
			Else
				n = -983
				i = -989
				mcinfo = -834
				diffcnt = 900
				b = True
				stepfound = False
				betak = 214
				v = -338
				vv = -686
			End If
			If state.rstate.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstate.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstate.stage = 2 Then
				GoTo lbl_2
			End If
			If state.rstate.stage = 3 Then
				GoTo lbl_3
			End If
			If state.rstate.stage = 4 Then
				GoTo lbl_4
			End If
			If state.rstate.stage = 5 Then
				GoTo lbl_5
			End If
			If state.rstate.stage = 6 Then
				GoTo lbl_6
			End If
			If state.rstate.stage = 7 Then
				GoTo lbl_7
			End If
			If state.rstate.stage = 8 Then
				GoTo lbl_8
			End If
			If state.rstate.stage = 9 Then
				GoTo lbl_9
			End If
			If state.rstate.stage = 10 Then
				GoTo lbl_10
			End If
			If state.rstate.stage = 11 Then
				GoTo lbl_11
			End If
			If state.rstate.stage = 12 Then
				GoTo lbl_12
			End If
			If state.rstate.stage = 13 Then
				GoTo lbl_13
			End If
			If state.rstate.stage = 14 Then
				GoTo lbl_14
			End If

			'
			' Routine body
			'

			'
			' Prepare
			'
			n = state.n
			state.repterminationtype = 0
			state.repiterationscount = 0
			state.repnfev = 0
			state.debugrestartscount = 0
			state.cgtype = 1
			For i_ = 0 To n - 1
				state.xk(i_) = state.x(i_)
			Next
			For i = 0 To n - 1
				If CDbl(state.xk(i)) = CDbl(state.bndl(i)) OrElse CDbl(state.xk(i)) = CDbl(state.bndu(i)) Then
					state.ak(i) = 0
				Else
					state.ak(i) = 1
				End If
			Next
			state.mu = 0.1
			state.curalgo = 0

			'
			' Calculate F/G, initialize algorithm
			'
			clearrequestfields(state)
			state.needfg = True
			state.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			state.needfg = False
			If Not state.xrep Then
				GoTo lbl_15
			End If

			'
			' progress report
			'
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			state.xupdated = False
			lbl_15:
			If CDbl(asaboundedantigradnorm(state)) <= CDbl(state.epsg) Then
				state.repterminationtype = 4
				result = False
				Return result
			End If
			state.repnfev = state.repnfev + 1
			lbl_17:

			'
			' Main cycle
			'
			' At the beginning of new iteration:
			' * CurAlgo stores current algorithm selector
			' * State.XK, State.F and State.G store current X/F/G
			' * State.AK stores current set of active constraints
			'
			If False Then
				GoTo lbl_18
			End If

			'
			' GPA algorithm
			'
			If state.curalgo <> 0 Then
				GoTo lbl_19
			End If
			state.k = 0
			state.acount = 0
			lbl_21:
			If False Then
				GoTo lbl_22
			End If

			'
			' Determine Dk = proj(xk - gk)-xk
			'
			For i = 0 To n - 1
				state.d(i) = apserv.boundval(state.xk(i) - state.g(i), state.bndl(i), state.bndu(i)) - state.xk(i)
			Next

			'
			' Armijo line search.
			' * exact search with alpha=1 is tried first,
			'   'exact' means that we evaluate f() EXACTLY at
			'   bound(x-g,bndl,bndu), without intermediate floating
			'   point operations.
			' * alpha<1 are tried if explicit search wasn't successful
			' Result is placed into XN.
			'
			' Two types of search are needed because we can't
			' just use second type with alpha=1 because in finite
			' precision arithmetics (x1-x0)+x0 may differ from x1.
			' So while x1 is correctly bounded (it lie EXACTLY on
			' boundary, if it is active), (x1-x0)+x0 may be
			' not bounded.
			'
			v = 0.0
			For i_ = 0 To n - 1
				v += state.d(i_) * state.g(i_)
			Next
			state.dginit = v
			state.finit = state.f
			If Not (CDbl(asad1norm(state)) <= CDbl(state.stpmax) OrElse CDbl(state.stpmax) = CDbl(0)) Then
				GoTo lbl_23
			End If

			'
			' Try alpha=1 step first
			'
			For i = 0 To n - 1
				state.x(i) = apserv.boundval(state.xk(i) - state.g(i), state.bndl(i), state.bndu(i))
			Next
			clearrequestfields(state)
			state.needfg = True
			state.rstate.stage = 2
			GoTo lbl_rcomm
			lbl_2:
			state.needfg = False
			state.repnfev = state.repnfev + 1
			stepfound = CDbl(state.f) <= CDbl(state.finit + gpaftol * state.dginit)
			GoTo lbl_24
			lbl_23:
			stepfound = False
			lbl_24:
			If Not stepfound Then
				GoTo lbl_25
			End If

			'
			' we are at the boundary(ies)
			'
			For i_ = 0 To n - 1
				state.xn(i_) = state.x(i_)
			Next
			state.stp = 1
			GoTo lbl_26
			lbl_25:

			'
			' alpha=1 is too large, try smaller values
			'
			state.stp = 1
			linmin.linminnormalized(state.d, state.stp, n)
			state.dginit = state.dginit / state.stp
			state.stp = gpadecay * state.stp
			If CDbl(state.stpmax) > CDbl(0) Then
				state.stp = System.Math.Min(state.stp, state.stpmax)
			End If
			lbl_27:
			If False Then
				GoTo lbl_28
			End If
			v = state.stp
			For i_ = 0 To n - 1
				state.x(i_) = state.xk(i_)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.x(i_) + v * state.d(i_)
			Next
			clearrequestfields(state)
			state.needfg = True
			state.rstate.stage = 3
			GoTo lbl_rcomm
			lbl_3:
			state.needfg = False
			state.repnfev = state.repnfev + 1
			If CDbl(state.stp) <= CDbl(stpmin) Then
				GoTo lbl_28
			End If
			If CDbl(state.f) <= CDbl(state.finit + state.stp * gpaftol * state.dginit) Then
				GoTo lbl_28
			End If
			state.stp = state.stp * gpadecay
			GoTo lbl_27
			lbl_28:
			For i_ = 0 To n - 1
				state.xn(i_) = state.x(i_)
			Next
			lbl_26:
			state.repiterationscount = state.repiterationscount + 1
			If Not state.xrep Then
				GoTo lbl_29
			End If

			'
			' progress report
			'
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 4
			GoTo lbl_rcomm
			lbl_4:
			state.xupdated = False
			lbl_29:

			'
			' Calculate new set of active constraints.
			' Reset counter if active set was changed.
			' Prepare for the new iteration
			'
			For i = 0 To n - 1
				If CDbl(state.xn(i)) = CDbl(state.bndl(i)) OrElse CDbl(state.xn(i)) = CDbl(state.bndu(i)) Then
					state.an(i) = 0
				Else
					state.an(i) = 1
				End If
			Next
			For i = 0 To n - 1
				If CDbl(state.ak(i)) <> CDbl(state.an(i)) Then
					state.acount = -1
					Exit For
				End If
			Next
			state.acount = state.acount + 1
			For i_ = 0 To n - 1
				state.xk(i_) = state.xn(i_)
			Next
			For i_ = 0 To n - 1
				state.ak(i_) = state.an(i_)
			Next

			'
			' Stopping conditions
			'
			If Not (state.repiterationscount >= state.maxits AndAlso state.maxits > 0) Then
				GoTo lbl_31
			End If

			'
			' Too many iterations
			'
			state.repterminationtype = 5
			If Not state.xrep Then
				GoTo lbl_33
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 5
			GoTo lbl_rcomm
			lbl_5:
			state.xupdated = False
			lbl_33:
			result = False
			Return result
			lbl_31:
			If CDbl(asaboundedantigradnorm(state)) > CDbl(state.epsg) Then
				GoTo lbl_35
			End If

			'
			' Gradient is small enough
			'
			state.repterminationtype = 4
			If Not state.xrep Then
				GoTo lbl_37
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 6
			GoTo lbl_rcomm
			lbl_6:
			state.xupdated = False
			lbl_37:
			result = False
			Return result
			lbl_35:
			v = 0.0
			For i_ = 0 To n - 1
				v += state.d(i_) * state.d(i_)
			Next
			If CDbl(System.Math.sqrt(v) * state.stp) > CDbl(state.epsx) Then
				GoTo lbl_39
			End If

			'
			' Step size is too small, no further improvement is
			' possible
			'
			state.repterminationtype = 2
			If Not state.xrep Then
				GoTo lbl_41
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 7
			GoTo lbl_rcomm
			lbl_7:
			state.xupdated = False
			lbl_41:
			result = False
			Return result
			lbl_39:
			If CDbl(state.finit - state.f) > CDbl(state.epsf * System.Math.Max(System.Math.Abs(state.finit), System.Math.Max(System.Math.Abs(state.f), 1.0))) Then
				GoTo lbl_43
			End If

			'
			' F(k+1)-F(k) is small enough
			'
			state.repterminationtype = 1
			If Not state.xrep Then
				GoTo lbl_45
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 8
			GoTo lbl_rcomm
			lbl_8:
			state.xupdated = False
			lbl_45:
			result = False
			Return result
			lbl_43:

			'
			' Decide - should we switch algorithm or not
			'
			If asauisempty(state) Then
				If CDbl(asaginorm(state)) >= CDbl(state.mu * asad1norm(state)) Then
					state.curalgo = 1
					GoTo lbl_22
				Else
					state.mu = state.mu * asarho
				End If
			Else
				If state.acount = n1 Then
					If CDbl(asaginorm(state)) >= CDbl(state.mu * asad1norm(state)) Then
						state.curalgo = 1
						GoTo lbl_22
					End If
				End If
			End If

			'
			' Next iteration
			'
			state.k = state.k + 1
			GoTo lbl_21
			lbl_22:
			lbl_19:

			'
			' CG algorithm
			'
			If state.curalgo <> 1 Then
				GoTo lbl_47
			End If

			'
			' first, check that there are non-active constraints.
			' move to GPA algorithm, if all constraints are active
			'
			b = True
			For i = 0 To n - 1
				If CDbl(state.ak(i)) <> CDbl(0) Then
					b = False
					Exit For
				End If
			Next
			If b Then
				state.curalgo = 0
				GoTo lbl_17
			End If

			'
			' CG iterations
			'
			state.fold = state.f
			For i_ = 0 To n - 1
				state.xk(i_) = state.x(i_)
			Next
			For i = 0 To n - 1
				state.dk(i) = -(state.g(i) * state.ak(i))
				state.gc(i) = state.g(i) * state.ak(i)
			Next
			lbl_49:
			If False Then
				GoTo lbl_50
			End If

			'
			' Store G[k] for later calculation of Y[k]
			'
			For i = 0 To n - 1
				state.yk(i) = -state.gc(i)
			Next

			'
			' Make a CG step in direction given by DK[]:
			' * calculate step. Step projection into feasible set
			'   is used. It has several benefits: a) step may be
			'   found with usual line search, b) multiple constraints
			'   may be activated with one step, c) activated constraints
			'   are detected in a natural way - just compare x[i] with
			'   bounds
			' * update active set, set B to True, if there
			'   were changes in the set.
			'
			For i_ = 0 To n - 1
				state.d(i_) = state.dk(i_)
			Next
			For i_ = 0 To n - 1
				state.xn(i_) = state.xk(i_)
			Next
			state.mcstage = 0
			state.stp = 1
			linmin.linminnormalized(state.d, state.stp, n)
			If CDbl(state.laststep) <> CDbl(0) Then
				state.stp = state.laststep
			End If
			linmin.mcsrch(n, state.xn, state.f, state.gc, state.d, state.stp, _
				state.stpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			lbl_51:
			If state.mcstage = 0 Then
				GoTo lbl_52
			End If

			'
			' preprocess data: bound State.XN so it belongs to the
			' feasible set and store it in the State.X
			'
			For i = 0 To n - 1
				state.x(i) = apserv.boundval(state.xn(i), state.bndl(i), state.bndu(i))
			Next

			'
			' RComm
			'
			clearrequestfields(state)
			state.needfg = True
			state.rstate.stage = 9
			GoTo lbl_rcomm
			lbl_9:
			state.needfg = False

			'
			' postprocess data: zero components of G corresponding to
			' the active constraints
			'
			For i = 0 To n - 1
				If CDbl(state.x(i)) = CDbl(state.bndl(i)) OrElse CDbl(state.x(i)) = CDbl(state.bndu(i)) Then
					state.gc(i) = 0
				Else
					state.gc(i) = state.g(i)
				End If
			Next
			linmin.mcsrch(n, state.xn, state.f, state.gc, state.d, state.stp, _
				state.stpmax, gtol, mcinfo, state.nfev, state.work, state.lstate, _
				state.mcstage)
			GoTo lbl_51
			lbl_52:
			diffcnt = 0
			For i = 0 To n - 1

				'
				' XN contains unprojected result, project it,
				' save copy to X (will be used for progress reporting)
				'
				state.xn(i) = apserv.boundval(state.xn(i), state.bndl(i), state.bndu(i))

				'
				' update active set
				'
				If CDbl(state.xn(i)) = CDbl(state.bndl(i)) OrElse CDbl(state.xn(i)) = CDbl(state.bndu(i)) Then
					state.an(i) = 0
				Else
					state.an(i) = 1
				End If
				If CDbl(state.an(i)) <> CDbl(state.ak(i)) Then
					diffcnt = diffcnt + 1
				End If
				state.ak(i) = state.an(i)
			Next
			For i_ = 0 To n - 1
				state.xk(i_) = state.xn(i_)
			Next
			state.repnfev = state.repnfev + state.nfev
			state.repiterationscount = state.repiterationscount + 1
			If Not state.xrep Then
				GoTo lbl_53
			End If

			'
			' progress report
			'
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 10
			GoTo lbl_rcomm
			lbl_10:
			state.xupdated = False
			lbl_53:

			'
			' Update info about step length
			'
			v = 0.0
			For i_ = 0 To n - 1
				v += state.d(i_) * state.d(i_)
			Next
			state.laststep = System.Math.sqrt(v) * state.stp

			'
			' Check stopping conditions.
			'
			If CDbl(asaboundedantigradnorm(state)) > CDbl(state.epsg) Then
				GoTo lbl_55
			End If

			'
			' Gradient is small enough
			'
			state.repterminationtype = 4
			If Not state.xrep Then
				GoTo lbl_57
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 11
			GoTo lbl_rcomm
			lbl_11:
			state.xupdated = False
			lbl_57:
			result = False
			Return result
			lbl_55:
			If Not (state.repiterationscount >= state.maxits AndAlso state.maxits > 0) Then
				GoTo lbl_59
			End If

			'
			' Too many iterations
			'
			state.repterminationtype = 5
			If Not state.xrep Then
				GoTo lbl_61
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 12
			GoTo lbl_rcomm
			lbl_12:
			state.xupdated = False
			lbl_61:
			result = False
			Return result
			lbl_59:
			If Not (CDbl(asaginorm(state)) >= CDbl(state.mu * asad1norm(state)) AndAlso diffcnt = 0) Then
				GoTo lbl_63
			End If

			'
			' These conditions (EpsF/EpsX) are explicitly or implicitly
			' related to the current step size and influenced
			' by changes in the active constraints.
			'
			' For these reasons they are checked only when we don't
			' want to 'unstick' at the end of the iteration and there
			' were no changes in the active set.
			'
			' NOTE: consition |G|>=Mu*|D1| must be exactly opposite
			' to the condition used to switch back to GPA. At least
			' one inequality must be strict, otherwise infinite cycle
			' may occur when |G|=Mu*|D1| (we DON'T test stopping
			' conditions and we DON'T switch to GPA, so we cycle
			' indefinitely).
			'
			If CDbl(state.fold - state.f) > CDbl(state.epsf * System.Math.Max(System.Math.Abs(state.fold), System.Math.Max(System.Math.Abs(state.f), 1.0))) Then
				GoTo lbl_65
			End If

			'
			' F(k+1)-F(k) is small enough
			'
			state.repterminationtype = 1
			If Not state.xrep Then
				GoTo lbl_67
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 13
			GoTo lbl_rcomm
			lbl_13:
			state.xupdated = False
			lbl_67:
			result = False
			Return result
			lbl_65:
			If CDbl(state.laststep) > CDbl(state.epsx) Then
				GoTo lbl_69
			End If

			'
			' X(k+1)-X(k) is small enough
			'
			state.repterminationtype = 2
			If Not state.xrep Then
				GoTo lbl_71
			End If
			clearrequestfields(state)
			state.xupdated = True
			state.rstate.stage = 14
			GoTo lbl_rcomm
			lbl_14:
			state.xupdated = False
			lbl_71:
			result = False
			Return result
			lbl_69:
			lbl_63:

			'
			' Check conditions for switching
			'
			If CDbl(asaginorm(state)) < CDbl(state.mu * asad1norm(state)) Then
				state.curalgo = 0
				GoTo lbl_50
			End If
			If diffcnt > 0 Then
				If asauisempty(state) OrElse diffcnt >= n2 Then
					state.curalgo = 1
				Else
					state.curalgo = 0
				End If
				GoTo lbl_50
			End If

			'
			' Calculate D(k+1)
			'
			' Line search may result in:
			' * maximum feasible step being taken (already processed)
			' * point satisfying Wolfe conditions
			' * some kind of error (CG is restarted by assigning 0.0 to Beta)
			'
			If mcinfo = 1 Then

				'
				' Standard Wolfe conditions are satisfied:
				' * calculate Y[K] and BetaK
				'
				For i_ = 0 To n - 1
					state.yk(i_) = state.yk(i_) + state.gc(i_)
				Next
				vv = 0.0
				For i_ = 0 To n - 1
					vv += state.yk(i_) * state.dk(i_)
				Next
				v = 0.0
				For i_ = 0 To n - 1
					v += state.gc(i_) * state.gc(i_)
				Next
				state.betady = v / vv
				v = 0.0
				For i_ = 0 To n - 1
					v += state.gc(i_) * state.yk(i_)
				Next
				state.betahs = v / vv
				If state.cgtype = 0 Then
					betak = state.betady
				End If
				If state.cgtype = 1 Then
					betak = System.Math.Max(0, System.Math.Min(state.betady, state.betahs))
				End If
			Else

				'
				' Something is wrong (may be function is too wild or too flat).
				'
				' We'll set BetaK=0, which will restart CG algorithm.
				' We can stop later (during normal checks) if stopping conditions are met.
				'
				betak = 0
				state.debugrestartscount = state.debugrestartscount + 1
			End If
			For i_ = 0 To n - 1
				state.dn(i_) = -state.gc(i_)
			Next
			For i_ = 0 To n - 1
				state.dn(i_) = state.dn(i_) + betak * state.dk(i_)
			Next
			For i_ = 0 To n - 1
				state.dk(i_) = state.dn(i_)
			Next

			'
			' update other information
			'
			state.fold = state.f
			state.k = state.k + 1
			GoTo lbl_49
			lbl_50:
			lbl_47:
			GoTo lbl_17
			lbl_18:
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstate.ia(0) = n
			state.rstate.ia(1) = i
			state.rstate.ia(2) = mcinfo
			state.rstate.ia(3) = diffcnt
			state.rstate.ba(0) = b
			state.rstate.ba(1) = stepfound
			state.rstate.ra(0) = betak
			state.rstate.ra(1) = v
			state.rstate.ra(2) = vv
			Return result
		End Function


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasaresults(state As minasastate, ByRef x As Double(), rep As minasareport)
			x = New Double(-1) {}

			minasaresultsbuf(state, x, rep)
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasaresultsbuf(state As minasastate, ByRef x As Double(), rep As minasareport)
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			If alglib.ap.len(x) < state.n Then
				x = New Double(state.n - 1) {}
			End If
			For i_ = 0 To state.n - 1
				x(i_) = state.x(i_)
			Next
			rep.iterationscount = state.repiterationscount
			rep.nfev = state.repnfev
			rep.terminationtype = state.repterminationtype
			rep.activeconstraints = 0
			For i = 0 To state.n - 1
				If CDbl(state.ak(i)) = CDbl(0) Then
					rep.activeconstraints = rep.activeconstraints + 1
				End If
			Next
		End Sub


		'************************************************************************
'        Obsolete optimization algorithm.
'        Was replaced by MinBLEIC subpackage.
'
'          -- ALGLIB --
'             Copyright 30.07.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minasarestartfrom(state As minasastate, x As Double(), bndl As Double(), bndu As Double())
			Dim i_ As Integer = 0

			alglib.ap.assert(alglib.ap.len(x) >= state.n, "MinASARestartFrom: Length(X)<N!")
			alglib.ap.assert(apserv.isfinitevector(x, state.n), "MinASARestartFrom: X contains infinite or NaN values!")
			alglib.ap.assert(alglib.ap.len(bndl) >= state.n, "MinASARestartFrom: Length(BndL)<N!")
			alglib.ap.assert(apserv.isfinitevector(bndl, state.n), "MinASARestartFrom: BndL contains infinite or NaN values!")
			alglib.ap.assert(alglib.ap.len(bndu) >= state.n, "MinASARestartFrom: Length(BndU)<N!")
			alglib.ap.assert(apserv.isfinitevector(bndu, state.n), "MinASARestartFrom: BndU contains infinite or NaN values!")
			For i_ = 0 To state.n - 1
				state.x(i_) = x(i_)
			Next
			For i_ = 0 To state.n - 1
				state.bndl(i_) = bndl(i_)
			Next
			For i_ = 0 To state.n - 1
				state.bndu(i_) = bndu(i_)
			Next
			state.laststep = 0
			state.rstate.ia = New Integer(3) {}
			state.rstate.ba = New Boolean(1) {}
			state.rstate.ra = New Double(2) {}
			state.rstate.stage = -1
			clearrequestfields(state)
		End Sub


		'************************************************************************
'        Returns norm of bounded anti-gradient.
'
'        Bounded antigradient is a vector obtained from  anti-gradient  by  zeroing
'        components which point outwards:
'            result = norm(v)
'            v[i]=0     if ((-g[i]<0)and(x[i]=bndl[i])) or
'                          ((-g[i]>0)and(x[i]=bndu[i]))
'            v[i]=-g[i] otherwise
'
'        This function may be used to check a stopping criterion.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function asaboundedantigradnorm(state As minasastate) As Double
			Dim result As Double = 0
			Dim i As Integer = 0
			Dim v As Double = 0

			result = 0
			For i = 0 To state.n - 1
				v = -state.g(i)
				If CDbl(state.x(i)) = CDbl(state.bndl(i)) AndAlso CDbl(-state.g(i)) < CDbl(0) Then
					v = 0
				End If
				If CDbl(state.x(i)) = CDbl(state.bndu(i)) AndAlso CDbl(-state.g(i)) > CDbl(0) Then
					v = 0
				End If
				result = result + Math.sqr(v)
			Next
			result = System.Math.sqrt(result)
			Return result
		End Function


		'************************************************************************
'        Returns norm of GI(x).
'
'        GI(x) is  a  gradient  vector  whose  components  associated  with  active
'        constraints are zeroed. It  differs  from  bounded  anti-gradient  because
'        components  of   GI(x)   are   zeroed  independently  of  sign(g[i]),  and
'        anti-gradient's components are zeroed with respect to both constraint  and
'        sign.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function asaginorm(state As minasastate) As Double
			Dim result As Double = 0
			Dim i As Integer = 0

			result = 0
			For i = 0 To state.n - 1
				If CDbl(state.x(i)) <> CDbl(state.bndl(i)) AndAlso CDbl(state.x(i)) <> CDbl(state.bndu(i)) Then
					result = result + Math.sqr(state.g(i))
				End If
			Next
			result = System.Math.sqrt(result)
			Return result
		End Function


		'************************************************************************
'        Returns norm(D1(State.X))
'
'        For a meaning of D1 see 'NEW ACTIVE SET ALGORITHM FOR BOX CONSTRAINED
'        OPTIMIZATION' by WILLIAM W. HAGER AND HONGCHAO ZHANG.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function asad1norm(state As minasastate) As Double
			Dim result As Double = 0
			Dim i As Integer = 0

			result = 0
			For i = 0 To state.n - 1
				result = result + Math.sqr(apserv.boundval(state.x(i) - state.g(i), state.bndl(i), state.bndu(i)) - state.x(i))
			Next
			result = System.Math.sqrt(result)
			Return result
		End Function


		'************************************************************************
'        Returns True, if U set is empty.
'
'        * State.X is used as point,
'        * State.G - as gradient,
'        * D is calculated within function (because State.D may have different
'          meaning depending on current optimization algorithm)
'
'        For a meaning of U see 'NEW ACTIVE SET ALGORITHM FOR BOX CONSTRAINED
'        OPTIMIZATION' by WILLIAM W. HAGER AND HONGCHAO ZHANG.
'
'          -- ALGLIB --
'             Copyright 20.03.2009 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function asauisempty(state As minasastate) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim d As Double = 0
			Dim d2 As Double = 0
			Dim d32 As Double = 0

			d = asad1norm(state)
			d2 = System.Math.sqrt(d)
			d32 = d * d2
			result = True
			For i = 0 To state.n - 1
				If CDbl(System.Math.Abs(state.g(i))) >= CDbl(d2) AndAlso CDbl(System.Math.Min(state.x(i) - state.bndl(i), state.bndu(i) - state.x(i))) >= CDbl(d32) Then
					result = False
					Return result
				End If
			Next
			Return result
		End Function


		'************************************************************************
'        Clears request fileds (to be sure that we don't forgot to clear something)
'        ************************************************************************

		Private Shared Sub clearrequestfields(state As minasastate)
			state.needfg = False
			state.xupdated = False
		End Sub


	End Class
	Public Class minnlc
		'************************************************************************
'        This object stores nonlinear optimizer state.
'        You should use functions provided by MinNLC subpackage to work  with  this
'        object
'        ************************************************************************

		Public Class minnlcstate
			Inherits apobject
			Public stabilizingpoint As Double
			Public initialinequalitymultiplier As Double
			Public solvertype As Integer
			Public prectype As Integer
			Public updatefreq As Integer
			Public rho As Double
			Public n As Integer
			Public epsg As Double
			Public epsf As Double
			Public epsx As Double
			Public maxits As Integer
			Public aulitscnt As Integer
			Public xrep As Boolean
			Public diffstep As Double
			Public teststep As Double
			Public s As Double()
			Public bndl As Double()
			Public bndu As Double()
			Public hasbndl As Boolean()
			Public hasbndu As Boolean()
			Public nec As Integer
			Public nic As Integer
			Public cleic As Double(,)
			Public ng As Integer
			Public nh As Integer
			Public x As Double()
			Public f As Double
			Public fi As Double()
			Public j As Double(,)
			Public needfij As Boolean
			Public needfi As Boolean
			Public xupdated As Boolean
			Public rstate As rcommstate
			Public rstateaul As rcommstate
			Public scaledbndl As Double()
			Public scaledbndu As Double()
			Public scaledcleic As Double(,)
			Public xc As Double()
			Public xstart As Double()
			Public xbase As Double()
			Public fbase As Double()
			Public dfbase As Double()
			Public fm2 As Double()
			Public fm1 As Double()
			Public fp1 As Double()
			Public fp2 As Double()
			Public dfm1 As Double()
			Public dfp1 As Double()
			Public bufd As Double()
			Public bufc As Double()
			Public bufw As Double(,)
			Public xk As Double()
			Public xk1 As Double()
			Public gk As Double()
			Public gk1 As Double()
			Public gammak As Double
			Public xkpresent As Boolean
			Public auloptimizer As minlbfgs.minlbfgsstate
			Public aulreport As minlbfgs.minlbfgsreport
			Public nubc As Double()
			Public nulc As Double()
			Public nunlc As Double()
			Public repinneriterationscount As Integer
			Public repouteriterationscount As Integer
			Public repnfev As Integer
			Public repvaridx As Integer
			Public repfuncidx As Integer
			Public repterminationtype As Integer
			Public repdbgphase0its As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
				s = New Double(-1) {}
				bndl = New Double(-1) {}
				bndu = New Double(-1) {}
				hasbndl = New Boolean(-1) {}
				hasbndu = New Boolean(-1) {}
				cleic = New Double(-1, -1) {}
				x = New Double(-1) {}
				fi = New Double(-1) {}
				j = New Double(-1, -1) {}
				rstate = New rcommstate()
				rstateaul = New rcommstate()
				scaledbndl = New Double(-1) {}
				scaledbndu = New Double(-1) {}
				scaledcleic = New Double(-1, -1) {}
				xc = New Double(-1) {}
				xstart = New Double(-1) {}
				xbase = New Double(-1) {}
				fbase = New Double(-1) {}
				dfbase = New Double(-1) {}
				fm2 = New Double(-1) {}
				fm1 = New Double(-1) {}
				fp1 = New Double(-1) {}
				fp2 = New Double(-1) {}
				dfm1 = New Double(-1) {}
				dfp1 = New Double(-1) {}
				bufd = New Double(-1) {}
				bufc = New Double(-1) {}
				bufw = New Double(-1, -1) {}
				xk = New Double(-1) {}
				xk1 = New Double(-1) {}
				gk = New Double(-1) {}
				gk1 = New Double(-1) {}
				auloptimizer = New minlbfgs.minlbfgsstate()
				aulreport = New minlbfgs.minlbfgsreport()
				nubc = New Double(-1) {}
				nulc = New Double(-1) {}
				nunlc = New Double(-1) {}
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minnlcstate()
				_result.stabilizingpoint = stabilizingpoint
				_result.initialinequalitymultiplier = initialinequalitymultiplier
				_result.solvertype = solvertype
				_result.prectype = prectype
				_result.updatefreq = updatefreq
				_result.rho = rho
				_result.n = n
				_result.epsg = epsg
				_result.epsf = epsf
				_result.epsx = epsx
				_result.maxits = maxits
				_result.aulitscnt = aulitscnt
				_result.xrep = xrep
				_result.diffstep = diffstep
				_result.teststep = teststep
				_result.s = DirectCast(s.Clone(), Double())
				_result.bndl = DirectCast(bndl.Clone(), Double())
				_result.bndu = DirectCast(bndu.Clone(), Double())
				_result.hasbndl = DirectCast(hasbndl.Clone(), Boolean())
				_result.hasbndu = DirectCast(hasbndu.Clone(), Boolean())
				_result.nec = nec
				_result.nic = nic
				_result.cleic = DirectCast(cleic.Clone(), Double(,))
				_result.ng = ng
				_result.nh = nh
				_result.x = DirectCast(x.Clone(), Double())
				_result.f = f
				_result.fi = DirectCast(fi.Clone(), Double())
				_result.j = DirectCast(j.Clone(), Double(,))
				_result.needfij = needfij
				_result.needfi = needfi
				_result.xupdated = xupdated
				_result.rstate = DirectCast(rstate.make_copy(), rcommstate)
				_result.rstateaul = DirectCast(rstateaul.make_copy(), rcommstate)
				_result.scaledbndl = DirectCast(scaledbndl.Clone(), Double())
				_result.scaledbndu = DirectCast(scaledbndu.Clone(), Double())
				_result.scaledcleic = DirectCast(scaledcleic.Clone(), Double(,))
				_result.xc = DirectCast(xc.Clone(), Double())
				_result.xstart = DirectCast(xstart.Clone(), Double())
				_result.xbase = DirectCast(xbase.Clone(), Double())
				_result.fbase = DirectCast(fbase.Clone(), Double())
				_result.dfbase = DirectCast(dfbase.Clone(), Double())
				_result.fm2 = DirectCast(fm2.Clone(), Double())
				_result.fm1 = DirectCast(fm1.Clone(), Double())
				_result.fp1 = DirectCast(fp1.Clone(), Double())
				_result.fp2 = DirectCast(fp2.Clone(), Double())
				_result.dfm1 = DirectCast(dfm1.Clone(), Double())
				_result.dfp1 = DirectCast(dfp1.Clone(), Double())
				_result.bufd = DirectCast(bufd.Clone(), Double())
				_result.bufc = DirectCast(bufc.Clone(), Double())
				_result.bufw = DirectCast(bufw.Clone(), Double(,))
				_result.xk = DirectCast(xk.Clone(), Double())
				_result.xk1 = DirectCast(xk1.Clone(), Double())
				_result.gk = DirectCast(gk.Clone(), Double())
				_result.gk1 = DirectCast(gk1.Clone(), Double())
				_result.gammak = gammak
				_result.xkpresent = xkpresent
				_result.auloptimizer = DirectCast(auloptimizer.make_copy(), minlbfgs.minlbfgsstate)
				_result.aulreport = DirectCast(aulreport.make_copy(), minlbfgs.minlbfgsreport)
				_result.nubc = DirectCast(nubc.Clone(), Double())
				_result.nulc = DirectCast(nulc.Clone(), Double())
				_result.nunlc = DirectCast(nunlc.Clone(), Double())
				_result.repinneriterationscount = repinneriterationscount
				_result.repouteriterationscount = repouteriterationscount
				_result.repnfev = repnfev
				_result.repvaridx = repvaridx
				_result.repfuncidx = repfuncidx
				_result.repterminationtype = repterminationtype
				_result.repdbgphase0its = repdbgphase0its
				Return _result
			End Function
		End Class


		'************************************************************************
'        This structure stores optimization report:
'        * IterationsCount           total number of inner iterations
'        * NFEV                      number of gradient evaluations
'        * TerminationType           termination type (see below)
'
'        TERMINATION CODES
'
'        TerminationType field contains completion code, which can be:
'          -8    internal integrity control detected  infinite  or  NAN  values  in
'                function/gradient. Abnormal termination signalled.
'          -7    gradient verification failed.
'                See MinNLCSetGradientCheck() for more information.
'           1    relative function improvement is no more than EpsF.
'           2    relative step is no more than EpsX.
'           4    gradient norm is no more than EpsG
'           5    MaxIts steps was taken
'           7    stopping conditions are too stringent,
'                further improvement is impossible,
'                X contains best point found so far.
'                
'        Other fields of this structure are not documented and should not be used!
'        ************************************************************************

		Public Class minnlcreport
			Inherits apobject
			Public iterationscount As Integer
			Public nfev As Integer
			Public varidx As Integer
			Public funcidx As Integer
			Public terminationtype As Integer
			Public dbgphase0its As Integer
			Public Sub New()
				init()
			End Sub
			Public Overrides Sub init()
			End Sub
			Public Overrides Function make_copy() As alglib.apobject
				Dim _result As New minnlcreport()
				_result.iterationscount = iterationscount
				_result.nfev = nfev
				_result.varidx = varidx
				_result.funcidx = funcidx
				_result.terminationtype = terminationtype
				_result.dbgphase0its = dbgphase0its
				Return _result
			End Function
		End Class




		Public Const aulmaxgrowth As Double = 10.0
		Public Const lbfgsfactor As Integer = 10
		Public Const hessesttol As Double = 1E-06


		'************************************************************************
'                          NONLINEARLY  CONSTRAINED  OPTIMIZATION
'                    WITH PRECONDITIONED AUGMENTED LAGRANGIAN ALGORITHM
'
'        DESCRIPTION:
'        The  subroutine  minimizes  function   F(x)  of N arguments subject to any
'        combination of:
'        * bound constraints
'        * linear inequality constraints
'        * linear equality constraints
'        * nonlinear equality constraints Gi(x)=0
'        * nonlinear inequality constraints Hi(x)<=0
'
'        REQUIREMENTS:
'        * user must provide function value and gradient for F(), H(), G()
'        * starting point X0 must be feasible or not too far away from the feasible
'          set
'        * F(), G(), H() are twice continuously differentiable on the feasible  set
'          and its neighborhood
'        * nonlinear constraints G() and H() must have non-zero gradient at  G(x)=0
'          and at H(x)=0. Say, constraint like x^2>=1 is supported, but x^2>=0   is
'          NOT supported.
'
'        USAGE:
'
'        Constrained optimization if far more complex than the  unconstrained  one.
'        Nonlinearly constrained optimization is one of the most esoteric numerical
'        procedures.
'
'        Here we give very brief outline  of  the  MinNLC  optimizer.  We  strongly
'        recommend you to study examples in the ALGLIB Reference Manual and to read
'        ALGLIB User Guide on optimization, which is available at
'        http://www.alglib.net/optimization/
'
'        1. User initializes algorithm state with MinNLCCreate() call  and  chooses
'           what NLC solver to use. There is some solver which is used by  default,
'           with default settings, but you should NOT rely on  default  choice.  It
'           may change in future releases of ALGLIB without notice, and no one  can
'           guarantee that new solver will be  able  to  solve  your  problem  with
'           default settings.
'           
'           From the other side, if you choose solver explicitly, you can be pretty
'           sure that it will work with new ALGLIB releases.
'           
'           In the current release following solvers can be used:
'           * AUL solver (activated with MinNLCSetAlgoAUL() function)
'
'        2. User adds boundary and/or linear and/or nonlinear constraints by  means
'           of calling one of the following functions:
'           a) MinNLCSetBC() for boundary constraints
'           b) MinNLCSetLC() for linear constraints
'           c) MinNLCSetNLC() for nonlinear constraints
'           You may combine (a), (b) and (c) in one optimization problem.
'           
'        3. User sets scale of the variables with MinNLCSetScale() function. It  is
'           VERY important to set  scale  of  the  variables,  because  nonlinearly
'           constrained problems are hard to solve when variables are badly scaled.
'
'        4. User sets  stopping  conditions  with  MinNLCSetCond(). If  NLC  solver
'           uses  inner/outer  iteration  layout,  this  function   sets   stopping
'           conditions for INNER iterations.
'           
'        5. User chooses one of the  preconditioning  methods.  Preconditioning  is
'           very  important  for  efficient  handling  of boundary/linear/nonlinear
'           constraints. Without preconditioning algorithm would require  thousands
'           of iterations even for simple problems.  Two  preconditioners  can   be
'           used:
'           * approximate LBFGS-based  preconditioner  which  should  be  used  for
'             problems with almost orthogonal  constraints  (activated  by  calling
'             MinNLCSetPrecInexact)
'           * exact low-rank preconditiner (activated by MinNLCSetPrecExactLowRank)
'             which should be used for problems with moderate number of constraints
'             which do not have to be orthogonal.
'
'        6. Finally, user calls MinNLCOptimize()  function  which  takes  algorithm
'           state and pointer (delegate, etc.) to callback function which calculates
'           F/G/H.
'
'        7. User calls MinNLCResults() to get solution
'
'        8. Optionally user may call MinNLCRestartFrom() to solve  another  problem
'           with same N but another starting point. MinNLCRestartFrom()  allows  to
'           reuse already initialized structure.
'
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size ofX
'            X       -   starting point, array[N]:
'                        * it is better to set X to a feasible point
'                        * but X can be infeasible, in which case algorithm will try
'                          to find feasible point first, using X as initial
'                          approximation.
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlccreate(n As Integer, x As Double(), state As minnlcstate)
			alglib.ap.assert(n >= 1, "MinNLCCreate: N<1")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinNLCCreate: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinNLCCreate: X contains infinite or NaN values")
			minnlcinitinternal(n, x, 0.0, state)
		End Sub


		'************************************************************************
'        This subroutine is a finite  difference variant of MinNLCCreate(). It uses
'        finite differences in order to differentiate target function.
'
'        Description below contains information which is specific to this  function
'        only. We recommend to read comments on MinNLCCreate() in order to get more
'        information about creation of NLC optimizer.
'
'        INPUT PARAMETERS:
'            N       -   problem dimension, N>0:
'                        * if given, only leading N elements of X are used
'                        * if not given, automatically determined from size ofX
'            X       -   starting point, array[N]:
'                        * it is better to set X to a feasible point
'                        * but X can be infeasible, in which case algorithm will try
'                          to find feasible point first, using X as initial
'                          approximation.
'            DiffStep-   differentiation step, >0
'
'        OUTPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'        NOTES:
'        1. algorithm uses 4-point central formula for differentiation.
'        2. differentiation step along I-th axis is equal to DiffStep*S[I] where
'           S[] is scaling vector which can be set by MinNLCSetScale() call.
'        3. we recommend you to use moderate values of  differentiation  step.  Too
'           large step will result in too large TRUNCATION  errors, while too small
'           step will result in too large NUMERICAL  errors.  1.0E-4  can  be  good
'           value to start from.
'        4. Numerical  differentiation  is   very   inefficient  -   one   gradient
'           calculation needs 4*N function evaluations. This function will work for
'           any N - either small (1...10), moderate (10...100) or  large  (100...).
'           However, performance penalty will be too severe for any N's except  for
'           small ones.
'           We should also say that code which relies on numerical  differentiation
'           is  less   robust   and  precise.  Imprecise  gradient  may  slow  down
'           convergence, especially on highly nonlinear problems.
'           Thus  we  recommend to use this function for fast prototyping on small-
'           dimensional problems only, and to implement analytical gradient as soon
'           as possible.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlccreatef(n As Integer, x As Double(), diffstep As Double, state As minnlcstate)
			alglib.ap.assert(n >= 1, "MinNLCCreateF: N<1")
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinNLCCreateF: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinNLCCreateF: X contains infinite or NaN values")
			alglib.ap.assert(Math.isfinite(diffstep), "MinNLCCreateF: DiffStep is infinite or NaN!")
			alglib.ap.assert(CDbl(diffstep) > CDbl(0), "MinNLCCreateF: DiffStep is non-positive!")
			minnlcinitinternal(n, x, diffstep, state)
		End Sub


		'************************************************************************
'        This function sets boundary constraints for NLC optimizer.
'
'        Boundary constraints are inactive by  default  (after  initial  creation).
'        They are preserved after algorithm restart with  MinNLCRestartFrom().
'
'        You may combine boundary constraints with  general  linear ones - and with
'        nonlinear ones! Boundary constraints are  handled  more  efficiently  than
'        other types.  Thus,  if  your  problem  has  mixed  constraints,  you  may
'        explicitly specify some of them as boundary and save some time/space.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            BndL    -   lower bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very small number or -INF.
'            BndU    -   upper bounds, array[N].
'                        If some (all) variables are unbounded, you may specify
'                        very large number or +INF.
'
'        NOTE 1:  it is possible to specify  BndL[i]=BndU[i].  In  this  case  I-th
'        variable will be "frozen" at X[i]=BndL[i]=BndU[i].
'
'        NOTE 2:  when you solve your problem  with  augmented  Lagrangian  solver,
'                 boundary constraints are  satisfied  only  approximately!  It  is
'                 possible   that  algorithm  will  evaluate  function  outside  of
'                 feasible area!
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetbc(state As minnlcstate, bndl As Double(), bndu As Double())
			Dim i As Integer = 0
			Dim n As Integer = 0

			n = state.n
			alglib.ap.assert(alglib.ap.len(bndl) >= n, "MinNLCSetBC: Length(BndL)<N")
			alglib.ap.assert(alglib.ap.len(bndu) >= n, "MinNLCSetBC: Length(BndU)<N")
			For i = 0 To n - 1
				alglib.ap.assert(Math.isfinite(bndl(i)) OrElse [Double].IsNegativeInfinity(bndl(i)), "MinNLCSetBC: BndL contains NAN or +INF")
				alglib.ap.assert(Math.isfinite(bndu(i)) OrElse [Double].IsPositiveInfinity(bndu(i)), "MinNLCSetBC: BndL contains NAN or -INF")
				state.bndl(i) = bndl(i)
				state.hasbndl(i) = Math.isfinite(bndl(i))
				state.bndu(i) = bndu(i)
				state.hasbndu(i) = Math.isfinite(bndu(i))
			Next
		End Sub


		'************************************************************************
'        This function sets linear constraints for MinNLC optimizer.
'
'        Linear constraints are inactive by default (after initial creation).  They
'        are preserved after algorithm restart with MinNLCRestartFrom().
'
'        You may combine linear constraints with boundary ones - and with nonlinear
'        ones! If your problem has mixed constraints, you  may  explicitly  specify
'        some of them as linear. It  may  help  optimizer   to   handle  them  more
'        efficiently.
'
'        INPUT PARAMETERS:
'            State   -   structure previously allocated with MinNLCCreate call.
'            C       -   linear constraints, array[K,N+1].
'                        Each row of C represents one constraint, either equality
'                        or inequality (see below):
'                        * first N elements correspond to coefficients,
'                        * last element corresponds to the right part.
'                        All elements of C (including right part) must be finite.
'            CT      -   type of constraints, array[K]:
'                        * if CT[i]>0, then I-th constraint is C[i,*]*x >= C[i,n+1]
'                        * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
'                        * if CT[i]<0, then I-th constraint is C[i,*]*x <= C[i,n+1]
'            K       -   number of equality/inequality constraints, K>=0:
'                        * if given, only leading K elements of C/CT are used
'                        * if not given, automatically determined from sizes of C/CT
'
'        NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
'                linear constraints are  satisfied  only   approximately!   It   is
'                possible   that  algorithm  will  evaluate  function  outside   of
'                feasible area!
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetlc(state As minnlcstate, c As Double(,), ct As Integer(), k As Integer)
			Dim n As Integer = 0
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			n = state.n

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(k >= 0, "MinNLCSetLC: K<0")
			alglib.ap.assert(alglib.ap.cols(c) >= n + 1 OrElse k = 0, "MinNLCSetLC: Cols(C)<N+1")
			alglib.ap.assert(alglib.ap.rows(c) >= k, "MinNLCSetLC: Rows(C)<K")
			alglib.ap.assert(alglib.ap.len(ct) >= k, "MinNLCSetLC: Length(CT)<K")
			alglib.ap.assert(apserv.apservisfinitematrix(c, k, n + 1), "MinNLCSetLC: C contains infinite or NaN values!")

			'
			' Handle zero K
			'
			If k = 0 Then
				state.nec = 0
				state.nic = 0
				Return
			End If

			'
			' Equality constraints are stored first, in the upper
			' NEC rows of State.CLEIC matrix. Inequality constraints
			' are stored in the next NIC rows.
			'
			' NOTE: we convert inequality constraints to the form
			' A*x<=b before copying them.
			'
			apserv.rmatrixsetlengthatleast(state.cleic, k, n + 1)
			state.nec = 0
			state.nic = 0
			For i = 0 To k - 1
				If ct(i) = 0 Then
					For i_ = 0 To n
						state.cleic(state.nec, i_) = c(i, i_)
					Next
					state.nec = state.nec + 1
				End If
			Next
			For i = 0 To k - 1
				If ct(i) <> 0 Then
					If ct(i) > 0 Then
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = -c(i, i_)
						Next
					Else
						For i_ = 0 To n
							state.cleic(state.nec + state.nic, i_) = c(i, i_)
						Next
					End If
					state.nic = state.nic + 1
				End If
			Next
		End Sub


		'************************************************************************
'        This function sets nonlinear constraints for MinNLC optimizer.
'
'        In fact, this function sets NUMBER of nonlinear  constraints.  Constraints
'        itself (constraint functions) are passed to MinNLCOptimize() method.  This
'        method requires user-defined vector function F[]  and  its  Jacobian  J[],
'        where:
'        * first component of F[] and first row  of  Jacobian  J[]  corresponds  to
'          function being minimized
'        * next NLEC components of F[] (and rows  of  J)  correspond  to  nonlinear
'          equality constraints G_i(x)=0
'        * next NLIC components of F[] (and rows  of  J)  correspond  to  nonlinear
'          inequality constraints H_i(x)<=0
'
'        NOTE: you may combine nonlinear constraints with linear/boundary ones.  If
'              your problem has mixed constraints, you  may explicitly specify some
'              of them as linear ones. It may help optimizer to  handle  them  more
'              efficiently.
'
'        INPUT PARAMETERS:
'            State   -   structure previously allocated with MinNLCCreate call.
'            NLEC    -   number of Non-Linear Equality Constraints (NLEC), >=0
'            NLIC    -   number of Non-Linear Inquality Constraints (NLIC), >=0
'
'        NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
'                nonlinear constraints are satisfied only  approximately!   It   is
'                possible   that  algorithm  will  evaluate  function  outside   of
'                feasible area!
'                
'        NOTE 2: algorithm scales variables  according  to   scale   specified   by
'                MinNLCSetScale()  function,  so  it can handle problems with badly
'                scaled variables (as long as we KNOW their scales).
'                   
'                However,  there  is  no  way  to  automatically  scale   nonlinear
'                constraints Gi(x) and Hi(x). Inappropriate scaling  of  Gi/Hi  may
'                ruin convergence. Solving problem with  constraint  "1000*G0(x)=0"
'                is NOT same as solving it with constraint "0.001*G0(x)=0".
'                   
'                It  means  that  YOU  are  the  one who is responsible for correct
'                scaling of nonlinear constraints Gi(x) and Hi(x). We recommend you
'                to scale nonlinear constraints in such way that I-th component  of
'                dG/dX (or dH/dx) has approximately unit  magnitude  (for  problems
'                with unit scale)  or  has  magnitude approximately equal to 1/S[i]
'                (where S is a scale set by MinNLCSetScale() function).
'
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetnlc(state As minnlcstate, nlec As Integer, nlic As Integer)
			alglib.ap.assert(nlec >= 0, "MinNLCSetNLC: NLEC<0")
			alglib.ap.assert(nlic >= 0, "MinNLCSetNLC: NLIC<0")
			state.ng = nlec
			state.nh = nlic
			state.fi = New Double(1 + state.ng + (state.nh - 1)) {}
			state.j = New Double(1 + state.ng + (state.nh - 1), state.n - 1) {}
		End Sub


		'************************************************************************
'        This function sets stopping conditions for inner iterations of  optimizer.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            EpsG    -   >=0
'                        The  subroutine  finishes  its  work   if   the  condition
'                        |v|<EpsG is satisfied, where:
'                        * |.| means Euclidian norm
'                        * v - scaled gradient vector, v[i]=g[i]*s[i]
'                        * g - gradient
'                        * s - scaling coefficients set by MinNLCSetScale()
'            EpsF    -   >=0
'                        The  subroutine  finishes  its work if on k+1-th iteration
'                        the  condition  |F(k+1)-F(k)|<=EpsF*max{|F(k)|,|F(k+1)|,1}
'                        is satisfied.
'            EpsX    -   >=0
'                        The subroutine finishes its work if  on  k+1-th  iteration
'                        the condition |v|<=EpsX is fulfilled, where:
'                        * |.| means Euclidian norm
'                        * v - scaled step vector, v[i]=dx[i]/s[i]
'                        * dx - step vector, dx=X(k+1)-X(k)
'                        * s - scaling coefficients set by MinNLCSetScale()
'            MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
'                        iterations is unlimited.
'
'        Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
'        to automatic stopping criterion selection.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetcond(state As minnlcstate, epsg As Double, epsf As Double, epsx As Double, maxits As Integer)
			alglib.ap.assert(Math.isfinite(epsg), "MinNLCSetCond: EpsG is not finite number")
			alglib.ap.assert(CDbl(epsg) >= CDbl(0), "MinNLCSetCond: negative EpsG")
			alglib.ap.assert(Math.isfinite(epsf), "MinNLCSetCond: EpsF is not finite number")
			alglib.ap.assert(CDbl(epsf) >= CDbl(0), "MinNLCSetCond: negative EpsF")
			alglib.ap.assert(Math.isfinite(epsx), "MinNLCSetCond: EpsX is not finite number")
			alglib.ap.assert(CDbl(epsx) >= CDbl(0), "MinNLCSetCond: negative EpsX")
			alglib.ap.assert(maxits >= 0, "MinNLCSetCond: negative MaxIts!")
			If ((CDbl(epsg) = CDbl(0) AndAlso CDbl(epsf) = CDbl(0)) AndAlso CDbl(epsx) = CDbl(0)) AndAlso maxits = 0 Then
				epsx = 1E-06
			End If
			state.epsg = epsg
			state.epsf = epsf
			state.epsx = epsx
			state.maxits = maxits
		End Sub


		'************************************************************************
'        This function sets scaling coefficients for NLC optimizer.
'
'        ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
'        size and gradient are scaled before comparison with tolerances).  Scale of
'        the I-th variable is a translation invariant measure of:
'        a) "how large" the variable is
'        b) how large the step should be to make significant changes in the function
'
'        Scaling is also used by finite difference variant of the optimizer  - step
'        along I-th axis is equal to DiffStep*S[I].
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            S       -   array[N], non-zero scaling coefficients
'                        S[i] may be negative, sign doesn't matter.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetscale(state As minnlcstate, s As Double())
			Dim i As Integer = 0

			alglib.ap.assert(alglib.ap.len(s) >= state.n, "MinNLCSetScale: Length(S)<N")
			For i = 0 To state.n - 1
				alglib.ap.assert(Math.isfinite(s(i)), "MinNLCSetScale: S contains infinite or NAN elements")
				alglib.ap.assert(CDbl(s(i)) <> CDbl(0), "MinNLCSetScale: S contains zero elements")
				state.s(i) = System.Math.Abs(s(i))
			Next
		End Sub


		'************************************************************************
'        This function sets preconditioner to "inexact LBFGS-based" mode.
'
'        Preconditioning is very important for convergence of  Augmented Lagrangian
'        algorithm because presence of penalty term makes problem  ill-conditioned.
'        Difference between  performance  of  preconditioned  and  unpreconditioned
'        methods can be as large as 100x!
'
'        MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
'        benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
'        It also provides special unpreconditioned mode of operation which  can  be
'        used for test purposes. Comments below discuss LBFGS-based preconditioner.
'
'        Inexact  LBFGS-based  preconditioner  uses L-BFGS  formula  combined  with
'        orthogonality assumption to perform very fast updates. For a N-dimensional
'        problem with K general linear or nonlinear constraints (boundary ones  are
'        not counted) it has O(N*K) cost per iteration.  This   preconditioner  has
'        best  quality  (less  iterations)  when   general   linear  and  nonlinear
'        constraints are orthogonal to each other (orthogonality  with  respect  to
'        boundary constraints is not required). Number of iterations increases when
'        constraints  are  non-orthogonal, because algorithm assumes orthogonality,
'        but still it is better than no preconditioner at all.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 26.09.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetprecinexact(state As minnlcstate)
			state.updatefreq = 0
			state.prectype = 1
		End Sub


		'************************************************************************
'        This function sets preconditioner to "exact low rank" mode.
'
'        Preconditioning is very important for convergence of  Augmented Lagrangian
'        algorithm because presence of penalty term makes problem  ill-conditioned.
'        Difference between  performance  of  preconditioned  and  unpreconditioned
'        methods can be as large as 100x!
'
'        MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
'        benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
'        It also provides special unpreconditioned mode of operation which  can  be
'        used for test purposes. Comments below discuss low rank preconditioner.
'
'        Exact low-rank preconditioner  uses  Woodbury  matrix  identity  to  build
'        quadratic model of the penalized function. It has no  special  assumptions
'        about orthogonality, so it is quite general. However, for a  N-dimensional
'        problem with K general linear or nonlinear constraints (boundary ones  are
'        not counted) it has O(N*K^2) cost per iteration (for  comparison:  inexact
'        LBFGS-based preconditioner has O(N*K) cost).
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'            UpdateFreq- update frequency. Preconditioner is  rebuilt  after  every
'                        UpdateFreq iterations. Recommended value: 10 or higher.
'                        Zero value means that good default value will be used.
'
'          -- ALGLIB --
'             Copyright 26.09.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetprecexactlowrank(state As minnlcstate, updatefreq As Integer)
			alglib.ap.assert(updatefreq >= 0, "MinNLCSetPrecExactLowRank: UpdateFreq<0")
			If updatefreq = 0 Then
				updatefreq = 10
			End If
			state.prectype = 2
			state.updatefreq = updatefreq
		End Sub


		'************************************************************************
'        This function sets preconditioner to "turned off" mode.
'
'        Preconditioning is very important for convergence of  Augmented Lagrangian
'        algorithm because presence of penalty term makes problem  ill-conditioned.
'        Difference between  performance  of  preconditioned  and  unpreconditioned
'        methods can be as large as 100x!
'
'        MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
'        benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
'        It also provides special unpreconditioned mode of operation which  can  be
'        used for test purposes.
'
'        This function activates this test mode. Do not use it in  production  code
'        to solve real-life problems.
'
'        INPUT PARAMETERS:
'            State   -   structure stores algorithm state
'
'          -- ALGLIB --
'             Copyright 26.09.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetprecnone(state As minnlcstate)
			state.updatefreq = 0
			state.prectype = 0
		End Sub


		'************************************************************************
'        This  function  tells MinNLC unit to use  Augmented  Lagrangian  algorithm
'        for nonlinearly constrained  optimization.  This  algorithm  is  a  slight
'        modification of one described in "A Modified Barrier-Augmented  Lagrangian
'        Method for  Constrained  Minimization  (1999)"  by  D.GOLDFARB,  R.POLYAK,
'        K. SCHEINBERG, I.YUZEFOVICH.
'
'        Augmented Lagrangian algorithm works by converting problem  of  minimizing
'        F(x) subject to equality/inequality constraints   to unconstrained problem
'        of the form
'
'            min[ f(x) + 
'                + Rho*PENALTY_EQ(x)   + SHIFT_EQ(x,Nu1) + 
'                + Rho*PENALTY_INEQ(x) + SHIFT_INEQ(x,Nu2) ]
'            
'        where:
'        * Rho is a fixed penalization coefficient
'        * PENALTY_EQ(x) is a penalty term, which is used to APPROXIMATELY  enforce
'          equality constraints
'        * SHIFT_EQ(x) is a special "shift"  term  which  is  used  to  "fine-tune"
'          equality constraints, greatly increasing precision
'        * PENALTY_INEQ(x) is a penalty term which is used to approximately enforce
'          inequality constraints
'        * SHIFT_INEQ(x) is a special "shift"  term  which  is  used to "fine-tune"
'          inequality constraints, greatly increasing precision
'        * Nu1/Nu2 are vectors of Lagrange coefficients which are fine-tuned during
'          outer iterations of algorithm
'
'        This  version  of  AUL  algorithm  uses   preconditioner,  which   greatly
'        accelerates convergence. Because this  algorithm  is  similar  to  penalty
'        methods,  it  may  perform  steps  into  infeasible  area.  All  kinds  of
'        constraints (boundary, linear and nonlinear ones) may   be   violated   in
'        intermediate points - and in the solution.  However,  properly  configured
'        AUL method is significantly better at handling  constraints  than  barrier
'        and/or penalty methods.
'
'        The very basic outline of algorithm is given below:
'        1) first outer iteration is performed with "default"  values  of  Lagrange
'           multipliers Nu1/Nu2. Solution quality is low (candidate  point  can  be
'           too  far  away  from  true  solution; large violation of constraints is
'           possible) and is comparable with that of penalty methods.
'        2) subsequent outer iterations  refine  Lagrange  multipliers  and improve
'           quality of the solution.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            Rho     -   penalty coefficient, Rho>0:
'                        * large enough  that  algorithm  converges  with   desired
'                          precision. Minimum value is 10*max(S'*diag(H)*S),  where
'                          S is a scale matrix (set by MinNLCSetScale) and H  is  a
'                          Hessian of the function being minimized. If you can  not
'                          easily estimate Hessian norm,  see  our  recommendations
'                          below.
'                        * not TOO large to prevent ill-conditioning
'                        * for unit-scale problems (variables and Hessian have unit
'                          magnitude), Rho=100 or Rho=1000 can be used.
'                        * it is important to note that Rho is internally multiplied
'                          by scaling matrix, i.e. optimum value of Rho depends  on
'                          scale of variables specified  by  MinNLCSetScale().
'            ItsCnt  -   number of outer iterations:
'                        * ItsCnt=0 means that small number of outer iterations  is
'                          automatically chosen (10 iterations in current version).
'                        * ItsCnt=1 means that AUL algorithm performs just as usual
'                          barrier method.
'                        * ItsCnt>1 means that  AUL  algorithm  performs  specified
'                          number of outer iterations
'                        
'        HOW TO CHOOSE PARAMETERS
'
'        Nonlinear optimization is a tricky area and Augmented Lagrangian algorithm
'        is sometimes hard to tune. Good values of  Rho  and  ItsCnt  are  problem-
'        specific.  In  order  to  help  you   we   prepared   following   set   of
'        recommendations:
'
'        * for  unit-scale  problems  (variables  and Hessian have unit magnitude),
'          Rho=100 or Rho=1000 can be used.
'
'        * start from  some  small  value of Rho and solve problem  with  just  one
'          outer iteration (ItcCnt=1). In this case algorithm behaves like  penalty
'          method. Increase Rho in 2x or 10x steps until you  see  that  one  outer
'          iteration returns point which is "rough approximation to solution".
'          
'          It is very important to have Rho so  large  that  penalty  term  becomes
'          constraining i.e. modified function becomes highly convex in constrained
'          directions.
'          
'          From the other side, too large Rho may prevent you  from  converging  to
'          the solution. You can diagnose it by studying number of inner iterations
'          performed by algorithm: too few (5-10 on  1000-dimensional  problem)  or
'          too many (orders of magnitude more than  dimensionality)  usually  means
'          that Rho is too large.
'
'        * with just one outer iteration you  usually  have  low-quality  solution.
'          Some constraints can be violated with very  large  margin,  while  other
'          ones (which are NOT violated in the true solution) can push final  point
'          too far in the inner area of the feasible set.
'          
'          For example, if you have constraint x0>=0 and true solution  x0=1,  then
'          merely a presence of "x0>=0" will introduce a bias towards larger values
'          of x0. Say, algorithm may stop at x0=1.5 instead of 1.0.
'          
'        * after you found good Rho, you may increase number of  outer  iterations.
'          ItsCnt=10 is a good value. Subsequent outer iteration will refine values
'          of  Lagrange  multipliers.  Constraints  which  were  violated  will  be
'          enforced, inactive constraints will be dropped (corresponding multipliers
'          will be decreased). Ideally, you  should  see  10-1000x  improvement  in
'          constraint handling (constraint violation is reduced).
'          
'        * if  you  see  that  algorithm  converges  to  vicinity  of solution, but
'          additional outer iterations do not refine solution,  it  may  mean  that
'          algorithm is unstable - it wanders around true  solution,  but  can  not
'          approach it. Sometimes algorithm may be stabilized by increasing Rho one
'          more time, making it 5x or 10x larger.
'
'        SCALING OF CONSTRAINTS [IMPORTANT]
'
'        AUL optimizer scales   variables   according   to   scale   specified   by
'        MinNLCSetScale() function, so it can handle  problems  with  badly  scaled
'        variables (as long as we KNOW their scales).   However,  because  function
'        being optimized is a mix  of  original  function and  constraint-dependent
'        penalty  functions, it  is   important  to   rescale  both  variables  AND
'        constraints.
'
'        Say,  if  you  minimize f(x)=x^2 subject to 1000000*x>=0,  then  you  have
'        constraint whose scale is different from that of target  function (another
'        example is 0.000001*x>=0). It is also possible to have constraints   whose
'        scales  are   misaligned:   1000000*x0>=0, 0.000001*x1<=0.   Inappropriate
'        scaling may ruin convergence because minimizing x^2 subject to x>=0 is NOT
'        same as minimizing it subject to 1000000*x>=0.
'
'        Because we  know  coefficients  of  boundary/linear  constraints,  we  can
'        automatically rescale and normalize them. However,  there  is  no  way  to
'        automatically rescale nonlinear constraints Gi(x) and  Hi(x)  -  they  are
'        black boxes.
'
'        It means that YOU are the one who is  responsible  for  correct scaling of
'        nonlinear constraints  Gi(x)  and  Hi(x).  We  recommend  you  to  rescale
'        nonlinear constraints in such way that I-th component of dG/dX (or  dH/dx)
'        has magnitude approximately equal to 1/S[i] (where S  is  a  scale  set by
'        MinNLCSetScale() function).
'
'        WHAT IF IT DOES NOT CONVERGE?
'
'        It is possible that AUL algorithm fails to converge to precise  values  of
'        Lagrange multipliers. It stops somewhere around true solution, but candidate
'        point is still too far from solution, and some constraints  are  violated.
'        Such kind of failure is specific for Lagrangian algorithms -  technically,
'        they stop at some point, but this point is not constrained solution.
'
'        There are exist several reasons why algorithm may fail to converge:
'        a) too loose stopping criteria for inner iteration
'        b) degenerate, redundant constraints
'        c) target function has unconstrained extremum exactly at the  boundary  of
'           some constraint
'        d) numerical noise in the target function
'
'        In all these cases algorithm is unstable - each outer iteration results in
'        large and almost random step which improves handling of some  constraints,
'        but violates other ones (ideally  outer iterations should form a  sequence
'        of progressively decreasing steps towards solution).
'           
'        First reason possible is  that  too  loose  stopping  criteria  for  inner
'        iteration were specified. Augmented Lagrangian algorithm solves a sequence
'        of intermediate problems, and requries each of them to be solved with high
'        precision. Insufficient precision results in incorrect update of  Lagrange
'        multipliers.
'
'        Another reason is that you may have specified degenerate constraints: say,
'        some constraint was repeated twice. In most cases AUL algorithm gracefully
'        handles such situations, but sometimes it may spend too much time figuring
'        out subtle degeneracies in constraint matrix.
'
'        Third reason is tricky and hard to diagnose. Consider situation  when  you
'        minimize  f=x^2  subject to constraint x>=0.  Unconstrained   extremum  is
'        located  exactly  at  the  boundary  of  constrained  area.  In  this case
'        algorithm will tend to oscillate between negative  and  positive  x.  Each
'        time it stops at x<0 it "reinforces" constraint x>=0, and each time it  is
'        bounced to x>0 it "relaxes" constraint (and is  attracted  to  x<0).
'
'        Such situation  sometimes  happens  in  problems  with  hidden  symetries.
'        Algorithm  is  got  caught  in  a  loop with  Lagrange  multipliers  being
'        continuously increased/decreased. Luckily, such loop forms after at  least
'        three iterations, so this problem can be solved by  DECREASING  number  of
'        outer iterations down to 1-2 and increasing  penalty  coefficient  Rho  as
'        much as possible.
'
'        Final reason is numerical noise. AUL algorithm is robust against  moderate
'        noise (more robust than, say, active set methods),  but  large  noise  may
'        destabilize algorithm.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetalgoaul(state As minnlcstate, rho As Double, itscnt As Integer)
			alglib.ap.assert(itscnt >= 0, "MinNLCSetAlgoAUL: negative ItsCnt")
			alglib.ap.assert(Math.isfinite(rho), "MinNLCSetAlgoAUL: Rho is not finite")
			alglib.ap.assert(CDbl(rho) > CDbl(0), "MinNLCSetAlgoAUL: Rho<=0")
			If itscnt = 0 Then
				itscnt = 10
			End If
			state.aulitscnt = itscnt
			state.rho = rho
			state.solvertype = 0
		End Sub


		'************************************************************************
'        This function turns on/off reporting.
'
'        INPUT PARAMETERS:
'            State   -   structure which stores algorithm state
'            NeedXRep-   whether iteration reports are needed or not
'
'        If NeedXRep is True, algorithm will call rep() callback function if  it is
'        provided to MinNLCOptimize().
'
'        NOTE: algorithm passes two parameters to rep() callback  -  current  point
'              and penalized function value at current point. Important -  function
'              value which is returned is NOT function being minimized. It  is  sum
'              of the value of the function being minimized - and penalty term.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetxrep(state As minnlcstate, needxrep As Boolean)
			state.xrep = needxrep
		End Sub


		'************************************************************************
'
'        NOTES:
'
'        1. This function has two different implementations: one which  uses  exact
'           (analytical) user-supplied Jacobian, and one which uses  only  function
'           vector and numerically  differentiates  function  in  order  to  obtain
'           gradient.
'
'           Depending  on  the  specific  function  used to create optimizer object
'           you should choose appropriate variant of MinNLCOptimize() -  one  which
'           accepts function AND Jacobian or one which accepts ONLY function.
'
'           Be careful to choose variant of MinNLCOptimize()  which  corresponds to
'           your optimization scheme! Table below lists different  combinations  of
'           callback (function/gradient) passed to MinNLCOptimize()   and  specific
'           function used to create optimizer.
'
'
'                             |         USER PASSED TO MinNLCOptimize()
'           CREATED WITH      |  function only   |  function and gradient
'           ------------------------------------------------------------
'           MinNLCCreateF()   |     works               FAILS
'           MinNLCCreate()    |     FAILS               works
'
'           Here "FAILS" denotes inappropriate combinations  of  optimizer creation
'           function  and  MinNLCOptimize()  version.   Attemps   to    use    such
'           combination will lead to exception. Either  you  did  not pass gradient
'           when it WAS needed or you passed gradient when it was NOT needed.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Function minnlciteration(state As minnlcstate) As Boolean
			Dim result As New Boolean()
			Dim i As Integer = 0
			Dim k As Integer = 0
			Dim n As Integer = 0
			Dim ng As Integer = 0
			Dim nh As Integer = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstate.stage >= 0 Then
				i = state.rstate.ia(0)
				k = state.rstate.ia(1)
				n = state.rstate.ia(2)
				ng = state.rstate.ia(3)
				nh = state.rstate.ia(4)
			Else
				i = -983
				k = -989
				n = -834
				ng = 900
				nh = -287
			End If
			If state.rstate.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstate.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstate.stage = 2 Then
				GoTo lbl_2
			End If
			If state.rstate.stage = 3 Then
				GoTo lbl_3
			End If
			If state.rstate.stage = 4 Then
				GoTo lbl_4
			End If
			If state.rstate.stage = 5 Then
				GoTo lbl_5
			End If
			If state.rstate.stage = 6 Then
				GoTo lbl_6
			End If
			If state.rstate.stage = 7 Then
				GoTo lbl_7
			End If
			If state.rstate.stage = 8 Then
				GoTo lbl_8
			End If

			'
			' Routine body
			'

			'
			' Init
			'
			state.repterminationtype = 0
			state.repinneriterationscount = 0
			state.repouteriterationscount = 0
			state.repnfev = 0
			state.repvaridx = 0
			state.repfuncidx = 0
			state.repdbgphase0its = 0
			n = state.n
			ng = state.ng
			nh = state.nh
			clearrequestfields(state)

			'
			' Test gradient
			'
			If Not (CDbl(state.diffstep) = CDbl(0) AndAlso CDbl(state.teststep) > CDbl(0)) Then
				GoTo lbl_9
			End If
			apserv.rvectorsetlengthatleast(state.xbase, n)
			apserv.rvectorsetlengthatleast(state.fbase, 1 + ng + nh)
			apserv.rvectorsetlengthatleast(state.dfbase, 1 + ng + nh)
			apserv.rvectorsetlengthatleast(state.fm1, 1 + ng + nh)
			apserv.rvectorsetlengthatleast(state.fp1, 1 + ng + nh)
			apserv.rvectorsetlengthatleast(state.dfm1, 1 + ng + nh)
			apserv.rvectorsetlengthatleast(state.dfp1, 1 + ng + nh)
			state.needfij = True
			For i_ = 0 To n - 1
				state.xbase(i_) = state.xstart(i_)
			Next
			k = 0
			lbl_11:
			If k > n - 1 Then
				GoTo lbl_13
			End If
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.rstate.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			For i_ = 0 To ng + nh
				state.fbase(i_) = state.fi(i_)
			Next
			For i_ = 0 To ng + nh
				state.dfbase(i_) = state.j(i_, k)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) - state.s(k) * state.teststep
			state.rstate.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			For i_ = 0 To ng + nh
				state.fm1(i_) = state.fi(i_)
			Next
			For i_ = 0 To ng + nh
				state.dfm1(i_) = state.j(i_, k)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) + state.s(k) * state.teststep
			state.rstate.stage = 2
			GoTo lbl_rcomm
			lbl_2:
			For i_ = 0 To ng + nh
				state.fp1(i_) = state.fi(i_)
			Next
			For i_ = 0 To ng + nh
				state.dfp1(i_) = state.j(i_, k)
			Next
			For i = 0 To ng + nh
				If Not optserv.derivativecheck(state.fm1(i), state.dfm1(i), state.fp1(i), state.dfp1(i), state.fbase(i), state.dfbase(i), _
					2 * state.s(k) * state.teststep) Then
					state.repfuncidx = i
					state.repvaridx = k
					state.repterminationtype = -7
					result = False
					Return result
				End If
			Next
			k = k + 1
			GoTo lbl_11
			lbl_13:
			state.needfij = False
			lbl_9:

			'
			' AUL solver
			'
			If state.solvertype <> 0 Then
				GoTo lbl_14
			End If
			If CDbl(state.diffstep) <> CDbl(0) Then
				apserv.rvectorsetlengthatleast(state.xbase, n)
				apserv.rvectorsetlengthatleast(state.fbase, 1 + ng + nh)
				apserv.rvectorsetlengthatleast(state.fm2, 1 + ng + nh)
				apserv.rvectorsetlengthatleast(state.fm1, 1 + ng + nh)
				apserv.rvectorsetlengthatleast(state.fp1, 1 + ng + nh)
				apserv.rvectorsetlengthatleast(state.fp2, 1 + ng + nh)
			End If
			state.rstateaul.ia = New Integer(8) {}
			state.rstateaul.ra = New Double(7) {}
			state.rstateaul.stage = -1
			lbl_16:
			If Not auliteration(state) Then
				GoTo lbl_17
			End If

			'
			' Numerical differentiation (if needed) - intercept NeedFiJ
			' request and replace it by sequence of NeedFi requests
			'
			If Not (CDbl(state.diffstep) <> CDbl(0) AndAlso state.needfij) Then
				GoTo lbl_18
			End If
			state.needfij = False
			state.needfi = True
			For i_ = 0 To n - 1
				state.xbase(i_) = state.x(i_)
			Next
			k = 0
			lbl_20:
			If k > n - 1 Then
				GoTo lbl_22
			End If
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) - state.s(k) * state.diffstep
			state.rstate.stage = 3
			GoTo lbl_rcomm
			lbl_3:
			For i_ = 0 To ng + nh
				state.fm2(i_) = state.fi(i_)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) - 0.5 * state.s(k) * state.diffstep
			state.rstate.stage = 4
			GoTo lbl_rcomm
			lbl_4:
			For i_ = 0 To ng + nh
				state.fm1(i_) = state.fi(i_)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) + 0.5 * state.s(k) * state.diffstep
			state.rstate.stage = 5
			GoTo lbl_rcomm
			lbl_5:
			For i_ = 0 To ng + nh
				state.fp1(i_) = state.fi(i_)
			Next
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.x(k) = state.x(k) + state.s(k) * state.diffstep
			state.rstate.stage = 6
			GoTo lbl_rcomm
			lbl_6:
			For i_ = 0 To ng + nh
				state.fp2(i_) = state.fi(i_)
			Next
			For i = 0 To ng + nh
				state.j(i, k) = (8 * (state.fp1(i) - state.fm1(i)) - (state.fp2(i) - state.fm2(i))) / (6 * state.diffstep * state.s(i))
			Next
			k = k + 1
			GoTo lbl_20
			lbl_22:
			For i_ = 0 To n - 1
				state.x(i_) = state.xbase(i_)
			Next
			state.rstate.stage = 7
			GoTo lbl_rcomm
			lbl_7:

			'
			' Restore previous values of fields and continue
			'
			state.needfi = False
			state.needfij = True
			GoTo lbl_16
			lbl_18:

			'
			' Forward request to caller
			'
			state.rstate.stage = 8
			GoTo lbl_rcomm
			lbl_8:
			GoTo lbl_16
			lbl_17:
			result = False
			Return result
			lbl_14:
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstate.ia(0) = i
			state.rstate.ia(1) = k
			state.rstate.ia(2) = n
			state.rstate.ia(3) = ng
			state.rstate.ia(4) = nh
			Return result
		End Function


		'************************************************************************
'        MinNLC results
'
'        INPUT PARAMETERS:
'            State   -   algorithm state
'
'        OUTPUT PARAMETERS:
'            X       -   array[0..N-1], solution
'            Rep     -   optimization report. You should check Rep.TerminationType
'                        in  order  to  distinguish  successful  termination  from
'                        unsuccessful one:
'                        * -8    internal integrity control  detected  infinite or
'                                NAN   values   in   function/gradient.   Abnormal
'                                termination signalled.
'                        * -7   gradient verification failed.
'                               See MinNLCSetGradientCheck() for more information.
'                        *  1   relative function improvement is no more than EpsF.
'                        *  2   scaled step is no more than EpsX.
'                        *  4   scaled gradient norm is no more than EpsG.
'                        *  5   MaxIts steps was taken
'                        More information about fields of this  structure  can  be
'                        found in the comments on MinNLCReport datatype.
'           
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcresults(state As minnlcstate, ByRef x As Double(), rep As minnlcreport)
			x = New Double(-1) {}

			minnlcresultsbuf(state, x, rep)
		End Sub


		'************************************************************************
'        NLC results
'
'        Buffered implementation of MinNLCResults() which uses pre-allocated buffer
'        to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
'        intended to be used in the inner cycles of performance critical algorithms
'        where array reallocation penalty is too large to be ignored.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcresultsbuf(state As minnlcstate, ByRef x As Double(), rep As minnlcreport)
			Dim i As Integer = 0
			Dim i_ As Integer = 0

			If alglib.ap.len(x) < state.n Then
				x = New Double(state.n - 1) {}
			End If
			rep.iterationscount = state.repinneriterationscount
			rep.nfev = state.repnfev
			rep.varidx = state.repvaridx
			rep.funcidx = state.repfuncidx
			rep.terminationtype = state.repterminationtype
			rep.dbgphase0its = state.repdbgphase0its
			If state.repterminationtype > 0 Then
				For i_ = 0 To state.n - 1
					x(i_) = state.xc(i_)
				Next
			Else
				For i = 0 To state.n - 1
					x(i) = [Double].NaN
				Next
			End If
		End Sub


		'************************************************************************
'        This subroutine restarts algorithm from new point.
'        All optimization parameters (including constraints) are left unchanged.
'
'        This  function  allows  to  solve multiple  optimization  problems  (which
'        must have  same number of dimensions) without object reallocation penalty.
'
'        INPUT PARAMETERS:
'            State   -   structure previously allocated with MinNLCCreate call.
'            X       -   new starting point.
'
'          -- ALGLIB --
'             Copyright 28.11.2010 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcrestartfrom(state As minnlcstate, x As Double())
			Dim n As Integer = 0
			Dim i_ As Integer = 0

			n = state.n

			'
			' First, check for errors in the inputs
			'
			alglib.ap.assert(alglib.ap.len(x) >= n, "MinNLCRestartFrom: Length(X)<N")
			alglib.ap.assert(apserv.isfinitevector(x, n), "MinNLCRestartFrom: X contains infinite or NaN values!")

			'
			' Set XC
			'
			For i_ = 0 To n - 1
				state.xstart(i_) = x(i_)
			Next

			'
			' prepare RComm facilities
			'
			state.rstate.ia = New Integer(4) {}
			state.rstate.stage = -1
			clearrequestfields(state)
		End Sub


		'************************************************************************
'        This  subroutine  turns  on  verification  of  the  user-supplied analytic
'        gradient:
'        * user calls this subroutine before optimization begins
'        * MinNLCOptimize() is called
'        * prior to  actual  optimization, for each component  of  parameters being
'          optimized X[i] algorithm performs following steps:
'          * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
'            where X[i] is i-th component of the initial point and S[i] is a  scale
'            of i-th parameter
'          * F(X) is evaluated at these trial points
'          * we perform one more evaluation in the middle point of the interval
'          * we  build  cubic  model using function values and derivatives at trial
'            points and we compare its prediction with actual value in  the  middle
'            point
'          * in case difference between prediction and actual value is higher  than
'            some predetermined threshold, algorithm stops with completion code -7;
'            Rep.VarIdx is set to index of the parameter with incorrect derivative,
'            and Rep.FuncIdx is set to index of the function.
'        * after verification is over, algorithm proceeds to the actual optimization.
'
'        NOTE 1: verification  needs  N (parameters count) gradient evaluations. It
'                is very costly and you should use  it  only  for  low  dimensional
'                problems,  when  you  want  to  be  sure  that  you've   correctly
'                calculated  analytic  derivatives.  You  should  not use it in the
'                production code (unless you want to check derivatives provided  by
'                some third party).
'
'        NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
'                (so large that function behaviour is significantly non-cubic) will
'                lead to false alarms. You may use  different  step  for  different
'                parameters by means of setting scale with MinNLCSetScale().
'
'        NOTE 3: this function may lead to false positives. In case it reports that
'                I-th  derivative was calculated incorrectly, you may decrease test
'                step  and  try  one  more  time  - maybe your function changes too
'                sharply  and  your  step  is  too  large for such rapidly chanding
'                function.
'
'        INPUT PARAMETERS:
'            State       -   structure used to store algorithm state
'            TestStep    -   verification step:
'                            * TestStep=0 turns verification off
'                            * TestStep>0 activates verification
'
'          -- ALGLIB --
'             Copyright 15.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcsetgradientcheck(state As minnlcstate, teststep As Double)
			alglib.ap.assert(Math.isfinite(teststep), "MinNLCSetGradientCheck: TestStep contains NaN or Infinite")
			alglib.ap.assert(CDbl(teststep) >= CDbl(0), "MinNLCSetGradientCheck: invalid argument TestStep(TestStep<0)")
			state.teststep = teststep
		End Sub


		'************************************************************************
'        Penalty function for equality constraints.
'        INPUT PARAMETERS:
'            Alpha   -   function argument. Penalty function becomes large when
'                        Alpha approaches -1 or +1. It is defined for Alpha<=-1 or
'                        Alpha>=+1 - in this case infinite value is returned.
'                        
'        OUTPUT PARAMETERS:
'            F       -   depending on Alpha:
'                        * for Alpha in (-1+eps,+1-eps), F=F(Alpha)
'                        * for Alpha outside of interval, F is some very large number
'            DF      -   depending on Alpha:
'                        * for Alpha in (-1+eps,+1-eps), DF=dF(Alpha)/dAlpha, exact
'                          numerical derivative.
'                        * otherwise, it is zero
'            D2F     -   second derivative
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcequalitypenaltyfunction(alpha As Double, ByRef f As Double, ByRef df As Double, ByRef d2f As Double)
			f = 0
			df = 0
			d2f = 0

			f = 0.5 * alpha * alpha
			df = alpha
			d2f = 1.0
		End Sub


		'************************************************************************
'        "Penalty" function  for  inequality  constraints,  which  is multiplied by
'        penalty coefficient Rho.
'
'        "Penalty" function plays only supplementary role - it helps  to  stabilize
'        algorithm when solving non-convex problems. Because it  is  multiplied  by
'        fixed and large  Rho  -  not  Lagrange  multiplier  Nu  which  may  become
'        arbitrarily small! - it enforces  convexity  of  the  problem  behind  the
'        boundary of the feasible area.
'
'        This function is zero at the feasible area and in the close  neighborhood,
'        it becomes non-zero only at some distance (scaling is essential!) and grows
'        quadratically.
'
'        Penalty function must enter augmented Lagrangian as
'            Rho*PENALTY(x-lowerbound)
'        with corresponding changes being made for upper bound or  other  kinds  of
'        constraints.
'
'        INPUT PARAMETERS:
'            Alpha   -   function argument. Typically, if we have active constraint
'                        with precise Lagrange multiplier, we have Alpha  around 1.
'                        Large positive Alpha's correspond to  inner  area  of  the
'                        feasible set. Alpha<1 corresponds to  outer  area  of  the
'                        feasible set.
'            StabilizingPoint- point where F becomes  non-zero.  Must  be  negative
'                        value, at least -1, large values (hundreds) are possible.
'                        
'        OUTPUT PARAMETERS:
'            F       -   F(Alpha)
'            DF      -   DF=dF(Alpha)/dAlpha, exact derivative
'            D2F     -   second derivative
'            
'        NOTE: it is improtant to  have  significantly  non-zero  StabilizingPoint,
'              because when it  is  large,  shift  term  does  not  interfere  with
'              Lagrange  multipliers  converging  to  their  final  values.   Thus,
'              convergence of such modified AUL algorithm is  still  guaranteed  by
'              same set of theorems.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcinequalitypenaltyfunction(alpha As Double, stabilizingpoint As Double, ByRef f As Double, ByRef df As Double, ByRef d2f As Double)
			f = 0
			df = 0
			d2f = 0

			If CDbl(alpha) >= CDbl(stabilizingpoint) Then
				f = 0.0
				df = 0.0
				d2f = 0.0
			Else
				alpha = alpha - stabilizingpoint
				f = 0.5 * alpha * alpha
				df = alpha
				d2f = 1.0
			End If
		End Sub


		'************************************************************************
'        "Shift" function  for  inequality  constraints,  which  is  multiplied  by
'        corresponding Lagrange multiplier.
'
'        "Shift" function is a main factor which enforces  inequality  constraints.
'        Inequality penalty function plays only supplementary role  -  it  prevents
'        accidental step deep into infeasible area  when  working  with  non-convex
'        problems (read comments on corresponding function for more information).
'
'        Shift function must enter augmented Lagrangian as
'            Nu/Rho*SHIFT((x-lowerbound)*Rho+1)
'        with corresponding changes being made for upper bound or  other  kinds  of
'        constraints.
'
'        INPUT PARAMETERS:
'            Alpha   -   function argument. Typically, if we have active constraint
'                        with precise Lagrange multiplier, we have Alpha  around 1.
'                        Large positive Alpha's correspond to  inner  area  of  the
'                        feasible set. Alpha<1 corresponds to  outer  area  of  the
'                        feasible set.
'                        
'        OUTPUT PARAMETERS:
'            F       -   F(Alpha)
'            DF      -   DF=dF(Alpha)/dAlpha, exact derivative
'            D2F     -   second derivative
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Public Shared Sub minnlcinequalityshiftfunction(alpha As Double, ByRef f As Double, ByRef df As Double, ByRef d2f As Double)
			f = 0
			df = 0
			d2f = 0

			If CDbl(alpha) >= CDbl(0.5) Then
				f = -System.Math.Log(alpha)
				df = -(1 / alpha)
				d2f = 1 / (alpha * alpha)
			Else
				f = 2 * alpha * alpha - 4 * alpha + (System.Math.Log(2) + 1.5)
				df = 4 * alpha - 4
				d2f = 4
			End If
		End Sub


		'************************************************************************
'        Clears request fileds (to be sure that we don't forget to clear something)
'        ************************************************************************

		Private Shared Sub clearrequestfields(state As minnlcstate)
			state.needfi = False
			state.needfij = False
			state.xupdated = False
		End Sub


		'************************************************************************
'        Internal initialization subroutine.
'        Sets default NLC solver with default criteria.
'        ************************************************************************

		Private Shared Sub minnlcinitinternal(n As Integer, x As Double(), diffstep As Double, state As minnlcstate)
			Dim i As Integer = 0
			Dim c As Double(,) = New Double(-1, -1) {}
			Dim ct As Integer() = New Integer(-1) {}


			'
			' Default params
			'
			state.stabilizingpoint = -100.0
			state.initialinequalitymultiplier = 1.0

			'
			' Initialize other params
			'
			state.teststep = 0
			state.n = n
			state.diffstep = diffstep
			state.bndl = New Double(n - 1) {}
			state.hasbndl = New Boolean(n - 1) {}
			state.bndu = New Double(n - 1) {}
			state.hasbndu = New Boolean(n - 1) {}
			state.s = New Double(n - 1) {}
			state.xstart = New Double(n - 1) {}
			state.xc = New Double(n - 1) {}
			state.x = New Double(n - 1) {}
			For i = 0 To n - 1
				state.bndl(i) = [Double].NegativeInfinity
				state.hasbndl(i) = False
				state.bndu(i) = [Double].PositiveInfinity
				state.hasbndu(i) = False
				state.s(i) = 1.0
				state.xstart(i) = x(i)
				state.xc(i) = x(i)
			Next
			minnlcsetlc(state, c, ct, 0)
			minnlcsetnlc(state, 0, 0)
			minnlcsetcond(state, 0.0, 0.0, 0.0, 0)
			minnlcsetxrep(state, False)
			minnlcsetalgoaul(state, 0.001, 0)
			minnlcsetprecinexact(state)
			minlbfgs.minlbfgscreate(n, System.Math.Min(lbfgsfactor, n), x, state.auloptimizer)
			minnlcrestartfrom(state, x)
		End Sub


		'************************************************************************
'        This function clears preconditioner for L-BFGS optimizer (sets it do default
'        state);
'
'        Parameters:
'            AULOptimizer    -   optimizer to tune
'            
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub clearpreconditioner(auloptimizer As minlbfgs.minlbfgsstate)
			minlbfgs.minlbfgssetprecdefault(auloptimizer)
		End Sub


		'************************************************************************
'        This function updates preconditioner for L-BFGS optimizer.
'
'        Parameters:
'            PrecType        -   preconditioner type:
'                                * 0 for unpreconditioned iterations
'                                * 1 for inexact LBFGS
'                                * 2 for exact preconditioner update after each UpdateFreq its
'            UpdateFreq      -   update frequency
'            PrecCounter     -   iterations counter, must be zero on the first call,
'                                automatically increased  by  this  function.  This
'                                counter is used to implement "update-once-in-X-iterations"
'                                scheme.
'            AULOptimizer    -   optimizer to tune
'            X               -   current point
'            Rho             -   penalty term
'            GammaK          -   current  estimate  of  Hessian  norm   (used   for
'                                initialization of preconditioner). Can be zero, in
'                                which case Hessian is assumed to be unit.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub updatepreconditioner(prectype As Integer, updatefreq As Integer, ByRef preccounter As Integer, auloptimizer As minlbfgs.minlbfgsstate, x As Double(), rho As Double, _
			gammak As Double, bndl As Double(), hasbndl As Boolean(), bndu As Double(), hasbndu As Boolean(), nubc As Double(), _
			cleic As Double(,), nulc As Double(), fi As Double(), jac As Double(,), nunlc As Double(), ByRef bufd As Double(), _
			ByRef bufc As Double(), ByRef bufw As Double(,), n As Integer, nec As Integer, nic As Integer, ng As Integer, _
			nh As Integer)
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim p As Double = 0
			Dim dp As Double = 0
			Dim d2p As Double = 0
			Dim i_ As Integer = 0

			alglib.ap.assert(CDbl(rho) > CDbl(0), "MinNLC: integrity check failed")
			apserv.rvectorsetlengthatleast(bufd, n)
			apserv.rvectorsetlengthatleast(bufc, nec + nic + ng + nh)
			apserv.rmatrixsetlengthatleast(bufw, nec + nic + ng + nh, n)

			'
			' Preconditioner before update from barrier/penalty functions
			'
			If CDbl(gammak) = CDbl(0) Then
				gammak = 1
			End If
			For i = 0 To n - 1
				bufd(i) = gammak
			Next

			'
			' Update diagonal Hessian using nonlinearity from boundary constraints:
			' * penalty term from equality constraints
			' * shift term from inequality constraints
			'
			' NOTE: penalty term for inequality constraints is ignored because it
			'       is large only in exceptional cases.
			'
			For i = 0 To n - 1
				If (hasbndl(i) AndAlso hasbndu(i)) AndAlso CDbl(bndl(i)) = CDbl(bndu(i)) Then
					minnlcequalitypenaltyfunction((x(i) - bndl(i)) * rho, p, dp, d2p)
					bufd(i) = bufd(i) + d2p * rho
					Continue For
				End If
				If hasbndl(i) Then
					minnlcinequalityshiftfunction((x(i) - bndl(i)) * rho + 1, p, dp, d2p)
					bufd(i) = bufd(i) + nubc(2 * i + 0) * d2p * rho
				End If
				If hasbndu(i) Then
					minnlcinequalityshiftfunction((bndu(i) - x(i)) * rho + 1, p, dp, d2p)
					bufd(i) = bufd(i) + nubc(2 * i + 1) * d2p * rho
				End If
			Next

			'
			' Process linear constraints
			'
			For i = 0 To nec + nic - 1
				For i_ = 0 To n - 1
					bufw(i, i_) = cleic(i, i_)
				Next
				v = 0.0
				For i_ = 0 To n - 1
					v += cleic(i, i_) * x(i_)
				Next
				v = v - cleic(i, n)
				If i < nec Then

					'
					' Equality constraint
					'
					minnlcequalitypenaltyfunction(v * rho, p, dp, d2p)
					bufc(i) = d2p * rho
				Else

					'
					' Inequality constraint
					'
					minnlcinequalityshiftfunction(-(v * rho) + 1, p, dp, d2p)
					bufc(i) = nulc(i) * d2p * rho
				End If
			Next

			'
			' Process nonlinear constraints
			'
			For i = 0 To ng + nh - 1
				For i_ = 0 To n - 1
					bufw(nec + nic + i, i_) = jac(1 + i, i_)
				Next
				v = fi(1 + i)
				If i < ng Then

					'
					' Equality constraint
					'
					minnlcequalitypenaltyfunction(v * rho, p, dp, d2p)
					bufc(nec + nic + i) = d2p * rho
				Else

					'
					' Inequality constraint
					'
					minnlcinequalityshiftfunction(-(v * rho) + 1, p, dp, d2p)
					bufc(nec + nic + i) = nunlc(i) * d2p * rho
				End If
			Next
			If prectype = 1 Then
				minlbfgs.minlbfgssetprecrankklbfgsfast(auloptimizer, bufd, bufc, bufw, nec + nic + ng + nh)
			End If
			If prectype = 2 AndAlso preccounter Mod updatefreq = 0 Then
				minlbfgs.minlbfgssetpreclowrankexact(auloptimizer, bufd, bufc, bufw, nec + nic + ng + nh)
			End If
			apserv.inc(preccounter)
		End Sub


		'************************************************************************
'        This subroutine adds penalty from boundary constraints to target  function
'        and its gradient. Penalty function is one which is used for main AUL cycle
'        - with Lagrange multipliers and infinite at the barrier and beyond.
'
'        Parameters:
'            X[] - current point
'            BndL[], BndU[] - boundary constraints
'            HasBndL[], HasBndU[] - I-th element is True if corresponding constraint is present
'            NuBC[] - Lagrange multipliers corresponding to constraints
'            Rho - penalty term
'            StabilizingPoint - branch point for inequality stabilizing term
'            F - function value to modify
'            G - gradient to modify
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub penaltybc(x As Double(), bndl As Double(), hasbndl As Boolean(), bndu As Double(), hasbndu As Boolean(), nubc As Double(), _
			n As Integer, rho As Double, stabilizingpoint As Double, ByRef f As Double, g As Double())
			Dim i As Integer = 0
			Dim p As Double = 0
			Dim dp As Double = 0
			Dim d2p As Double = 0

			For i = 0 To n - 1
				If (hasbndl(i) AndAlso hasbndu(i)) AndAlso CDbl(bndl(i)) = CDbl(bndu(i)) Then

					'
					' I-th boundary constraint is of equality-type
					'
					minnlcequalitypenaltyfunction((x(i) - bndl(i)) * rho, p, dp, d2p)
					f = f + p / rho - nubc(2 * i + 0) * (x(i) - bndl(i))
					g(i) = g(i) + dp - nubc(2 * i + 0)
					Continue For
				End If
				If hasbndl(i) Then

					'
					' Handle lower bound
					'
					minnlcinequalitypenaltyfunction(x(i) - bndl(i), stabilizingpoint, p, dp, d2p)
					f = f + rho * p
					g(i) = g(i) + rho * dp
					minnlcinequalityshiftfunction((x(i) - bndl(i)) * rho + 1, p, dp, d2p)
					f = f + p / rho * nubc(2 * i + 0)
					g(i) = g(i) + dp * nubc(2 * i + 0)
				End If
				If hasbndu(i) Then

					'
					' Handle upper bound
					'
					minnlcinequalitypenaltyfunction(bndu(i) - x(i), stabilizingpoint, p, dp, d2p)
					f = f + rho * p
					g(i) = g(i) - rho * dp
					minnlcinequalityshiftfunction((bndu(i) - x(i)) * rho + 1, p, dp, d2p)
					f = f + p / rho * nubc(2 * i + 1)
					g(i) = g(i) - dp * nubc(2 * i + 1)
				End If
			Next
		End Sub


		'************************************************************************
'        This subroutine adds penalty from  linear  constraints to target  function
'        and its gradient. Penalty function is one which is used for main AUL cycle
'        - with Lagrange multipliers and infinite at the barrier and beyond.
'
'        Parameters:
'            X[] - current point
'            CLEIC[] -   constraints matrix, first NEC rows are equality ones, next
'                        NIC rows are inequality ones. array[NEC+NIC,N+1]
'            NuLC[]  -   Lagrange multipliers corresponding to constraints,
'                        array[NEC+NIC]
'            N       -   dimensionalty
'            NEC     -   number of equality constraints
'            NIC     -   number of inequality constraints.
'            Rho - penalty term
'            StabilizingPoint - branch point for inequality stabilizing term
'            F - function value to modify
'            G - gradient to modify
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub penaltylc(x As Double(), cleic As Double(,), nulc As Double(), n As Integer, nec As Integer, nic As Integer, _
			rho As Double, stabilizingpoint As Double, ByRef f As Double, g As Double())
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim p As Double = 0
			Dim dp As Double = 0
			Dim d2p As Double = 0
			Dim i_ As Integer = 0

			For i = 0 To nec + nic - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += cleic(i, i_) * x(i_)
				Next
				v = v - cleic(i, n)
				If i < nec Then

					'
					' Equality constraint
					'
					minnlcequalitypenaltyfunction(v * rho, p, dp, d2p)
					f = f + p / rho
					vv = dp
					For i_ = 0 To n - 1
						g(i_) = g(i_) + vv * cleic(i, i_)
					Next
					f = f - nulc(i) * v
					vv = nulc(i)
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * cleic(i, i_)
					Next
				Else

					'
					' Inequality constraint
					'
					minnlcinequalitypenaltyfunction(-v, stabilizingpoint, p, dp, d2p)
					f = f + p * rho
					vv = dp * rho
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * cleic(i, i_)
					Next
					minnlcinequalityshiftfunction(-(v * rho) + 1, p, dp, d2p)
					f = f + p / rho * nulc(i)
					vv = dp * nulc(i)
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * cleic(i, i_)
					Next
				End If
			Next
		End Sub


		'************************************************************************
'        This subroutine adds penalty from nonlinear constraints to target function
'        and its gradient. Penalty function is one which is used for main AUL cycle
'        - with Lagrange multipliers and infinite at the barrier and beyond.
'
'        Parameters:
'            Fi[] - function vector:
'                  * 1 component for function being minimized
'                  * NG components for equality constraints G_i(x)=0
'                  * NH components for inequality constraints H_i(x)<=0
'            J[]  - Jacobian matrix, array[1+NG+NH,N]
'            NuNLC[]  -   Lagrange multipliers corresponding to constraints,
'                        array[NG+NH]
'            N - number of dimensions
'            NG - number of equality constraints
'            NH - number of inequality constraints
'            Rho - penalty term
'            StabilizingPoint - branch point for inequality stabilizing term
'            F - function value to modify
'            G - gradient to modify
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Sub penaltynlc(fi As Double(), j As Double(,), nunlc As Double(), n As Integer, ng As Integer, nh As Integer, _
			rho As Double, stabilizingpoint As Double, ByRef f As Double, g As Double())
			Dim i As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim p As Double = 0
			Dim dp As Double = 0
			Dim d2p As Double = 0
			Dim i_ As Integer = 0


			'
			' IMPORTANT: loop starts from 1, not zero!
			'
			For i = 1 To ng + nh
				v = fi(i)
				If i <= ng Then

					'
					' Equality constraint
					'
					minnlcequalitypenaltyfunction(v * rho, p, dp, d2p)
					f = f + p / rho
					vv = dp
					For i_ = 0 To n - 1
						g(i_) = g(i_) + vv * j(i, i_)
					Next
					f = f - nunlc(i - 1) * v
					vv = nunlc(i - 1)
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * j(i, i_)
					Next
				Else

					'
					' Inequality constraint
					'
					minnlcinequalitypenaltyfunction(-v, stabilizingpoint, p, dp, d2p)
					f = f + p * rho
					vv = dp * rho
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * j(i, i_)
					Next
					minnlcinequalityshiftfunction(-(v * rho) + 1, p, dp, d2p)
					f = f + p / rho * nunlc(i - 1)
					vv = dp * nunlc(i - 1)
					For i_ = 0 To n - 1
						g(i_) = g(i_) - vv * j(i, i_)
					Next
				End If
			Next
		End Sub


		'************************************************************************
'        This function performs actual processing for AUL algorith. It expects that
'        caller redirects its reverse communication  requests  NeedFiJ/XUpdated  to
'        external user who will provide analytic derivative (or handle reports about
'        progress).
'
'        In case external user does not have analytic derivative, it is responsibility
'        of caller to intercept NeedFiJ request and  replace  it  with  appropriate
'        numerical differentiation scheme.
'
'          -- ALGLIB --
'             Copyright 06.06.2014 by Bochkanov Sergey
'        ************************************************************************

		Private Shared Function auliteration(state As minnlcstate) As Boolean
			Dim result As New Boolean()
			Dim n As Integer = 0
			Dim nec As Integer = 0
			Dim nic As Integer = 0
			Dim ng As Integer = 0
			Dim nh As Integer = 0
			Dim i As Integer = 0
			Dim j As Integer = 0
			Dim outerit As Integer = 0
			Dim preccounter As Integer = 0
			Dim v As Double = 0
			Dim vv As Double = 0
			Dim p As Double = 0
			Dim dp As Double = 0
			Dim d2p As Double = 0
			Dim v0 As Double = 0
			Dim v1 As Double = 0
			Dim v2 As Double = 0
			Dim i_ As Integer = 0


			'
			' Reverse communication preparations
			' I know it looks ugly, but it works the same way
			' anywhere from C++ to Python.
			'
			' This code initializes locals by:
			' * random values determined during code
			'   generation - on first subroutine call
			' * values from previous call - on subsequent calls
			'
			If state.rstateaul.stage >= 0 Then
				n = state.rstateaul.ia(0)
				nec = state.rstateaul.ia(1)
				nic = state.rstateaul.ia(2)
				ng = state.rstateaul.ia(3)
				nh = state.rstateaul.ia(4)
				i = state.rstateaul.ia(5)
				j = state.rstateaul.ia(6)
				outerit = state.rstateaul.ia(7)
				preccounter = state.rstateaul.ia(8)
				v = state.rstateaul.ra(0)
				vv = state.rstateaul.ra(1)
				p = state.rstateaul.ra(2)
				dp = state.rstateaul.ra(3)
				d2p = state.rstateaul.ra(4)
				v0 = state.rstateaul.ra(5)
				v1 = state.rstateaul.ra(6)
				v2 = state.rstateaul.ra(7)
			Else
				n = 364
				nec = 214
				nic = -338
				ng = -686
				nh = 912
				i = 585
				j = 497
				outerit = -271
				preccounter = -581
				v = 745
				vv = -533
				p = -77
				dp = 678
				d2p = -293
				v0 = 316
				v1 = 647
				v2 = -756
			End If
			If state.rstateaul.stage = 0 Then
				GoTo lbl_0
			End If
			If state.rstateaul.stage = 1 Then
				GoTo lbl_1
			End If
			If state.rstateaul.stage = 2 Then
				GoTo lbl_2
			End If

			'
			' Routine body
			'
			alglib.ap.assert(state.solvertype = 0, "MinNLC: internal error")
			n = state.n
			nec = state.nec
			nic = state.nic
			ng = state.ng
			nh = state.nh

			'
			' Prepare scaled problem
			'
			apserv.rvectorsetlengthatleast(state.scaledbndl, n)
			apserv.rvectorsetlengthatleast(state.scaledbndu, n)
			apserv.rmatrixsetlengthatleast(state.scaledcleic, nec + nic, n + 1)
			For i = 0 To n - 1
				If state.hasbndl(i) Then
					state.scaledbndl(i) = state.bndl(i) / state.s(i)
				End If
				If state.hasbndu(i) Then
					state.scaledbndu(i) = state.bndu(i) / state.s(i)
				End If
				state.xc(i) = state.xstart(i) / state.s(i)
			Next
			For i = 0 To nec + nic - 1

				'
				' Scale and normalize linear constraints
				'
				vv = 0.0
				For j = 0 To n - 1
					v = state.cleic(i, j) * state.s(j)
					state.scaledcleic(i, j) = v
					vv = vv + v * v
				Next
				vv = System.Math.sqrt(vv)
				state.scaledcleic(i, n) = state.cleic(i, n)
				If CDbl(vv) > CDbl(0) Then
					For j = 0 To n
						state.scaledcleic(i, j) = state.scaledcleic(i, j) / vv
					Next
				End If
			Next

			'
			' Prepare stopping criteria
			'
			minlbfgs.minlbfgssetcond(state.auloptimizer, state.epsg, state.epsf, state.epsx, state.maxits)

			'
			' Main AUL cycle:
			' * prepare Lagrange multipliers NuNB/NuLC
			' * set GammaK (current estimate of Hessian norm) to 0.0 and XKPresent to False
			'
			apserv.rvectorsetlengthatleast(state.nubc, 2 * n)
			apserv.rvectorsetlengthatleast(state.nulc, nec + nic)
			apserv.rvectorsetlengthatleast(state.nunlc, ng + nh)
			apserv.rvectorsetlengthatleast(state.xk, n)
			apserv.rvectorsetlengthatleast(state.gk, n)
			apserv.rvectorsetlengthatleast(state.xk1, n)
			apserv.rvectorsetlengthatleast(state.gk1, n)
			For i = 0 To n - 1
				state.nubc(2 * i + 0) = 0.0
				state.nubc(2 * i + 1) = 0.0
				If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
					Continue For
				End If
				If state.hasbndl(i) Then
					state.nubc(2 * i + 0) = state.initialinequalitymultiplier
				End If
				If state.hasbndu(i) Then
					state.nubc(2 * i + 1) = state.initialinequalitymultiplier
				End If
			Next
			For i = 0 To nec - 1
				state.nulc(i) = 0.0
			Next
			For i = 0 To nic - 1
				state.nulc(nec + i) = state.initialinequalitymultiplier
			Next
			For i = 0 To ng - 1
				state.nunlc(i) = 0.0
			Next
			For i = 0 To nh - 1
				state.nunlc(ng + i) = state.initialinequalitymultiplier
			Next
			state.gammak = 0.0
			state.xkpresent = False
			alglib.ap.assert(state.aulitscnt > 0, "MinNLC: integrity check failed")
			clearpreconditioner(state.auloptimizer)
			outerit = 0
			lbl_3:
			If outerit > state.aulitscnt - 1 Then
				GoTo lbl_5
			End If

			'
			' Optimize with current Lagrange multipliers
			'
			' NOTE: this code expects and checks that line search ends in the
			'       point which is used as beginning for the next search. Such
			'       guarantee is given by MCSRCH function.  L-BFGS  optimizer
			'       does not formally guarantee it, but it follows same rule.
			'       Below we a) rely on such property of the optimizer, and b)
			'       assert that it is true, in order to fail loudly if it is
			'       not true.
			'
			' NOTE: security check for NAN/INF in F/G is responsibility of
			'       LBFGS optimizer. AUL optimizer checks for NAN/INF only
			'       when we update Lagrange multipliers.
			'
			preccounter = 0
			minlbfgs.minlbfgssetxrep(state.auloptimizer, True)
			minlbfgs.minlbfgsrestartfrom(state.auloptimizer, state.xc)
			lbl_6:
			If Not minlbfgs.minlbfgsiteration(state.auloptimizer) Then
				GoTo lbl_7
			End If
			If Not state.auloptimizer.needfg Then
				GoTo lbl_8
			End If

			'
			' Un-scale X, evaluate F/G/H, re-scale Jacobian
			'
			For i = 0 To n - 1
				state.x(i) = state.auloptimizer.x(i) * state.s(i)
			Next
			state.needfij = True
			state.rstateaul.stage = 0
			GoTo lbl_rcomm
			lbl_0:
			state.needfij = False
			For i = 0 To ng + nh
				For j = 0 To n - 1
					state.j(i, j) = state.j(i, j) * state.s(j)
				Next
			Next

			'
			' Store data for estimation of Hessian norm:
			' * current point (re-scaled)
			' * gradient of the target function (re-scaled, unmodified)
			'
			For i_ = 0 To n - 1
				state.xk1(i_) = state.auloptimizer.x(i_)
			Next
			For i_ = 0 To n - 1
				state.gk1(i_) = state.j(0, i_)
			Next

			'
			' Function being optimized
			'
			state.auloptimizer.f = state.fi(0)
			For i = 0 To n - 1
				state.auloptimizer.g(i) = state.j(0, i)
			Next

			'
			' Penalty for violation of boundary/linear/nonlinear constraints
			'
			penaltybc(state.auloptimizer.x, state.scaledbndl, state.hasbndl, state.scaledbndu, state.hasbndu, state.nubc, _
				n, state.rho, state.stabilizingpoint, state.auloptimizer.f, state.auloptimizer.g)
			penaltylc(state.auloptimizer.x, state.scaledcleic, state.nulc, n, nec, nic, _
				state.rho, state.stabilizingpoint, state.auloptimizer.f, state.auloptimizer.g)
			penaltynlc(state.fi, state.j, state.nunlc, n, ng, nh, _
				state.rho, state.stabilizingpoint, state.auloptimizer.f, state.auloptimizer.g)

			'
			' To optimizer
			'
			GoTo lbl_6
			lbl_8:
			If Not state.auloptimizer.xupdated Then
				GoTo lbl_10
			End If

			'
			' Report current point (if needed)
			'
			If Not state.xrep Then
				GoTo lbl_12
			End If
			For i = 0 To n - 1
				state.x(i) = state.auloptimizer.x(i) * state.s(i)
			Next
			state.f = state.auloptimizer.f
			state.xupdated = True
			state.rstateaul.stage = 1
			GoTo lbl_rcomm
			lbl_1:
			state.xupdated = False
			lbl_12:

			'
			' Update GammaK
			'
			If state.xkpresent Then

				'
				' XK/GK store beginning of current line search, and XK1/GK1
				' store data for the end of the line search:
				' * first, we Assert() that XK1 (last point where function
				'   was evaluated) is same as AULOptimizer.X (what is
				'   reported by RComm interface
				' * calculate step length V2.
				'
				' If V2>HessEstTol, then:
				' * calculate V0 - directional derivative at XK,
				'   and V1 - directional derivative at XK1
				' * set GammaK to Max(GammaK, |V1-V0|/V2)
				'
				For i = 0 To n - 1
					alglib.ap.assert(CDbl(System.Math.Abs(state.auloptimizer.x(i) - state.xk1(i))) <= CDbl(100 * Math.machineepsilon), "MinNLC: integrity check failed, unexpected behavior of LBFGS optimizer")
				Next
				v2 = 0.0
				For i = 0 To n - 1
					v2 = v2 + Math.sqr(state.xk(i) - state.xk1(i))
				Next
				v2 = System.Math.sqrt(v2)
				If CDbl(v2) > CDbl(hessesttol) Then
					v0 = 0.0
					v1 = 0.0
					For i = 0 To n - 1
						v = (state.xk(i) - state.xk1(i)) / v2
						v0 = v0 + state.gk(i) * v
						v1 = v1 + state.gk1(i) * v
					Next
					state.gammak = System.Math.Max(state.gammak, System.Math.Abs(v1 - v0) / v2)
				End If
			Else

				'
				' Beginning of the first line search, XK is not yet initialized.
				'
				For i_ = 0 To n - 1
					state.xk(i_) = state.xk1(i_)
				Next
				For i_ = 0 To n - 1
					state.gk(i_) = state.gk1(i_)
				Next
				state.xkpresent = True
			End If

			'
			' Update preconsitioner using current GammaK
			'
			updatepreconditioner(state.prectype, state.updatefreq, preccounter, state.auloptimizer, state.auloptimizer.x, state.rho, _
				state.gammak, state.scaledbndl, state.hasbndl, state.scaledbndu, state.hasbndu, state.nubc, _
				state.scaledcleic, state.nulc, state.fi, state.j, state.nunlc, state.bufd, _
				state.bufc, state.bufw, n, nec, nic, ng, _
				nh)
			GoTo lbl_6
			lbl_10:
			alglib.ap.assert(False, "MinNLC: integrity check failed")
			GoTo lbl_6
			lbl_7:
			minlbfgs.minlbfgsresultsbuf(state.auloptimizer, state.xc, state.aulreport)
			state.repinneriterationscount = state.repinneriterationscount + state.aulreport.iterationscount
			state.repnfev = state.repnfev + state.aulreport.nfev
			state.repterminationtype = state.aulreport.terminationtype
			apserv.inc(state.repouteriterationscount)
			If state.repterminationtype <= 0 Then
				GoTo lbl_5
			End If

			'
			' 1. Evaluate F/J
			' 2. Check for NAN/INF in F/J: we just calculate sum of their
			'    components, it should be enough to reduce vector/matrix to
			'    just one value which either "normal" (all summands were "normal")
			'    or NAN/INF (at least one summand was NAN/INF).
			' 3. Update Lagrange multipliers
			'
			For i = 0 To n - 1
				state.x(i) = state.xc(i) * state.s(i)
			Next
			state.needfij = True
			state.rstateaul.stage = 2
			GoTo lbl_rcomm
			lbl_2:
			state.needfij = False
			v = 0.0
			For i = 0 To ng + nh
				v = 0.1 * v + state.fi(i)
				For j = 0 To n - 1
					v = 0.1 * v + state.j(i, j)
				Next
			Next
			If Not Math.isfinite(v) Then

				'
				' Abnormal termination - infinities in function/gradient
				'
				state.repterminationtype = -8
				result = False
				Return result
			End If
			For i = 0 To ng + nh
				For j = 0 To n - 1
					state.j(i, j) = state.j(i, j) * state.s(j)
				Next
			Next
			For i = 0 To n - 1

				'
				' Process coefficients corresponding to equality-type
				' constraints.
				'
				If (state.hasbndl(i) AndAlso state.hasbndu(i)) AndAlso CDbl(state.bndl(i)) = CDbl(state.bndu(i)) Then
					minnlcequalitypenaltyfunction((state.xc(i) - state.scaledbndl(i)) * state.rho, p, dp, d2p)
					state.nubc(2 * i + 0) = state.nubc(2 * i + 0) - dp
					Continue For
				End If

				'
				' Process coefficients corresponding to inequality-type
				' constraints. These coefficients have limited growth/decay
				' per iteration which helps to stabilize algorithm.
				'
				alglib.ap.assert(CDbl(aulmaxgrowth) > CDbl(1.0), "MinNLC: integrity error")
				If state.hasbndl(i) Then
					minnlcinequalityshiftfunction((state.xc(i) - state.scaledbndl(i)) * state.rho + 1, p, dp, d2p)
					v = System.Math.Abs(dp)
					v = System.Math.Min(v, aulmaxgrowth)
					v = System.Math.Max(v, 1 / aulmaxgrowth)
					state.nubc(2 * i + 0) = state.nubc(2 * i + 0) * v
				End If
				If state.hasbndu(i) Then
					minnlcinequalityshiftfunction((state.scaledbndu(i) - state.xc(i)) * state.rho + 1, p, dp, d2p)
					v = System.Math.Abs(dp)
					v = System.Math.Min(v, aulmaxgrowth)
					v = System.Math.Max(v, 1 / aulmaxgrowth)
					state.nubc(2 * i + 1) = state.nubc(2 * i + 1) * v
				End If
			Next
			For i = 0 To nec + nic - 1
				v = 0.0
				For i_ = 0 To n - 1
					v += state.scaledcleic(i, i_) * state.xc(i_)
				Next
				v = v - state.scaledcleic(i, n)
				If i < nec Then
					minnlcequalitypenaltyfunction(v * state.rho, p, dp, d2p)
					state.nulc(i) = state.nulc(i) - dp
				Else
					minnlcinequalityshiftfunction(-(v * state.rho) + 1, p, dp, d2p)
					v = System.Math.Abs(dp)
					v = System.Math.Min(v, aulmaxgrowth)
					v = System.Math.Max(v, 1 / aulmaxgrowth)
					state.nulc(i) = state.nulc(i) * v
				End If
			Next
			For i = 1 To ng + nh

				'
				' NOTE: loop index must start from 1, not zero!
				'
				v = state.fi(i)
				If i <= ng Then
					minnlcequalitypenaltyfunction(v * state.rho, p, dp, d2p)
					state.nunlc(i - 1) = state.nunlc(i - 1) - dp
				Else
					minnlcinequalityshiftfunction(-(v * state.rho) + 1, p, dp, d2p)
					v = System.Math.Abs(dp)
					v = System.Math.Min(v, aulmaxgrowth)
					v = System.Math.Max(v, 1 / aulmaxgrowth)
					state.nunlc(i - 1) = state.nunlc(i - 1) * v
				End If
			Next
			outerit = outerit + 1
			GoTo lbl_3
			lbl_5:
			For i = 0 To n - 1
				state.xc(i) = state.xc(i) * state.s(i)
			Next
			result = False
			Return result
			lbl_rcomm:

			'
			' Saving state
			'
			result = True
			state.rstateaul.ia(0) = n
			state.rstateaul.ia(1) = nec
			state.rstateaul.ia(2) = nic
			state.rstateaul.ia(3) = ng
			state.rstateaul.ia(4) = nh
			state.rstateaul.ia(5) = i
			state.rstateaul.ia(6) = j
			state.rstateaul.ia(7) = outerit
			state.rstateaul.ia(8) = preccounter
			state.rstateaul.ra(0) = v
			state.rstateaul.ra(1) = vv
			state.rstateaul.ra(2) = p
			state.rstateaul.ra(3) = dp
			state.rstateaul.ra(4) = d2p
			state.rstateaul.ra(5) = v0
			state.rstateaul.ra(6) = v1
			state.rstateaul.ra(7) = v2
			Return result
		End Function


	End Class
End Class

